{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.907032181168057,
  "eval_steps": 500,
  "global_step": 1600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011918951132300357,
      "grad_norm": 0.7020679116249084,
      "learning_rate": 4.9702026221692494e-05,
      "loss": 3.8087,
      "step": 10
    },
    {
      "epoch": 0.023837902264600714,
      "grad_norm": 0.742351233959198,
      "learning_rate": 4.9404052443384986e-05,
      "loss": 3.6385,
      "step": 20
    },
    {
      "epoch": 0.03575685339690107,
      "grad_norm": 0.9017781019210815,
      "learning_rate": 4.910607866507748e-05,
      "loss": 3.6666,
      "step": 30
    },
    {
      "epoch": 0.04767580452920143,
      "grad_norm": 0.9957247376441956,
      "learning_rate": 4.880810488676997e-05,
      "loss": 3.5786,
      "step": 40
    },
    {
      "epoch": 0.05959475566150179,
      "grad_norm": 0.949219822883606,
      "learning_rate": 4.851013110846246e-05,
      "loss": 3.271,
      "step": 50
    },
    {
      "epoch": 0.07151370679380215,
      "grad_norm": 0.8393499851226807,
      "learning_rate": 4.821215733015495e-05,
      "loss": 3.3631,
      "step": 60
    },
    {
      "epoch": 0.08343265792610251,
      "grad_norm": 1.0037214756011963,
      "learning_rate": 4.791418355184744e-05,
      "loss": 3.2165,
      "step": 70
    },
    {
      "epoch": 0.09535160905840286,
      "grad_norm": 0.9481014609336853,
      "learning_rate": 4.7616209773539935e-05,
      "loss": 3.3117,
      "step": 80
    },
    {
      "epoch": 0.10727056019070322,
      "grad_norm": 0.9377284646034241,
      "learning_rate": 4.7318235995232426e-05,
      "loss": 3.1507,
      "step": 90
    },
    {
      "epoch": 0.11918951132300358,
      "grad_norm": 0.7452619671821594,
      "learning_rate": 4.702026221692492e-05,
      "loss": 3.0512,
      "step": 100
    },
    {
      "epoch": 0.13110846245530394,
      "grad_norm": 1.182600498199463,
      "learning_rate": 4.672228843861741e-05,
      "loss": 3.1949,
      "step": 110
    },
    {
      "epoch": 0.1430274135876043,
      "grad_norm": 0.9979214072227478,
      "learning_rate": 4.6424314660309894e-05,
      "loss": 3.0256,
      "step": 120
    },
    {
      "epoch": 0.15494636471990464,
      "grad_norm": 0.8867535591125488,
      "learning_rate": 4.6126340882002386e-05,
      "loss": 3.0151,
      "step": 130
    },
    {
      "epoch": 0.16686531585220502,
      "grad_norm": 1.6302587985992432,
      "learning_rate": 4.582836710369488e-05,
      "loss": 3.0546,
      "step": 140
    },
    {
      "epoch": 0.17878426698450536,
      "grad_norm": 1.7845615148544312,
      "learning_rate": 4.553039332538737e-05,
      "loss": 2.9025,
      "step": 150
    },
    {
      "epoch": 0.1907032181168057,
      "grad_norm": 1.2045034170150757,
      "learning_rate": 4.523241954707986e-05,
      "loss": 2.883,
      "step": 160
    },
    {
      "epoch": 0.2026221692491061,
      "grad_norm": 1.038744330406189,
      "learning_rate": 4.4934445768772345e-05,
      "loss": 2.907,
      "step": 170
    },
    {
      "epoch": 0.21454112038140644,
      "grad_norm": 0.9265843629837036,
      "learning_rate": 4.463647199046484e-05,
      "loss": 2.9687,
      "step": 180
    },
    {
      "epoch": 0.22646007151370678,
      "grad_norm": 1.000428557395935,
      "learning_rate": 4.433849821215733e-05,
      "loss": 2.7992,
      "step": 190
    },
    {
      "epoch": 0.23837902264600716,
      "grad_norm": 1.2161349058151245,
      "learning_rate": 4.404052443384982e-05,
      "loss": 2.7197,
      "step": 200
    },
    {
      "epoch": 0.25029797377830754,
      "grad_norm": 0.8771137595176697,
      "learning_rate": 4.374255065554231e-05,
      "loss": 2.7808,
      "step": 210
    },
    {
      "epoch": 0.2622169249106079,
      "grad_norm": 1.0601040124893188,
      "learning_rate": 4.34445768772348e-05,
      "loss": 2.7224,
      "step": 220
    },
    {
      "epoch": 0.27413587604290823,
      "grad_norm": 0.9046211838722229,
      "learning_rate": 4.3146603098927295e-05,
      "loss": 2.7486,
      "step": 230
    },
    {
      "epoch": 0.2860548271752086,
      "grad_norm": 1.0880966186523438,
      "learning_rate": 4.2848629320619786e-05,
      "loss": 2.7114,
      "step": 240
    },
    {
      "epoch": 0.29797377830750893,
      "grad_norm": 0.7910630106925964,
      "learning_rate": 4.255065554231228e-05,
      "loss": 2.8494,
      "step": 250
    },
    {
      "epoch": 0.3098927294398093,
      "grad_norm": 1.1601769924163818,
      "learning_rate": 4.225268176400477e-05,
      "loss": 2.7249,
      "step": 260
    },
    {
      "epoch": 0.3218116805721097,
      "grad_norm": 1.1358641386032104,
      "learning_rate": 4.195470798569726e-05,
      "loss": 2.7484,
      "step": 270
    },
    {
      "epoch": 0.33373063170441003,
      "grad_norm": 1.1555254459381104,
      "learning_rate": 4.165673420738975e-05,
      "loss": 2.7973,
      "step": 280
    },
    {
      "epoch": 0.3456495828367104,
      "grad_norm": 1.0267014503479004,
      "learning_rate": 4.1358760429082244e-05,
      "loss": 2.6115,
      "step": 290
    },
    {
      "epoch": 0.3575685339690107,
      "grad_norm": 1.1858642101287842,
      "learning_rate": 4.1060786650774736e-05,
      "loss": 2.6838,
      "step": 300
    },
    {
      "epoch": 0.3694874851013111,
      "grad_norm": 1.0578930377960205,
      "learning_rate": 4.076281287246723e-05,
      "loss": 2.8439,
      "step": 310
    },
    {
      "epoch": 0.3814064362336114,
      "grad_norm": 1.1167423725128174,
      "learning_rate": 4.046483909415972e-05,
      "loss": 2.653,
      "step": 320
    },
    {
      "epoch": 0.3933253873659118,
      "grad_norm": 1.2659839391708374,
      "learning_rate": 4.016686531585221e-05,
      "loss": 2.5218,
      "step": 330
    },
    {
      "epoch": 0.4052443384982122,
      "grad_norm": 1.722832202911377,
      "learning_rate": 3.98688915375447e-05,
      "loss": 2.7167,
      "step": 340
    },
    {
      "epoch": 0.4171632896305125,
      "grad_norm": 1.1252179145812988,
      "learning_rate": 3.9570917759237194e-05,
      "loss": 2.6402,
      "step": 350
    },
    {
      "epoch": 0.42908224076281287,
      "grad_norm": 1.316868543624878,
      "learning_rate": 3.927294398092968e-05,
      "loss": 2.5415,
      "step": 360
    },
    {
      "epoch": 0.4410011918951132,
      "grad_norm": 1.1327240467071533,
      "learning_rate": 3.897497020262217e-05,
      "loss": 2.5276,
      "step": 370
    },
    {
      "epoch": 0.45292014302741357,
      "grad_norm": 1.1305328607559204,
      "learning_rate": 3.867699642431466e-05,
      "loss": 2.644,
      "step": 380
    },
    {
      "epoch": 0.464839094159714,
      "grad_norm": 1.266366720199585,
      "learning_rate": 3.837902264600715e-05,
      "loss": 2.4714,
      "step": 390
    },
    {
      "epoch": 0.4767580452920143,
      "grad_norm": 1.2873718738555908,
      "learning_rate": 3.8081048867699645e-05,
      "loss": 2.5719,
      "step": 400
    },
    {
      "epoch": 0.48867699642431467,
      "grad_norm": 1.8069028854370117,
      "learning_rate": 3.7783075089392136e-05,
      "loss": 2.6548,
      "step": 410
    },
    {
      "epoch": 0.5005959475566151,
      "grad_norm": 1.4463634490966797,
      "learning_rate": 3.748510131108463e-05,
      "loss": 2.7069,
      "step": 420
    },
    {
      "epoch": 0.5125148986889154,
      "grad_norm": 1.5263224840164185,
      "learning_rate": 3.718712753277712e-05,
      "loss": 2.6149,
      "step": 430
    },
    {
      "epoch": 0.5244338498212158,
      "grad_norm": 1.619038701057434,
      "learning_rate": 3.688915375446961e-05,
      "loss": 2.6005,
      "step": 440
    },
    {
      "epoch": 0.5363528009535161,
      "grad_norm": 1.9581713676452637,
      "learning_rate": 3.6591179976162096e-05,
      "loss": 2.5803,
      "step": 450
    },
    {
      "epoch": 0.5482717520858165,
      "grad_norm": 2.3323769569396973,
      "learning_rate": 3.629320619785459e-05,
      "loss": 2.6133,
      "step": 460
    },
    {
      "epoch": 0.5601907032181168,
      "grad_norm": 1.4092613458633423,
      "learning_rate": 3.599523241954708e-05,
      "loss": 2.6412,
      "step": 470
    },
    {
      "epoch": 0.5721096543504172,
      "grad_norm": 1.6637107133865356,
      "learning_rate": 3.569725864123957e-05,
      "loss": 2.6556,
      "step": 480
    },
    {
      "epoch": 0.5840286054827175,
      "grad_norm": 2.091116428375244,
      "learning_rate": 3.539928486293206e-05,
      "loss": 2.7041,
      "step": 490
    },
    {
      "epoch": 0.5959475566150179,
      "grad_norm": 1.2741117477416992,
      "learning_rate": 3.5101311084624553e-05,
      "loss": 2.5859,
      "step": 500
    },
    {
      "epoch": 0.6078665077473182,
      "grad_norm": 1.3032504320144653,
      "learning_rate": 3.4803337306317045e-05,
      "loss": 2.4441,
      "step": 510
    },
    {
      "epoch": 0.6197854588796186,
      "grad_norm": 1.7293461561203003,
      "learning_rate": 3.4505363528009537e-05,
      "loss": 2.5472,
      "step": 520
    },
    {
      "epoch": 0.6317044100119189,
      "grad_norm": 1.5969918966293335,
      "learning_rate": 3.420738974970203e-05,
      "loss": 2.6209,
      "step": 530
    },
    {
      "epoch": 0.6436233611442194,
      "grad_norm": 2.2652831077575684,
      "learning_rate": 3.390941597139452e-05,
      "loss": 2.4807,
      "step": 540
    },
    {
      "epoch": 0.6555423122765197,
      "grad_norm": 1.9970368146896362,
      "learning_rate": 3.361144219308701e-05,
      "loss": 2.5007,
      "step": 550
    },
    {
      "epoch": 0.6674612634088201,
      "grad_norm": 1.9108145236968994,
      "learning_rate": 3.33134684147795e-05,
      "loss": 2.3761,
      "step": 560
    },
    {
      "epoch": 0.6793802145411204,
      "grad_norm": 1.655799388885498,
      "learning_rate": 3.3015494636471994e-05,
      "loss": 2.5881,
      "step": 570
    },
    {
      "epoch": 0.6912991656734208,
      "grad_norm": 1.6004278659820557,
      "learning_rate": 3.2717520858164486e-05,
      "loss": 2.5344,
      "step": 580
    },
    {
      "epoch": 0.7032181168057211,
      "grad_norm": 1.4465957880020142,
      "learning_rate": 3.241954707985698e-05,
      "loss": 2.551,
      "step": 590
    },
    {
      "epoch": 0.7151370679380215,
      "grad_norm": 1.2498652935028076,
      "learning_rate": 3.212157330154946e-05,
      "loss": 2.4495,
      "step": 600
    },
    {
      "epoch": 0.7270560190703218,
      "grad_norm": 1.7920255661010742,
      "learning_rate": 3.1823599523241954e-05,
      "loss": 2.5264,
      "step": 610
    },
    {
      "epoch": 0.7389749702026222,
      "grad_norm": 2.0306766033172607,
      "learning_rate": 3.1525625744934445e-05,
      "loss": 2.5196,
      "step": 620
    },
    {
      "epoch": 0.7508939213349225,
      "grad_norm": 2.018720865249634,
      "learning_rate": 3.122765196662694e-05,
      "loss": 2.6554,
      "step": 630
    },
    {
      "epoch": 0.7628128724672228,
      "grad_norm": 1.8473377227783203,
      "learning_rate": 3.092967818831943e-05,
      "loss": 2.5402,
      "step": 640
    },
    {
      "epoch": 0.7747318235995232,
      "grad_norm": 2.2267520427703857,
      "learning_rate": 3.063170441001192e-05,
      "loss": 2.585,
      "step": 650
    },
    {
      "epoch": 0.7866507747318237,
      "grad_norm": 1.414315104484558,
      "learning_rate": 3.0333730631704412e-05,
      "loss": 2.392,
      "step": 660
    },
    {
      "epoch": 0.798569725864124,
      "grad_norm": 1.5827754735946655,
      "learning_rate": 3.00357568533969e-05,
      "loss": 2.5136,
      "step": 670
    },
    {
      "epoch": 0.8104886769964244,
      "grad_norm": 1.4913722276687622,
      "learning_rate": 2.973778307508939e-05,
      "loss": 2.309,
      "step": 680
    },
    {
      "epoch": 0.8224076281287247,
      "grad_norm": 2.1802589893341064,
      "learning_rate": 2.9439809296781883e-05,
      "loss": 2.5227,
      "step": 690
    },
    {
      "epoch": 0.834326579261025,
      "grad_norm": 1.769221305847168,
      "learning_rate": 2.9141835518474375e-05,
      "loss": 2.4121,
      "step": 700
    },
    {
      "epoch": 0.8462455303933254,
      "grad_norm": 2.41013503074646,
      "learning_rate": 2.8843861740166866e-05,
      "loss": 2.6258,
      "step": 710
    },
    {
      "epoch": 0.8581644815256257,
      "grad_norm": 1.7796913385391235,
      "learning_rate": 2.8545887961859358e-05,
      "loss": 2.5764,
      "step": 720
    },
    {
      "epoch": 0.8700834326579261,
      "grad_norm": 1.2652126550674438,
      "learning_rate": 2.824791418355185e-05,
      "loss": 2.4073,
      "step": 730
    },
    {
      "epoch": 0.8820023837902264,
      "grad_norm": 1.3145133256912231,
      "learning_rate": 2.794994040524434e-05,
      "loss": 2.4032,
      "step": 740
    },
    {
      "epoch": 0.8939213349225268,
      "grad_norm": 1.7842053174972534,
      "learning_rate": 2.7651966626936832e-05,
      "loss": 2.462,
      "step": 750
    },
    {
      "epoch": 0.9058402860548271,
      "grad_norm": 1.7293181419372559,
      "learning_rate": 2.7353992848629324e-05,
      "loss": 2.4376,
      "step": 760
    },
    {
      "epoch": 0.9177592371871275,
      "grad_norm": 1.6333868503570557,
      "learning_rate": 2.7056019070321816e-05,
      "loss": 2.4594,
      "step": 770
    },
    {
      "epoch": 0.929678188319428,
      "grad_norm": 1.5476481914520264,
      "learning_rate": 2.6758045292014307e-05,
      "loss": 2.4583,
      "step": 780
    },
    {
      "epoch": 0.9415971394517283,
      "grad_norm": 1.6864641904830933,
      "learning_rate": 2.6460071513706795e-05,
      "loss": 2.4146,
      "step": 790
    },
    {
      "epoch": 0.9535160905840286,
      "grad_norm": 1.51480233669281,
      "learning_rate": 2.6162097735399287e-05,
      "loss": 2.4863,
      "step": 800
    },
    {
      "epoch": 0.965435041716329,
      "grad_norm": 1.7184711694717407,
      "learning_rate": 2.586412395709178e-05,
      "loss": 2.257,
      "step": 810
    },
    {
      "epoch": 0.9773539928486293,
      "grad_norm": 1.7182347774505615,
      "learning_rate": 2.556615017878427e-05,
      "loss": 2.4306,
      "step": 820
    },
    {
      "epoch": 0.9892729439809297,
      "grad_norm": 2.1889264583587646,
      "learning_rate": 2.526817640047676e-05,
      "loss": 2.6288,
      "step": 830
    },
    {
      "epoch": 1.0011918951132301,
      "grad_norm": 2.0607995986938477,
      "learning_rate": 2.497020262216925e-05,
      "loss": 2.3735,
      "step": 840
    },
    {
      "epoch": 1.0131108462455305,
      "grad_norm": 1.656325101852417,
      "learning_rate": 2.467222884386174e-05,
      "loss": 2.4325,
      "step": 850
    },
    {
      "epoch": 1.0250297973778308,
      "grad_norm": 2.2888541221618652,
      "learning_rate": 2.4374255065554233e-05,
      "loss": 2.4741,
      "step": 860
    },
    {
      "epoch": 1.0369487485101312,
      "grad_norm": 1.8828660249710083,
      "learning_rate": 2.4076281287246724e-05,
      "loss": 2.4224,
      "step": 870
    },
    {
      "epoch": 1.0488676996424315,
      "grad_norm": 2.2429473400115967,
      "learning_rate": 2.3778307508939216e-05,
      "loss": 2.4012,
      "step": 880
    },
    {
      "epoch": 1.0607866507747319,
      "grad_norm": 2.754037380218506,
      "learning_rate": 2.3480333730631707e-05,
      "loss": 2.3663,
      "step": 890
    },
    {
      "epoch": 1.0727056019070322,
      "grad_norm": 1.8462945222854614,
      "learning_rate": 2.3182359952324196e-05,
      "loss": 2.3193,
      "step": 900
    },
    {
      "epoch": 1.0846245530393326,
      "grad_norm": 1.6229530572891235,
      "learning_rate": 2.2884386174016687e-05,
      "loss": 2.465,
      "step": 910
    },
    {
      "epoch": 1.096543504171633,
      "grad_norm": 1.813696026802063,
      "learning_rate": 2.258641239570918e-05,
      "loss": 2.6162,
      "step": 920
    },
    {
      "epoch": 1.1084624553039333,
      "grad_norm": 2.6518237590789795,
      "learning_rate": 2.228843861740167e-05,
      "loss": 2.4862,
      "step": 930
    },
    {
      "epoch": 1.1203814064362336,
      "grad_norm": 1.9904345273971558,
      "learning_rate": 2.1990464839094162e-05,
      "loss": 2.2868,
      "step": 940
    },
    {
      "epoch": 1.132300357568534,
      "grad_norm": 2.098189353942871,
      "learning_rate": 2.169249106078665e-05,
      "loss": 2.4653,
      "step": 950
    },
    {
      "epoch": 1.1442193087008343,
      "grad_norm": 2.092559814453125,
      "learning_rate": 2.139451728247914e-05,
      "loss": 2.4481,
      "step": 960
    },
    {
      "epoch": 1.1561382598331347,
      "grad_norm": 2.276566505432129,
      "learning_rate": 2.1096543504171633e-05,
      "loss": 2.3755,
      "step": 970
    },
    {
      "epoch": 1.168057210965435,
      "grad_norm": 2.1359381675720215,
      "learning_rate": 2.0798569725864125e-05,
      "loss": 2.4579,
      "step": 980
    },
    {
      "epoch": 1.1799761620977354,
      "grad_norm": 2.8341267108917236,
      "learning_rate": 2.0500595947556616e-05,
      "loss": 2.4534,
      "step": 990
    },
    {
      "epoch": 1.1918951132300357,
      "grad_norm": 2.1327433586120605,
      "learning_rate": 2.0202622169249108e-05,
      "loss": 2.343,
      "step": 1000
    },
    {
      "epoch": 1.203814064362336,
      "grad_norm": 1.9339457750320435,
      "learning_rate": 1.99046483909416e-05,
      "loss": 2.4081,
      "step": 1010
    },
    {
      "epoch": 1.2157330154946364,
      "grad_norm": 1.4487553834915161,
      "learning_rate": 1.9606674612634088e-05,
      "loss": 2.566,
      "step": 1020
    },
    {
      "epoch": 1.2276519666269368,
      "grad_norm": 2.1201043128967285,
      "learning_rate": 1.930870083432658e-05,
      "loss": 2.4742,
      "step": 1030
    },
    {
      "epoch": 1.2395709177592371,
      "grad_norm": 3.382166624069214,
      "learning_rate": 1.901072705601907e-05,
      "loss": 2.4944,
      "step": 1040
    },
    {
      "epoch": 1.2514898688915377,
      "grad_norm": 2.7532896995544434,
      "learning_rate": 1.8712753277711562e-05,
      "loss": 2.4105,
      "step": 1050
    },
    {
      "epoch": 1.2634088200238378,
      "grad_norm": 2.1670501232147217,
      "learning_rate": 1.8414779499404054e-05,
      "loss": 2.5492,
      "step": 1060
    },
    {
      "epoch": 1.2753277711561384,
      "grad_norm": 2.9009439945220947,
      "learning_rate": 1.8116805721096545e-05,
      "loss": 2.3123,
      "step": 1070
    },
    {
      "epoch": 1.2872467222884385,
      "grad_norm": 2.21317982673645,
      "learning_rate": 1.7818831942789037e-05,
      "loss": 2.3185,
      "step": 1080
    },
    {
      "epoch": 1.299165673420739,
      "grad_norm": 2.342164993286133,
      "learning_rate": 1.7520858164481525e-05,
      "loss": 2.3018,
      "step": 1090
    },
    {
      "epoch": 1.3110846245530392,
      "grad_norm": 1.9670519828796387,
      "learning_rate": 1.7222884386174017e-05,
      "loss": 2.3785,
      "step": 1100
    },
    {
      "epoch": 1.3230035756853398,
      "grad_norm": 2.016437530517578,
      "learning_rate": 1.692491060786651e-05,
      "loss": 2.4064,
      "step": 1110
    },
    {
      "epoch": 1.3349225268176401,
      "grad_norm": 2.6575284004211426,
      "learning_rate": 1.6626936829559e-05,
      "loss": 2.4077,
      "step": 1120
    },
    {
      "epoch": 1.3468414779499405,
      "grad_norm": 2.257981300354004,
      "learning_rate": 1.632896305125149e-05,
      "loss": 2.4845,
      "step": 1130
    },
    {
      "epoch": 1.3587604290822408,
      "grad_norm": 1.580819010734558,
      "learning_rate": 1.603098927294398e-05,
      "loss": 2.353,
      "step": 1140
    },
    {
      "epoch": 1.3706793802145412,
      "grad_norm": 1.7312827110290527,
      "learning_rate": 1.573301549463647e-05,
      "loss": 2.4381,
      "step": 1150
    },
    {
      "epoch": 1.3825983313468415,
      "grad_norm": 1.9444750547409058,
      "learning_rate": 1.5435041716328963e-05,
      "loss": 2.2892,
      "step": 1160
    },
    {
      "epoch": 1.3945172824791419,
      "grad_norm": 1.7008605003356934,
      "learning_rate": 1.5137067938021454e-05,
      "loss": 2.1865,
      "step": 1170
    },
    {
      "epoch": 1.4064362336114422,
      "grad_norm": 2.091810464859009,
      "learning_rate": 1.4839094159713946e-05,
      "loss": 2.3347,
      "step": 1180
    },
    {
      "epoch": 1.4183551847437426,
      "grad_norm": 2.1566684246063232,
      "learning_rate": 1.4541120381406437e-05,
      "loss": 2.4012,
      "step": 1190
    },
    {
      "epoch": 1.430274135876043,
      "grad_norm": 2.338925838470459,
      "learning_rate": 1.4243146603098927e-05,
      "loss": 2.3952,
      "step": 1200
    },
    {
      "epoch": 1.4421930870083433,
      "grad_norm": 3.124746322631836,
      "learning_rate": 1.3945172824791419e-05,
      "loss": 2.5055,
      "step": 1210
    },
    {
      "epoch": 1.4541120381406436,
      "grad_norm": 2.7192347049713135,
      "learning_rate": 1.364719904648391e-05,
      "loss": 2.3329,
      "step": 1220
    },
    {
      "epoch": 1.466030989272944,
      "grad_norm": 2.9920654296875,
      "learning_rate": 1.3349225268176402e-05,
      "loss": 2.4546,
      "step": 1230
    },
    {
      "epoch": 1.4779499404052443,
      "grad_norm": 2.7703635692596436,
      "learning_rate": 1.3051251489868894e-05,
      "loss": 2.523,
      "step": 1240
    },
    {
      "epoch": 1.4898688915375446,
      "grad_norm": 2.3719518184661865,
      "learning_rate": 1.2753277711561385e-05,
      "loss": 2.5271,
      "step": 1250
    },
    {
      "epoch": 1.5017878426698452,
      "grad_norm": 1.7674814462661743,
      "learning_rate": 1.2455303933253875e-05,
      "loss": 2.3092,
      "step": 1260
    },
    {
      "epoch": 1.5137067938021453,
      "grad_norm": 2.231431722640991,
      "learning_rate": 1.2157330154946365e-05,
      "loss": 2.4185,
      "step": 1270
    },
    {
      "epoch": 1.525625744934446,
      "grad_norm": 1.711674690246582,
      "learning_rate": 1.1859356376638856e-05,
      "loss": 2.4229,
      "step": 1280
    },
    {
      "epoch": 1.537544696066746,
      "grad_norm": 1.8609817028045654,
      "learning_rate": 1.1561382598331346e-05,
      "loss": 2.3431,
      "step": 1290
    },
    {
      "epoch": 1.5494636471990466,
      "grad_norm": 1.9311186075210571,
      "learning_rate": 1.1263408820023838e-05,
      "loss": 2.4015,
      "step": 1300
    },
    {
      "epoch": 1.5613825983313467,
      "grad_norm": 2.9493601322174072,
      "learning_rate": 1.096543504171633e-05,
      "loss": 2.2748,
      "step": 1310
    },
    {
      "epoch": 1.5733015494636473,
      "grad_norm": 2.4651882648468018,
      "learning_rate": 1.0667461263408821e-05,
      "loss": 2.3486,
      "step": 1320
    },
    {
      "epoch": 1.5852205005959474,
      "grad_norm": 2.403562545776367,
      "learning_rate": 1.0369487485101313e-05,
      "loss": 2.4653,
      "step": 1330
    },
    {
      "epoch": 1.597139451728248,
      "grad_norm": 1.8787879943847656,
      "learning_rate": 1.0071513706793802e-05,
      "loss": 2.4644,
      "step": 1340
    },
    {
      "epoch": 1.6090584028605481,
      "grad_norm": 1.7324281930923462,
      "learning_rate": 9.773539928486292e-06,
      "loss": 2.3029,
      "step": 1350
    },
    {
      "epoch": 1.6209773539928487,
      "grad_norm": 1.775671362876892,
      "learning_rate": 9.475566150178784e-06,
      "loss": 2.4108,
      "step": 1360
    },
    {
      "epoch": 1.6328963051251488,
      "grad_norm": 2.23872971534729,
      "learning_rate": 9.177592371871275e-06,
      "loss": 2.3762,
      "step": 1370
    },
    {
      "epoch": 1.6448152562574494,
      "grad_norm": 1.9942522048950195,
      "learning_rate": 8.879618593563767e-06,
      "loss": 2.3678,
      "step": 1380
    },
    {
      "epoch": 1.6567342073897497,
      "grad_norm": 1.751981258392334,
      "learning_rate": 8.581644815256259e-06,
      "loss": 2.2067,
      "step": 1390
    },
    {
      "epoch": 1.66865315852205,
      "grad_norm": 1.8187514543533325,
      "learning_rate": 8.28367103694875e-06,
      "loss": 2.3667,
      "step": 1400
    },
    {
      "epoch": 1.6805721096543504,
      "grad_norm": 2.694084882736206,
      "learning_rate": 7.98569725864124e-06,
      "loss": 2.3934,
      "step": 1410
    },
    {
      "epoch": 1.6924910607866508,
      "grad_norm": 2.0177016258239746,
      "learning_rate": 7.68772348033373e-06,
      "loss": 2.4458,
      "step": 1420
    },
    {
      "epoch": 1.7044100119189511,
      "grad_norm": 1.5488580465316772,
      "learning_rate": 7.389749702026222e-06,
      "loss": 2.4745,
      "step": 1430
    },
    {
      "epoch": 1.7163289630512515,
      "grad_norm": 2.0793838500976562,
      "learning_rate": 7.091775923718713e-06,
      "loss": 2.3974,
      "step": 1440
    },
    {
      "epoch": 1.7282479141835518,
      "grad_norm": 1.4844335317611694,
      "learning_rate": 6.7938021454112046e-06,
      "loss": 2.32,
      "step": 1450
    },
    {
      "epoch": 1.7401668653158522,
      "grad_norm": 2.029982089996338,
      "learning_rate": 6.495828367103695e-06,
      "loss": 2.4522,
      "step": 1460
    },
    {
      "epoch": 1.7520858164481525,
      "grad_norm": 2.202042818069458,
      "learning_rate": 6.197854588796186e-06,
      "loss": 2.1853,
      "step": 1470
    },
    {
      "epoch": 1.7640047675804529,
      "grad_norm": 1.8675615787506104,
      "learning_rate": 5.8998808104886775e-06,
      "loss": 2.4642,
      "step": 1480
    },
    {
      "epoch": 1.7759237187127532,
      "grad_norm": 1.8323594331741333,
      "learning_rate": 5.601907032181168e-06,
      "loss": 2.3795,
      "step": 1490
    },
    {
      "epoch": 1.7878426698450536,
      "grad_norm": 1.6003588438034058,
      "learning_rate": 5.30393325387366e-06,
      "loss": 2.3868,
      "step": 1500
    },
    {
      "epoch": 1.7997616209773541,
      "grad_norm": 1.4848159551620483,
      "learning_rate": 5.0059594755661505e-06,
      "loss": 2.428,
      "step": 1510
    },
    {
      "epoch": 1.8116805721096543,
      "grad_norm": 1.9038580656051636,
      "learning_rate": 4.707985697258641e-06,
      "loss": 2.2415,
      "step": 1520
    },
    {
      "epoch": 1.8235995232419548,
      "grad_norm": 1.7458528280258179,
      "learning_rate": 4.410011918951133e-06,
      "loss": 2.2834,
      "step": 1530
    },
    {
      "epoch": 1.835518474374255,
      "grad_norm": 1.8820786476135254,
      "learning_rate": 4.1120381406436235e-06,
      "loss": 2.3109,
      "step": 1540
    },
    {
      "epoch": 1.8474374255065555,
      "grad_norm": 2.203167676925659,
      "learning_rate": 3.8140643623361143e-06,
      "loss": 2.3785,
      "step": 1550
    },
    {
      "epoch": 1.8593563766388557,
      "grad_norm": 2.7055768966674805,
      "learning_rate": 3.516090584028606e-06,
      "loss": 2.4235,
      "step": 1560
    },
    {
      "epoch": 1.8712753277711562,
      "grad_norm": 2.2815723419189453,
      "learning_rate": 3.218116805721097e-06,
      "loss": 2.3627,
      "step": 1570
    },
    {
      "epoch": 1.8831942789034564,
      "grad_norm": 2.1566054821014404,
      "learning_rate": 2.920143027413588e-06,
      "loss": 2.34,
      "step": 1580
    },
    {
      "epoch": 1.895113230035757,
      "grad_norm": 2.8675637245178223,
      "learning_rate": 2.622169249106079e-06,
      "loss": 2.3419,
      "step": 1590
    },
    {
      "epoch": 1.907032181168057,
      "grad_norm": 1.8472950458526611,
      "learning_rate": 2.32419547079857e-06,
      "loss": 2.3586,
      "step": 1600
    }
  ],
  "logging_steps": 10,
  "max_steps": 1678,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.0372079337472e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
