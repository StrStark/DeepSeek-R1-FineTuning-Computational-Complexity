{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9982132221560454,
  "eval_steps": 500,
  "global_step": 1678,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011911852293031567,
      "grad_norm": 0.8331869840621948,
      "learning_rate": 4.9702026221692494e-05,
      "loss": 3.7724,
      "step": 10
    },
    {
      "epoch": 0.023823704586063133,
      "grad_norm": 0.8037112355232239,
      "learning_rate": 4.9404052443384986e-05,
      "loss": 3.6957,
      "step": 20
    },
    {
      "epoch": 0.0357355568790947,
      "grad_norm": 0.9495511651039124,
      "learning_rate": 4.910607866507748e-05,
      "loss": 3.5004,
      "step": 30
    },
    {
      "epoch": 0.047647409172126266,
      "grad_norm": 0.8503285050392151,
      "learning_rate": 4.880810488676997e-05,
      "loss": 3.4538,
      "step": 40
    },
    {
      "epoch": 0.05955926146515783,
      "grad_norm": 0.9518029093742371,
      "learning_rate": 4.851013110846246e-05,
      "loss": 3.3857,
      "step": 50
    },
    {
      "epoch": 0.0714711137581894,
      "grad_norm": 0.7812421917915344,
      "learning_rate": 4.821215733015495e-05,
      "loss": 3.3294,
      "step": 60
    },
    {
      "epoch": 0.08338296605122096,
      "grad_norm": 0.8994767665863037,
      "learning_rate": 4.791418355184744e-05,
      "loss": 3.3507,
      "step": 70
    },
    {
      "epoch": 0.09529481834425253,
      "grad_norm": 0.9923936724662781,
      "learning_rate": 4.7616209773539935e-05,
      "loss": 3.165,
      "step": 80
    },
    {
      "epoch": 0.1072066706372841,
      "grad_norm": 1.095488429069519,
      "learning_rate": 4.7318235995232426e-05,
      "loss": 2.9983,
      "step": 90
    },
    {
      "epoch": 0.11911852293031566,
      "grad_norm": 0.6779934763908386,
      "learning_rate": 4.702026221692492e-05,
      "loss": 2.6345,
      "step": 100
    },
    {
      "epoch": 0.13103037522334723,
      "grad_norm": 0.9980449676513672,
      "learning_rate": 4.672228843861741e-05,
      "loss": 2.927,
      "step": 110
    },
    {
      "epoch": 0.1429422275163788,
      "grad_norm": 1.3400304317474365,
      "learning_rate": 4.6424314660309894e-05,
      "loss": 3.0266,
      "step": 120
    },
    {
      "epoch": 0.15485407980941035,
      "grad_norm": 1.1539180278778076,
      "learning_rate": 4.6126340882002386e-05,
      "loss": 2.962,
      "step": 130
    },
    {
      "epoch": 0.16676593210244192,
      "grad_norm": 1.362911343574524,
      "learning_rate": 4.582836710369488e-05,
      "loss": 2.9862,
      "step": 140
    },
    {
      "epoch": 0.1786777843954735,
      "grad_norm": 1.1137619018554688,
      "learning_rate": 4.553039332538737e-05,
      "loss": 2.9164,
      "step": 150
    },
    {
      "epoch": 0.19058963668850507,
      "grad_norm": 1.068885087966919,
      "learning_rate": 4.523241954707986e-05,
      "loss": 2.7142,
      "step": 160
    },
    {
      "epoch": 0.20250148898153664,
      "grad_norm": 1.5746004581451416,
      "learning_rate": 4.4934445768772345e-05,
      "loss": 2.7634,
      "step": 170
    },
    {
      "epoch": 0.2144133412745682,
      "grad_norm": 1.4264402389526367,
      "learning_rate": 4.463647199046484e-05,
      "loss": 2.9374,
      "step": 180
    },
    {
      "epoch": 0.22632519356759975,
      "grad_norm": 1.0312334299087524,
      "learning_rate": 4.433849821215733e-05,
      "loss": 2.7409,
      "step": 190
    },
    {
      "epoch": 0.23823704586063132,
      "grad_norm": 1.1290526390075684,
      "learning_rate": 4.404052443384982e-05,
      "loss": 2.7563,
      "step": 200
    },
    {
      "epoch": 0.25014889815366287,
      "grad_norm": 1.2668434381484985,
      "learning_rate": 4.374255065554231e-05,
      "loss": 2.75,
      "step": 210
    },
    {
      "epoch": 0.26206075044669447,
      "grad_norm": 1.447741150856018,
      "learning_rate": 4.34445768772348e-05,
      "loss": 2.6486,
      "step": 220
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 1.126152753829956,
      "learning_rate": 4.3146603098927295e-05,
      "loss": 2.6745,
      "step": 230
    },
    {
      "epoch": 0.2858844550327576,
      "grad_norm": 1.2109392881393433,
      "learning_rate": 4.2848629320619786e-05,
      "loss": 2.6507,
      "step": 240
    },
    {
      "epoch": 0.29779630732578916,
      "grad_norm": 1.3083012104034424,
      "learning_rate": 4.255065554231228e-05,
      "loss": 2.6699,
      "step": 250
    },
    {
      "epoch": 0.3097081596188207,
      "grad_norm": 0.9998019933700562,
      "learning_rate": 4.225268176400477e-05,
      "loss": 2.5862,
      "step": 260
    },
    {
      "epoch": 0.3216200119118523,
      "grad_norm": 1.2239547967910767,
      "learning_rate": 4.195470798569726e-05,
      "loss": 2.6009,
      "step": 270
    },
    {
      "epoch": 0.33353186420488384,
      "grad_norm": 2.5089659690856934,
      "learning_rate": 4.165673420738975e-05,
      "loss": 2.5918,
      "step": 280
    },
    {
      "epoch": 0.34544371649791544,
      "grad_norm": 1.9186148643493652,
      "learning_rate": 4.1358760429082244e-05,
      "loss": 2.5955,
      "step": 290
    },
    {
      "epoch": 0.357355568790947,
      "grad_norm": 1.768919587135315,
      "learning_rate": 4.1060786650774736e-05,
      "loss": 2.5537,
      "step": 300
    },
    {
      "epoch": 0.36926742108397853,
      "grad_norm": 1.3619683980941772,
      "learning_rate": 4.076281287246723e-05,
      "loss": 2.6401,
      "step": 310
    },
    {
      "epoch": 0.38117927337701013,
      "grad_norm": 1.456097960472107,
      "learning_rate": 4.046483909415972e-05,
      "loss": 2.634,
      "step": 320
    },
    {
      "epoch": 0.3930911256700417,
      "grad_norm": 1.66647469997406,
      "learning_rate": 4.016686531585221e-05,
      "loss": 2.5691,
      "step": 330
    },
    {
      "epoch": 0.4050029779630733,
      "grad_norm": 1.7197571992874146,
      "learning_rate": 3.98688915375447e-05,
      "loss": 2.5725,
      "step": 340
    },
    {
      "epoch": 0.4169148302561048,
      "grad_norm": 0.9992815852165222,
      "learning_rate": 3.9570917759237194e-05,
      "loss": 2.6029,
      "step": 350
    },
    {
      "epoch": 0.4288266825491364,
      "grad_norm": 1.6268552541732788,
      "learning_rate": 3.927294398092968e-05,
      "loss": 2.6413,
      "step": 360
    },
    {
      "epoch": 0.44073853484216796,
      "grad_norm": 1.219160795211792,
      "learning_rate": 3.897497020262217e-05,
      "loss": 2.4722,
      "step": 370
    },
    {
      "epoch": 0.4526503871351995,
      "grad_norm": 1.3183561563491821,
      "learning_rate": 3.867699642431466e-05,
      "loss": 2.5172,
      "step": 380
    },
    {
      "epoch": 0.4645622394282311,
      "grad_norm": 1.4275727272033691,
      "learning_rate": 3.837902264600715e-05,
      "loss": 2.5209,
      "step": 390
    },
    {
      "epoch": 0.47647409172126265,
      "grad_norm": 1.657426118850708,
      "learning_rate": 3.8081048867699645e-05,
      "loss": 2.4935,
      "step": 400
    },
    {
      "epoch": 0.48838594401429425,
      "grad_norm": 1.6949247121810913,
      "learning_rate": 3.7783075089392136e-05,
      "loss": 2.6291,
      "step": 410
    },
    {
      "epoch": 0.5002977963073257,
      "grad_norm": 1.4263520240783691,
      "learning_rate": 3.748510131108463e-05,
      "loss": 2.5819,
      "step": 420
    },
    {
      "epoch": 0.5122096486003573,
      "grad_norm": 2.180471181869507,
      "learning_rate": 3.718712753277712e-05,
      "loss": 2.5061,
      "step": 430
    },
    {
      "epoch": 0.5241215008933889,
      "grad_norm": 1.4686040878295898,
      "learning_rate": 3.688915375446961e-05,
      "loss": 2.1739,
      "step": 440
    },
    {
      "epoch": 0.5360333531864205,
      "grad_norm": 1.17300546169281,
      "learning_rate": 3.6591179976162096e-05,
      "loss": 2.5186,
      "step": 450
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 1.42994225025177,
      "learning_rate": 3.629320619785459e-05,
      "loss": 2.389,
      "step": 460
    },
    {
      "epoch": 0.5598570577724836,
      "grad_norm": 1.7544924020767212,
      "learning_rate": 3.599523241954708e-05,
      "loss": 2.6046,
      "step": 470
    },
    {
      "epoch": 0.5717689100655152,
      "grad_norm": 1.4461973905563354,
      "learning_rate": 3.569725864123957e-05,
      "loss": 2.4709,
      "step": 480
    },
    {
      "epoch": 0.5836807623585467,
      "grad_norm": 1.6270467042922974,
      "learning_rate": 3.539928486293206e-05,
      "loss": 2.5544,
      "step": 490
    },
    {
      "epoch": 0.5955926146515783,
      "grad_norm": 1.5565637350082397,
      "learning_rate": 3.5101311084624553e-05,
      "loss": 2.4702,
      "step": 500
    },
    {
      "epoch": 0.6075044669446099,
      "grad_norm": 1.6378673315048218,
      "learning_rate": 3.4803337306317045e-05,
      "loss": 2.4331,
      "step": 510
    },
    {
      "epoch": 0.6194163192376414,
      "grad_norm": 2.032350778579712,
      "learning_rate": 3.4505363528009537e-05,
      "loss": 2.5549,
      "step": 520
    },
    {
      "epoch": 0.631328171530673,
      "grad_norm": 1.6773067712783813,
      "learning_rate": 3.420738974970203e-05,
      "loss": 2.2164,
      "step": 530
    },
    {
      "epoch": 0.6432400238237046,
      "grad_norm": 1.2518166303634644,
      "learning_rate": 3.390941597139452e-05,
      "loss": 2.3613,
      "step": 540
    },
    {
      "epoch": 0.6551518761167362,
      "grad_norm": 1.6483936309814453,
      "learning_rate": 3.361144219308701e-05,
      "loss": 2.5606,
      "step": 550
    },
    {
      "epoch": 0.6670637284097677,
      "grad_norm": 1.524681568145752,
      "learning_rate": 3.33134684147795e-05,
      "loss": 2.4932,
      "step": 560
    },
    {
      "epoch": 0.6789755807027993,
      "grad_norm": 1.7687208652496338,
      "learning_rate": 3.3015494636471994e-05,
      "loss": 2.4274,
      "step": 570
    },
    {
      "epoch": 0.6908874329958309,
      "grad_norm": 1.5266315937042236,
      "learning_rate": 3.2717520858164486e-05,
      "loss": 2.3303,
      "step": 580
    },
    {
      "epoch": 0.7027992852888624,
      "grad_norm": 1.522974967956543,
      "learning_rate": 3.241954707985698e-05,
      "loss": 2.5184,
      "step": 590
    },
    {
      "epoch": 0.714711137581894,
      "grad_norm": 1.596190094947815,
      "learning_rate": 3.212157330154946e-05,
      "loss": 2.3835,
      "step": 600
    },
    {
      "epoch": 0.7266229898749256,
      "grad_norm": 1.7242026329040527,
      "learning_rate": 3.1823599523241954e-05,
      "loss": 2.5825,
      "step": 610
    },
    {
      "epoch": 0.7385348421679571,
      "grad_norm": 1.677919626235962,
      "learning_rate": 3.1525625744934445e-05,
      "loss": 2.4108,
      "step": 620
    },
    {
      "epoch": 0.7504466944609887,
      "grad_norm": 1.6081762313842773,
      "learning_rate": 3.122765196662694e-05,
      "loss": 2.488,
      "step": 630
    },
    {
      "epoch": 0.7623585467540203,
      "grad_norm": 1.6532959938049316,
      "learning_rate": 3.092967818831943e-05,
      "loss": 2.3725,
      "step": 640
    },
    {
      "epoch": 0.7742703990470519,
      "grad_norm": 1.926618218421936,
      "learning_rate": 3.063170441001192e-05,
      "loss": 2.4243,
      "step": 650
    },
    {
      "epoch": 0.7861822513400833,
      "grad_norm": 1.7899537086486816,
      "learning_rate": 3.0333730631704412e-05,
      "loss": 2.3725,
      "step": 660
    },
    {
      "epoch": 0.798094103633115,
      "grad_norm": 1.6685471534729004,
      "learning_rate": 3.00357568533969e-05,
      "loss": 2.5048,
      "step": 670
    },
    {
      "epoch": 0.8100059559261465,
      "grad_norm": 1.3715529441833496,
      "learning_rate": 2.973778307508939e-05,
      "loss": 2.3748,
      "step": 680
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 1.7303473949432373,
      "learning_rate": 2.9439809296781883e-05,
      "loss": 2.3572,
      "step": 690
    },
    {
      "epoch": 0.8338296605122096,
      "grad_norm": 1.6842427253723145,
      "learning_rate": 2.9141835518474375e-05,
      "loss": 2.3532,
      "step": 700
    },
    {
      "epoch": 0.8457415128052412,
      "grad_norm": 1.4827057123184204,
      "learning_rate": 2.8843861740166866e-05,
      "loss": 2.2804,
      "step": 710
    },
    {
      "epoch": 0.8576533650982728,
      "grad_norm": 1.32364821434021,
      "learning_rate": 2.8545887961859358e-05,
      "loss": 2.2645,
      "step": 720
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 1.4762771129608154,
      "learning_rate": 2.824791418355185e-05,
      "loss": 2.5986,
      "step": 730
    },
    {
      "epoch": 0.8814770696843359,
      "grad_norm": 1.8222447633743286,
      "learning_rate": 2.794994040524434e-05,
      "loss": 2.3083,
      "step": 740
    },
    {
      "epoch": 0.8933889219773675,
      "grad_norm": 1.4409270286560059,
      "learning_rate": 2.7651966626936832e-05,
      "loss": 2.3244,
      "step": 750
    },
    {
      "epoch": 0.905300774270399,
      "grad_norm": 1.2801510095596313,
      "learning_rate": 2.7353992848629324e-05,
      "loss": 2.3166,
      "step": 760
    },
    {
      "epoch": 0.9172126265634306,
      "grad_norm": 1.4996535778045654,
      "learning_rate": 2.7056019070321816e-05,
      "loss": 2.3031,
      "step": 770
    },
    {
      "epoch": 0.9291244788564622,
      "grad_norm": 1.2773430347442627,
      "learning_rate": 2.6758045292014307e-05,
      "loss": 2.2532,
      "step": 780
    },
    {
      "epoch": 0.9410363311494937,
      "grad_norm": 1.5293323993682861,
      "learning_rate": 2.6460071513706795e-05,
      "loss": 2.3325,
      "step": 790
    },
    {
      "epoch": 0.9529481834425253,
      "grad_norm": 1.4878816604614258,
      "learning_rate": 2.6162097735399287e-05,
      "loss": 2.3281,
      "step": 800
    },
    {
      "epoch": 0.9648600357355569,
      "grad_norm": 1.5827871561050415,
      "learning_rate": 2.586412395709178e-05,
      "loss": 2.2785,
      "step": 810
    },
    {
      "epoch": 0.9767718880285885,
      "grad_norm": 1.3327616453170776,
      "learning_rate": 2.556615017878427e-05,
      "loss": 2.3398,
      "step": 820
    },
    {
      "epoch": 0.98868374032162,
      "grad_norm": 1.6732133626937866,
      "learning_rate": 2.526817640047676e-05,
      "loss": 2.5643,
      "step": 830
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.6273694038391113,
      "learning_rate": 2.497020262216925e-05,
      "loss": 2.3472,
      "step": 840
    },
    {
      "epoch": 1.0119118522930315,
      "grad_norm": 2.067218542098999,
      "learning_rate": 2.467222884386174e-05,
      "loss": 2.4122,
      "step": 850
    },
    {
      "epoch": 1.0238237045860632,
      "grad_norm": 1.4873181581497192,
      "learning_rate": 2.4374255065554233e-05,
      "loss": 2.4088,
      "step": 860
    },
    {
      "epoch": 1.0357355568790947,
      "grad_norm": 1.6014041900634766,
      "learning_rate": 2.4076281287246724e-05,
      "loss": 2.3817,
      "step": 870
    },
    {
      "epoch": 1.0476474091721262,
      "grad_norm": 1.9448353052139282,
      "learning_rate": 2.3778307508939216e-05,
      "loss": 2.4088,
      "step": 880
    },
    {
      "epoch": 1.0595592614651579,
      "grad_norm": 1.7703160047531128,
      "learning_rate": 2.3480333730631707e-05,
      "loss": 2.2214,
      "step": 890
    },
    {
      "epoch": 1.0714711137581894,
      "grad_norm": 1.7674610614776611,
      "learning_rate": 2.3182359952324196e-05,
      "loss": 2.4276,
      "step": 900
    },
    {
      "epoch": 1.0833829660512209,
      "grad_norm": 2.051823854446411,
      "learning_rate": 2.2884386174016687e-05,
      "loss": 2.1589,
      "step": 910
    },
    {
      "epoch": 1.0952948183442526,
      "grad_norm": 1.3520689010620117,
      "learning_rate": 2.258641239570918e-05,
      "loss": 2.2656,
      "step": 920
    },
    {
      "epoch": 1.107206670637284,
      "grad_norm": 1.9318726062774658,
      "learning_rate": 2.228843861740167e-05,
      "loss": 2.3355,
      "step": 930
    },
    {
      "epoch": 1.1191185229303158,
      "grad_norm": 1.9838063716888428,
      "learning_rate": 2.1990464839094162e-05,
      "loss": 2.5123,
      "step": 940
    },
    {
      "epoch": 1.1310303752233473,
      "grad_norm": 1.3179174661636353,
      "learning_rate": 2.169249106078665e-05,
      "loss": 2.3676,
      "step": 950
    },
    {
      "epoch": 1.1429422275163788,
      "grad_norm": 1.643439769744873,
      "learning_rate": 2.139451728247914e-05,
      "loss": 2.3823,
      "step": 960
    },
    {
      "epoch": 1.1548540798094105,
      "grad_norm": 1.5513259172439575,
      "learning_rate": 2.1096543504171633e-05,
      "loss": 2.4744,
      "step": 970
    },
    {
      "epoch": 1.166765932102442,
      "grad_norm": 1.808224081993103,
      "learning_rate": 2.0798569725864125e-05,
      "loss": 2.3316,
      "step": 980
    },
    {
      "epoch": 1.1786777843954734,
      "grad_norm": 3.0821874141693115,
      "learning_rate": 2.0500595947556616e-05,
      "loss": 2.3655,
      "step": 990
    },
    {
      "epoch": 1.1905896366885051,
      "grad_norm": 1.5047787427902222,
      "learning_rate": 2.0202622169249108e-05,
      "loss": 2.3095,
      "step": 1000
    },
    {
      "epoch": 1.2025014889815366,
      "grad_norm": 1.6033228635787964,
      "learning_rate": 1.99046483909416e-05,
      "loss": 2.32,
      "step": 1010
    },
    {
      "epoch": 1.2144133412745681,
      "grad_norm": 1.3269734382629395,
      "learning_rate": 1.9606674612634088e-05,
      "loss": 2.3661,
      "step": 1020
    },
    {
      "epoch": 1.2263251935675998,
      "grad_norm": 1.8193552494049072,
      "learning_rate": 1.930870083432658e-05,
      "loss": 2.3594,
      "step": 1030
    },
    {
      "epoch": 1.2382370458606313,
      "grad_norm": 1.755932092666626,
      "learning_rate": 1.901072705601907e-05,
      "loss": 2.3547,
      "step": 1040
    },
    {
      "epoch": 1.2501488981536628,
      "grad_norm": 2.019310712814331,
      "learning_rate": 1.8712753277711562e-05,
      "loss": 2.19,
      "step": 1050
    },
    {
      "epoch": 1.2620607504466945,
      "grad_norm": 1.755027413368225,
      "learning_rate": 1.8414779499404054e-05,
      "loss": 2.4669,
      "step": 1060
    },
    {
      "epoch": 1.273972602739726,
      "grad_norm": 1.2714399099349976,
      "learning_rate": 1.8116805721096545e-05,
      "loss": 2.2005,
      "step": 1070
    },
    {
      "epoch": 1.2858844550327575,
      "grad_norm": 1.8046023845672607,
      "learning_rate": 1.7818831942789037e-05,
      "loss": 2.2367,
      "step": 1080
    },
    {
      "epoch": 1.2977963073257892,
      "grad_norm": 1.3572839498519897,
      "learning_rate": 1.7520858164481525e-05,
      "loss": 2.423,
      "step": 1090
    },
    {
      "epoch": 1.3097081596188207,
      "grad_norm": 2.1495413780212402,
      "learning_rate": 1.7222884386174017e-05,
      "loss": 2.3795,
      "step": 1100
    },
    {
      "epoch": 1.3216200119118522,
      "grad_norm": 1.61260986328125,
      "learning_rate": 1.692491060786651e-05,
      "loss": 2.5513,
      "step": 1110
    },
    {
      "epoch": 1.333531864204884,
      "grad_norm": 1.5561422109603882,
      "learning_rate": 1.6626936829559e-05,
      "loss": 2.419,
      "step": 1120
    },
    {
      "epoch": 1.3454437164979154,
      "grad_norm": 1.4184824228286743,
      "learning_rate": 1.632896305125149e-05,
      "loss": 2.2199,
      "step": 1130
    },
    {
      "epoch": 1.3573555687909469,
      "grad_norm": 1.6324427127838135,
      "learning_rate": 1.603098927294398e-05,
      "loss": 2.1693,
      "step": 1140
    },
    {
      "epoch": 1.3692674210839786,
      "grad_norm": 1.8703070878982544,
      "learning_rate": 1.573301549463647e-05,
      "loss": 2.2841,
      "step": 1150
    },
    {
      "epoch": 1.38117927337701,
      "grad_norm": 1.5572154521942139,
      "learning_rate": 1.5435041716328963e-05,
      "loss": 2.3698,
      "step": 1160
    },
    {
      "epoch": 1.3930911256700416,
      "grad_norm": 1.6125736236572266,
      "learning_rate": 1.5137067938021454e-05,
      "loss": 2.2858,
      "step": 1170
    },
    {
      "epoch": 1.4050029779630733,
      "grad_norm": 1.6269760131835938,
      "learning_rate": 1.4839094159713946e-05,
      "loss": 2.2926,
      "step": 1180
    },
    {
      "epoch": 1.4169148302561048,
      "grad_norm": 1.2340846061706543,
      "learning_rate": 1.4541120381406437e-05,
      "loss": 2.275,
      "step": 1190
    },
    {
      "epoch": 1.4288266825491365,
      "grad_norm": 1.500461459159851,
      "learning_rate": 1.4243146603098927e-05,
      "loss": 2.3358,
      "step": 1200
    },
    {
      "epoch": 1.440738534842168,
      "grad_norm": 1.2770748138427734,
      "learning_rate": 1.3945172824791419e-05,
      "loss": 2.2652,
      "step": 1210
    },
    {
      "epoch": 1.4526503871351995,
      "grad_norm": 1.2782433032989502,
      "learning_rate": 1.364719904648391e-05,
      "loss": 2.2458,
      "step": 1220
    },
    {
      "epoch": 1.4645622394282312,
      "grad_norm": 2.0666635036468506,
      "learning_rate": 1.3349225268176402e-05,
      "loss": 2.2725,
      "step": 1230
    },
    {
      "epoch": 1.4764740917212626,
      "grad_norm": 1.5859870910644531,
      "learning_rate": 1.3051251489868894e-05,
      "loss": 2.2567,
      "step": 1240
    },
    {
      "epoch": 1.4883859440142944,
      "grad_norm": 1.5281895399093628,
      "learning_rate": 1.2753277711561385e-05,
      "loss": 2.3762,
      "step": 1250
    },
    {
      "epoch": 1.5002977963073256,
      "grad_norm": 1.9030601978302002,
      "learning_rate": 1.2455303933253875e-05,
      "loss": 2.2009,
      "step": 1260
    },
    {
      "epoch": 1.5122096486003573,
      "grad_norm": 1.6866012811660767,
      "learning_rate": 1.2157330154946365e-05,
      "loss": 2.4375,
      "step": 1270
    },
    {
      "epoch": 1.524121500893389,
      "grad_norm": 1.8849905729293823,
      "learning_rate": 1.1859356376638856e-05,
      "loss": 2.2826,
      "step": 1280
    },
    {
      "epoch": 1.5360333531864205,
      "grad_norm": 1.576212763786316,
      "learning_rate": 1.1561382598331346e-05,
      "loss": 2.429,
      "step": 1290
    },
    {
      "epoch": 1.547945205479452,
      "grad_norm": 1.8793307542800903,
      "learning_rate": 1.1263408820023838e-05,
      "loss": 2.3525,
      "step": 1300
    },
    {
      "epoch": 1.5598570577724837,
      "grad_norm": 1.425812005996704,
      "learning_rate": 1.096543504171633e-05,
      "loss": 2.382,
      "step": 1310
    },
    {
      "epoch": 1.5717689100655152,
      "grad_norm": 1.5043357610702515,
      "learning_rate": 1.0667461263408821e-05,
      "loss": 2.4539,
      "step": 1320
    },
    {
      "epoch": 1.5836807623585467,
      "grad_norm": 1.1537679433822632,
      "learning_rate": 1.0369487485101313e-05,
      "loss": 2.2848,
      "step": 1330
    },
    {
      "epoch": 1.5955926146515784,
      "grad_norm": 2.4558358192443848,
      "learning_rate": 1.0071513706793802e-05,
      "loss": 2.371,
      "step": 1340
    },
    {
      "epoch": 1.60750446694461,
      "grad_norm": 1.5501887798309326,
      "learning_rate": 9.773539928486292e-06,
      "loss": 2.319,
      "step": 1350
    },
    {
      "epoch": 1.6194163192376414,
      "grad_norm": 1.958216905593872,
      "learning_rate": 9.475566150178784e-06,
      "loss": 2.4019,
      "step": 1360
    },
    {
      "epoch": 1.631328171530673,
      "grad_norm": 2.388273239135742,
      "learning_rate": 9.177592371871275e-06,
      "loss": 2.3175,
      "step": 1370
    },
    {
      "epoch": 1.6432400238237046,
      "grad_norm": 1.5088335275650024,
      "learning_rate": 8.879618593563767e-06,
      "loss": 2.3884,
      "step": 1380
    },
    {
      "epoch": 1.655151876116736,
      "grad_norm": 1.6471893787384033,
      "learning_rate": 8.581644815256259e-06,
      "loss": 2.2425,
      "step": 1390
    },
    {
      "epoch": 1.6670637284097678,
      "grad_norm": 2.4210045337677,
      "learning_rate": 8.28367103694875e-06,
      "loss": 2.3452,
      "step": 1400
    },
    {
      "epoch": 1.6789755807027993,
      "grad_norm": 1.7973328828811646,
      "learning_rate": 7.98569725864124e-06,
      "loss": 2.2811,
      "step": 1410
    },
    {
      "epoch": 1.6908874329958308,
      "grad_norm": 1.5109117031097412,
      "learning_rate": 7.68772348033373e-06,
      "loss": 2.4405,
      "step": 1420
    },
    {
      "epoch": 1.7027992852888625,
      "grad_norm": 1.159056544303894,
      "learning_rate": 7.389749702026222e-06,
      "loss": 2.2554,
      "step": 1430
    },
    {
      "epoch": 1.714711137581894,
      "grad_norm": 1.2405815124511719,
      "learning_rate": 7.091775923718713e-06,
      "loss": 2.2882,
      "step": 1440
    },
    {
      "epoch": 1.7266229898749255,
      "grad_norm": 1.460023283958435,
      "learning_rate": 6.7938021454112046e-06,
      "loss": 2.326,
      "step": 1450
    },
    {
      "epoch": 1.7385348421679572,
      "grad_norm": 1.5156199932098389,
      "learning_rate": 6.495828367103695e-06,
      "loss": 2.1946,
      "step": 1460
    },
    {
      "epoch": 1.7504466944609887,
      "grad_norm": 1.5196524858474731,
      "learning_rate": 6.197854588796186e-06,
      "loss": 2.3398,
      "step": 1470
    },
    {
      "epoch": 1.7623585467540201,
      "grad_norm": 1.8714206218719482,
      "learning_rate": 5.8998808104886775e-06,
      "loss": 2.4027,
      "step": 1480
    },
    {
      "epoch": 1.7742703990470519,
      "grad_norm": 1.6636935472488403,
      "learning_rate": 5.601907032181168e-06,
      "loss": 2.5278,
      "step": 1490
    },
    {
      "epoch": 1.7861822513400833,
      "grad_norm": 1.5672438144683838,
      "learning_rate": 5.30393325387366e-06,
      "loss": 2.3213,
      "step": 1500
    },
    {
      "epoch": 1.7980941036331148,
      "grad_norm": 1.4658167362213135,
      "learning_rate": 5.0059594755661505e-06,
      "loss": 2.2555,
      "step": 1510
    },
    {
      "epoch": 1.8100059559261465,
      "grad_norm": 1.5514888763427734,
      "learning_rate": 4.707985697258641e-06,
      "loss": 2.3126,
      "step": 1520
    },
    {
      "epoch": 1.821917808219178,
      "grad_norm": 2.6841838359832764,
      "learning_rate": 4.410011918951133e-06,
      "loss": 2.4099,
      "step": 1530
    },
    {
      "epoch": 1.8338296605122095,
      "grad_norm": 1.8611688613891602,
      "learning_rate": 4.1120381406436235e-06,
      "loss": 2.4622,
      "step": 1540
    },
    {
      "epoch": 1.8457415128052412,
      "grad_norm": 1.6732250452041626,
      "learning_rate": 3.8140643623361143e-06,
      "loss": 2.3307,
      "step": 1550
    },
    {
      "epoch": 1.857653365098273,
      "grad_norm": 1.5709197521209717,
      "learning_rate": 3.516090584028606e-06,
      "loss": 2.4066,
      "step": 1560
    },
    {
      "epoch": 1.8695652173913042,
      "grad_norm": 1.4511183500289917,
      "learning_rate": 3.218116805721097e-06,
      "loss": 2.3794,
      "step": 1570
    },
    {
      "epoch": 1.881477069684336,
      "grad_norm": 1.371374487876892,
      "learning_rate": 2.920143027413588e-06,
      "loss": 2.391,
      "step": 1580
    },
    {
      "epoch": 1.8933889219773676,
      "grad_norm": 1.77414870262146,
      "learning_rate": 2.622169249106079e-06,
      "loss": 2.1076,
      "step": 1590
    },
    {
      "epoch": 1.905300774270399,
      "grad_norm": 1.913577675819397,
      "learning_rate": 2.32419547079857e-06,
      "loss": 2.3358,
      "step": 1600
    },
    {
      "epoch": 1.9172126265634306,
      "grad_norm": 1.862631916999817,
      "learning_rate": 2.026221692491061e-06,
      "loss": 2.4162,
      "step": 1610
    },
    {
      "epoch": 1.9291244788564623,
      "grad_norm": 1.4007962942123413,
      "learning_rate": 1.728247914183552e-06,
      "loss": 2.3534,
      "step": 1620
    },
    {
      "epoch": 1.9410363311494936,
      "grad_norm": 1.717026948928833,
      "learning_rate": 1.430274135876043e-06,
      "loss": 2.1617,
      "step": 1630
    },
    {
      "epoch": 1.9529481834425253,
      "grad_norm": 1.8610985279083252,
      "learning_rate": 1.132300357568534e-06,
      "loss": 2.3605,
      "step": 1640
    },
    {
      "epoch": 1.964860035735557,
      "grad_norm": 2.265563488006592,
      "learning_rate": 8.343265792610251e-07,
      "loss": 2.2244,
      "step": 1650
    },
    {
      "epoch": 1.9767718880285885,
      "grad_norm": 1.492172360420227,
      "learning_rate": 5.363528009535162e-07,
      "loss": 2.2334,
      "step": 1660
    },
    {
      "epoch": 1.98868374032162,
      "grad_norm": 1.726231336593628,
      "learning_rate": 2.3837902264600714e-07,
      "loss": 2.2636,
      "step": 1670
    }
  ],
  "logging_steps": 10,
  "max_steps": 1678,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.18432269303808e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
