[
  {
    "prompt": "Scientific context on Eyal, Ori, Itai and their role in graph theory.",
    "response": "On the Importance of Error Mitigation for Quantum Computation Dorit Aharonov∗1,2, Ori Alberton1, Itai Arad1,3, Yosi Atia1, Eyal Bairey1, Zvika Brakerski1,4, Itsik Cohen1, Omri Golan†1, Ilya Gurwich1, Oded Kenneth1,5, Eyal Leviatan1, Netanel H."
  },
  {
    "prompt": "This is a technical discussion about Israel, Quantum, Science in theoretical computer science.",
    "response": "Lindner1,5, Ron Aharon Melcer1, Adiel Meyer1, Gili Schul1, and Maor Shutman1 1Qedma Quantum Computing, Tel Aviv, Israel 2The Benin School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel 3Centre for Quantum Technologies, National University of Singapore, Singapore 4Faculty of Mathematics and Computer Science, Weizmann Institute of Science, Israel 5Department of Physics, Technion, Haifa, Israel Abstract Quantum error mitigation (EM) is a family of hybrid quantum-classical methods for eliminating or reducing the effect of noise and decoherence on quantum algorithms run on quantum hardware, without applying quantum error correction (EC)."
  },
  {
    "prompt": "Read and understand this section about mild, exponent, overhead from a research paper.",
    "response": "While EM has many benefits compared to EC, specifically that it requires no (or little) qubit overhead, this benefit comes with a painful price: EM seems to necessitate an overhead in quantum run time which grows as a (mild) exponent [1–4]. Accordingly, recent results show that EM alone cannot enable exponential quantum advantages (QAs), for an average variant of the expectation value estimation problem [5]. These works raised concerns regarding the role of EM in the road map towards QAs."
  },
  {
    "prompt": "Explain a concept involving QAs, clear, achieving in computational complexity.",
    "response": "We aim to demystify the discussion, and provide a clear picture of the role of EM in achieving QAs, both in the near and long term. We first propose a clear distinction between finite QA and asymptotic QA, which is crucial to the understanding of the question, and present the notion of circuit volume boost, which we claim is an adequate way to quantify the benefits of EM. Using these notions we can argue straightforwardly that EM is expected to have a significant role in achieving QAs."
  },
  {
    "prompt": "Understanding the computational aspects of expected, Specifically, QAs.",
    "response": "Specifically, that EM is likely to be the first error reduction method for useful finite QAs, before EC; that the first such QAs are expected to be achieved using EM in the very near future; and that EM is expected to maintain its important role in quantum computation even when EC will be routinely used – for as long as high-quality qubits remain a scarce resource. Contents 1 Introduction 1 1.1 Error correction (EC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Explain a concept involving Error, mitigation, Asymptotic in computational complexity.",
    "response": "1 1.2 Error mitigation (EM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 This paper, in a nutshell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.4 Overview of key points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2 Asymptotic vs. finite quantum advantage (QA) 6 2.1 Asymptotic QA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Finite QA . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about Scalable, Asymptotic, Quantifying from a research paper.",
    "response": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Scalable QA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4 Asymptotic QA vs. Finite QA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.5 Quantifying resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about Mar, INTRODUCTION, Expectation from a research paper.",
    "response": "9 ∗dorit.aharonov@qedma.com †omri.golan@qedma.com arXiv:2503.17243v1 [quant-ph] 21 Mar 2025 1 INTRODUCTION 3 Expectation value estimation 10 3.1 Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.2 Active circuit volume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4 Evidence against asymptotic QA with EM 12 4.1 Evidence for exponential sampling overhead of EM . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about exponential, general, lower from a research paper.",
    "response": "12 4.2 Towards general exponential lower bounds on the sampling overhead . . . . . . . . . . . . 13 4.3 EM with exponential overheads cannot enable exponential asymptotic QA . . . . . . . . . 14 4.4 No-go for exponential asymptotic QA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5 Useful finite QAs expected in the near future using EM 16 5.1 Circuit Volume Boosts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "This is a technical discussion about Predictions, finite, QAs in theoretical computer science.",
    "response": "17 5.2 Predictions for finite QAs enabled using EM alone . . . . . . . . . . . . . . . . . . . . . . 19 6 EM in the era of fault tolerant EC 23 6.1 The fault tolerance misconception: Why additional error reduction is needed even when fault tolerant EC is available . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 6.2 Logical EM: Combining EM with EC to provide further circuit volume boosts . . . . . . 24 6.3 Concluding remarks about logical EM . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about Introduction, quantum, Summary from a research paper.",
    "response": ". . . . . . . . . . . . . 27 7 Summary and forecast: QA timeline 27 1 Introduction 1.1 Error correction (EC) The necessity of protecting quantum computations from noise had been realized already close to three decades ago, soon after the remarkable theoretical discoveries indicating a potential for exponential algorithmic speedups (or quantum advantages, QAs) over classical computing devices [6, 7], including Shor’s algorithm [8]."
  },
  {
    "prompt": "Explain a concept involving quantum, QPUs, fault-tolerant in computational complexity.",
    "response": "The no less remarkable discovery of quantum error correction (EC) [9, 10] and the follow-up proofs of threshold theorems showing that EC can be made fault-tolerant [11–14], laid the foundations for the approach taken today to handle noise: large scale quantum computers (or quantum processing units, QPUs) are to be protected from the detrimental effect of noise by redundantly encoding computational logical qubits into physical qubits, and performing fault-tolerant EC, periodically measuring and correcting errors."
  },
  {
    "prompt": "Understanding the computational aspects of Fault-tolerant, roadmap, QPUs.",
    "response": "Fault-tolerant EC has since become a lively active field on its own, and most quantum hardware companies have adopted this path as the main approach in their roadmap, to handle noise in future large scale QPUs [15–20]. However, the EC approach suffers from several severe practical drawbacks. First, it requires very large overheads in qubit numbers, due to the redundant encoding of physical qubits into logical qubits, as well additional qubits needed to periodically measure error syndromes."
  },
  {
    "prompt": "Scientific context on fault, tolerance, threshold and their role in graph theory.",
    "response": "The second drawback is of course the requirement for extremely high quality qubits; fault tolerance requires that the gates applied on the qubits be of fidelity better than a certain threshold, the “fault tolerance threshold”. EC is only beneficial if the infidelity is below that threshold. Moreover, the closer to the threshold the actual infidelity is, the larger the qubit overhead required in order to achieve the same effective (logical) error rate."
  },
  {
    "prompt": "Read and understand this section about quantum, fault, threshold from a research paper.",
    "response": "This importantly implies that even when the infidelity is below the quantum fault tolerance threshold, the noise still limits the size of the quantum computation. By far the most important leading candidate for a quantum error correcting code to be used on fault tolerant quantum devices, had been, for a long time, the surface code [21], due to its high threshold value [22]."
  },
  {
    "prompt": "Understanding the computational aspects of codes, qLDPC, due.",
    "response": "However, due to the large qubit overhead it requires, recently the interest of the industry had shifted to other codes, and in particular to quantum low-density parity check (qLDPC) codes [23–25], as well as concatenated codes [26, 27]. The past year has seen tremendous progress on the experimental side of various fault tolerance components [28–33]."
  },
  {
    "prompt": "Read and understand this section about INTRODUCTION, unclear, quantum from a research paper.",
    "response": "However, it is still unclear when quantum computers with 1 1 INTRODUCTION sufficiently many high quality logical qubits will be available to enable the first useful QAs using EC alone."
  },
  {
    "prompt": "Explain a concept involving QPU, IBM, Quantinuum in computational complexity.",
    "response": "By useful QA we mean any computation performed by a QPU faster or better than possible by classical computers, and whose output is of interest outside of the question of whether it was achieved by a QPU or by some other computational means.1 According to the public roadmaps of both IBM [16] and Quantinuum [18], for example, achieving the first useful QAs with EC is a matter of at least five years."
  },
  {
    "prompt": "This is a technical discussion about mitigation, Error, side in theoretical computer science.",
    "response": "1.2 Error mitigation (EM) In light of these hurdles, and the long anticipated time until useful QA with EC, side by side with the immense effort and many successes in improving fault tolerant constructions and reducing their requirements, another approach had been developed - a family of protocols which goes by the name of quantum error mitigation (EM) [41]."
  },
  {
    "prompt": "This is a technical discussion about circuit, target, protocols in theoretical computer science.",
    "response": "Such EM protocols replace the execution of a given target circuit with multiple noisy circuit executions; the results of which are post-processed to provide an estimate for the error-free output of the target circuit. The first EM protocols, based on quasi-probability (QP) distributions and zero-noise extrapolation (ZNE) were developed in [42–44] and followed by a plethora of additional protocols and extensions [41, 45–47]. The advantages of EM methods over EC are threefold. 1."
  },
  {
    "prompt": "Explain a concept involving Unlike, relaxed, adaptive in computational complexity.",
    "response": "Unlike EC, the EM approach does not require adaptive operations.2 Thus the requirements on the hardware are significantly relaxed. This lack of adaptive operations is the theoretically defining feature of EM [1–4]. 2. EM methods entail no (or little) overhead in qubit number."
  },
  {
    "prompt": "This is a technical discussion about critical, process, ions in theoretical computer science.",
    "response": "This is critical, given the fact that increasing the number of high quality qubits has proven to be a steady but unfortunately slow process, at least in the currently leading publicly available hardware platforms of superconducting qubits and trapped ions [16, 18]. Qubit numbers may be larger, though still limited, in upcoming neutral atom platforms [17]. 3. Last but not least, EM methods do not require the error rate to be below any threshold! They work with any infidelity."
  },
  {
    "prompt": "Read and understand this section about quantum, current, processors from a research paper.",
    "response": "These advantages make EM applicable on current quantum processors. Indeed, EM protocols are now being routinely used in essentially all large scale quantum computing experiments, showcasing impressive achievements and demonstrating the ability of those methods to bring current quantum processors much closer to QA [48–55]."
  },
  {
    "prompt": "Explain a concept involving protocols, computation, price in computational complexity.",
    "response": "Importantly, already from the first EM protocols, it was clear that these protocols come with a painful price: they exhibit a sampling overhead that grows like a mild exponent with the size of the computation. This exponential price of EM raises important questions: Is EM worth the effort and the time overhead? Is it a viable approach for addressing quantum noise on the way to achieving useful QAs? And very importantly, what would its role be when EC becomes available for end-users?"
  },
  {
    "prompt": "Explain a concept involving concerns, papers, exponential in computational complexity.",
    "response": "In particular, concerns were recently raised following several papers [1–4] that highlighted exponential lower bounds on the sampling overhead of EM. Importantly, a recent strong result by [5] managed to rule out the possibility that EM can provide exponential QA for a certain computational problem (we discuss the results of those papers and their implications in Sec. 4). In light of all this, what is the role of EM in the future of quantum computation?"
  },
  {
    "prompt": "Read and understand this section about method, interesting, RCS from a research paper.",
    "response": "Perhaps it should be considered as a temporary method, interesting for investigations but to be abandoned very soon when EC takes over? 1QAs have already been argued in several experiments [28, 34–39], including random circuit sampling (RCS) experiments; we will mention those in Section 2. As remarkable as these achievements are, they do not belong to this class of useful QAs, since their output is in general not useful, for any purpose other than demonstrating QA (though Ref."
  },
  {
    "prompt": "Scientific context on RCS, suggested, randomness and their role in graph theory.",
    "response": "[40] suggested using RCS to generate certifiable randomness). 2Namely, mid-circuit measurements, followed by real-time classical processing of measurement results, which are in turn followed by unitary gates conditioned on the classical processing output."
  },
  {
    "prompt": "Scientific context on INTRODUCTION, quantum, QAs and their role in graph theory.",
    "response": "2 1 INTRODUCTION 1.3 This paper, in a nutshell The question regarding the role of EM in the evolution of quantum computation is crucial for the clarification of the path that the quantum eco-system will take as it moves forward towards algorithmic QAs. In this manuscript, we aim to remove some of the misconceptions in the public discussion regarding this question, and by that, clarify the role of EM in this roadmap towards algorithmic QAs."
  },
  {
    "prompt": "Read and understand this section about notion, asymptotic, finite from a research paper.",
    "response": "The main message of this paper can be summarized as follows: There are two different types of QA which should not be confused: asymptotic QA, which is a mathematical notion, and finite QA, which is a practical notion. While asymptotic QA is probably indeed not achievable by EM alone, EM is expected to provide dramatic finite QA in the near future, and in fact be the first error reduction approach to provide useful QA, before this will be achieved by EC."
  },
  {
    "prompt": "Read and understand this section about computation, quantum, fault-tolerant from a research paper.",
    "response": "Furthermore, even when fault-tolerant EC becomes available, it is a misconception that this will completely resolve the problem of noise in quantum computation. Since quantum computers are expected to have a limited number of high-fidelity qubits in the foreseeable future, we expect severe limitations on the quantum circuit volumes available even when fault tolerant EC becomes available."
  },
  {
    "prompt": "Understanding the computational aspects of time, solution, leading.",
    "response": "At that time, EM combined with EC would provide a much more effective error reduction solution, leading to circuit volumes that are orders of magnitude larger than those available by EC alone. Thus, EM is here to stay, and is expected to play a crucial role in all important milestones of the quantum ecosystem in the near and far future. The paper follows the above sequence of claims and provides details behind each of the key points."
  },
  {
    "prompt": "This is a technical discussion about Section, paper, overview in theoretical computer science.",
    "response": "Section 1.4 provides a more detailed overview of the flow of the paper. We attempt to keep the discussion simple, and gloss over some details, referring the reader to other resources where these details appear. Table 1 provides an overview of different methods for handling errors in quantum computation, along with the main take-away messages we propose for each method. EM, EC, and logical error mitigation (LEM) are discussed throughout the paper, as detailed below."
  },
  {
    "prompt": "Read and understand this section about bare’, noisy’, Sec from a research paper.",
    "response": "We do not discuss error suppression (ES) except in this table, and assume throughout the paper that ‘bare’, or ‘noisy’, quantum circuit execution already includes the best available ES methods. 1.4 Overview of key points ■Asymptotic vs. finite Quantum advantages Our starting point (Sec. 2) is a distinction, that we believe is too often overlooked, between two notions of QA."
  },
  {
    "prompt": "This is a technical discussion about Shor, factoring, algorithm in theoretical computer science.",
    "response": "One is an asymptotic notion, where the most famous example, perhaps, is Shor’s factoring algorithm that, as far as we know, performs exponentially better than any classical factoring algorithm, when considering the asymptotic cost. The other is finite QA, which refers to a concrete experiment where a QPU computes “much faster” than a classical computer."
  },
  {
    "prompt": "Understanding the computational aspects of larger, Ideally, behavior.",
    "response": "Ideally, the experiment can be repeated with larger and larger settings to establish a scalable behavior, where the QA is expected to strengthen as the problem size grows. We discuss some of the subtleties involved in defining finite QA, such as the proper way to define what is meant by “much faster”, i.e., what is the proper way to quantify the cost of the computation."
  },
  {
    "prompt": "Read and understand this section about expectation, EVE, estimation from a research paper.",
    "response": "■The expectation value estimation problem, and definition of active volume We next present the computational problem on which we focus our discussion of the above notions of QA: expectation value estimation (EVE), in which for a given circuit C and an observable O, one is required to estimate the expectation value of O in the state obtained by applying C to the all zero input state, with a prescribed accuracy."
  },
  {
    "prompt": "Understanding the computational aspects of EVE, algorithms, basic.",
    "response": "EVE is one of the most basic problems in physics and has a variety of applications within the field of quantum algorithms. Furthermore, EVE is particularly appropriate for the discussion 3Realistically, dissipative dynamics is due to coherent interaction with an environment."
  },
  {
    "prompt": "Explain a concept involving coherent’, dissipative’, slow’ in computational complexity.",
    "response": "The distinction we make between ‘coherent’ and ‘dissipative’ errors then depends on the dynamical time scales of the environment relative to the time scales for implementing quantum gates: interaction with a ‘slow’ (‘fast’) environment manifests as ‘coherent’ (‘dissipative’)."
  },
  {
    "prompt": "Understanding the computational aspects of QPU, stated, parallelization.",
    "response": "4Beyond the main resources stated, we note that EM can actually benefit from extra qubits via e.g., parallelization and coherent operations on multiple copies of the noisy computation, and EC does actually require a QPU time overhead, since logical operations are composed of multiple physical gate layers."
  },
  {
    "prompt": "Understanding the computational aspects of allowed, LEM, Sec..",
    "response": "5For EM and LEM, we assume a negligible mitigation bias b ≪ϵ, and the range 1 −10 depends the details of the EM method used, and on the allowed shot overhead (fixing the allowed shot number leads to a logarithmic dependence on ϵ, see Sec. 5.1). 3 1 INTRODUCTION Table 1: Overview of different approaches for dealing with errors in quantum computation."
  },
  {
    "prompt": "Read and understand this section about Error, Sec, bare’ from a research paper.",
    "response": "ES is not further discussed in this paper, and we assume throughout that the best available ES methods have been incorporated into the ‘bare’, or ‘noisy’ circuit execution. EM is described in Sec. 4-5, while EC and LEM are discussed in Sec. 6. Error Suppression (ES) Error Mitigation (EM) Error Correction (EC) Logical EM (LEM) Relation Always done. On top of ES, instead of EC. On top of ES, instead of EM. On top of ES and EC."
  },
  {
    "prompt": "This is a technical discussion about Run, Cancel, directly in theoretical computer science.",
    "response": "Main idea Cancel coherent errors (directly, or between layers or shots).3 Run multiple noisy computations and post-process. Encode qubits redundantly, measure and invert errors in real time. Perform EC and mitigate remaining logical errors. Main resource Essentially free. QPU time.4 Qubits.4 Trade QPU time for qubits. Examples Gate calibration, Pauli twirling, dynamical decoupling. Zero Noise Extrapolation (ZNE), Quasi Probability (QP), Clifford Data Regression (CDR)."
  },
  {
    "prompt": "Understanding the computational aspects of Surface, Check, Low.",
    "response": "Steane, Surface, quantum Low Density Parity Check (qLDPC), concatenation. External LEM, Error correction with post-selection (EC+PS), Syndrome-aware LEM. Maximal circuit volume Vbare := VES ∼ϵ/γ, for allowed inaccuracy ϵ and (un-suppressed) gate infidelity γ. VEM ∼(1 −10)/γ, independent of ϵ.5 VEC ∼ϵ/γ′, for allowed inaccuracy ϵ and logical gate infidelity γ′."
  },
  {
    "prompt": "Explain a concept involving VLEM, Take-away, volume in computational complexity.",
    "response": "VLEM ∼(1 −10)/γ′, independent of ϵ.5 Take-away message Should be done as much as possible, but fundamentally cannot handle dissipation,3 and therefore limited to small volume. Volume boosted by a factor ∼(1 −10)/ϵ over ES. If physical qubit number is limited, tension between logical accuracy (or depth) and width: Higher accuracy requires smaller γ′, which requires more physical qubits per logical qubit, leaving less logical qubits. Break EM volume barrier with EC."
  },
  {
    "prompt": "This is a technical discussion about Ease, Volume, EVE in theoretical computer science.",
    "response": "Ease accuracy-width tension in EC. Volume boosted by a factor ∼(1 −10)/ϵ over EC. Volumes anticipated based on Hardware roadmaps Not indicated. V ∼103 within 2025 [16, 18]. V ∼104 by 2027-2028 [16, 18]. Larger volumes not anticipated with EM [16, 18]. V ∼103 −104 within 2026 [17]. V ∼104 −109 within 2029 [18]. V ∼108 within 2029 [16]. Not indicated. of EM. We define EVE in Sec. 3, together with the notion of active volume – the relevant notion of quantum circuit volume within the context of EVE."
  },
  {
    "prompt": "Explain a concept involving Equipped, EVE, question in computational complexity.",
    "response": "■EM is likely not capable of providing exponential asymptotic QA Equipped with the definitions of EVE and active volume, we next proceed to discuss the question of whether and in what context EM is useful for QA. We first consider the question of asymptotic QA. Section 4 describes the barriers towards the usability of EM for such QA."
  },
  {
    "prompt": "Scientific context on results, providing, accuracy and their role in graph theory.",
    "response": "We start by describing examples and general results, providing strong evidence that EM is expected to require a number of shots that grows exponentially with γV , in order to ensure a required accuracy. Here γ is the gate infidelity and V the active volume of a given instance of EVE."
  },
  {
    "prompt": "Understanding the computational aspects of TEM, EVE, time.",
    "response": "In cases where this exponential behavior holds, we show that if EM solves EVE for a given circuit in time TEM, the same task can be accomplished classically in a time which is bounded (roughly) as Tc = O(T 1/γ EM), i.e., Tc is at most polynomial, with degree 1/γ, in TEM. This rules out exponential (and in fact any non-polynomial) asymptotic QA."
  },
  {
    "prompt": "This is a technical discussion about EVE, problem, exponential in theoretical computer science.",
    "response": "We explain that it is still an open question whether the above exponential scaling as a function of γV is a generic behavior for all EM protocols and inputs to the EVE problem. Nevertheless, recent work significantly strengthened the case against exponential asymptotic QA with EM [5]."
  },
  {
    "prompt": "Understanding the computational aspects of circuit, polynomial, inputs.",
    "response": "By deriving a classical polynomial time simulation of any noisy quantum circuit (albeit when averaged over the inputs, and with the polynomial degree scaling as 1/γ2), the authors deduced that for the EVE problem in which the required accuracy is defined on average over inputs to the circuit, exponential asymptotic QA is ruled out. Interestingly, this leaves room for the 4 1 INTRODUCTION possibility of exponential asymptotic QA with EM in non-generic cases; however, the main message of Sec."
  },
  {
    "prompt": "Understanding the computational aspects of generically, provide, exponential.",
    "response": "4 is that EM cannot generically provide exponential asymptotic QA. ■The polynomial bound on the advantage of EM leaves ample room for finite QA Naively, the above statement could be interpreted to mean that the execution of quantum circuits using EM can be replaced by a classical simulation, so there is no point in using EM on a QPU at all."
  },
  {
    "prompt": "This is a technical discussion about TEM, large, power in theoretical computer science.",
    "response": "However, we observe that the large power 1/γ (e.g., a thousand!) in the above-mentioned polynomial relation between Tc and TEM quantifies a large slowdown of classical computation compared to EM. This implies that even though EM probably cannot exhibit exponential asymptotic QA, there is ample room for EM to enable dramatic finite QAs. In Sec. 5 we indeed argue why EM is expected to enable such finite QA in the very near future, for the useful EVE problem."
  },
  {
    "prompt": "This is a technical discussion about contribution, Boosts, Volume in theoretical computer science.",
    "response": "■Circuit Volume Boosts can quantify the contribution of EM We therefore put forth a quantity that properly quantifies the contribution of EM to the execution of quantum circuits. Indeed, we claim that using EM, it is possible to use noisy hardware to execute quantum circuits of higher volume than is possible with the ‘bare’ execution, without EM."
  },
  {
    "prompt": "Read and understand this section about active, maximal, volume from a research paper.",
    "response": "Namely, we fix the allowed shot overhead and the desired output accuracy, and consider the maximal active volume that can be obtained by naive circuit execution, as opposed to the maximal active volume possible with EM. In Sec. 5.1 we define the ratio of these maximal volumes to be the circuit volume boost (CVB) of EM in the particular setting; Fig. 1 shows the expected CVBs as a function of the required output accuracy."
  },
  {
    "prompt": "Read and understand this section about shot, required, accuracy from a research paper.",
    "response": "For example, EM is expected to enable circuit volume boosts of over a 100 (!) for required accuracy of 99% with a mild allowed shot overhead of 10. Furthermore, the better the required accuracy (and allowed shot overhead), the larger the CVBs that will be provided, namely the more beneficial it is to apply EM. We argue that the CVB is the proper way to quantify the contribution of EM to quantum circuit execution on noisy hardware. ■Dramatic finite QAs expected using EM, way before EC In Sec."
  },
  {
    "prompt": "Scientific context on future, discuss, expected and their role in graph theory.",
    "response": "5.2 we discuss the expected impact of EM in the near future. We argue that EM is likely to be the first error reduction method to enable useful finite QA. We predict that this will be achieved for the fundamental task of EVE, namely, extracting accurate expectation value estimations from ideal (noise free) generic quantum circuits. We expect this milestone to be achieved as soon as hardware platforms reach 50 qubits with an average two-qubit gate fidelity of 99.9%."
  },
  {
    "prompt": "Read and understand this section about Tab, achieved, logical from a research paper.",
    "response": "We note that these hardware requirements will certainly be achieved before quantum computations on 50 logical qubits will be available using error correction alone (see Tab. 2); which means that finite QAs will be achieved first, and by a large margin, by EM before it is achieved by EC! Our predictions are given in Fig. 2."
  },
  {
    "prompt": "Scientific context on circuit, volume, geometry and their role in graph theory.",
    "response": "We additionally discuss the relation between the circuit geometry (shape of the active volume) and the relative run time of EM compared to classical simulation, providing geometrical guidelines for the construction of early finite QA demonstrations. ■EM, combined with EC, will continue to provide large circuit volume boosts even in the fault tolerant EC era In Sec. 6 we turn to the important discussion of the role of quantum EM when fault-tolerant EC becomes available."
  },
  {
    "prompt": "This is a technical discussion about users, end, methods in theoretical computer science.",
    "response": "It is a common misconception that once EC is available to end users, there will not be any need in other error reduction methods. We explain why this is indeed a misconception: even when fault-tolerant EC is available to end users, the fact that the number of high quality qubits is likely to remain limited in the foreseeable future, means that the logical error cannot be reduced to an arbitrary small value, and thus, fault tolerant quantum computations will also be limited in their volumes."
  },
  {
    "prompt": "Scientific context on circuit, volumes, making and their role in graph theory.",
    "response": "We then explain how EM can provide large circuit volume boosts over those available by EC alone, making it extremely useful also when EC is available. This will be achieved by combining EC with EM, and will dramatically push the limits of circuit volumes provided by EC alone, multiplying available circuit volumes by orders of magnitude. The expected CVBs provided by this combination, which we call logical error mitigation, are given in Fig. 3."
  },
  {
    "prompt": "Understanding the computational aspects of Tab, Steane, figure.",
    "response": "This figure is based on simulations of the prototypical Steane EC code combined with EM, and we provide in Tab. 2 order of magnitude estimates for state of the art EC codes. Thus, we argue that EM is not just a “transition technology” for the NISQ era that bridges the gap until fault tolerant EC is achieved and becomes available to end-users, but rather an important resource that is applicable whenever the number of high-quality qubits cannot be scaled without a significant cost."
  },
  {
    "prompt": "Scientific context on Specifically, MegaQuOp, gates and their role in graph theory.",
    "response": "Specifically, we believe that it is the combination of EM and EC that will first enable reaching the milestone of MegaQuOp (one million logical gates) [56, 57]. 5 2 ASYMPTOTIC VS. FINITE QUANTUM ADVANTAGE (QA) ■Conclusions regarding the central role of EM and the quantum advantage timeline We conclude our manuscript (Sec. 7) with our future prospects, expressed as a QA timeline (Tab. 3)."
  },
  {
    "prompt": "Explain a concept involving Table, quantum, advantage in computational complexity.",
    "response": "This Table draws the implications of the main points in the paper, and highlights the central role that EM is expected to play in the future of the field. 2 Asymptotic vs. finite quantum advantage (QA) The notion of quantum algorithmic advantage or in short quantum advantage (QA) aims to capture the ability to solve problems with a real quantum computer (QPU) using much fewer resources than without it."
  },
  {
    "prompt": "Scientific context on concept, notion, academia and their role in graph theory.",
    "response": "This notion is of major interest to both industry and academia; however, it is not well defined in the literature and there are substantial misconceptions about its meaning. In this section we point out an important distinction between different flavors of this notion: the mathematical concept of asymptotic QA; the concept of finite QA achieved in a finite-size real system - and the concept of scalable QA which is in fact a finite sequence of finite QAs whose behavior fits a certain scaling."
  },
  {
    "prompt": "This is a technical discussion about interchangeably, notions, current in theoretical computer science.",
    "response": "In the current public discourse about QA, all these notions (and perhaps others as well) are often used interchangeably, as if they are one and the same. However, for the discussion regarding the usefulness of EM, the question of which QA we are considering is crucial. We devote this section to clarifying these notions and the differences between them."
  },
  {
    "prompt": "This is a technical discussion about formal, variants, Asymptotic in theoretical computer science.",
    "response": "The definitions are not fully formal, and there are also many possible variants; however the important points that distinguish between the various flavors is what we care about here. 2.1 Asymptotic QA Let us first recall the well known mathematical notion of QA, which we call asymptotic QA."
  },
  {
    "prompt": "Scientific context on QPU, complexity, computer and their role in graph theory.",
    "response": "It considers the relation between the computational complexity (complexity, in short) of solving a certain problem using a classical computer versus a QPU (the QPU may also interact with a classical computer in a hybrid manner). The classical and quantum complexities are denoted SC(n) and SQ(n), respectively where n is the size of the input to the problem (or some parameter closely related to the input size that characterizes the scaling of the problem)."
  },
  {
    "prompt": "Explain a concept involving computation, Asymptotic, alternatively in computational complexity.",
    "response": "These complexities are often measured in the quantum computer science literature by the number of elementary gates used in the computation; alternatively, those can refer to the depth of the circuits used, which is an abstraction of the run time that the computation requires. Definition 2.1 (Asymptotic QA) Asymptotic QA on a given computational problem is achieved if limn→∞ SQ(n) SC(n) = 0. If in addition SC(n) ≤poly(SQ(n), n) we say that the QA is (at most) polynomial."
  },
  {
    "prompt": "Explain a concept involving poly, log, exponential in computational complexity.",
    "response": "If SQ(n) ≤poly(log(SC(n)), n) we say that the asymptotic QA is (at least) exponential."
  },
  {
    "prompt": "Scientific context on due, QPU, quantum and their role in graph theory.",
    "response": "It is widely agreed by the scientific community that asymptotic QA holds for a variety of problems [58], in particular, the factoring problem (due to Shor’s quantum algorithm [8]) and the problem of simulating the dynamics of quantum systems [59].6 We note that Definition 2.1 can be instantiated where the underlying quantum model is either an ideal QPU or a noisy QPU, due to the quantum fault tolerance theorem [11–14]."
  },
  {
    "prompt": "Explain a concept involving noise, rate, model in computational complexity.",
    "response": "Therefore, the asymptotic notion is robust to the noise in the model, as long as the noise rate is below the fault tolerance threshold, and the number of physical qubits with this noise rate is unlimited. 2.2 Finite QA A major question that concerns the quantum community is whether and when we will see experimental demonstrations of QAs for a variety of useful problems."
  },
  {
    "prompt": "Understanding the computational aspects of PSPACE, science, providing.",
    "response": "We thus need to formalize notions of QA that 6In computer science, providing a rigorous proof that quantum computation cannot be simulated in polynomial time by classical computers, would imply a resolution of the long standing seminal open problem of whether P = PSPACE. However it is widely conjectured that exponential asymptotic QA holds, and restricted proofs exist in the black box model for several oracle problems, e.g., Simon’s problem [7]. 6 2 ASYMPTOTIC VS."
  },
  {
    "prompt": "Understanding the computational aspects of FINITE, ADVANTAGE, QUANTUM.",
    "response": "FINITE QUANTUM ADVANTAGE (QA) are closer to experimental realizations. In particular, this requires quantifying the physical resources that are used by real physical quantum and classical computers. Indeed, the complexity measures mentioned above do not seem to directly correspond to a physically measurable quantity of interest in the real world."
  },
  {
    "prompt": "Explain a concept involving settings, place, classical in computational complexity.",
    "response": "Before we discuss the appropriate complexity measures that are more adequate to experimental settings, we first clarify the context in which such experiments might take place. We consider two competitors attempting to solve a computational problem: one with access only to a classical computer, and the other with access to a quantum and a classical computer combined in a hybrid manner."
  },
  {
    "prompt": "Scientific context on resources, problem, scale and their role in graph theory.",
    "response": "To compare the physical resources required by the two competitors for solving a certain computational problem, we need to put their resources on the same scale. This is a challenging task since the resources used by quantum and classical computational devices are very different in character, and strongly depend on the particulars of the hardware."
  },
  {
    "prompt": "This is a technical discussion about hardware, software, classical in theoretical computer science.",
    "response": "We will return to this question at the end of this section, but for now presume that we can quantify the cost for solving a given computational problem using the best available quantum and classical hardware and software as Q = Q(n), (n being the aforementioned scaling parameter of the problem) and its cost using the best available classical hardware and software as C = C(n)."
  },
  {
    "prompt": "Explain a concept involving Importantly, software, hardware in computational complexity.",
    "response": "Importantly, these definitions consider the best available hardware and software, rather than the best possible as in the asymptotic definition. This is because when we discuss experimental demonstrations of QA, we attempt to capture a snapshot of the added value of quantum computers over classical at a specific point in time, rather than provide a prediction for idealized future hardware."
  },
  {
    "prompt": "Understanding the computational aspects of question, theory, devices.",
    "response": "The latter question is in the regime of computational complexity theory, while here we are attempting to formally address the question of achieving QA on existing physical devices, at a given point in time. We first define QA in a non-asymptotic setting, i.e. finite QA which considers a fixed value of n. It is somewhat harder to make a precise definition in this context, but we can consider the rather informal one below."
  },
  {
    "prompt": "Understanding the computational aspects of Finite, inputs, size.",
    "response": "Definition 2.2 (Finite QA) For a given computational problem L, let In be the set of inputs to L of size n. Finite QA for L on inputs of size n is achieved if Q(n) ≪C(n), where Q(n) is the expected cost of the quantum algorithm over the inputs in In, and similarly C(n) is the expected cost of the classical algorithm over the inputs in In."
  },
  {
    "prompt": "Explain a concept involving large, arbitrariness, reason in computational complexity.",
    "response": "The reason for requiring that Q(n) ≪C(n) and not just Q(n) < C(n) is that the evaluation of the costs for the best currently available computers involves a large amount of arbitrariness, and may change from one day to the next. Indeed a robust definition will require a large concrete gap between the two that will be agreed upon universally."
  },
  {
    "prompt": "Read and understand this section about Definition, achieved, finite from a research paper.",
    "response": "We note that Definition 2.2 does not specify a procedure for the jury in a QA competition to verify that finite QA had been achieved; this may be a challenging task on its own. One may imagine a competition attempting to demonstrate finite QA for some problem, for some given fixed n. How would that work?"
  },
  {
    "prompt": "Explain a concept involving judge, algorithms, determine in computational complexity.",
    "response": "The competition may involve asking the competitors to run their respective algorithms on inputs chosen from some distribution over In for the fixed n, and the judge would then need to determine first whether the algorithms indeed output correct results, and secondly, the judge would also need to estimate the average costs and determine whether indeed Q(n) ≪C(n). Remark 1 (The size of In) We note that it is desirable to choose In to be a large set, rather than a single instance or just a few."
  },
  {
    "prompt": "Understanding the computational aspects of instance, practice, reason.",
    "response": "The reason is that in practice, if the competition is defined to be on a single instance, both players will fine-tune their apparatus and algorithm to work optimally on that specific instance. Demonstrating an advantage for that particular instance will then not be indicative regarding performance on any other instance."
  },
  {
    "prompt": "This is a technical discussion about question, QPU, achieve in theoretical computer science.",
    "response": "Presumably, the goal of the QA competition is to provide a proof of principle that the QPU can (or cannot) achieve the advantage not just for the particular instance on which it was tested, but also more generally, for some interesting class of instances.7 7In the theory of computer science this question is related to the question of whether or not the resources invested in preprocessing are limited or not; more formally, this can be viewed as the question of whether we are working in the uniform or non-uniform circuit model."
  },
  {
    "prompt": "Understanding the computational aspects of ASYMPTOTIC, FINITE, ADVANTAGE.",
    "response": "7 2 ASYMPTOTIC VS. FINITE QUANTUM ADVANTAGE (QA) While there are still debates about particular experiments, it is generally agreed in the community that finite QA had already been demonstrated experimentally. Experiments claiming such finite QA include RCS and Boson Sampling, as well as other examples [28, 34–39].8 As mentioned in the introduction, useful finite QA is yet to be demonstrated, and we discuss in Sec."
  },
  {
    "prompt": "Read and understand this section about goal, expectation, estimation from a research paper.",
    "response": "3 a leading candidate problem for this goal, namely, expectation value estimation. 2.3 Scalable QA The experimental demonstration of finite QA is a remarkable achievement of the quantum community, but it is still not close enough to the fantastic notion of exponential asymptotic QAs promised by the theoretical discoveries of quantum algorithms."
  },
  {
    "prompt": "Scientific context on finite, QAs, scaling and their role in graph theory.",
    "response": "One may consider a notion which is closer to asymptotic QA, in which one could demonstrate a sequence of finite QAs, over some range of n’s, such that those finite QAs fit a certain scaling (polynomial or exponential) as a function of n."
  },
  {
    "prompt": "Explain a concept involving prove, notion, scaling in computational complexity.",
    "response": "Given that this is a real world notion, it is impossible to “prove” that this scaling is maintained for all n, as one would hope for in order to validate the mathematical asymptotic notion of QA – however one may consider it a pretty convincing evidence if the scaling seems to behave like an exponent or a polynomial in some large regime of n’s. One can refer to this type of QA as scalable finite QA."
  },
  {
    "prompt": "Explain a concept involving scalable, Achieving, community in computational complexity.",
    "response": "Achieving experimental demonstration of scalable QA is a major goal of the quantum community; we do not consider this notion of QA further in this exposition (for recent progress on scalable QA see, e.g., [60]). 2.4 Asymptotic QA vs. Finite QA Despite the evident difference between finite and asymptotic QA, there is often a confusion in the literature regarding which of these notions of QA is appropriate to use in different contexts."
  },
  {
    "prompt": "Understanding the computational aspects of Asymptotic, notions, make.",
    "response": "We make several important remarks regarding the relations between these two notions. Remark 2 (Does Asymptotic QA imply finite QA, or vice versa?) We note that asymptotic QA and finite QA are in general incomparable, and it is possible that one holds but not the other, for a given problem. For example consider a problem for which we believe that asymptotic QA holds (e.g., factoring)."
  },
  {
    "prompt": "Scientific context on Def, Mathematically, reasonable and their role in graph theory.",
    "response": "Mathematically, for any reasonable choice of definition for Q(n) and C(n) (and their relation to SQ(n) and SC(n) of Def. 2.1), respectively, such asymptotic QA implies that for a large enough n we will have Q(n) ≪C(n). Based on this, and using quantum fault tolerance, in principle one should be able to experimentally demonstrate finite QA, for some large enough finite n."
  },
  {
    "prompt": "Scientific context on barriers, world, problem and their role in graph theory.",
    "response": "However it might be that this requires working with numbers of qubits which are too large to reach due to some technological barriers, and hence finite QA would not be achievable in our physical world, at least for this particular computational problem."
  },
  {
    "prompt": "This is a technical discussion about hand, finite, problem in theoretical computer science.",
    "response": "On the other hand, it is conceivable that finite QA is achievable in our real world for some problem, for some fixed n, but on the other hand this problem does not exhibit asymptotic QA at all, and the finite QA is simply an artifact of some finite size phenomenon that does not scale. Which of the two notions, finite QAs or asymptotic QAs, is more interesting or relevant for quantum computation? This is a difficult and possibly confusing question."
  },
  {
    "prompt": "This is a technical discussion about QPU, interest, practical in theoretical computer science.",
    "response": "Much of the interest in quantum computation is practical, and comes from the industry and applications side; it relies on the strong belief that QAs will be realizable in the real world, and in particular, that for certain sizes of inputs for problems of practical interest, the actual relevant measure of costs on a QPU, say the wall-clock run time, will be shorter than that required to achieve this task on a classical computer. Achieving finite QAs is thus a leading goal."
  },
  {
    "prompt": "Read and understand this section about finite, QAs, effect from a research paper.",
    "response": "However, since such finite QAs may be due to a “finite size effect” that may become irrelevant as the system sizes grow. It is theoretical asymptotic algorithmic results such as Shor’s algorithm, namely asymptotic QAs, that provide the scientific basis and motivation to seek for the experimental realization of finite QAs in the first place."
  },
  {
    "prompt": "Explain a concept involving Remark, Def, RCS in computational complexity.",
    "response": "As explained in Remark 2, asymptotic QA implies that finite QAs for this 8As an example, let us see how to view the RCS experiments of [34, 38] as instances of Def. 2.2. In this case In is the ensemble of n-qubits circuits from which the random circuit is drawn. The sampling task is to output a sequence of k n-bit strings xi such that the cross entropy benchmark fidelity, FXEB = 2n k Pk i=1 |⟨0n|C|xi⟩|2 −1, is above a certain threshold. 8 2 ASYMPTOTIC VS."
  },
  {
    "prompt": "Explain a concept involving ADVANTAGE, QUANTUM, FINITE in computational complexity.",
    "response": "FINITE QUANTUM ADVANTAGE (QA) problem are to be expected starting from some “crossover point”. Indeed, humanity’s current endeavour towards experimental demonstrations of QA implicitly assumes that the crossover point is within our reach, and that the required n is not too large for our finite world. Whereas theoretical study does not guarantee that this is the case, we may draw inspiration from the technological ability to scale classical computing systems quite rapidly."
  },
  {
    "prompt": "Read and understand this section about Sec., manuscript, hardware from a research paper.",
    "response": "As we argue in this manuscript, the important role of EM is to enable higher circuit volumes for a given hardware, thus reaching the crossover point earlier (see Sec. 5). This discussion reveals an inherent tension between the practical, finite world and the asymptotic approach of computational complexity; it is indeed rightfully confusing, and has been a constant source of misunderstandings in the literature and study of QAs."
  },
  {
    "prompt": "This is a technical discussion about misunderstandings, maintaining, distinction in theoretical computer science.",
    "response": "We believe that maintaining the distinction between asymptotic and finite QAs may significantly help avoid these misunderstandings. 2.5 Quantifying resources Before we move on, we remark on the question which we have allowed ourselves to evade so far in this section - how to quantify the realistic costs Q and C. We discuss several possibilities in broad strokes."
  },
  {
    "prompt": "Understanding the computational aspects of classical, gates, size.",
    "response": "One possibility is to adapt the notion of circuit size (the number of quantum/classical gates used in an execution) to a metric which is adequate for finite QA. Quantum and classical gates are not comparable. To make Q, C measure resources on the same scale, we can decide on a conversion factor between them. Such a conversion factor does not matter from an asymptotic point of view, so does not concern us when we discuss asymptotic QA. However there seem to be severe drawbacks to this approach."
  },
  {
    "prompt": "Scientific context on Firstly, crucial, involved and their role in graph theory.",
    "response": "Firstly, this metric seems to be much more cumbersome to use in the context of finite QA, where the constants are crucial - especially when hybrid quantum computation making use of classical computers is involved. More importantly, the circuit complexity metric of number of gates does not distinguish between time and space (i.e. number of bits/qubits being used), since it considers, essentially, only their product."
  },
  {
    "prompt": "Scientific context on importance, counting, time and their role in graph theory.",
    "response": "However space and time might be very different in their importance for resource counting; and their importance within quantum and classical computation is also very different, a difference which is hard to assign a convergence rate to. One can define instead Q, C using wall-clock time (which roughly corresponds to another metric commonly used in computational complexity: the depth of the circuit, namely the number of its layers)."
  },
  {
    "prompt": "Read and understand this section about energy, consumption, Ref from a research paper.",
    "response": "This choice is in line with contemporary QA demonstrations, where the goal is to beat the best available classical super-computer in terms of run time, regardless of its size, energy consumption and other considerations. Both above metrics ignore many resources, such as cost in dollars, energy consumption, economic costs, and more. For example, see Ref. [61] for an estimation of the classical cost for simulating RCS in $s, and Ref. [34] for energy consumption estimations."
  },
  {
    "prompt": "Understanding the computational aspects of Ref, resource, number.",
    "response": "The number of qubits is another important resource, of course; see, e.g., Ref. [62] for a detailed exposition of qubit-number estimates. We note that no matter what metric is chosen, it can be mapped to a volume crossover point above which finite QA is reached w.r.t. this metric."
  },
  {
    "prompt": "Scientific context on Remark, preprocessing, resources and their role in graph theory.",
    "response": "We further note that in all these metrics we did not account for the resources that go into preprocessing; it does not seem realistic nor practical to take into account the resources invested by the different competitors in preprocessing, as it seems extremely difficult to put a measure on such efforts.9 However, to a certain extent, the requirement that In is large (Remark 1) is a practical way to significantly reduce the importance of preprocessing performed by the competitors, and prevent it from making the competition meaningless."
  },
  {
    "prompt": "Read and understand this section about inputs, large, competitors from a research paper.",
    "response": "Once In is large enough, the competitors would have to put an enormous amount of effort to fine tune their systems so that the algorithm will work for all (or most of) these inputs. 9In the language of theoretical computer science, this means that we are working in the non-uniform model that allows infinite preprocessing."
  },
  {
    "prompt": "Understanding the computational aspects of EXPECTATION, ESTIMATION, EVE.",
    "response": "9 3 EXPECTATION VALUE ESTIMATION 3 Expectation value estimation To further discuss finite and asymptotic QA, we focus on a concrete and ubiquitous problem, which is of great importance in quantum physics and chemistry, as well as in a variety of quantum algorithms, namely, the expectation value estimation (EVE) problem. This is the problem of estimating the expected value of some observable in a pure quantum state that is generated in some prescribed manner."
  },
  {
    "prompt": "Scientific context on Practically, noise-free, HPC and their role in graph theory.",
    "response": "Achieving finite QA for EVE is fundamentally important, as it would imply that an existing QPU is better at approximating ideal (noise-free) quantum computation than classical simulation methods run on HPC hardware.10 Practically, we expect that a QPU capable of demonstrating such finite QA will be useful for two purposes."
  },
  {
    "prompt": "Read and understand this section about physics, Floquet, Hamiltonian from a research paper.",
    "response": "First, such a machine will allow for the otherwise-impossible simulation of important problems in quantum many-body physics, and in particular condensed matter physics, e.g., Floquet and Hamiltonian time evolution [63]. Secondly, this may have an important impact on researchers working on designing quantum algorithms and adapting them to specific applications."
  },
  {
    "prompt": "Explain a concept involving HPC, quantum, alternative in computational complexity.",
    "response": "For lack of an alternative, researchers currently working on quantum algorithm design make extensive use of classical HPC to simulate their quantum algorithm’s performance [64], and are thus very limited in the sizes of the instances they can study. The ability to solve the EVE problem for larger and more general input quantum circuits would allow those researchers much more effective exploration of the behavior of their algorithms."
  },
  {
    "prompt": "This is a technical discussion about EVE, problems, methods in theoretical computer science.",
    "response": "The important EVE problem lends itself naturally to EM methods, and thus it will be very convenient to focus on it in the remainder of this paper. We remark that one may consider other computational problems as a basis for finite QA, such as deterministic problems like Factoring [8], sampling problems like RCS [34], Boson-Sampling and IQP [65] and even more generally interactive protocols for computational tasks such as certifiable randomness [66]."
  },
  {
    "prompt": "This is a technical discussion about EVE, Problem, methods in theoretical computer science.",
    "response": "While EM methods are commonly referred to as appropriate mainly for EVE, there is an ongoing effort towards the application of EM to sampling problems (e.g., [67]). 3.1 Problem statement We define the problem of expectation value estimation in the realistic setting where we are interested in circuits and observables taken from some restricted prespecified family."
  },
  {
    "prompt": "Explain a concept involving acting, pairs, output in computational complexity.",
    "response": "Hence we consider a fixed family of sets I = {In}n where for each n, In is a set of pairs (C, O) with C a circuit acting on n qubits and O a tensor product observable acting on its output."
  },
  {
    "prompt": "This is a technical discussion about polynomial, size, EVE in theoretical computer science.",
    "response": "We require that there is an a-priori polynomial bound on the size of the circuits, as a function of n (formally, that there is a polynomial p, so that all circuits C in In have size at most p(n).) Definition 3.1 (Expectation Value Estimation (EVE)) The input to the expectation value estimation problem EVE[I] (we omit the parameter I when it is clear from the context) is a pair (C, O) ∈In for some n, comprising of a description of a quantum circuit C acting on n qubits and a description of a tensor product observable O over the output domain of C."
  },
  {
    "prompt": "Understanding the computational aspects of EVE, state, input.",
    "response": "The output EVE(C, O) is an estimation of the expected value of O over the state C|0n⟩= |C(0n)⟩derived by applying C to the all 0 input state, namely, ⟨O⟩C := ⟨C(0n)|O|C(0n)⟩. We say that an algorithm A ϵ-approximates EVE(C, O) if the input pair (C, O) the mean-square error in the approximation (averaged over the randomness in the algorithm) is bounded by ϵ: q EA \u0002 |A(C, O) −⟨O⟩C|2\u0003 ≤ϵ = ϵ(n) ."
  },
  {
    "prompt": "Understanding the computational aspects of norm, convention, normalized.",
    "response": "(1) With the convention that O is normalized in operator norm, ∥O∥op = 1, we consider a small ϵ ≪1, which may be a function of n. Note that the value ⟨O⟩C is a fixed number given the pair (C, O), and the expectation EA is taken over the randomness in the algorithm A, due to classical coin flips, quantum measurements, or both. The problem of ϵ-approximating EVE[I] is denoted EVEϵ[I]."
  },
  {
    "prompt": "This is a technical discussion about HPC, QPU, RCS in theoretical computer science.",
    "response": "10This should be contrasted with finite QA for RCS [34], which may be viewed as implying that an existing QPU is better at approximating noisy quantum computation than classical HPC. Indeed, in RCS it is sufficient to sample from a distribution whose distance from the output distribution of an ideal quantum circuit can be as large as 1 −F, where F is very small - it is the experimental circuit fidelity (e.g., F ∼10−3 −10−2 [34, 38])."
  },
  {
    "prompt": "Scientific context on EVE, error, bound and their role in graph theory.",
    "response": "The large error bound 1 −F ≲1 should be contrasted with the small additive error ϵ ≪1 in EVE. 10 3 EXPECTATION VALUE ESTIMATION Remark 3 (Variations of EVE) The above definition considers worst-case accuracy, requiring that A performs well on the entire set In. We may also consider average case versions, where ϵ corresponds to the mean-squared-error over In. We also note that in the above definition, we allow only the all-zero state as an input to the circuit."
  },
  {
    "prompt": "This is a technical discussion about circuits, input, implementations in theoretical computer science.",
    "response": "Indeed, this is how circuits are executed in the lab in current implementations. In a variety of settings it is natural to consider all 2n inputs to the circuit; this is equivalent to considering 2n circuits with the all zero input, where the circuits are derived from the original one by adding a layer of single qubit gates at the beginning, which change the all-zero input to some other input string. 3.2 Active circuit volume In the remainder of this paper, in particular Sec."
  },
  {
    "prompt": "Understanding the computational aspects of total, EVEϵ, methods.",
    "response": "4 and 5, we will study the potential for both asymptotic and finite QA in EVEϵ, by comparing the run time of EM protocols with that of classical simulation methods. Each instance of EVEϵ is defined by the circuit and observable (C, O), and one may naively expect the hardness of each instance to be controlled by the total number of qubits on which C is defined, and by the total volume of C, measured as e.g., the total number of two-qubit gates in the circuit."
  },
  {
    "prompt": "Scientific context on circuit, active, size and their role in graph theory.",
    "response": "These, however, are naive metrics for the circuit size. A more refined convenient notion is the circuit active volume, which takes into account also the observable O and the accuracy ϵ: Definition 3.2 (Active circuit volume) Given an instance (C, O) of EVEϵ, its active circuit volume V is given by the size of the minimal subset of 2-qubit gates in C, such that arbitrary modifications to the gates in the complement of the subset lead to a negligible change of ⟨O⟩C relative to ϵ."
  },
  {
    "prompt": "Explain a concept involving complement, min, Allowed in computational complexity.",
    "response": "That is, V = min n |S| : S ⊂C, ∀˜Sc | ⟨O⟩(C\\Sc)∪˜ Sc −⟨O⟩C | ≪ϵ o , (2) where Sc is the complement of a subset S ⊂C, and ˜Sc is any modification to the complement. Remark 4 (Allowed modifications) We intentionally leave vague the precise details of the allowed ‘arbitrary modification’ in Definition 3.2, as different conventions can be appropriate for different applications."
  },
  {
    "prompt": "Read and understand this section about subset, paper, k-qubit from a research paper.",
    "response": "For the purpose of this paper, it will suffice to consider the replacement of each k-qubit unitary gate in the complement by an arbitrary k-qubit unitary.11 By a small abuse of notation, we sometime refer to V as a subset of 2-qubit gates, and not just as the size of a subset. In the remainder of this paper, whenever we use the shorthand ‘active volume’, ‘circuit volume’, or simply ‘volume’, we always refer to active circuit volume, unless stated otherwise."
  },
  {
    "prompt": "Understanding the computational aspects of volume, relevant, intuitively.",
    "response": "The notion of active circuit volume captures, in some sense, the “relevant” volume for the problem; intuitively, one doesn’t care about gates outside of it. Classical algorithms for EVEϵ may benefit in run time from the removal of gates outside of V . Quantum-classical algorithms (such as EM), may alternatively ignore errors occurring outside of V ."
  },
  {
    "prompt": "Explain a concept involving active, subroutine, volume in computational complexity.",
    "response": "Note that algorithms that require the active volume to be a-priori identified in order to benefit from it must incorporate an active volume identification subroutine [68], and their performance will be affected by the performance of this subroutine. Finally, we also restrict attention to the “relevant” subset of qubits: Remark 5 (Active qubits) The active qubits are those qubits acted upon by any of the gates in the active volume."
  },
  {
    "prompt": "Understanding the computational aspects of circuit, qubits, active.",
    "response": "In the remainder of the paper we make the assumption that all n qubits in the circuit are indeed active. Otherwise some qubits are not acted upon by any 2-qubit gate (after the removal of all gates outside of V ) and in principle can be removed from the circuit. The number n thus represents the ‘number of active qubits’ in the circuit C, with respect to the observable O and accuracy ϵ, and in particular, satisfies n ≤V ."
  },
  {
    "prompt": "Scientific context on Pauli, k-qubit, unitary and their role in graph theory.",
    "response": "11The unitary replacement captures both the removal of gates outside of V , as well as the addition of errors outside of V , with commonly used error models (e.g., local Pauli channels), corresponding to independent probability distributions over k-qubit unitaries occurring after each k-qubit gate."
  },
  {
    "prompt": "Read and understand this section about EVIDENCE, ASYMPTOTIC, errors from a research paper.",
    "response": "11 4 EVIDENCE AGAINST ASYMPTOTIC QA WITH EM 4 Evidence against asymptotic QA with EM It is long known that in order to handle the effect of the most disruptive type of errors in quantum computers, namely non-unitary or dissipative errors, one must constantly remove the entropy accumulated in the quantum system (e.g., [11, 69, 70])."
  },
  {
    "prompt": "Read and understand this section about qubits, Quantum, correction from a research paper.",
    "response": "Quantum error correction (EC) does this by employing a large overhead in qubit numbers due to (i) the encoding of logical qubits onto physical qubits, and (ii) the addition of measurement qubits, which are periodically reset or replaced between EC cycles."
  },
  {
    "prompt": "Read and understand this section about computation, circuit, mitigation from a research paper.",
    "response": "In contrast to EC, error mitigation (EM) is a collective name for a family of methods to reduce the effect of errors in quantum computation without introducing fresh qubits during the computation, and importantly, while avoiding the large qubit overheads of EC. To do this, EM replaces the execution of a given target circuit by multiple noisy circuit executions; the results of which are post-processed to provide an estimate for the error-free output of the target circuit."
  },
  {
    "prompt": "This is a technical discussion about benefit, price, section in theoretical computer science.",
    "response": "The fact that EM has little or no qubit overhead is a great benefit of EM over EC; however, this benefit comes at a painful price, which is the subject of this section. 4.1 Evidence for exponential sampling overhead of EM From its inception, it was understood that EM probably cannot lead to exponential QA."
  },
  {
    "prompt": "This is a technical discussion about protocols, Definition, infidelity in theoretical computer science.",
    "response": "The reason is that, starting with the development of the first EM protocols [42–44], and continuing in subsequent extensions and protocols [41, 45–47], it was observed that, in order to produce an accurate output, the proposed protocols generically require a number of shots M which is exponential in the total infidelity affecting the measured observable: M ≥1 ϵ2 eλγV , (3) where ϵ is the required statistical error, γ is the 2-qubit (average) gate infidelity, V is the active circuit volume (Definition 3.2)."
  },
  {
    "prompt": "Understanding the computational aspects of volume, product, total.",
    "response": "The product γV is the total infidelity within the active volume. The parameter λ appearing in the exponent may be referred to as the ‘blow-up rate’ of the specific EM method being used, and in the above described cases, it was found to be a constant (independent of n). For example, in EM based on QP distributions [42], it has been argued that λ ≥4, with the lower bound achieved in e.g., commonly-used constructions for Pauli channels [71–73]. As another example, in exponential ZNE (i.e."
  },
  {
    "prompt": "Explain a concept involving ZNE, lower, bound in computational complexity.",
    "response": "ZNE with an exponential extrapolation function), it has been argued that λ ≥2, with the lower bound achieved by the two-point scheme described in [47].12 The above case by case study seems to suggest that a lower bound in the form of Eq. (3) may hold more generally; the following toy example may give some intuition for why that might be true."
  },
  {
    "prompt": "Explain a concept involving circuit, depolarizing, qtr in computational complexity.",
    "response": "Consider the global n-qubit depolarizing error model, where a channel Dq : ρ 7→(1−q)ρ+qtr(ρ)I/2n, with depolarizing probability q ∈[0, 1], acts after each of the V ideal unitary gates in the circuit. Since Dq commutes with all unitary gates, all V copies of Dq can be pulled to the end of the circuit, where they contract a trace-less observable O as O 7→(1 −q)V O. To mitigate this effect, we may consider the following toy EM protocol."
  },
  {
    "prompt": "Understanding the computational aspects of QPU, noisy, protocol.",
    "response": "The protocol estimates the noisy expectation value ⟨O⟩noisy by running M shots on a QPU leading to a statistical error ≤1/ √ M. Assuming perfect knowledge of q, the protocol then needs to \"stretch\" the result by a factor (1 −q)−V , producing an unbiased estimate of ⟨O⟩ideal. However, unfortunately, the statistical error is also stretched, to (1 −q)−V / √ M."
  },
  {
    "prompt": "This is a technical discussion about protocols, subtleties, involved in theoretical computer science.",
    "response": "Thus, M ≥ϵ−2(1 −q)−2V shots must be used to 12There are a few subtleties involved in accurately determining the shot overhead of EM protocols. First, there are generically O(γ2V ) corrections in the exponent. Additionally, there may be multiplicative factors which are polynomial in γV . Finally, we work with the active circuit volume as it is an intrinsic property of instances of the problem EVEϵ."
  },
  {
    "prompt": "Scientific context on Veff, circuit, ZNE and their role in graph theory.",
    "response": "However, the shot overhead of some EM protocols (e.g., exponential ZNE) depends on the effective circuit volume, Veff = γ−1 log(⟨O⟩ideal / ⟨O⟩noisy), which corresponds not to a subset of gates in the circuit, but to the decay of the expectation value due to noise. The latter is generically proportional to the active volume (with an n-independent proportionality constant), but the two can scale differently with n in some cases (see e.g., Ref. [74])."
  },
  {
    "prompt": "Scientific context on volume, distributions, mitigation and their role in graph theory.",
    "response": "As another example, for EM based on QP distributions, mitigation is usually performed locally, and the relevant volume corresponds to the gates (or gate errors) one chooses to mitigate to insure a negligible bias, see [68, 75, 76]. This is closely related, but not identical, to the active volume. Since Eq. (3) is expressed in terms of the active volume, in methods in which other volumes are relevant, the difference is swallowed into λ."
  },
  {
    "prompt": "Understanding the computational aspects of EVIDENCE, ASYMPTOTIC, ensure.",
    "response": "12 4 EVIDENCE AGAINST ASYMPTOTIC QA WITH EM ensure a statistical estimation error at most ϵ. Using the infidelity γ = (1 −4−n)q of the depolarizing channel,13 this can be written as M ≥ϵ−2e2γV/(1−4−n)+O(γ2V ), which demonstrates Eq. (3), with blowup rate λ = 2/(1 −4−n) ∈(2, 8/3]."
  },
  {
    "prompt": "Scientific context on order, output, over-simplified and their role in graph theory.",
    "response": "Though this toy model is over-simplified, it seems to capture an essential point - the need to effectively stretch the output by an exponentially large factor, in order to combat the effect of exponential decay due to noise. This stretch increases the variance of the output exponentially, implying a similar increase in the number of shots, which is needed in order to meet the required statistical error with such a large variance."
  },
  {
    "prompt": "Understanding the computational aspects of shots, toy, protocol.",
    "response": "Another toy EM protocol that one may consider is given by an idealized post-selection that manages to reject all erred shots [41]. In this case only a fraction (1 −q)V of shots is maintained, leading to a sampling overhead (1 −q)−V , which again reproduces Eq. (3), now with λ = 1/(1 −4−n). The above arguments have led researchers to seek for a lower bound along the lines of Eq. (3) that may hold for any EM method. We survey those below."
  },
  {
    "prompt": "Understanding the computational aspects of bounds, lower, general.",
    "response": "4.2 Towards general exponential lower bounds on the sampling overhead There are a number of results providing lower bounds on the required shot number that hold for general, rather than specific, EM methods [1–4]. Before we survey these results, we note that known EM protocols [41], as well as the lower bounds of Ref. [1–4], show that one may avoid the exponential scaling of Eq."
  },
  {
    "prompt": "Scientific context on bias, bound, guarantee and their role in graph theory.",
    "response": "(3) only at the expense of a significant bias, that is, a large estimation error that persists even with infinitely many shots.14 Such biases are notoriously difficult to bound, often leading to EM protocols with no accuracy guarantee; a situation which is especially problematic in the context of computational problems that are not efficiently verifiable."
  },
  {
    "prompt": "Understanding the computational aspects of Ref, bias, protocols.",
    "response": "In the remainder of this paper we restrict attention to EM protocols that produce a negligible bias b ≪ϵ for the circuits in question.15 This includes (i) unbiased EM protocols, where the bias is rigorously guaranteed to be negligible for any circuit and observable; and (ii) heuristic EM protocols, where no such guarantee exists, but nevertheless the bias is still negligible in particular cases.16 We now survey the results of Ref. [1–4], for EM protocols with negligible bias."
  },
  {
    "prompt": "Read and understand this section about Ref, results, derived from a research paper.",
    "response": "These results are derived under a general definition of EM protocols provided in Ref. [1], the essence of which is the exclusion of adaptive quantum operations. In a nutshell, a lower bound along the lines of Eq. (3) (with λ a constant independent of n) is not known to hold for all circuits and observables; in fact, recent work suggests the existence of fine-tuned counter examples. Nevertheless, Eq. (3) does seem to hold very generally."
  },
  {
    "prompt": "Explain a concept involving circuit, knowledge, observable in computational complexity.",
    "response": "To the best of our knowledge, the strongest known lower bounds on the number of shots M that hold for any circuit, observable and EM protocol, are exponential in the circuit depth D as opposed to the volume V [2–4]."
  },
  {
    "prompt": "This is a technical discussion about assuming, iii, eλγD in theoretical computer science.",
    "response": "In particular, assuming (i) a local-depolarizing error model, (ii) a trace-less observable O, and (iii) a large-enough ideal expectation value | ⟨O⟩ideal | > 2ϵ; the number of shots M required by any EM protocol (with negligible bias) to reach a statistical error at most ϵ in estimating the expectation value ⟨O⟩C for any observable O and circuit C, satisfies M ≥1 ϵ2 eλγD."
  },
  {
    "prompt": "Explain a concept involving state, assuming, model in computational complexity.",
    "response": "(4) This result is based on the known decay of the output state towards the maximally mixed state, which, assuming a local depolarizing model, decreases exponentially with γD [78–81]. The works [3, 4] further demonstrated the existence of circuits for which the number of shots M for any EM protocol must be exponential in the total circuit volume nD/2 ≥V and not only in D. By 13We work with the ‘entanglement infidelity’, as opposed to the ‘average gate infidelity’ [77]."
  },
  {
    "prompt": "This is a technical discussion about ZNE, polynomial, small in theoretical computer science.",
    "response": "14This is the behavior of e.g., polynomial ZNE (with a low-degree polynomial) which requires a small sampling overhead, but can lead to significant biases unless γV ≪1 [42]. 15There’s no problem allowing for a small, but non-negligible, bias, say b < ϵ/2. This effectively replaces ϵ by ϵ/2 in Eq. (3), as well as in Eq. (14) below. In fact, trading bias for error bar in a controlled manner is an important aspect of designing practical EM protocols."
  },
  {
    "prompt": "Scientific context on protocols, type, channel and their role in graph theory.",
    "response": "16An example for the first type, namely unbiased protocols, is EM protocols based on QP distributions that rely on a good-enough characterization of error channel. An example for heuristic EM protocols in which the bias is negligible for specific cases, is exponential ZNE, where the bias vanishes for Clifford circuits with Pauli errors, but not generally."
  },
  {
    "prompt": "Read and understand this section about Reference, volume, EVIDENCE from a research paper.",
    "response": "13 4 EVIDENCE AGAINST ASYMPTOTIC QA WITH EM limiting their circuit construction to the connectivity of a d-dimensional lattice, Reference [4] further showed compelling evidence that, for the above circuits, it is the active volume (or ‘light cone’) that controls the shot overhead of EM, as opposed to the total circuit volume. We describe these results, which are closely related to the above global depolarizing toy model, in more detail in Appendix A.1."
  },
  {
    "prompt": "This is a technical discussion about results, compelling, evidence in theoretical computer science.",
    "response": "Despite the above results, compelling evidence that Eq. (3) is violated for a different family of circuits, for restricted input states and observables, was recently presented in Ref. [74]. In the examples discussed in [74] the behavior of the shot overhead is exponential in D and not in V ."
  },
  {
    "prompt": "Read and understand this section about Trotter, circuits, depth from a research paper.",
    "response": "The authors consider certain 2d Trotter circuits, where n ∼D2 and V ∼nD ∼D3, and observe that for very particular initial states and observables, the decay of the observable due to errors can scale with depth, as e−Ω(γD), rather than with volume. Accordingly, EM methods with a sampling overhead of only eO(γD) may in principle be used in these particular cases. We conclude that while Eq."
  },
  {
    "prompt": "Scientific context on hold, broadly, exponent and their role in graph theory.",
    "response": "(3) seems to hold very broadly, it fails to hold in all cases with V in the exponent; it does hold generally with D in place of V . This is a strong lower bound only for circuits whose depth D grows super-logarithmically with n. We now proceed to deduce the negative implications of such exponential lower bounds on the prospects for asymptotic QA with EM."
  },
  {
    "prompt": "Understanding the computational aspects of asymptotic, exponential, ruled.",
    "response": "4.3 EM with exponential overheads cannot enable exponential asymptotic QA It is easy to see that in cases where the EM shot overhead scales according to Eq. (3), exponential asymptotic QA is ruled out. Furthermore, for any negligibly-biased EM protocol, applied to any circuit and observable with D > n, exponential asymptotic QA is ruled out due to the general lower bound in Eq. (4)."
  },
  {
    "prompt": "Scientific context on protocol, time, Corollary and their role in graph theory.",
    "response": "Let Tc be the time required to classically solve the expectation value estimation problem EVEϵ using the best available algorithm, and TEM the time to solve it on a quantum processor using an EM protocol; we claim Corollary 1 (Exponentially scaling EM cannot provide exponential asymptotic QA) Consider an EM protocol, and a family of circuits and observables for which one of the following two conditions hold: (i) The exponential lower bound in V , Eq."
  },
  {
    "prompt": "This is a technical discussion about bound, satisfied, n-independent in theoretical computer science.",
    "response": "(3), is satisfied with λγ ≥γ for some n-independent bound γ ∈(0, 1], or (ii) the depths of the circuits satisfy D ≥n, and we also have that the conditions for the exponential lower bound in D, Eq. (4), hold; namely, the circuit is affected by a local depolarizing model, the observable O is trace-less, and its ideal expectation value is non-negligible, | ⟨O⟩ideal | > 2ϵ. In either case we have that the classical run time is polynomially bounded by the EM run time: Tc = O(T 1/γ EM)."
  },
  {
    "prompt": "This is a technical discussion about polynomial, TEM, asymptotic in theoretical computer science.",
    "response": "(5) Since the classical run time Tc is at most polynomial (with degree 1/γ) in the EM run time TEM, any asymptotic QA with EM is at most polynomial. In particular, there is no exponential asymptotic QA of EM over classical computation. Proof: We prove the first case, the second one is similar. First, we translate the shot number in Eq. (3) to a lower bound on the run time of EM for a quantum circuit of depth D, TEM = Ω(DM) = Ω \u0012 1 ϵ2 DeλγV \u0013 ."
  },
  {
    "prompt": "Understanding the computational aspects of Note, run, bound.",
    "response": "(6) Note that we use here the total circuit depth over all quantum circuits in the EM protocol to bound the run time,17 without including the classical run time which will of course only increase the lower bound. 17We comment here on the relation of this simple bound to realistic experimental settings. In experiments, one can bound TEM ≥tsM, where ts is the shot time, which can itself be bounded as ts ≥tlD, where tl is the time per circuit layer."
  },
  {
    "prompt": "Understanding the computational aspects of time, QPU, shot.",
    "response": "The lower bound for the shot time assumes that the relevant QPU can parallelize all gates appearing in each circuit layer, and accounts only for the time to implement these layers. Limited parallelization, or additional delays due to e.g., state preparation and measurement, will increase the shot time. Similarly, the bound TEM ≥tsM accounts only for the physical time to implement the quantum operations in each shot."
  },
  {
    "prompt": "Understanding the computational aspects of practice, current, delays.",
    "response": "In practice, current QPUs suffer significant additional delays (‘circuit time’), due to the loading of circuit instructions onto classical control electronics. 14 4 EVIDENCE AGAINST ASYMPTOTIC QA WITH EM We can now compare this run time to that of the best available classical simulation."
  },
  {
    "prompt": "Understanding the computational aspects of Sec, quantum, simulation.",
    "response": "The run time of that would be at most that of the simplest classical simulation of quantum circuits, namely, state-vector simulation, which would require Tc = O(V 2n), (7) floating-point operations, for an ideal n-qubit quantum circuit including V 2-qubit gates; see details in Sec. 5.2. Further note that V is bounded by the maximal number of 2-qubit gates in the circuit, V ≤nD/2, where D is the circuit depth and n/2 is the maximal number of 2-qubit gates per layer."
  },
  {
    "prompt": "Understanding the computational aspects of Deγn, Remark, Equations.",
    "response": "Equations (6) and (7) then together imply the desired relation: Using ϵ < 1, V ≥n (see Remark 5) and en > n2n (for, say, n > 10), the corollary follows from ϵ−2DeλγV ≥DeγV ≥Deγn > D(n2n)γ > (Dn2n)γ > (V 2n)γ. □ Remark 6 (Role of the lower bound γ) The requirement that λγ ≥γ may be refined to separate n- independent lower bounds on γ and λ. The first lower bound is a very realistic assumption, since there is no experimental indication for error rates decreasing with the number of qubits."
  },
  {
    "prompt": "This is a technical discussion about fact, error, qubits in theoretical computer science.",
    "response": "In fact, error rates tend to increase with the number of qubits [82], since maintaining high quality at scale is a more challenging engineering task. The second lower bound, on λ, is meant to exclude the known examples in which the sampling overhead is exponential in D instead of V (see Subsection 4.2). As we expect Eq. (3) to hold generically (with λ bounded from below by a constant independent of n), we expect that, generically, EM cannot provide exponential asymptotic QA."
  },
  {
    "prompt": "Read and understand this section about Remark, polynomial, Corollary from a research paper.",
    "response": "Remark 7 (What about polynomial QA?) Corollary 1 does not exclude polynomial QA using EM. However, we note that classical simulation becomes more efficient compared to EM (namely, the upper bound of Eq. (5) becomes less tight), the larger V is relative to n, such that polynomial asymptotic QA is only possible when V = O(n), namely for constant depth circuits."
  },
  {
    "prompt": "Scientific context on IQP, Polytime, Instantaneous and their role in graph theory.",
    "response": "In the noiseless setting, constant depth circuits are conjectured to provide exponential asymptotic QA for sampling problems, e.g., in the model of Instantaneous Quantum Polytime (IQP) [83–87]. We pose here as an open problem whether EM for expectation values of heavy weight tensor product observables measured at the end of constant depth quantum circuits may lead to polynomial asymptotic QAs. The results in this section still do not rule out exponential asymptotic QA using EM in special cases."
  },
  {
    "prompt": "Explain a concept involving no-go, setting, Corollary in computational complexity.",
    "response": "We next describe a recent result which proves a no-go theorem in a more general setting. 4.4 No-go for exponential asymptotic QA The assumptions stated in Corollary 1 (namely, that Eq. (3) holds or that D ≥n) were needed because of the current limitations of rigorous lower bounds on the shot overhead of EM, as described in Sec. 4.2."
  },
  {
    "prompt": "Scientific context on assumptions, protocol, observable and their role in graph theory.",
    "response": "In this section we describe recent work that excludes exponential asymptotic QA without relying on such assumptions, and which holds for every EM protocol, circuit and observable, but only on average over input states [5]. The main result of Ref. [5] is a classical algorithm for approximating expectation values in any noisy quantum circuit (up to error ϵ′)."
  },
  {
    "prompt": "Read and understand this section about degree, algorithm, polynomial from a research paper.",
    "response": "The algorithm’s run time is O((1/ϵ′) ˜O(1/γ2)), which is polynomial in the required accuracy ϵ′, with degree ˜O(1/γ2); albeit a large degree, this is a polynomial algorithm. Notably, this performance guarantee holds for any fixed circuit and observable, improving over previous work [88] which provided a similar result but only on average over random circuits. Importantly, the accuracy guarantee of Ref."
  },
  {
    "prompt": "Scientific context on input, states, hold and their role in graph theory.",
    "response": "[5] does not hold for any input to the quantum circuit, but only on average over computational basis input states. Ref. [5] use their algorithm to provide limitations on any EM protocol for the problem of EVEϵ, based on the following idea. They use their classical algorithm for simulating noisy circuits, in order to simulate each of the M shots of the EM protocol."
  },
  {
    "prompt": "Understanding the computational aspects of total, run, classical.",
    "response": "In order to arrive at total error ϵ in the final estimation of the expectation value, the authors run their classical simulation algorithm for each of the M shots with required accuracy ϵ′ = O(ϵ/M). This leads to a total classical run time Tc = O((M/ϵ) ˜O(1/γ2)) = O((TEM/ϵ) ˜O(1/γ2)). (8) 15 5 USEFUL FINITE QAS EXPECTED IN THE NEAR FUTURE USING EM Notably, a lower bound which is significantly weaker than Eq. (5), but still a polynomial."
  },
  {
    "prompt": "Scientific context on Reference, EVEϵ, polynomial and their role in graph theory.",
    "response": "Thus, whenever the number of shots M in the EM protocol for EVEϵ is polynomial, Reference [5] constructs a simulation of EVEϵ for the ideal quantum circuit which takes polynomial time. We stress that the guarantee for the error to be smaller than ϵ does not hold for any input state, but only on average over all inputs."
  },
  {
    "prompt": "Scientific context on Reference, Sec, asymptotic and their role in graph theory.",
    "response": "Thus, Reference [5] shows that EM alone cannot lead to exponential asymptotic QA (or in fact to any non-polynomial asymptotic QA), for the variant of the problem EVEϵ in which ϵ is the root-mean-squared error over all 2n input states (see Sec. 3).18 We note that this no-go result still does not completely rule out exponential QA using EM, due to this average over all inputs to the circuits involved."
  },
  {
    "prompt": "Read and understand this section about Ref, small, states from a research paper.",
    "response": "This leaves a small crack in the door for exponential asymptotic QA to be achievable using EM when restricted to a special small set of input states, that will not significantly affect the average over inputs; in line with the recent examples of Ref. [74], suggesting a violation of Eq. (3) with fine-tuned input states. We do not concern ourselves further with this highly specific case; the main message is that, generically, EM alone cannot enable exponential asymptotic QA."
  },
  {
    "prompt": "Understanding the computational aspects of asymptotic, exponential, QAs.",
    "response": "To summarize, we see that EM cannot on its own enable generic exponential asymptotic QAs. In light of this, and the immense importance of exponential asymptotic QAs in motivating the whole field of quantum computation, one must ask – why bother with EM at all? 5 Useful finite QAs expected in the near future using EM We now explain how, despite the lessons from Sec. 4, EM does in fact provide a viable path to QA. The point is exactly the distinction between exponential asymptotic QA, which Sec."
  },
  {
    "prompt": "Scientific context on finite, achieved, earlier and their role in graph theory.",
    "response": "4 argues cannot be achieved with EM alone, and finite QA. We argue here that EM is an extremely promising path to finite QAs, and is in fact expected to enable such advantages in the very near future – much earlier than EC. Furthermore, based on the same reasoning which we will outline in this section, we will see in Sec. 6 that though EM cannot provide exponential QA on its own, it can in fact crucially help in bringing it much earlier than would be possible without EM, namely with EC alone."
  },
  {
    "prompt": "Explain a concept involving exponent, lower, bound in computational complexity.",
    "response": "The starting point is a very important observation regarding the exponent in the lower bound on the sampling overhead of EM, Eq. (3). The power of the exponent in this lower bound contains a very small constant - the infidelity per gate γ. Thus, though the sampling overhead indeed grows exponentially, it is a very mild exponential growth. Current values of γ are in the range 10−3 −10−2, and may reach 10−4 within the next few years [18, 89]."
  },
  {
    "prompt": "Understanding the computational aspects of complexity, Corollary, quantum.",
    "response": "Closely related, the degree 1/γ of the polynomial connecting the complexity of classically simulating quantum circuits and the complexity of EM protocols (Corollary 1), is very large, currently in the range 102 −103, and possibly 104 in the near term. Thus, while the classical run time is indeed only polynomial in the quantum run time as is derived in Corollary 1, this polynomial classical computation could still be tremendously expensive compared to the quantum one."
  },
  {
    "prompt": "This is a technical discussion about Sec, circuits, polynomial in theoretical computer science.",
    "response": "The upshot is that though EM is not expected to lead to exponential QA (and in fact not even polynomial, except possibly for very special cases of constant depth circuits, as discussed at the end of Sec. 4) it does offer a very challenging fight, for shallow circuits, to the classical competition, due to this large power of 1/γ. This is what leaves a slack for EM to provide very significant finite QAs."
  },
  {
    "prompt": "This is a technical discussion about Sec, computations, section in theoretical computer science.",
    "response": "In this section we clarify the resulting potential benefits of using EM in real world quantum computations, in two ways. First, in Sec. 5.1 we introduce an important notion, which we term “circuit volume boost” (CVB), that can be used to quantify the benefit of using EM over not using it. In Sec. 5.2, we provide estimation of the finite QAs that can be achieved using EM, for different fidelity values of the hardware."
  },
  {
    "prompt": "This is a technical discussion about Ref, section, bound in theoretical computer science.",
    "response": "18One may notice that we can now turn back to the first question discussed in this section, and ask whether the results of Ref. [5] can be used to deduce the lower bound of Eq. (6) for generic EM protocols. As mentioned above, there are specific counter examples in which this bound does not hold [74]; however, one might expect the lower bound to still hold generically."
  },
  {
    "prompt": "Explain a concept involving Appendix, lower, bound in computational complexity.",
    "response": "In Appendix A.2 we show that such a line of reasoning can indeed be applied, to yield a lower bound which in some cases is stronger than the exponential in D lower bound, however it is weaker than Eq. (6). 16 5 USEFUL FINITE QAS EXPECTED IN THE NEAR FUTURE USING EM 5.1 Circuit Volume Boosts We introduce a useful definition for comparing the performance of quantum computation with and without EM, which we call circuit volume boost (or CVB in short)."
  },
  {
    "prompt": "Understanding the computational aspects of CVB, quantum, circuits.",
    "response": "The CVB captures the factor by which one can increase the volume of quantum circuits being run using EM, compared to the circuit volumes available without EM. In fact, the CVB can be used for comparing the performance of any two methods for executing quantum circuits. Recall that the number of shots required to ensure accuracy 1 −ϵ in the ideal (noiseless) computation is ϵ−2, since in this case the error is entirely statistical. In EM, we allow a larger shot budget."
  },
  {
    "prompt": "Scientific context on allowed, shot, accuracy and their role in graph theory.",
    "response": "We find it convenient to write the allowed shot budget as a multiple of the number of shots required by the ideal computation for the same accuracy, M = R/ϵ2, (9) with R > 1. We thus introduce the allowed shot overhead R := Mϵ2. In the discussion below, we will study the behavior of EM for fixed values of the allowed shot overhead R. With this convention, we now define the CVB."
  },
  {
    "prompt": "This is a technical discussion about CVB, Circuit, denote in theoretical computer science.",
    "response": "Definition 5.1 Circuit volume boost (CVB) Let VA(ϵ, R) denote the maximal circuit volume that can be achieved with circuit execution method A, given an allowed shot overhead R ≥1, and while meeting a required output accuracy 1 −ϵ. Let VB(ϵ, R) denote the maximal volume possible with circuit execution method B, under the same constraints."
  },
  {
    "prompt": "Understanding the computational aspects of method, circuit, volume.",
    "response": "The circuit volume boost of method A over method B, given an allowed shot overhead R and required accuracy 1 −ϵ, is then given by CV B(ϵ, R) = VA(ϵ, R) VB(ϵ, R). (10) This gives a very clear quantification of the benefit of one method over another, in terms of the ratio between the active volumes that the two methods can achieve. We would now like to estimate the circuit volume boost provided by executing circuits using EM, compared to ‘bare’ circuit execution, without EM."
  },
  {
    "prompt": "Understanding the computational aspects of VEM, Vbare, estimate.",
    "response": "We first estimate the circuit volume Vbare which can be run without EM, then estimate VEM, and then compare. 5.1.1 Estimating Vbare We do this under the same global-depolarizing toy-model used in Sec. 4, where ⟨O⟩noisy = e−γV ⟨O⟩ideal,19 and ∥O∥op = 1. The bias in the bare computation is given by b = | ⟨O⟩ideal −⟨O⟩noisy | ≤1 −e−γV , and the statistical error is ≤1/ √ M."
  },
  {
    "prompt": "This is a technical discussion about error, Vbare, γVbare in theoretical computer science.",
    "response": "Using these bounds, we can ensure that the total error, given by the sum of the statistical error and the bias,20 is at most ϵ, if ϵ = (1 −e−γVbare) + 1/ √ M. (11) Solving for Vbare and using Eq. (9) then gives Vbare = −1 γ log \u0012 1 −ϵ + 1 √ M \u0013 (12) = −1 γ log \u0012 1 −ϵ + ϵ √ R \u0013 ∼ϵ γ \u0012 1 − 1 √ R \u0013 . 19We suppress here the small differences between (1 −q)V = e−γV/(1−4−n)+O(γ2V ) and e−γV . 20One may similarly work with the mean squared error instead of the sum."
  },
  {
    "prompt": "Read and understand this section about FINITE, QAS, EXPECTED from a research paper.",
    "response": "17 5 USEFUL FINITE QAS EXPECTED IN THE NEAR FUTURE USING EM Note that the bare execution requires a non-trivial allowed shot overhead R > 1 to support a volume Vbare > 0. The reason is that the systematic error takes up part of the total allowed error budget ϵ, leaving ‘less room’ for the statistical error. On the other hand, due to the bias, even if the number of shots M is taken to infinity, there is a hard upper bound on the available volume: Vbare ∼ϵ/γ."
  },
  {
    "prompt": "Explain a concept involving regime, accuracy, small in computational complexity.",
    "response": "(13) This upper bound is particularly strict in the high accuracy (small ϵ) regime. Equation (13) is a good rule of thumb for the maximal circuit volume possible without EM. 5.1.2 Estimating VEM Let us now compute the maximal active circuit volume VEM possible with EM. We assume here that there’s no systematic error - this is the case, e.g., when performing EM based on QP distributions and assuming an exact characterization. However, the statistical error is the bottle neck - following Eq."
  },
  {
    "prompt": "Scientific context on VEM, Vbare, log and their role in graph theory.",
    "response": "(3), the statistical error grows exponentially with the circuit volume, but with a small pre-factor γ, the infidelity per gate: ϵ = 0 + q eλγVEM /M. (14) Solving for VEM we get, VEM = 1 γ log(ϵ2M) λ = 1 γ log R λ . (15) As opposed to Vbare, this is independent of ϵ but is only a function of R. 5.1.3 Estimating the circuit volume boost Given the above two estimates, Eq. (12) and Eq. (15), the CVB is given by CV B(ϵ, R) = VEM Vbare ∼1 ϵ log R λ(1 −1/ √ R) ."
  },
  {
    "prompt": "This is a technical discussion about Note, CVB, range in theoretical computer science.",
    "response": "(16) Note that the CVB is independent of the infidelity γ! The function log(R)/(1 −1/ √ R) depends only weakly on R, for example for allowed shot overhead R in the range [2, 109] it takes values in the range ≈[2, 20].21 As described in Sec. 4, the blowup rate λ is a constant of order 1. Thus, an order of magnitude estimate for the CVB due to EM is CV B(ϵ, R) ∼1–10 ϵ . (17) The important feature is that the CV B scales like ∼1/ϵ (see Fig."
  },
  {
    "prompt": "This is a technical discussion about accuracy, meaning, smaller in theoretical computer science.",
    "response": "1) meaning that the higher the required accuracy is (smaller ϵ), the larger the CV B. In particular, chemistry applications usually require reaching ‘chemical accuracy’, which translates to ϵ ∼10−3 [90]. As an example, consider e.g., λ = 2 and a mild overhead R = 10. This gives CV B > 10 for 85% accuracy; a CV B > 100 is obtained for 99% accuracy; and a CV B > 1000 is obtained for 99.9% accuracy."
  },
  {
    "prompt": "Understanding the computational aspects of CVB, overheads, platforms.",
    "response": "21Shot overheads as high as 109 are reasonable in superconducting-qubit platforms, where gate times are of the order of tens of nanoseconds. 18 5 USEFUL FINITE QAS EXPECTED IN THE NEAR FUTURE USING EM 75 80 85 90 95 100 101 102 103 Accuracy 1-ϵ [%] CVB=VEM/Vbare R=10 R=103 R=109 Figure 1: Circuit volume boost (CVB) due to unbiased error mitigation with blow up rate λ = 2."
  },
  {
    "prompt": "Understanding the computational aspects of maximal, allowed, volume.",
    "response": "As the allowed inaccuracy ϵ →0, the maximal volume possible without EM vanishes, while the maximal volume possible with unbiased EM is independent of ϵ, given the same allowed shot overhead R for both circuit execution methods. 5.2 Predictions for finite QAs enabled using EM alone Given the impressive expected circuit volume boosts described in Sec. 5.1, what can be said about the expected actual values of V , the volumes of the quantum circuits, to be enabled by EM in the near future?"
  },
  {
    "prompt": "Read and understand this section about QPU, finite, subsection from a research paper.",
    "response": "In this subsection, we will argue that EM acting on devices with γ = 10−3 will already enable achieving finite QA within reasonable QPU time. This is the origin of our assertion in the introduction that finite QA in EVE (as defined in Sec. 2) is to be expected already in the very near future. This prediction relies on two claims: first, we provide estimates of the QPU time for EM as a function of the active volumes with such fidelities."
  },
  {
    "prompt": "Understanding the computational aspects of QPU, active, volumes.",
    "response": "According to these estimates, active volumes of over a thousand (natively available on a given QPU) 2-qubit gates are achievable in reasonable time with γ = 10−3. Secondly, we claim that achieving active volume of over a thousand gates for general quantum circuits suffices for finite QA in EVE. This is done by showing that for generic circuits, such active volumes seem to be well-beyond the reach of state of the art HPC."
  },
  {
    "prompt": "Explain a concept involving claim, subsection, devoted in computational complexity.",
    "response": "Most of this subsection is devoted to explaining the analysis that supports this latter claim."
  },
  {
    "prompt": "This is a technical discussion about volume, active, quantum in theoretical computer science.",
    "response": "Figure 2 summarizes the analysis of both the quantum and the classical run time as a function of volume, and indicates where finite QA is expected for two types of quantum hardware platforms: superconducting qubits and trapped ions.22 Note that we consider here the ability of EM to execute generic quantum circuits of a given active volume and geometry faster than the performance of classical HPC for the same active volume and geometry, as marking the point of finite QA.23 5.2.1 Estimates of active volume achievable using EM Similarly to the derivation of Eq."
  },
  {
    "prompt": "This is a technical discussion about VEM, Fig., log in theoretical computer science.",
    "response": "(17), we can use Eq. (15) to obtain an order of magnitude estimate VEM = log R/λγ ∼(1–10)/γ for the volumes enabled by EM, as a function of the infidelity γ, for a 22In drawing Fig. 2, we assume that both EM and classical simulation have access to the same active volume identification subroutine, and so n and V have the same meaning in the analysis of both. 23 We stress, however, that designing finite QA demonstrations that can be considered as rigorously justified requires further work."
  },
  {
    "prompt": "Explain a concept involving Definition, family, meet in computational complexity.",
    "response": "In particular, to meet the formal requirements of Definition 2.2, one must choose a specific circuit family In and demonstrate finite QA with respect to this family."
  },
  {
    "prompt": "Understanding the computational aspects of Ref, gates, hardware.",
    "response": "The natural choice of defining In to be the set of all quantum circuits of a given size consisting of native gates (for a given hardware), accompanied with some fixed observable, will likely not suffice, since random (or unstructured) circuit families tend to exhibit exponentially small expectation values with overwhelming probability, making the computational task trivial; see e.g., Ref. [97]."
  },
  {
    "prompt": "Explain a concept involving circuits, non-negligible, time in computational complexity.",
    "response": "Instead, it is required to construct a family of circuits In containing a large number of circuits, such that the variance of the ideal expectation values is non-negligible (in particular, it needs to be larger than the accuracy ϵ that is possible to achieve with EM in reasonable time); this is beyond the scope of the current paper. 19 5 USEFUL FINITE QAS EXPECTED IN THE NEAR FUTURE USING EM reasonable allowed shot overhead R ∈[2, 109]."
  },
  {
    "prompt": "Explain a concept involving VEM, QPU, near-term in computational complexity.",
    "response": "For near-term 2-qubit gate infidelities γ ∼10−3, this means VEM ∼103–104. (18) Going beyond such order of magnitude estimates requires specifying a particular EM protocol and QPU, and accurately estimating the required number of shots and distinct circuits required by the protocol, as well as the shot and circuit times of the QPU (as already mentioned in Footnote 17). Figure 2 demonstrates the results of such an analysis, based on QESEM, an unbiased EM protocol introduced in Ref."
  },
  {
    "prompt": "Scientific context on trapped-ions, Fig, characteristic and their role in graph theory.",
    "response": "[68]; and with characteristic time scales for current QPUs based on either superconducting-qubits or trapped-ions. The solid colored lines in Fig."
  },
  {
    "prompt": "This is a technical discussion about thousand, gates, time in theoretical computer science.",
    "response": "2 show estimates of the QPU time for QESEM as a function of the active volume, for a required output accuracy 1−ϵ = 95%.24 We see that for both superconducting-qubits and trapped-ions, a 2-qubit gate infidelity γ = 10−3 (green lines) suffices to achieve accurate expectation values from circuits with an active volume of over a thousand 2-qubit gates in reasonable time – a few hours for trapped ions, and under an hour for superconducting qubits.25 We devote the remainder of this section to justify the claim that the ability of EM to produce accurate output from circuits actively involving over a thousand 2-qubit gates in reasonable time suffices for finite QA in EVE, namely suffices to exceed state of the art quantum circuit simulation by classical HPC."
  },
  {
    "prompt": "Scientific context on Sec, simulation, methods and their role in graph theory.",
    "response": "We start in Sec. 5.2.2 by considering state-vector simulation, and restrict attention to circuits with a particular geometry to enable comparison to EM. In Sec. 5.2.3 we argue that, for circuits with this geometry, state of the art simulation methods, based on tensor-network contraction, will not provide a significant advantage over the run time of state-vector simulation. We comment on additional classical simulation methods, including approximate methods, in Sec. 5.2.4."
  },
  {
    "prompt": "Scientific context on Advantage, gates, Volume and their role in graph theory.",
    "response": "0 500 1000 1500 2000 2500 3000 0.1 1 10 100 Active Volume [#2q gates] Run time [hours] Trapped ions Quantum Advantage 0 500 1000 1500 2000 2500 3000 0.1 1 10 100 Active Volume [#2q gates] Superconducting qubits Quantum Advantage F2 q=99.5% F2 q=99.8% F2 q=99.9% HPC (d = 2, v = 0.1) v 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Figure 2: Predictions for finite QA with EM. Coloured lines indicate the QPU time as a function of active circuit volume, when using QESEM, an unbiased EM method introduced in Ref."
  },
  {
    "prompt": "Understanding the computational aspects of trapped-ions, gate, fidelities.",
    "response": "[68], on QPUs based on superconducting-qubits and trapped-ions, for different 2-qubit gate fidelities F2q = 1 −γ, and requiring 95% output accuracy. Differences between the left and right panels stem from the different time scales in the two types of QPUs. The dashed line estimates the run time of state-of-the-art classical simulation algorithms on HPC hardware, using Eq. (20), with dimension d = 2 and operator spreading velocity v = 0.1, as an example."
  },
  {
    "prompt": "Understanding the computational aspects of active, Note, connectivity.",
    "response": "Note that since we measure the active volume in native gates and work with d = 2, the all-to-all connectivity advantage of trapped-ions is not explicitly indicated. Finite QA may be achieved when QESEM’s QPU time is significantly below the dashed line. The purple background shows the velocity v (for d = 2) as a function of the active volume V and classical run time Tc, demonstrating how the dashed line would have looked like for with different choices of the velocity v."
  },
  {
    "prompt": "Explain a concept involving Veff, Ref, log in computational complexity.",
    "response": "24We assume here, for simplicity, that the observable is a single Pauli string, with ideal expectation value ⟨O⟩ideal = 1, and with an effective volume Veff := γ−1 log(⟨O⟩ideal / ⟨O⟩noisy) identical to the active volume V . We refer to Ref. [68] for derivations, more general analytic QPU time estimates, improved estimates based on empirical data, and a comparison to experimental performance."
  },
  {
    "prompt": "Read and understand this section about QPU, time, pre from a research paper.",
    "response": "25We consider here EM protocols whose run time is dominated by the required QPU time, with a negligible classical time for pre- and post-processing. It is possible to allow for significant HPC usage in EM, which can reduce the QPU time [91–93]. In such cases one must consider the total run time (or other appropriate resource), including both classical and quantum processing."
  },
  {
    "prompt": "Explain a concept involving Comparison, HPC, State in computational complexity.",
    "response": "20 5 USEFUL FINITE QAS EXPECTED IN THE NEAR FUTURE USING EM 5.2.2 Comparison to classical HPC (1): State vector simulation We consider here the classical complexity of EVE, focusing on finite problem sizes and time scales, as opposed to asymptotics. The world’s top supercomputers can now perform ∼1018 floating-point operations per second (FLOPS) [94]."
  },
  {
    "prompt": "Understanding the computational aspects of volume, run, active.",
    "response": "A classical state-vector simulation of quantum circuits with volume V on n qubits requires 4V 2n complex floating-point operations,26 leading to a run time27 Tc ≥V 2n10−18sec. (19) Equation (19) gives the run time of state vector simulation as a function of both the active volume V and the number of active qubits n. On the other hand, as discussed in Sec. 4, the run time of known EM methods is generically determined solely by the active volume V , as in Eq. (3)."
  },
  {
    "prompt": "Read and understand this section about functional, relation, circuits from a research paper.",
    "response": "Therefore, in order to compare the two run times, we must fix a functional relation between n and V , by restricting attention to a particular family of circuits, as we next describe. Once a functional relation n(V ) is fixed, it can be plugged into Eq. (19) to derive a classical HPC run time estimate as a function of V , as demonstrated in Fig. 2 (dashed line)."
  },
  {
    "prompt": "Explain a concept involving hyper, circuit, lattice in computational complexity.",
    "response": "To demonstrate how a family of circuits with a fixed relation n(V ) may be defined, we consider a (hyper) cubic lattice of qubits in d ≥1 dimensions on which we define a circuit of D layers that is made of identical 2-qubit gates. The gates act on the edges of the lattice, such that each qubit experiences a 2-qubit gate in each of the D circuit layers. In addition, we shall assume that at the end of the circuit single-qubit observable is measured."
  },
  {
    "prompt": "Understanding the computational aspects of operator, support, active.",
    "response": "In such uniform circuits, it is natural to characterize the active volume V in terms of the operator spreading velocity v ∈[0, 1] [96], This velocity determines the number of active qubits in the support of the operator after evolving through the D circuit layers in the Heisenberg picture. When v = 1, the support of the operator grows by one qubit whenever the operator evolves through a 2-qubit gate that crosses its support."
  },
  {
    "prompt": "Scientific context on circuit, velocity, practice and their role in graph theory.",
    "response": "In practice, the spreading velocity can be much lower than its maximal value 1, e.g., when the gates in the circuit have small rotation angles. For the above circuit family, the active volume V takes the shape of a d-dimensional pyramid with base area n (the number of active qubits) and height D (the circuit depth), with the velocity v determining the opening angle of the pyramid. A simple geometrical calculation then gives the relation n(V ) = (4vV/cd)cd, with cd = d/(d + 1), see Appendix A.3."
  },
  {
    "prompt": "Read and understand this section about Substituting, classical, run from a research paper.",
    "response": "Substituting into Eq. (19), we get the classical run time Tc of state vector simulation, as a function of the active volume V , for the above circuit family: Tc ∼V 2(4vV/cd)cd. (20) We use Eq. (20) with the conservative values d = 2 and v = 0.1 to obtain the dashed ‘HPC’ line in Fig. 2. Since the classical run time is very sensitive to the velocity parameter v, we also include a colored background in Fig."
  },
  {
    "prompt": "Explain a concept involving circuit, finite, showing in computational complexity.",
    "response": "2, where the color scale corresponds to varying velocities v for fixed d = 2, showing how the graph Tc(V ) changes with different choices of the velocity v. In the darker areas in Fig. 2, finite QA is less fine tuned, as it holds for broader circuit families of a given volume (namely, also for lower velocities). Remark 8 (Circuit geometry and finite QA with EM) The above discussion provides insights regarding the choice of circuit geometry where it is best to look for early finite QAs with EM."
  },
  {
    "prompt": "Explain a concept involving Equation, shows, worse’ in computational complexity.",
    "response": "Equation (20) shows that Tc grows exponentially with V with an exponent that becomes ‘worse’ the larger v or d become. The dimension d is essentially unlimited in QPUs based on trapped ions and neutral atoms, but is restricted to ≤2 in current superconducting qubit QPUs. For fixed d, lower velocities correspond to more challenging circuits for EM relative to classical simulation."
  },
  {
    "prompt": "Understanding the computational aspects of gate, matrix-vector, dimension.",
    "response": "Thus, for early finite QA using EM, we may therefore 26For each 2-qubit gate, 2n−2 matrix-vector multiplications of complex dimension 4. Conversion between complex and real floating-point operations leads to an additional factor of 7.5 [95]. 27We ignore here memory requirements, which only increase the run time, and revisit these in Sec. 5.2.3. 21 5 USEFUL FINITE QAS EXPECTED IN THE NEAR FUTURE USING EM seek circuits with high velocity. Intuitively, we can understand this directly from Eq."
  },
  {
    "prompt": "Scientific context on circuits, demonstrations, finite and their role in graph theory.",
    "response": "(19), in which Tc grows exponentially with n and only linearly with V . It follows that, for fixed volume, ‘balanced’ circuits, where D ∼n1/d, and so n ∼V cd, are harder to classically simulate than circuits that are narrow and deep (D ≫n1/d, or n ≪V cd), and therefore balanced circuits are more suited for early demonstrations of finite QA.28 We expect such geometrical considerations to play an important role in near term demonstrations of finite QA with EM."
  },
  {
    "prompt": "This is a technical discussion about Comparison, HPC, Tensor in theoretical computer science.",
    "response": "5.2.3 Comparison to classical HPC (2): Tensor network contraction So far, we explicitly considered the run time of state-vector simulation, ignoring memory requirements and additional simulation methods. In practice, state-vector simulation is limited by memory requirements to under 50 qubits, and the current leading method for simulating generic quantum circuits seems to be tensor network contraction [98–100]."
  },
  {
    "prompt": "Explain a concept involving network, circuit, method in computational complexity.",
    "response": "The method is based on the representation of a given quantum circuit as a single tensor network (TN), which is contracted via an optimized ordering of tensors [101, 102]. Apart from potentially reducing memory requirements relative to state-vector simulation (at least for circuits with uniform connectivity, e.g., a d-dimensional lattice) [99], TN contraction can significantly reduce the run time, if the circuit has a ‘tree-width’ which is significantly smaller than the number of qubits [101]."
  },
  {
    "prompt": "Understanding the computational aspects of lattice, qubits, diameter.",
    "response": "For circuits on a d-dimensional lattice of n qubits, this corresponds to the circuit depth D being smaller than the lattice diameter n1/d. However, for D > n1/d, the expression in Eq. (19) is a good estimate for the run time of TN contraction with state of the art classical algorithms and hardware (see e.g. Sec.IV.A in Ref. [98]). In our example D = (d/2v)n1/d (see Eq. (23) in Appendix A.3), so for d = 2, the above condition is met for velocities v which are not too close to 1."
  },
  {
    "prompt": "Read and understand this section about Fig., simulation, contraction from a research paper.",
    "response": "The color scale in Fig. 2 emphasizes low velocities, for which TN contraction cannot significantly improve upon the run time of state-vector simulation. 5.2.4 Comparison to classical HPC (3): Additional simulation methods Though TN contraction is arguably the leading method for simulating generic quantum circuits [98–100], a convincing claim of finite QA would need to address additional classical simulation methods. Each known simulation method can evade the lower bound in Eq."
  },
  {
    "prompt": "This is a technical discussion about families, TNS, restricted in theoretical computer science.",
    "response": "(19) only for a very restricted family of circuits; and finite QA can only be achieved for circuit families that do not significantly overlap with any of these easy-to-simulate families. As a example, an important set of classical methods to consider is based on a TN representation of the initial state (TNS), and the evolution of this representation through the circuit. Such methods are advantageous for circuits that create limited entanglement (see e.g., Ref."
  },
  {
    "prompt": "Scientific context on entangled, ensure, admit and their role in graph theory.",
    "response": "[103]), so to ensure that they do not admit a run time significantly lower than Eq. (19), we must consider circuits in which all n qubits are sufficiently entangled. It is indeed expected that in generic cases the n active qubits in Eq. (19) are significantly entangled, in which case TNS methods would not be advantageous over state vector.29 We note that Ref."
  },
  {
    "prompt": "This is a technical discussion about number, qubits, constructed in theoretical computer science.",
    "response": "[105] constructed examples for high dimensional qudits where this is not the case, and the number of active qubits is much larger than the number of qubits which are significantly entangled. For such circuits, the classical run time of TNS methods might be significantly better than Eq. (19)."
  },
  {
    "prompt": "Explain a concept involving velocity, dense, generic in computational complexity.",
    "response": "However, while the entanglement growth velocity is generally only upper bounded by the operator spreading velocity v [96, 105], we are not aware of examples where the two velocities differ significantly for generic, dense, quantum circuits, defined on qubits. 28To compensate for the ‘un-balanced’ shape of the active volume in circuits with low velocity v, we may choose to measure observables supported on more than a single qubit, essentially chopping the top of the pyramid-shaped active volume."
  },
  {
    "prompt": "Read and understand this section about velocity, direction, circuits from a research paper.",
    "response": "Such higher-weight observables will not, however, be advantageous for circuits with high velocity, where they lead to an imbalance in the opposite direction, D < n1/d. As explained below, such circuits admit a reduced classical run time relative to Eq. (19), via tensor-network contraction algorithms. 29As an interesting recent example, the TNS approach of [104] proved useful for simulating the naively large volume experiment of [48]."
  },
  {
    "prompt": "Scientific context on number, qubits, circuit and their role in graph theory.",
    "response": "However, as shown in [98], the number of active qubits was significantly smaller than the total number of qubits in the circuit, such that efficient state-vector simulations were also possible. 22 6 EM IN THE ERA OF FAULT TOLERANT EC Two additional notable examples which would require significantly less run time for classical simulation than Eq."
  },
  {
    "prompt": "Scientific context on simulated, efficiently, based and their role in graph theory.",
    "response": "(19), include circuits with mostly Clifford gates, which can be simulated efficiently based on the stabilizer formalism [106]; as well as circuits with mostly ‘matchgates’, which can be simulated efficiently based on a mapping of qubits to free fermions [107]. The above easy-to-simulate circuit families and corresponding simulation algorithms were discovered as part of extensive (and ongoing) research effort in quantum computational complexity."
  },
  {
    "prompt": "Understanding the computational aspects of additional, efficiently, studies.",
    "response": "It is possible that additional studies will identify additional restricted circuit families that may be simulated efficiently. Nevertheless, we take Eq. (19) as an estimate for the run time of state-of-the-art classical simulation of generic quantum circuits actively involving n qubits and V gates; and with the connectivity of a d- dimensional lattice and circuit depth D > n1/d. This may be viewed as a finite-size complexity assumption, on which claims of finite QA may be based."
  },
  {
    "prompt": "Understanding the computational aspects of run, time, remark.",
    "response": "As a final remark, we note that the run time for EM scales as ϵ−2 (as in Eq. (3)) with the allowed inaccuracy ϵ, while the expression (19) for the classical run time is naively independent of ϵ. As a result, finite QA with EM seems to be easier for larger ϵ, and more challenging with small ϵ. Of course, one may ask whether classical simulation methods can utilize the allowed inaccuracy to save in run time. First, note that since Eq."
  },
  {
    "prompt": "Read and understand this section about run, time, active from a research paper.",
    "response": "(19) involves the active volume and active qubit number, the run time is already reduced based on the allowed inaccuracy ϵ. Beyond this, despite results showing significant reductions in the run time for noisy circuit simulation based on its large deviation from the ideal circuit [61, 108], we are not aware of significant reductions in the run time of ideal circuit simulation given a small allowed inaccuracy ϵ ≪1."
  },
  {
    "prompt": "Scientific context on tolerant, fault, reduction and their role in graph theory.",
    "response": "6 EM in the era of fault tolerant EC 6.1 The fault tolerance misconception: Why additional error reduction is needed even when fault tolerant EC is available A common misconception in the quantum ecosystem is that fault tolerant EC is thought to handle errors in the best and most efficient way, and therefore, once EC abilities become available to end-users, no other error reduction methods will be needed."
  },
  {
    "prompt": "Scientific context on gates, results, assumed and their role in graph theory.",
    "response": "The origin of this misconception comes from the fact that the original fault tolerance results [11–14] assumed that the model of quantum computation can be described by the set of gates available, accompanied by a single parameter γ indicating the fidelity of the individual qubits or gates in the system."
  },
  {
    "prompt": "Understanding the computational aspects of fixed, gates, demand.",
    "response": "The assumption in all those results is that once the desired below-threshold infidelity γ is achieved for single gates, an unbounded number of qubits with this fixed error rate is available upon demand. This picture is also in line with the usual picture of condensed matter physics, in which the local properties of the physical system are fixed and then one takes the size of the system to infinity to arrive at the thermodynamic limit."
  },
  {
    "prompt": "Read and understand this section about picture, implementations, idealistic from a research paper.",
    "response": "This idealistic picture, however, is not how things work in the reality of quantum computer implementations. Over the past decade, qubit numbers in existing QPUs have been increasing steadily but slowly."
  },
  {
    "prompt": "This is a technical discussion about platforms, ions, future in theoretical computer science.",
    "response": "This had been the case in all leading hardware platforms, in particular those based on superconducting qubits and trapped ions, and it is expected to continue to be the situation in the foreseeable future, according to the road maps of all major quantum computing manufacturers, see e.g., [16, 18]. The qubit numbers predicted for neutral atom based quantum processors are larger, but nevertheless restricted, see e.g., [17]."
  },
  {
    "prompt": "Explain a concept involving number, qubits, QPUs in computational complexity.",
    "response": "The situation is somewhat different in photon based QPUs, which we exclude from this discussion. We therefore consider here hardware platforms in which the increase in the number of high quality qubits is gradual; at any point in time in the foreseeable future, the number of qubits is upper bounded and limited by some fixed number, and the end-users at that point in time can only make use of QPUs of that size. This limit on the number of qubits has severe consequences."
  },
  {
    "prompt": "Understanding the computational aspects of number, qubits, logical.",
    "response": "When the number of high quality qubits is limited, the user must choose between spending the qubits on EC, or on logical qubits for computation (namely, the number of qubits in the ideal circuit). Thus there is a trade-off between the number of logical qubits, and their quality, namely, the logical error. Say, for example, that we are looking a few years ahead into the future, when QPUs consisting of 104 high quality 23 6 EM IN THE ERA OF FAULT TOLERANT EC (below threshold) qubits are available."
  },
  {
    "prompt": "Read and understand this section about qubits, logical, Suppose from a research paper.",
    "response": "Suppose then that we want to run a quantum circuit on 103 logical qubits; in this case we can only use EC with a rate of 1/10 logical qubits to physical qubits. Such an EC code can only reduce the logical error rate by a limited amount; denote the resulting logical error by γ′. Using Eq. (12), this directly translates to an upper bound on the volume of the logical quantum circuit that can be run, V ∼ϵ/γ′, for a required accuracy ϵ."
  },
  {
    "prompt": "This is a technical discussion about qubits, logical, user in theoretical computer science.",
    "response": "Perhaps the user would resort to running a circuit of just 100 logical qubits? That would leave her the possibility of using an EC scheme with rate 1/100, which if chosen correctly, can reduce the logical error by many orders of magnitudes compared to the physical one, and allow her to run very deep quantum circuits on those 100 qubits."
  },
  {
    "prompt": "This is a technical discussion about exponentially, protocols, achieved in theoretical computer science.",
    "response": "We note that the effectiveness of EC protocols (namely, the value of the logical error rate γ′ that is achieved) depends very strongly (in fact, exponentially) on the distance of the EC code used."
  },
  {
    "prompt": "Explain a concept involving rate, code, reducing in computational complexity.",
    "response": "However, increasing the code distance usually comes at the expense of reducing the code rate, entailing an increase in the required number of physical qubits.30 This means that reducing the number of logical qubits in the above example from 103 to a 100, namely by a factor of ten, leads to a dramatic improvement in the logical error rate, which in turn allows an increase by a factor much larger than 10 in the volume."
  },
  {
    "prompt": "Read and understand this section about logical, circuits, run from a research paper.",
    "response": "We see that there is an important trade off: if we want to reduce the logical error rate significantly (in order to run logical circuits with larger volume), we will need to invest a lot of our qubit resources in EC, which will limit the logical circuits we can run to much fewer logical qubits."
  },
  {
    "prompt": "Read and understand this section about due, limited, volumes from a research paper.",
    "response": "The upshot is that in the foreseeable future, and for as long as the number of high quality qubits is limited, circuit volumes and logical qubit numbers will be limited due to noise, even in the era of fault tolerant EC. Hence, while volumes available by EM are severely limited due to limitations on available QPU time (cf. Sec. 4), the volumes possible with fault tolerant EC are also limited – due to restricted qubit numbers."
  },
  {
    "prompt": "Understanding the computational aspects of future, demonstrating, circuit.",
    "response": "Here is an example demonstrating the circuit volumes expected to be available when just using EC in the near future, say with an optimistic fidelity γ = 5 × 10−4. In this case, the bare volume available for required accuracy ϵ = 1% is Vbare ∼ϵ/γ = 20 gates (following the rule of thumb of Eq. (13)). How much can we improve on this when using EC? This, as explained above, depends on how much of our qubit resource we want to spend on EC."
  },
  {
    "prompt": "Scientific context on Suppose, hand, fidelity and their role in graph theory.",
    "response": "Suppose we have 103 physical qubits at hand, with this fidelity. If we want to run a circuit on 100 logical qubits, we can only afford using an EC code of rate roughly 1/10; we could use Steane’s code. We note that as in the bare case, we can estimate the available logical volume by the same rule of thumb, VEC ∼ϵ/γ′, (21) where the physical infidelity γ is replaced by the logical infidelity γ′."
  },
  {
    "prompt": "Read and understand this section about compared, error, rate from a research paper.",
    "response": "We see that EC only increases the available volume compared to the bare case, by the same factor by which Steane’s code improves the logical error rate compared to the physical error rate. With a fidelity of 5 × 10−4 and taking into account that the threshold for Steane’s code with state-of-the-art fault-tolerant constructions is ∼10−3, we get that the increase in volume would be less than a factor 2 (see Fig. 3, blue line)."
  },
  {
    "prompt": "Read and understand this section about circuit, enables, reaching from a research paper.",
    "response": "This enables reaching logical circuit volumes < 40, certainly not sufficient for achieving finite QA. Luckily, this situation can be remedied. While noise limits the circuit volumes that can be executed even when fault tolerant EC is used, the volume available can be significantly increased, in fact by several orders of magnitude, by using EM in conjunction with EC. We explain this combination now."
  },
  {
    "prompt": "Understanding the computational aspects of Combining, recently, limitations.",
    "response": "6.2 Logical EM: Combining EM with EC to provide further circuit volume boosts To address the challenges of EC and EM, and go beyond their respective limitations, the combination of the two methods was recently proposed [109–113], and very recently demonstrated [114]. We refer to this general approach as logical error mitigation (LEM)."
  },
  {
    "prompt": "Explain a concept involving resources, LEM, QPU in computational complexity.",
    "response": "In terms of quantum resources, LEM enables to optimize the use of both available QPU time and physical qubit numbers, to maximize accessible circuit volume and output accuracy, with the given resources. 30The inverse relation between code distance and code rate holds in commonly used code families, such as Surface codes or code concatenations."
  },
  {
    "prompt": "Scientific context on constant, rate, codes and their role in graph theory.",
    "response": "An exception occurs in qLDPC codes of constant rate – codes in which the rate is bounded from below by a constant > 0 independent of the code length that may grow unboundedly. However even for such codes, increasing the code distance means investing more physical qubits; see Tab. 2 and surrounding text regarding the LP qLDPC codes."
  },
  {
    "prompt": "Scientific context on LEM, operations, External and their role in graph theory.",
    "response": "24 6 EM IN THE ERA OF FAULT TOLERANT EC 6.2.1 External LEM The LEM methods described in the literature may be roughly summarized as follows: apply EC to generate logical operations with a reduced error rate relative to physical operations; then apply EM to these logical operations, as if they are physical operations. One can view this is as follows."
  },
  {
    "prompt": "Scientific context on LEM, including, measurements and their role in graph theory.",
    "response": "To create LEM, we start with some EM protocol, and replace each gate used by it by an error-corrected logical gate, including syndrome measurements, decoding and recovery operations. This results in the LEM protocol in which EM acts on the logical qubits."
  },
  {
    "prompt": "Read and understand this section about Assuming, Sec, ExtLEM from a research paper.",
    "response": "We refer to this approach as ‘external LEM’ (ExtLEM), since it does not make use of the wealth of internal syndrome data generated by logical operations.31 Assuming that ExtLEM is based on an EM protocol which is either unbiased, or produces a bias which is negligible relative to the required accuracy ϵ, we immediately obtain from Sec. 5.1 the order of magnitude estimate VLEM ∼1–10 γ′ (22) for the logical circuit volume available with LEM."
  },
  {
    "prompt": "This is a technical discussion about Fig., CVBs, circuit in theoretical computer science.",
    "response": "Accordingly, the circuit volume boosts (CVBs) of ∼1–10/ϵ shown in Fig. 1 for EM over bare circuit execution, are maintained when comparing ExtLEM to EC. Thus, LEM enables ∼1–10/ϵ larger logical circuit volumes compared to those provides by EC alone, on the same hardware and with the same output accuracy, see blue and green curves in Fig. 3."
  },
  {
    "prompt": "Understanding the computational aspects of Vbare, Figure, SALEM.",
    "response": "75 80 85 90 95 100 100 101 102 103 1-ϵ [%] CVB=V/Vbare bare EC EM ExtLEM EC+PS SALEM Figure 3: An example for circuit volume boosts (CVBs) with different error reduction strategies, with and without EC. The different curves indicate the CVBs relative to bare circuit execution, due to EC, EM, an ‘external’ mitigation of logical errors (ExtLEM), the combination of EC and post-selection (EC+PS), and a syndrome-aware mitigation of logical errors (SALEM) [115]."
  },
  {
    "prompt": "This is a technical discussion about code, methods, mild in theoretical computer science.",
    "response": "All methods are given a very mild allowed shot overhead R = 2. The data presented was obtained from numerical simulations of repeated EC cycles (a logical memory circuit), with the well-known Steane code (7-qubit color code), based on the fault-tolerant construction of Ref. [116]. The results hold qualitatively for any EC code, but will be quantitatively different, e.g. the blue line will be higher with better EC schemes (cf. Tab. 2). For ease of reference, focus on 95% output accuracy."
  },
  {
    "prompt": "This is a technical discussion about methods, CVB, errors in theoretical computer science.",
    "response": "We compare the volume boosts for different methods. For an infidelity of 5 × 10−4, slightly below the Steane code’s threshold, logical errors are slightly smaller than physical errors, and EC provides a small CVB (blue) compared to bare execution. This CVB does not improve with required accuracy, because EC suffers a significant bias due to logical errors (similarly to the bias in the bare computation, discussed in Sec. 5.1)."
  },
  {
    "prompt": "Understanding the computational aspects of Steane, yellow, Unbiased.",
    "response": "Unbiased EM (yellow), applied directly to physical qubits, is better than EC for the Steane code and chosen infidelity of 5 × 10−4. ExtLEM (green), where EM is naively applied to logical gates, enjoys the benefits of both EC and EM, and provides a larger CVB than EM. The remaining methods, EC+PS and SALEM, are discussed in the main text."
  },
  {
    "prompt": "Understanding the computational aspects of Pauli, operations, Clifford.",
    "response": "31We note that existing work does make use of several features of logical operations which are not present in physical operations, such as the ability to virtually apply Pauli operations by keeping track of a ‘Pauli frame’ [109], as well as the expected lower logical error of Clifford gates relative to non-Clifford gates [110, 111, 113]."
  },
  {
    "prompt": "Scientific context on ERA, FAULT, TOLERANT and their role in graph theory.",
    "response": "25 6 EM IN THE ERA OF FAULT TOLERANT EC 6.2.2 Error correction combined with post-selection An alternative strategy to mitigate logical errors, which is routinely used in EC experiments, is based on post-selection (PS) [117, 118]. Here, one employs decoders that reject certain syndromes, discarding the circuit repetitions (shots) in which they are observed."
  },
  {
    "prompt": "Read and understand this section about syndromes, set, standard from a research paper.",
    "response": "We refer to this strategy as EC+PS, to highlight the fact that the accepted set of syndromes is not only kept but also decoded and recovered from, as in standard EC.32 Let us compare ExtLEM to the approach of EC+PS. In the latter method, one rejects a set of syndromes, which means that errors associated with these syndrome will not contribute to the output distribution, which is computed by conditioning on accepted (and corrected) syndromes only."
  },
  {
    "prompt": "Explain a concept involving logical, errors, mitigated in computational complexity.",
    "response": "Thus, in EC+PS only the logical errors in rejected syndromes are mitigated (by rejection of the corresponding shots), while leaving the logical errors in accepted syndromes intact. This generically leads to a significant bias b. On the other hand, if the rejected set of syndromes is chosen appropriately, the above ‘rejected’ logical errors are mitigated (by shot rejection) very efficiently, in the sense of a blowup rate λ that can be as low as 1 (cf. toy example in Sec. 4)."
  },
  {
    "prompt": "Understanding the computational aspects of SALEM, Fig., Syndrome.",
    "response": "These features determine the behavior of the CVB due to EC+PS, which is generically better than that of ExtLEM when b ≪ϵ, but significantly under-performs when b ≫ϵ, see e.g., purple curve in Fig. 3. 6.2.3 Syndrome Aware Logical EM (SALEM) In fact, it turns out that both ExtLEM and EC+PS may be viewed as instances of a more general framework, that extends and improves upon both. We refer to this framework as syndrome-aware LEM (SALEM), and provide its detailed technical description in Ref."
  },
  {
    "prompt": "Explain a concept involving SALEM, ExtLEM, Logical in computational complexity.",
    "response": "[115]. In essence, SALEM allows to maintain the negligible bias possible with ExtLEM, while significantly improving upon its blowup rate. Figure 3 (red curve) demonstrates these performance improvements of SALEM relative to ExtLEM and EC+PS. 6.2.4 Logical EM with state of the art EC codes Our discussion of LEM so far used the prototypical Steane code as an example."
  },
  {
    "prompt": "Understanding the computational aspects of Steane, experiments, makes.",
    "response": "And while the simplicity and high rate of the Steane code made it a useful example for recent EC experiments, its low threshold value ∼10−3 makes it impractical for useful EC in the near term. This is reflected by the small CVB of EC relative to bare execution, even with an optimistic infidelity of 5×10−4 (blue curve in Fig. 3). We now provide examples for the benefits of LEM over mere EC."
  },
  {
    "prompt": "Explain a concept involving Surface, qLDPC, codes in computational complexity.",
    "response": "We consider three state of the art code families: Surface codes, BB qLDPC codes, and LP qLDPC codes [23, 24]. These are examples of codes expected to lead to significant performance improvements over bare circuit execution by the end of the decade, at least according to current hardware road maps [15–18]. In our analysis, we set aside the details of how LEM is performed, and use only the order of magnitude estimates in Eq. (21) and (22). This is summarized in Tab. 2."
  },
  {
    "prompt": "Scientific context on families, code, logical and their role in graph theory.",
    "response": "Within each of the three code families, we consider two codes, the second one having a lower logical error than the former. For the Surface and BB qLDPC code families, the improvement in logical error rate comes at the cost of reducing the rate of the code, thus requiring more physical qubits for a fixed number of logical qubits."
  },
  {
    "prompt": "Understanding the computational aspects of rate, family, unboundedly.",
    "response": "The case of the LP qLDPC family is different, since the distance of these codes can increase unboundedly (leading to an exponential decrease in the logical error rate per qubit) while keeping the rate of the code above a fixed constant > 0. However, this requires working with codes in the family with whose block size (ntot in Tab. 1) also grows unboundedly, eventually surpassing the total number of physical qubits in a given QPU."
  },
  {
    "prompt": "Scientific context on rate, increase, number and their role in graph theory.",
    "response": "Thus, again, an increase in the number of physical qubits is generally required in order to reduce the logical error rate. In the examples given in Tab."
  },
  {
    "prompt": "Explain a concept involving method, LEM, rate in computational complexity.",
    "response": "2, the improvement in logical error rate from the first LP qLDPC code to the second, is in fact accompanied by a slightly higher rate, but comes at the price of a 1300-qubit increase in ntot – possibly requiring more physical qubits than 32We view PS without EC, that is, without decoding and recovering from accepted syndromes, as an EM method, as opposed to a LEM method. Note that PS can be performed even without EC codes, based on known symmetries of the ideal circuit [41]."
  },
  {
    "prompt": "This is a technical discussion about SUMMARY, FORECAST, QPU in theoretical computer science.",
    "response": "26 7 SUMMARY AND FORECAST: QA TIMELINE available on a near term QPU. The upshot is that in all three examples of code families, moving from the first code to the second code to improve the logical error rate requires increasing the required number of physical qubits for implementation. Assuming ϵ = 0.1, for each code family in Tab. 2, LEM of the first code enables to at least reproduce the available volumes of the second code, thus trading run time for physical qubits."
  },
  {
    "prompt": "Scientific context on LEM, Ref, volumes and their role in graph theory.",
    "response": "For estimation error ϵ = 0.01, LEM of the first code provides a significant improvement over the second code (without LEM). In particular, without LEM, and for ϵ = 0.01, none of the listed codes can reach volumes ∼106, the so-called MegaQuOp milestone [57], where one may expect quantum applications with industrial value (see e.g., Ref. [119] for proposed quantum simulation algorithms of industrially-relevant materials at or below these circuit volumes)."
  },
  {
    "prompt": "Scientific context on LEM, shot, allowed and their role in graph theory.",
    "response": "However, with LEM, the second code in each family reaches MegaQuOp with a small allowed shot overhead R (see Eq. (9)) while the first code can also reach MegaQuOp, given a more significant allowed shot overhead but with less physical qubits. Table 2: Examples of state of the art EC codes, and corresponding logical circuit volumes possible with EC alone, and with LEM."
  },
  {
    "prompt": "This is a technical discussion about logical, qubits, ntot in theoretical computer science.",
    "response": "The total number of physical qubits ntot includes the both code qubits and ancilla qubits used for fault-tolerant syndrome measurements. The net rate is given by k/ntot. The logical error γ′ quoted here is defined per syndrome measurement cycle and per logical qubit, and at a physical infidelity γ = 10−3. Note that we consider here logical memory circuits, without any logical operations."
  },
  {
    "prompt": "Read and understand this section about Current, patches, rates from a research paper.",
    "response": "Current proposals for logical operations for qLDPC codes are based on teleportation to surface code patches, which degrades net rates [23, 24]. The columns VEC and VLEM represent the maximal logical circuit volumes possible with EC and LEM, based on the order of magnitude estimates (21) and (22), respectively. Code family Jn, k, dK ntot k/ntot γ′ VEC VLEM Surface, J81, 1, 9K 161 1/161 9 × 10−6 105ϵ 105 −106 Ref."
  },
  {
    "prompt": "Read and understand this section about Ref, qLDPC from a research paper.",
    "response": "[24] J121, 1, 11K 241 1/241 9 × 10−7 106ϵ 106 −107 BB qLDPC, J72, 12, 6K 144 1/12 6 × 10−6 2 × 105ϵ 2 × (105 −106) Ref. [24] J144, 12, 12K 288 1/24 2 × 10−8 6 × 107ϵ 6 × (107 −108) LP qLDPC, J544, 80, ≤12K 1367 1/17 10−6 8 × 105ϵ 8 × (105 −106) Ref."
  },
  {
    "prompt": "Read and understand this section about Concluding, LEM, QPU from a research paper.",
    "response": "[23] J1428, 184, ≤24K 2670 1/15 10−7 9 × 106ϵ 9 × (106 −107) 6.3 Concluding remarks about logical EM To summarize, given a restricted number of physical qubits, LEM provides access to significantly larger logical circuit volumes relative to just EC, at the expense of a tolerable overhead in QPU time."
  },
  {
    "prompt": "This is a technical discussion about QPU, users, manufacturers in theoretical computer science.",
    "response": "And while adding more physical qubits to a given QPU is generally impossible for end users (and a formidable challenge for hardware manufacturers), allowing for more QPU time is always a possibility, which should be exploited based on its cost-effectiveness for the application at hand."
  },
  {
    "prompt": "This is a technical discussion about time, resources, provide in theoretical computer science.",
    "response": "The implication is that at any given moment in time, when EC becomes available, the combination of EM and EC, namely logical EM, will provide significantly larger circuit volumes than with EC alone, and thus should be viewed as the way to optimally use the available quantum resources. 7 Summary and forecast: QA timeline This paper refutes two common misconceptions ■Misconception 1: Error mitigation cannot provide any quantum advantage due to its exponentially scaling shot overhead."
  },
  {
    "prompt": "Scientific context on Sec, misconception, explained and their role in graph theory.",
    "response": "We explained in this paper why this is a misconception. The origin of the misconception is a confusion between asymptotic QA and finite QA; we clarify the difference between these two notions in Sec. 2. 27 7 SUMMARY AND FORECAST: QA TIMELINE We then show that exponential asymptotic QA is indeed generically impossible using EM alone (Sec. 4) and at the same time, dramatic finite QAs are expected very soon using EM (Sec. 5.2)."
  },
  {
    "prompt": "This is a technical discussion about Misconception, Sec, mitigation in theoretical computer science.",
    "response": "■Misconception 2: Error mitigation will not be important once error correction becomes available. In Sec. 6 we explained why even when EC becomes available, this will not eliminate the problem of errors, as residual logical errors will still limit the volumes of quantum circuits that can be executed."
  },
  {
    "prompt": "Understanding the computational aspects of compared, platform, circuit.",
    "response": "We described how circuit volume boosts of a few orders of magnitude are expected using logical EM – the combination of EM and EC – compared to what can be achieved using EC alone on a given hardware platform."
  },
  {
    "prompt": "Read and understand this section about finite, QAs, expected from a research paper.",
    "response": "We conclude that the expected role to be played by EM on the road to the first finite QAs, and beyond, is crucial; EM is highly likely to be the first to provide the first useful finite QAs, and is expected to continue to have an important role also in the long run, when EC becomes a daily routine, by enabling significantly larger quantum circuits than what EC will enable on its own; thus enabling reaching a variety of finite QAs much earlier than would be possible with EC alone."
  },
  {
    "prompt": "Understanding the computational aspects of Table, timeline, finite.",
    "response": "Based on the arguments presented here, Table 3 provides predictions regarding the expected advances along the finite QA timeline, with and without EM. In particular, it highlights two milestones that demonstrate the crucial role of EM on the road to QAs: 1. The first milestone is to experimentally demonstrate finite QA for the expectation value estimation problem. As argued in Sec. 5.2, and based on the hardware road maps cited in Tab."
  },
  {
    "prompt": "Understanding the computational aspects of future, two-qubit, gates.",
    "response": "1, this is expected to be achieved in the very near future, by demonstrating the ability to execute generic quantum circuits of over 1000 two-qubit gates, using EM alone. This will constitute a demonstration of the first useful finite QAs. 2. The second milestone is to reach (logical) circuit volumes ∼105 −107, where the first industry- relevant applications are expected, using logical EM (LEM). As discussed in Sec."
  },
  {
    "prompt": "Scientific context on LEM, Tab, circuit and their role in graph theory.",
    "response": "6.2.4, LEM significantly reduces the physical qubit numbers required for reaching these (logical) circuit volumes, relative to EC alone, thus expediting the corresponding finite QAs. Based on the hardware road maps cited in Tab. 1, we expect this milestone to be achieved within the next few years. Table 3: Conservative road map for finite QAs with EM and logical EM (LEM). In the LEM column we consider the state of the art qLDPC codes in Tab."
  },
  {
    "prompt": "Scientific context on LEM, larger, rate and their role in graph theory.",
    "response": "2, but we allow for a larger net rate, up to 1/100, as may be needed for logical operations (beyond memory). For both EM and LEM we assume a mild allowed shot overhead, leading to a circuit volume boost ∼1/ϵ, as a function of the required output accuracy 1−ϵ. Note that while a 99.9% fidelity suffices for the third milestone with LEM, any improvement in fidelity translates to a significantly larger improvement in logical fidelity, enabling much larger volumes."
  },
  {
    "prompt": "Understanding the computational aspects of Achieved, LEM, Hardware.",
    "response": "Milestone V ∼103 V ∼104 V ∼105 −107 Achieved by EM EM LEM Hardware requirements 50 physical qubits. 2-qubit gate fidelity 99.9%. Existing circuit execution rates. 50-100 physical qubits. Higher circuit execution rates, and/or higher 2-qubit gate infidelity 99.95 −99.99%. 2,000-10,000 physical qubits. 100 logical qubits. 2-qubit gate fidelity 99.9%."
  },
  {
    "prompt": "Scientific context on LEM, REFERENCES, Quantum and their role in graph theory.",
    "response": "Use cases Quantum many-body physics research + Quantum algorithm development + First industry relevant applications Volumes without EM/LEM V ∼103ϵ V ∼104ϵ V ∼(105 −107)ϵ 28 REFERENCES This manuscript makes a case for the importance of EM both as the enabler of the first useful QAs, as well as an accelerator of further finite QAs when combined with EC, once the latter becomes available. We believe these insights place EM as a crucial component towards truly useful quantum computation."
  },
  {
    "prompt": "Explain a concept involving Babbush, Sergio, Boixo in computational complexity.",
    "response": "Acknowledgements We thank Ryan Babbush, Sergio Boixo, Jay Gambetta, David Hayes, Abhinav Kandala, Chris Langer, Sabrina Maniscalco, Tom O’Brien, John Preskill, Ittai Rubinstein, Thomas Schuster, Sarah Sheldon and Norman Yao for useful discussions and helpful comments. Note added Towards finalization of this manuscript, Reference [120] appeared on the arXiv, which provides a perspective on the role of EM in quantum computing."
  },
  {
    "prompt": "This is a technical discussion about present, paper, views in theoretical computer science.",
    "response": "The views and arguments presented in this reference are generally in line with the present paper, and we were pleased to see a consensus with the authors as to the important role of EM. Relative to Ref. [120], the present paper provides a more detailed and technical account to explain both the misconceptions and the justification of our predictions. We hope these expositions will be of use to the community. References [1] R. Takagi, S. Endo, S. Minagawa, and M."
  },
  {
    "prompt": "Understanding the computational aspects of Information, Quantum, Phys.",
    "response": "Gu, npj Quantum Information 8, 114 (2022). [2] R. Takagi, H. Tajima, and M. Gu, Phys. Rev. Lett. 131, 210602 (2023). [3] K. Tsubouchi, T. Sagawa, and N. Yoshioka, Phys. Rev. Lett. 131, 210601 (2023). [4] Y. Quek, D. Stilck França, S. Khatri, J. J. Meyer, and J. Eisert, Nature Physics 20, 1648 (2024). [5] T. Schuster, C. Yin, X. Gao, and N. Y. Yao, arXiv preprint arXiv:2407.12768 (2024). [6] E. Bernstein and U. Vazirani, SIAM Journal on Computing 26, 1411 (1997). [7] D. R."
  },
  {
    "prompt": "Explain a concept involving SIAM, Journal, Simon in computational complexity.",
    "response": "Simon, SIAM Journal on Computing 26, 1474 (1997). [8] P. W. Shor, in Proceedings of the 35th Annual Symposium on Foundations of Computer Science (IEEE, 1994) pp. 124–134. [9] A. M. Steane, Physical Review Letters 77, 793 (1996). [10] P. W. Shor, Physical Review A 52, R2493 (1995). [11] D. Aharonov and M. Ben-Or, in Proceedings of the 29th Annual ACM Symposium on Theory of Computing (ACM, 1997) pp. 176–188. [12] A. Kitaev, Russian Mathematical Surveys 52, 1191 (1997). [13] E. Knill, R."
  },
  {
    "prompt": "This is a technical discussion about quantum, Zurek, Physical in theoretical computer science.",
    "response": "Laflamme, and W. H. Zurek, Physical Review Letters 77, 411 (1996). [14] P. Aliferis, D. Gottesman, and J. Preskill, “Accuracy threshold for postselected quantum computation,” (2007), arXiv:quant-ph/0703264 [quant-ph] . [15] Google Quantum AI, “Google Quantum AI,” (2024), accessed: 2024-10-21. [16] IBM Quantum, “Ibm quantum technology,” (2024), accessed: 2024-10-21. [17] QuEra Computing, “Quera quantum error correction (qec),” (2024), accessed: 2024-10-21."
  },
  {
    "prompt": "Read and understand this section about Quantinuum, accessed, Accelerated from a research paper.",
    "response": "[18] Quantinuum, “Quantinuum Unveils Accelerated Roadmap to Achieve Universal Fault-Tolerant Quantum Computing by 2030,” (2023), accessed: 2024-10-21. 29 REFERENCES [19] IonQ, “Ionq unveils accelerated roadmap and new technical milestones to propel quantum computing,” (2024), accessed: 2024-10-21. [20] IQM Quantum Computers, “Iqm quantum computers technology roadmap,” (2024), accessed: 2024- 10-21. [21] A. G. Fowler, M. Mariantoni, J. M. Martinis, and A. N."
  },
  {
    "prompt": "Understanding the computational aspects of Atomic, Molecular, Physics.",
    "response": "Cleland, Physical Review A—Atomic, Molecular, and Optical Physics 86, 032324 (2012). [22] D. S. Wang, A. G. Fowler, and L. C. Hollenberg, Physical Review A—Atomic, Molecular, and Optical Physics 83, 020302 (2011). [23] Q. Xu, J. P. Bonilla Ataides, C. A. Pattison, N. Raveendran, D. Bluvstein, J. Wurtz, B. Vasić, M. D. Lukin, L. Jiang, and H. Zhou, Nature Physics , 1 (2024). [24] S. Bravyi, A. W. Cross, J. M. Gambetta, D. Maslov, P. Rall, and T. J."
  },
  {
    "prompt": "This is a technical discussion about Nature, Yoder, accessed in theoretical computer science.",
    "response": "Yoder, Nature 627, 78 (2024), accessed: 2024-10-21. [25] T. R. Scruby, T. Hillmann, and J. Roffe, arXiv preprint arXiv:2406.14445 (2024). [26] H. Yamasaki and M. Koashi, Nature Physics 20, 247 (2024). [27] H. Goto et al., Science Advances 10, eadp6388 (2024). [28] D. Bluvstein, S. J. Evered, A. A. Geim, S. H. Li, H. Zhou, T. Manovitz, S. Ebadi, M. Cain, M. Kalinowski, D. Hangleiter, et al., Nature 626, 58 (2024). [29] K. Mayer, C. Ryan-Anderson, N. Brown, E. Durso-Sabina, C. H. Baldwin, D."
  },
  {
    "prompt": "Explain a concept involving arXiv, Gaebler, Foltz in computational complexity.",
    "response": "Hayes, J. M. Dreiling, C. Foltz, J. P. Gaebler, T. M. Gatterman, et al., arXiv preprint arXiv:2404.08616 (2024). [30] M. da Silva, C. Ryan-Anderson, J. Bello-Rivas, A. Chernoguzov, J. Dreiling, C. Foltz, F. Frachon, J. Gaebler, T. Gatterman, L. Grans-Samuelsson, et al., arXiv preprint arXiv:2404.02280 . [31] B. W. Reichardt, D. Aasen, R. Chao, A. Chernoguzov, W. van Dam, J. P. Gaebler, D. Gresh, D. Lucchetti, M. Mills, S. A. Moses, et al., arXiv preprint arXiv:2409.04628 (2024). [32] R."
  },
  {
    "prompt": "Scientific context on Aleiner, Andersen, Ansmann and their role in graph theory.",
    "response": "Acharya, L. Aghababaie-Beni, I. Aleiner, T. I. Andersen, M. Ansmann, F. Arute, K. Arya, A. Asfaw, N. Astrakhantsev, J. Atalaya, et al., arXiv preprint arXiv:2408.13687 (2024). [33] H. Putterman, K. Noh, C. T. Hann, G. S. MacCabe, S. Aghaeimeibodi, R. N. Patel, M. Lee, W. M. Jones, H. Moradinejad, R. Rodriguez, et al., arXiv preprint arXiv:2409.13025 (2024). [34] F. Arute, K. Arya, R. Babbush, et al., Nature 574, 505 (2019). [35] Y. Wu, W.-S. Bao, S. Cao, F. Chen, M.-C. Chen, X. Chen, T.-H."
  },
  {
    "prompt": "Explain a concept involving Physical, Chen, Deng in computational complexity.",
    "response": "Chung, H. Deng, Y. Du, D. Fan, et al., Physical review letters 127, 180501 (2021). [36] J.-W. Pan, C.-Y. Lu, Y. He, et al., Science 370, 1460 (2020). [37] Q. Zhu, S. Cao, F. Chen, M.-C. Chen, X. Chen, T.-H. Chung, H. Deng, Y. Du, D. Fan, M. Gong, et al., Science bulletin 67, 240 (2022). [38] A. Morvan, B. Villalonga, X. Mi, S. Mandrà, A. Bengtsson, P. V. Klimov, Z. Chen, S. Hong, C. Erickson, I. K. Drozdov, et al., Nature 618, 56 (2024). [39] M. DeCross, R. Haghshenas, M. Liu, E. Rinaldi, J."
  },
  {
    "prompt": "Explain a concept involving Alexeev, Baldwin, Bartolotta in computational complexity.",
    "response": "Gray, Y. Alexeev, C. H. Baldwin, J. P. Bartolotta, M. Bohn, E. Chertkov, et al., arXiv preprint arXiv:2406.02501 (2024). 30 REFERENCES [40] S. Aaronson and S.-H. Hung, in Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023 (Association for Computing Machinery, New York, NY, USA, 2023) p. 933–944. [41] Z. Cai, R. Babbush, S. C. Benjamin, S. Endo, W. J. Huggins, Y. Li, J. R. McClean, and T. E. O’Brien, Rev. Mod. Phys. 95, 045005 (2023). [42] K. Temme, S. Bravyi, and J."
  },
  {
    "prompt": "Explain a concept involving Phys, Gambetta, Rev in computational complexity.",
    "response": "M. Gambetta, Phys. Rev. Lett. 119, 180509 (2017). [43] Y. Li and S. C. Benjamin, Physical Review X 7, 021050 (2017). [44] S. Endo, S. C. Benjamin, and Y. Li, Phys. Rev. X 8, 031027 (2018). [45] A. Mari, N. Shammah, and W. J. Zeng, Phys. Rev. A 104, 052607 (2021). [46] Z. Cai, npj Quantum Information 7, 80 (2021). [47] S. N. Filippov, S. Maniscalco, and G. García-Pérez, arXiv preprint arXiv:2403.13542 (2024). [48] Y. Kim, A. Eddins, S. Anand, K. X. Wei, E. Van Den Berg, S. Rosenblatt, H."
  },
  {
    "prompt": "This is a technical discussion about Nature, Zaletel, Temme in theoretical computer science.",
    "response": "Nayfeh, Y. Wu, M. Zaletel, K. Temme, et al., Nature 618, 500 (2023). [49] K. Shinjo, K. Seki, T. Shirakawa, R.-Y. Sun, and S. Yunoki, arXiv preprint arXiv:2403.16718 (2024). [50] H. Yu, Y. Zhao, and T.-C. Wei, Physical Review Research 5, 013183 (2023). [51] X. Mi, A. Michailidis, S. Shabani, K. Miao, P. Klimov, J. Lloyd, E. Rosenberg, R. Acharya, I. Aleiner, T. Andersen, et al., Science 383, 1332 (2024). [52] G. Gyawali, T. Cochran, Y. Lensky, E. Rosenberg, A. H. Karamlou, K. Kechedzhi, J."
  },
  {
    "prompt": "Explain a concept involving arXiv, Westerhout, Asfaw in computational complexity.",
    "response": "Berndtsson, T. Westerhout, A. Asfaw, D. Abanin, et al., arXiv preprint arXiv:2410.06557 (2024). [53] M. Iqbal, A. Lyons, C. F. B. Lo, N. Tantivasadakarn, J. Dreiling, C. Foltz, T. M. Gatterman, D. Gresh, N. Hewitt, C. A. Holliman, et al., arXiv preprint arXiv:2411.04185 (2024). [54] G. Greene-Diniz, C. N. Self, M. Krompiec, L. Coopmans, M. Benedetti, D. M. Ramo, and M. Rosenkranz, arXiv preprint arXiv:2409.15908 (2024). [55] J.-S. Chen, E. Nielsen, M. Ebert, V. Inlek, K. Wright, V. Chaplin, A."
  },
  {
    "prompt": "Scientific context on Quantum, Páez, Poudel and their role in graph theory.",
    "response": "Maksymov, E. Páez, A. Poudel, P. Maunz, et al., Quantum 8, 1516 (2024). [56] Riverlane, “Quantum error correction report 2024,” (2024), accessed: 2025-03-10. [57] J. Preskill, “Beyond nisq: The megaquop machine,” (2024), keynote address at the Q2B 2024 Conference, Silicon Valley. [58] D. W. Berry, R. S. T. Oliver, L. M. D. Ashley, and D. M. W. P. Brown, “Quantum algorithm zoo,” https://quantumalgorithmzoo.org/ (2020), accessed: 2025-02-16. [59] S. Lloyd, Science 273, 1073 (1996). [60] P."
  },
  {
    "prompt": "Explain a concept involving Kasatkin, Zhou, Quiroz in computational complexity.",
    "response": "Singkanipa, V. Kasatkin, Z. Zhou, G. Quiroz, and D. Lidar, Quant-Ph (2025). [61] I. L. Markov, A. Fatima, S. V. Isakov, and S. Boixo, arXiv preprint arXiv:1807.10749 (2018). [62] A. M. Dalzell, A. W. Harrow, D. E. Koh, and R. L. La Placa, Quantum 4, 264 (2020). [63] T. Ayral, P. Besserve, D. Lacroix, and E. A. R. Guzman, The European Physical Journal A 59, 227 (2023). 31 REFERENCES [64] I. Corporation, arXiv preprint arXiv:2001.10554 (2020), accessed: 2025-02-24. [65] A. P. Lund, M. J."
  },
  {
    "prompt": "Explain a concept involving Ralph, Bremner, IEEE in computational complexity.",
    "response": "Bremner, and T. C. Ralph, njp quantum information 3 (2017). [66] Z. Brakerski, P. F. Christiano, U. Mahadev, U. V. Vazirani, and T. Vidick, in 59th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9, 2018, edited by M. Thorup (IEEE Computer Society, 2018) pp. 320–331. [67] K. Liu and Z. Cai, arXiv preprint arXiv:2502.11285 (2025). [68] Qedma Quantum Computing, To appear (2025). [69] J."
  },
  {
    "prompt": "Understanding the computational aspects of Chap, Introduction, World.",
    "response": "Preskill, “Introduction to quantum computation and information,” (World Scientific, 1998) Chap. FAULT-TOLERANT QUANTUM COMPUTATION, pp. 213–269. [70] M. Ben-Or, D. Gottesman, and A. Hassidim, “Quantum refrigerator,” (2013), arXiv:1301.1995 [quant-ph] . [71] Y. Xiong, D. Chandra, S. X. Ng, and L. Hanzo, IEEE Access 8, 228967 (2020). [72] S. Ferracin, A. Hashim, J.-L. Ville, R. Naik, A. Carignan-Dugas, H. Qassim, A. Morvan, D. I. Santiago, I. Siddiqi, and J. J. Wallman, Quantum 8, 1410 (2024)."
  },
  {
    "prompt": "This is a technical discussion about arXiv, Temme, preprint in theoretical computer science.",
    "response": "[73] E. Van Den Berg, Z. K. Minev, A. Kandala, and K. Temme, Nature physics 19, 1116 (2023). [74] E. Granet and H. Dreyer, arXiv preprint arXiv:2409.04254 (2024). [75] M. C. Tran, K. Sharma, and K. Temme, arXiv preprint arXiv:2303.06496 (2023). [76] A. Eddins, M. C. Tran, and P. Rall, arXiv preprint arXiv:2409.04401 (2024). [77] M. Kliesch and I. Roth, PRX Quantum 2, 010201 (2021). [78] D. Aharonov, M. Ben-Or, R. Impagliazzo, and N."
  },
  {
    "prompt": "Explain a concept involving quant-ph, arXiv, Limitations in computational complexity.",
    "response": "Nisan, “Limitations of noisy reversible computation,” (1996), arXiv:quant-ph/9611028 [quant-ph] . [79] X. Gao and L. Duan, “Efficient classical simulation of noisy quantum computation,” (2018), arXiv:1810.03176 [quant-ph] . [80] A. Deshpande, P. Niroula, O. Shtanko, A. V. Gorshkov, B. Fefferman, and M. J. Gullans, PRX Quantum 3 (2022), 10.1103/prxquantum.3.040329. [81] M. J. Kastoryano and K."
  },
  {
    "prompt": "Read and understand this section about Journal, Physics, Mathematical from a research paper.",
    "response": "Temme, Journal of Mathematical Physics 54, 052202 (2013), https://pubs.aip.org/aip/jmp/article-pdf/doi/10.1063/1.4804995/13369764/052202_1_online.pdf . [82] D. C. McKay, I. Hincks, E. J. Pritchett, M. Carroll, L. C. Govia, and S. T. Merkel, arXiv preprint arXiv:2311.05933 (2023). [83] D. Shepherd and M. J. Bremner, Proc. R. Soc. A 465, 1413 (2009). [84] M. J. Bremner and R. J. D. Shepherd, Proc. R. Soc. A 467, 459 (2011). [85] M. J. Bremner, D. Shepherd, and A. Montanaro, Phys. Rev. Lett."
  },
  {
    "prompt": "This is a technical discussion about Liu, IEEE, Quantum in theoretical computer science.",
    "response": "117 (2016). [86] M. J. Bremner, A. Montanaro, and D. Shepherd, Quantum 1 (2017). [87] T. Bergamaschi, C.-F. Chen, and Y. Liu, in 65th IEEE Annual Symposium on Foundations of Computer Science, FOCS (IEEE Computer Society, 2024). [88] D. Aharonov, X. Gao, Z. Landau, Y. Liu, and U. Vazirani, arXiv preprint arXiv:2211.03999 (2022). 32 REFERENCES [89] C. Löschnauer, J. M. Toba, A. Hughes, S. King, M. Weber, R. Srinivas, R. Matt, R. Nourshargh, D. Allcock, C."
  },
  {
    "prompt": "This is a technical discussion about arXiv, Ballance, preprint in theoretical computer science.",
    "response": "Ballance, et al., arXiv preprint arXiv:2407.07694 (2024). [90] S. McArdle, S. Endo, A. Aspuru-Guzik, S. C. Benjamin, and X. Yuan, Reviews of Modern Physics 92, 015003 (2020), arXiv:1808.10402 . [91] S. Filippov, M. Leahy, M. A. Rossi, and G. García-Pérez, arXiv preprint arXiv:2307.11740 (2023). [92] N. Lindner and S. Yunoki, “Q2b24 tokyo | pushing forward for quantum advantage with error mitigation and hpc,” (2024), accessed: 2025-03-10. [93] B. Fuller, M. C. Tran, D. Lykov, C. Johnson, M."
  },
  {
    "prompt": "Explain a concept involving Wei, Kim, Sharma in computational complexity.",
    "response": "Rossmannek, K. X. Wei, A. He, Y. Kim, D. Vu, K. Sharma, et al., arXiv preprint arXiv:2502.01897 (2025). [94] TOP500 Authors, “Top500 november 2024 list,” https://top500.org/lists/top500/2024/11/ (2024), accessed: 2024-12-10. [95] B. Fang, M. Y. Özkaya, A. Li, Ü. V. Çatalyürek, and S. Krishnamoorthy, in 2022 IEEE International Conference on Cluster Computing (CLUSTER) (IEEE, 2022) pp. 289–300. [96] A. Nahum, S. Vijay, and J. Haah, Phys. Rev. X 8, 021014 (2018). [97] M. Larocca, S. Thanasilp, S."
  },
  {
    "prompt": "Explain a concept involving Sharma, Biamonte, Coles in computational complexity.",
    "response": "Wang, K. Sharma, J. Biamonte, P. J. Coles, L. Cincio, J. R. McClean, Z. Holmes, and M. Cerezo, arXiv preprint arXiv:2405.00781 (2024). [98] K. Kechedzhi, S. V. Isakov, S. Mandrà, B. Villalonga, X. Mi, S. Boixo, and V. Smelyanskiy, Future Generation Computer Systems 153, 431 (2024). [99] M. Vallero, F. Vella, and P. Rech, arXiv preprint arXiv:2401.06188 (2024). [100] F. Pan and P. Zhang, Phys. Rev. Lett. 128, 030501 (2022). [101] I. L. Markov and Y. Shi, SIAM Journal on Computing 38, 963 (2008)."
  },
  {
    "prompt": "This is a technical discussion about Quantum, Phys, Stoudenmire in theoretical computer science.",
    "response": "[102] J. Gray and S. Kourtis, Quantum 5, 410 (2021). [103] G. Vidal, Phys. Rev. Lett. 91, 147902 (2003). [104] J. Tindall, M. Fishman, E. M. Stoudenmire, and D. Sels, PRX Quantum 5, 010308 (2024). [105] A. Nahum, J. Ruhman, S. Vijay, and J. Haah, Phys. Rev. X 7, 031016 (2017). [106] S. Bravyi, D. Browne, P. Calpin, E. Campbell, D. Gosset, and M. Howard, Quantum 3, 181 (2019). [107] O. Reardon-Smith, M. Oszmaniec, and K. Korzekwa, Quantum 8, 1549 (2024). [108] Y. Zhou, E. M. Stoudenmire, and X."
  },
  {
    "prompt": "Scientific context on Phys, Rev, Waintal and their role in graph theory.",
    "response": "Waintal, Phys. Rev. X 10, 041038 (2020). [109] Y. Suzuki, S. Endo, K. Fujii, and Y. Tokunaga, PRX Quantum 3, 010345 (2022). [110] C. Piveteau, D. Sutter, S. Bravyi, J. M. Gambetta, and K. Temme, Phys. Rev. Lett. 127, 200505 (2021). [111] M. Lostaglio and A. Ciani, Phys. Rev. Lett. 127, 200506 (2021). [112] Y. Xiong, D. Chandra, S. X. Ng, and L. Hanzo, IEEE Access 8, 228967 (2020). [113] K. Tsubouchi, Y. Mitsuhashi, K. Sharma, and N. Yoshioka, arXiv preprint arXiv:2405.07720 (2024)."
  },
  {
    "prompt": "Read and understand this section about APPENDIX, Zhang, Reichardt from a research paper.",
    "response": "33 A APPENDIX [114] A. Zhang, H. Xie, Y. Gao, J.-N. Yang, Z. Bao, Z. Zhu, J. Chen, N. Wang, C. Zhang, J. Zhong, et al., arXiv preprint arXiv:2501.09079 (2025). [115] Qedma Quantum Computing, To appear (2025). [116] B. W. Reichardt, Quantum Science and Technology 6, 015007 (2020). [117] S. C. Smith, B. J. Brown, and S. D. Bartlett, Communications Physics 7, 386 (2024). [118] P. Prabhu and B. W. Reichardt, Phys. Rev. A 110, 012419 (2024). [119] L. Clinton, T. Cubitt, B. Flynn, F. M. Gambetta, J."
  },
  {
    "prompt": "Explain a concept involving Montanaro, Piddock, Santos in computational complexity.",
    "response": "Klassen, A. Montanaro, S. Piddock, R. A. Santos, and E. Sheridan, Nature Communications 15, 211 (2024). [120] Z. Zimborás, B. Koczor, Z. Holmes, E.-M. Borrelli, A. Gilyén, H.-Y. Huang, Z. Cai, A. Acín, L. Aolita, L. Banchi, et al., arXiv preprint arXiv:2501.05694 (2025)."
  },
  {
    "prompt": "Explain a concept involving Appendix, References, demonstrated in computational complexity.",
    "response": "A Appendix A.1 Known lower bounds on sampling overhead of general EM methods References [3, 4] demonstrated the existence of circuits for which the sample complexity of any EM protocol must be exponential in the total circuit volume nD/2, and not only in D. The construction is based on random circuits made of layers sampled from an n-qubit unitary 2-design."
  },
  {
    "prompt": "Explain a concept involving channel, error, local in computational complexity.",
    "response": "Assuming a fixed local error channel after each layer, and averaging over the above ensemble of circuits, leads to a twirling of the local error channel into an n-qubit depolarizing channel. The latter leads to a decay of the output state towards the maximally mixed state, with a decay constant that is proportional to the total circuit volume (this prototypical example is discussed in more detail in Sec. 4.1)."
  },
  {
    "prompt": "Scientific context on decay, exists, individual and their role in graph theory.",
    "response": "Accordingly, there exists an individual circuit in the ensemble which exhibits the same decay. Reference [4] showed that the same conclusion holds when constructing the above 2-designs from 2-qubit Clifford gates followed by a local error channel. By transpiling the above construction to QPUs with the connectivity of a d-dimensional lattice, Ref."
  },
  {
    "prompt": "Understanding the computational aspects of volume, circuit, ensemble.",
    "response": "[4] further showed compelling evidence that, for the above circuit ensemble, it is the active volume (or ‘light cone’) that controls the sampling overhead of EM, as opposed to the total circuit volume. Technically, the bounds in Ref. [4] are phrased in terms of the total volume, but are only valid for D large enough such that all n qubits can be entangled by the circuit, given the restricted circuit connectivity."
  },
  {
    "prompt": "Understanding the computational aspects of Deducing, Section, Ref.",
    "response": "A.2 Deducing exponential lower bounds on number of shots, from classical simulation of noisy circuits Here we turn back to the first question discussed in Section 4, and ask whether the results of Ref. [5] can be used to deduce the lower bound of Eq. (6) for generic EM protocols. As mentioned above, there are specific counter examples in which this bound does not hold [74]; however, one might expect the lower bound to still hold generically."
  },
  {
    "prompt": "Read and understand this section about Definition, circuit, focus from a research paper.",
    "response": "We focus on classically-hard circuit families In (see Definition 3.1) and assume that classical simulation algorithms for the given class of ideal circuits and observables In must scale as Ω(2n). We consider any EM protocol, which for any circuit and observable (C, O) ∈In, achieves an accuracy ϵ on average over computational input states. Using Eq. (8), we can then reverse the logic of Corollary 1, yielding a lower bound M = Ω(e˜Ω(γ2n)) for the shot overhead."
  },
  {
    "prompt": "Scientific context on protocol, applied, state and their role in graph theory.",
    "response": "This may be compared with the known result M = Ω(eΩ(γD)) for any EM protocol, applied to any (C, O), that achieves accuracy ϵ on a fixed input state [1–4]. In cases where n /∈O(D) the bound obtained from Ref. [5] is stronger (asymptotically, though with a weaker constant γ2 as opposed to γ). For example, in 2d circuits where n ∼D2, the sampling overhead scales exponentially in D2 rather than in D. Note that this still does not guarantee an overhead exponential in V as in Eq."
  },
  {
    "prompt": "This is a technical discussion about circuit, case, states in theoretical computer science.",
    "response": "(3) (which is, e.g., ∼D3 in the 2d circuit case). Interestingly, this still leaves open the 34 A APPENDIX following possibility (suggested in Ref. [74]): there may be a family with fine tuned input states, which are on one hand classically hard (namely their classical simulation must scale as Ω(2n)) for which there exists an EM protocol that exhibits a sampling overhead which is exponential only in D (and not in n) when restricted to those fine tuned input states."
  },
  {
    "prompt": "Read and understand this section about TEM, Fig., circuits from a research paper.",
    "response": "Such cases, if they exist, would lead to a quasi-polynomial QA (in between polynomial and exponential), such that, e.g., for 2d circuits, Tc = Ω(2n), but TEM = O(eγ√n). A.3 Calculating V = V (n, v) for the HPC run time plot in Fig. 2 The plots of the classical run time estimations for Fig. 2 require calculating the active volume V as a function of n and the operator spread velocity v. We detail here how these calculations are done to derive Eq. (20). Let us start with the simple case: v = 1."
  },
  {
    "prompt": "Read and understand this section about support, active, naïve from a research paper.",
    "response": "Here, the active volume corresponds to the “naïve” light-cone that is determined by the circuit connectivity. In such cases the qubit support of this lightcone (which is the number of active qubits), is n = \u0000 2 d · D \u0001d. The 2/d factor accounts for the fact that each layer extends the support only in one of the d dimensions, but in both the positive and negative directions along that dimension. Similarly, for a general v ∈[0, 1], the qubit support is given by n = \u00122v d D \u0013d ."
  },
  {
    "prompt": "Read and understand this section about volume, fixed, dimensional from a research paper.",
    "response": "(23) Once n is fixed, the active volume is given by the volume of a (d + 1)-dimensional pyramid with base area n and height D. We additionally divide by a factor of 2, to take into account that a dense layer on m qubits involves m/2 2-qubit gates. We thus derive: V = nD 2(d + 1) = cd 4vn1/cd, (24) where cd = d/(d + 1). This relation can be easily inverted to give n as a function of V : n = (4vV/cd)cd . (25) 35"
  },
  {
    "prompt": "Understanding the computational aspects of Science, Computer, Department.",
    "response": "Recognizing and Realizing Temporal Reachability Graphs Thomas Erlebach # \u001a Department of Computer Science, Durham University, UK Othon Michail # \u001a Department of Computer Science, University of Liverpool, UK Nils Morawietz # \u001a Friedrich Schiller University Jena, Institute of Computer Science, Germany LaBRI, Université de Bordeaux, France Abstract A temporal graph G = (G, λ) can be represented by an underlying graph G = (V, E) together with a function λ that assigns to each edge e ∈E the set of time steps during which e is present."
  },
  {
    "prompt": "This is a technical discussion about graph, reachability, temporal in theoretical computer science.",
    "response": "The reachability graph of G is the directed graph D = (V, A) with (u, v) ∈A if only if there is a temporal path from u to v. We study the Reachability Graph Realizability (RGR) problem that asks whether a given directed graph D = (V, A) is the reachability graph of some temporal graph. The question can be asked for undirected or directed temporal graphs, for reachability defined via strict or non-strict temporal paths, and with or without restrictions on λ (proper, simple, or happy)."
  },
  {
    "prompt": "This is a technical discussion about Casteigts, Answering, open in theoretical computer science.",
    "response": "Answering an open question posed by Casteigts et al. (Theoretical Computer Science 991 (2024)), we show that all variants of the problem are NP-complete, except for two variants that become trivial in the directed case. For undirected temporal graphs, we consider the complexity of the problem with respect to the solid graph, that is, the graph containing all edges that could potentially receive a label in any realization."
  },
  {
    "prompt": "Explain a concept involving solid, graph, RGR in computational complexity.",
    "response": "We show that the RGR problem is polynomial-time solvable if the solid graph is a tree and fixed-parameter tractable with respect to the feedback edge set number of the solid graph. As we show, the latter parameter can presumably not be replaced by smaller parameters like feedback vertex set or treedepth, since the problem is W[2]-hard with respect to these parameters."
  },
  {
    "prompt": "Explain a concept involving ACM, Design, FPT in computational complexity.",
    "response": "2012 ACM Subject Classification Theory of computation →Design and analysis of algorithms Keywords and phrases parameterized complexity, temporal graphs, FPT algorithm, feedback edge set, directed graph recognition 1 Introduction Temporal graphs are graphs whose edge set can change over time. The vertex set is often assumed to be fixed, and the edge set can differ from one time step to the next. The study of temporal graphs has attracted significant attention in recent years [4, 20, 21]."
  },
  {
    "prompt": "Read and understand this section about time, present, graph from a research paper.",
    "response": "A temporal graph G with vertex set V can be represented as a sequence (Gi)i∈[L], where Gi = (V, Ei) is the graph containing the edges that are present in time step i. An alternative way to represent G is to specify a graph G = (V, E) together with a labeling function λ : E →2[L] that maps each edge e ∈E to the (possibly empty) set of time steps in which e is present. We write G = (G, λ) in this case. If an edge e is present in time step i, we refer to (e, i) as a time edge."
  },
  {
    "prompt": "Understanding the computational aspects of time, edges, increasing.",
    "response": "A strict temporal path from u to v in G is a sequence of time edges such that the edges form a u-v-path in the underlying graph and the time steps are strictly increasing. A non-strict temporal path is defined analogously, except that the time steps only need to be non-decreasing."
  },
  {
    "prompt": "This is a technical discussion about temporal, graph, reachability in theoretical computer science.",
    "response": "Given a temporal graph G, the reachability relation (with respect to strict or non-strict temporal paths) can be represented as a directed graph D = (V, A), called the reachability graph, with (u, v) ∈A for u ̸= v if and only if there exists a (strict or non-strict, respectively) temporal path from u to v in G."
  },
  {
    "prompt": "Explain a concept involving Mar, Recognizing, Table in computational complexity.",
    "response": "Note that self-loops, which would represent arXiv:2503.15771v1 [cs.CC] 20 Mar 2025 2 Recognizing and Realizing Temporal Reachability Graphs Table 1 The complexity results for all variants of RGR. Red cells indicate NP-hard cases and green cells indicate cases that are trivial (and thus polynomial-time solvable)."
  },
  {
    "prompt": "Explain a concept involving Theorem, Proper, Happy in computational complexity.",
    "response": "Strict Non-strict Any Theorem 27 Theorem 29 Simple Theorem 27 Theorem 29 Proper Theorem 29 Happy Theorem 29 (a) undirected Strict Non-strict Any Lemma 64 and [10] Theorem 67 Simple Lemma 64 and [10] Theorem 67 Proper Theorem 67 Happy Theorem 67 (b) directed the trivial reachability from a vertex to itself, are omitted from D."
  },
  {
    "prompt": "Read and understand this section about graphs, graph, directed from a research paper.",
    "response": "A natural question is then which directed graphs D = (V, A) can arise as reachability graphs of temporal graphs, and how difficult it is to determine for a given directed graph D = (V, A) whether there exists a temporal graph G with reachability graph D. This question was posed as an open problem by Casteigts et al. [3, Open question 5]. In this paper, we answer this open question by showing that this decision problem is NP-hard."
  },
  {
    "prompt": "Read and understand this section about temporal, simple, proper from a research paper.",
    "response": "Actually, there are a number of variations of the question, as we may ask for an undirected or a directed temporal graph G, for a temporal graph with a restricted kind of labeling (simple, proper, happy), and may consider reachability with respect to strict or non-strict temporal paths."
  },
  {
    "prompt": "Scientific context on temporal, graph, variations and their role in graph theory.",
    "response": "We show that all these variations are NP-hard if we ask for an undirected temporal graph, and also if we ask for a directed temporal graph except for two variations (strict temporal paths with arbitrary or simple labelings) that are known to become trivial in the directed case [10]. See Table 1 for an overview of these complexity results. From the positive side, we present the following algorithmic results."
  },
  {
    "prompt": "Scientific context on solid, graph, edge and their role in graph theory.",
    "response": "For a given digraph D = (V, A), we refer to {u, v} for u, v ∈V as a solid edge if both (u, v) and (v, u) are in A. Let G = (V, E) be the undirected graph on V whose edge set is the set of solid edges. We refer to G as the solid graph of D. We show that all undirected problem variants can be solved in polynomial time if the solid graph is a tree. Furthermore, we give an FPT algorithm with respect to the feedback edge set number of the solid graph."
  },
  {
    "prompt": "Understanding the computational aspects of treedepth, set, pathwidth.",
    "response": "This parameter can presumably not be replaced by smaller parameters like feedback vertex set, treedepth, or pathwidth, since two undirected versions of our problem turn out to be W[2]-hard for these parameters. Related work Casteigts et al."
  },
  {
    "prompt": "Explain a concept involving temporal, graphs, reachability in computational complexity.",
    "response": "[3] studied the relationships between the classes of reachability graphs that arise from undirected temporal graphs if different restrictions are placed on the graph (proper, simple, happy [3]) and depending on whether strict or non-strict temporal paths are considered. They showed that reachability with respect to strict temporal paths in arbitrary temporal graphs yields the widest class of reachability graphs while reachability in happy temporal graphs yields the narrowest class."
  },
  {
    "prompt": "Scientific context on graphs, class, temporal and their role in graph theory.",
    "response": "The class of reachability graphs that arise from proper temporal graphs is the same as for non-strict paths in arbitrary temporal graphs, and this class is larger than the class of reachability graphs arising from non-strict reachability in simple temporal graphs. Strict reachability in simple temporal graphs was also shown to lie between the happy case and the general strict case."
  },
  {
    "prompt": "Read and understand this section about simple, non-strict, graphs from a research paper.",
    "response": "Döring [10] showed that the class “strict & simple” is incomparable to “non-strict & simple” and to “non-strict / proper”, completing the picture of a two-stranded hierarchy for undirected temporal graphs. She also extended the study to directed temporal graphs and showed that their classes of reachability graphs form a single-stranded hierarchy from happy to strict & simple, the latter being equivalent to the general strict case. Thomas Erlebach, Othon Michail, Nils Morawietz 3 Casteigts et al."
  },
  {
    "prompt": "Explain a concept involving graphs, temporal, reachability in computational complexity.",
    "response": "[3] posed the open question whether there is a characterization of the directed graphs that arise as reachability graphs of temporal graphs (or of some restricted subclass of temporal graphs), and how hard it is to decide whether a given directed graph is the reachability graph of some temporal graph. These questions were posed again by Döring [10], also in relation to the setting of directed temporal graphs. This is in particular of interest because Casteigts et al."
  },
  {
    "prompt": "Understanding the computational aspects of FPT, showed, temporal.",
    "response": "[5] showed that several temporal graph problems can be solved in FPT time with respect to temporal parameters defined over the reachability graph. In this paper we resolve these open questions regarding the complexity of all directed and undirected variants. Besides that, our work falls into the field of temporal graph realization problems."
  },
  {
    "prompt": "Explain a concept involving temporal, graph, behavior in computational complexity.",
    "response": "In these problems, one is given some data about the behavior of a temporal graph, and the goal is to detect whether there actually is a temporal graph with this behavior (and to compute such a temporal graph if one exists). Klobas et al."
  },
  {
    "prompt": "Understanding the computational aspects of period, matrix, fastest.",
    "response": "[16] introduced the problem of deciding for a given matrix of fastest travel durations and a period ∆whether there exists a simple temporal graph G with period ∆with the property that the duration of the fastest temporal path between any pair of nodes in G is equal to the value specified in the input matrix. Erlebach et al. [12] extended the problem to a multi-label version, assuming that the input specifies a bound ℓon the maximum number of labels per edge."
  },
  {
    "prompt": "Explain a concept involving Mertzios, problem, temporal in computational complexity.",
    "response": "Motivated by the constraints in the design of transportation networks, Mertzios et al. [19] considered the modified version of the problem (with one label per edge) where the fastest temporal path between any pair of nodes is only upper-bounded by the value specified in the input matrix. The problem of generating a temporal graph realizing a given reachability graph is an instance of the more general class of temporal network design problem."
  },
  {
    "prompt": "This is a technical discussion about measures, problems, objective in theoretical computer science.",
    "response": "In such problems, the objective is to construct a temporal network that satisfies specified constraints while optimizing certain network-quality measures. These problems have natural applications in transportation and logistics, communication networks, social networks, and epidemiology. In an early example studied by Kempe et al."
  },
  {
    "prompt": "Read and understand this section about set, temporal, edge from a research paper.",
    "response": "[14], the goal was to reconstruct a temporal labeling restricted to a single label per edge, so that a designated root reaches via temporal paths all vertices in a set P while avoiding those in a set N. For multi-labeled temporal graphs, Mertzios et al."
  },
  {
    "prompt": "Scientific context on number, temporal, graph and their role in graph theory.",
    "response": "[18] studied the problem of designing a temporal graph that preserves all reachabilities or paths of an underlying static graph while minimizing either the temporality (the maximum number of labels per edge) or the temporal cost (the total number of labels used). Göbel et al. [13] showed that it is NP-complete to decide whether the edges of a given undirected graph can be labeled with a single label per edge in such a way that each vertex can reach every other vertex via a strict temporal path."
  },
  {
    "prompt": "This is a technical discussion about deletions, motivated, edge in theoretical computer science.",
    "response": "Other studies have focused on variants of minimizing edge deletions [11], vertex deletions [24], or edge delays [8] to restrict reachability, motivated, for instance, by epidemic containment strategies that limit interactions. Temporal network design is an active area of research, with further related questions explored for example in [1, 15, 6]. For general introductory texts to the area of dynamic networks the reader is referred to [4, 20, 21]. Organization of the paper."
  },
  {
    "prompt": "Explain a concept involving Section, problem, provide in computational complexity.",
    "response": "In Section 2 we provide a formal problem definition and define the notions used in this work. In Section 3, we show upper and lower bounds for the required number of labels per edge in any realization and provide a single exponential algorithm for all problem variants. Afterwards, in Section 4, we analyze properties and define splitting operations based on bridge edges in the solid graph of the undirected versions of our problem."
  },
  {
    "prompt": "Understanding the computational aspects of Section, Recognizing, FPT.",
    "response": "These structural insights will be mainly used in our FPT algorithm in Section 7, but also 4 Recognizing and Realizing Temporal Reachability Graphs immediately let us describe a polynomial-time algorithm for instances where the solid graph is a tree in Section 5. In Section 6, we then provide NP-hardness results for all undirected problem versions as well as parameterized intractability results for two of them with respect to feedback vertex set number and treedepth."
  },
  {
    "prompt": "This is a technical discussion about Section, FPT, result in theoretical computer science.",
    "response": "Afterwards, in Section 7, we provide our main algorithmic result: an FPT algorithm with respect to the feedback edge set number fes of the solid graph."
  },
  {
    "prompt": "Understanding the computational aspects of Firstly, Section, fes.",
    "response": "This algorithm is achieved in three steps: Firstly, we apply our splitting operations of Section 4 and provide a polynomial-time reduction rule to simplify the instance at hand, such that all we need to deal with is a subset X∗of vertices of size O(fes) for which the remainder of the graph decomposes into edge-disjoint trees that only interact with X∗via two leaves each. We call such trees connector trees."
  },
  {
    "prompt": "Explain a concept involving set, fes, show in computational complexity.",
    "response": "Secondly, we show that we can efficiently extend the set X∗to a set W ∗of size O(fes) such that each respective connector tree is more or less independent from the remainder of the graph with respect to the interactions of temporal paths in any realization. All these preprocessing steps run in polynomial time. Afterwards, our algorithm enumerates all reasonable labelings on the edges incident with vertices of W ∗and tries to extend each such labeling to a realization for D."
  },
  {
    "prompt": "Explain a concept involving FPT, show, checked in computational complexity.",
    "response": "As we show, there are only FPT many such reasonable labelings and for each such labeling, there are only FPT many possible extensions that need to be checked. Finally, in Section 8 we briefly discuss the directed version of the problem and provide NP-hardness results for all but the two trivial cases. 2 Preliminaries For definitions on parameterized complexity, the Exponential Time Hypothesis (ETH), or parameters like treedepth, we refer to the textbooks [9, 7]."
  },
  {
    "prompt": "Explain a concept involving set, write, edge in computational complexity.",
    "response": "For natural numbers i, j with i ≤j we write [i] for the set {1, 2, 3, . . . , i} and [i, j] for the set {i, i + 1, i + 2, . . . , j}. For an undirected graph G = (V, E), we denote an edge between vertices u and v as {u, v} or uv. By N(v) = {u ∈V | vu ∈E} we denote the set of neighbors of u. The degree of v is the size of N(v). An edge e ∈E is a bridge or bridge edge in a graph G = (V, E) if deleting e increases the number of connected components."
  },
  {
    "prompt": "Explain a concept involving pendant, called, directed in computational complexity.",
    "response": "A vertex of degree 1 is called a pendant vertex, and an edge incident with a pendant vertex is called a pendant edge. For directed graphs, we denote an arc from u to v by (u, v). A directed graph D is simple if it has no parallel arcs and no self-loops, and it is a directed acyclic graph (DAG) if it does not contain a directed cycle. The in-degree of a vertex v in D is the number of incoming arcs of v, i.e., the number of arcs with head v."
  },
  {
    "prompt": "This is a technical discussion about vertex, Similarly, out-degree in theoretical computer science.",
    "response": "Similarly, the out-degree of a vertex v is the number of arcs with tail v. The degree of a vertex in D is then the sum of its in-degree and out-degree. A temporal graph G with vertex set V and lifetime L is given by a sequence (Gt)t∈[L] of L static graphs Gt = (V, Et) referred to as snapshots or layers. The graph G↓= (V, E↓) with E↓= S i∈[L] Ei is called the underlying graph of G."
  },
  {
    "prompt": "Read and understand this section about edges, Alternatively, possibly from a research paper.",
    "response": "Alternatively, G can be represented by an undirected graph G = (V, E) with E ⊇S t∈[L] Et and a labeling function λ : E →2[1,L] that assigns to each edge e the (possibly empty) set of time steps during which e is present, i.e., λ(e) = {t ∈[L] | e ∈Et}. We write G = (G, λ) in this case. In this representation we allow G to contain extra edges in addition to the edges of the underlying graph; such edges e satisfy λ(e) = ∅."
  },
  {
    "prompt": "Scientific context on graph, paper, problems and their role in graph theory.",
    "response": "This is useful because in the problems we consider in this paper, there is a natural choice of a graph G = (V, E), the so-called solid graph defined below, that contains all edges of the underlying graph of every realization but may contain additional edges. We assume that each layer of a temporal graph is an undirected graph unless we explicitly Thomas Erlebach, Othon Michail, Nils Morawietz 5 refer to directed temporal graphs. If t ∈λ(e), we refer to (e, t) as a time edge."
  },
  {
    "prompt": "Scientific context on sequence, time, ℓsuch and their role in graph theory.",
    "response": "A temporal path from u to v in G is a sequence of time edges ((ej, tj))j∈[ℓ] for some ℓsuch that (e1, e2, . . . , eℓ) is a u-v path in G and t1 ≤t2 ≤· · · ≤tℓholds in the non-strict case and t1 < t2 < · · · < tℓ holds in the strict case. A temporal graph is proper if no two adjacent edges share a label, simple if every edge has a single label, and happy if it is both proper and simple [3]."
  },
  {
    "prompt": "Read and understand this section about temporal, graph, Note from a research paper.",
    "response": "Note that for proper and happy temporal graphs there is no distinction between strict and non-strict temporal paths. The reachability graph R(G) of a temporal graph G with vertex set V is the directed graph (V, A) with the same vertex set V and (u, v) ∈A if and only if u ̸= v and G contains a temporal path from u to v. Note that R(G) depends on whether we consider strict or non-strict temporal paths and can be computed in polynomial time in both cases [2]."
  },
  {
    "prompt": "Understanding the computational aspects of Input, Graph, Reachability.",
    "response": "We are interested in the following problem: Reachability Graph Realizability (RGR) Input: A simple directed graph D = (V, A). Question: Does there exist a temporal graph G with R(G) = D? For yes-instances of RGR, we are also interested in computing a temporal graph G with R(G) = D. We refer to such a temporal graph as a solution or a realization for D, and we typically represent it by a labeling function."
  },
  {
    "prompt": "This is a technical discussion about Duv, mind, temporal in theoretical computer science.",
    "response": "With the adjacency matrix representation of D in mind, we also write Duv = 1 for (u, v) ∈A and Duv = 0 for (u, v) /∈A. We can consider RGR with respect to reachability via strict temporal paths or with respect to non-strict temporal paths. Furthermore, we can require the realization of D to be simple, proper, or happy. For proper and happy temporal graphs, strict and non-strict reachability coincide."
  },
  {
    "prompt": "Read and understand this section about RGR, Simple, Strict from a research paper.",
    "response": "Therefore, the distinct problem variants that we can consider are Any Strict RGR, Any Non-strict RGR, Simple Strict RGR, Simple Non-strict RGR, Proper RGR, and Happy RGR. Finally, we write DRGR instead of RGR if we are asking for a directed temporal graph that realizes D. We sometimes write URGR if we want to make it explicit that we are asking for an undirected realization. The solid graph. If Duv = 1 and Dvu = 1 for some u ̸= v, we say that there is a solid edge between u and v."
  },
  {
    "prompt": "Scientific context on Duv, Dvu, solid and their role in graph theory.",
    "response": "If Duv = 1 and Dvu = 0, we say that there is a dashed arc from u to v. We use G = (V, E) to denote the graph on V whose edge set is the set of solid edges, and we refer to this graph as the solid graph (of D). It is clear that only solid edges can receive labels in a realization of D, as the two endpoints of an edge that is present in at least one time step can reach each other."
  },
  {
    "prompt": "This is a technical discussion about Solid, receive, Observation in theoretical computer science.",
    "response": "Solid edges that are bridges of the solid graph must receive labels, but a solid edge e that is not a bridge need not necessarily receive labels, as the endpoints of e could reach each other via temporal paths of length greater than one in a realization. ▶Observation 1. Let D be an instance of URGR. Let λ be a realization of D and let {u, v} and {v, w} be solid edges that both receive at least one label under λ."
  },
  {
    "prompt": "This is a technical discussion about URGR, arc, Observation in theoretical computer science.",
    "response": "If D contains neither the arc (u, w) nor the arc (w, u), then there is a label α such that λ({u, v}) = λ({v, w}) = {α}. Note that Observation 1 implies that for Proper URGR, Happy URGR, and Non-strict URGR, no realization for D can assign labels to both of two adjacent edges {u, v} and {v, w} if D contains neither (u, w) nor (w, u). A path P = (u0 = u, u1, u2, . . . , uℓ= v) from u to v in G is a dense u-v-path if there exist dashed arcs (ui, uj) for all 0 ≤i < j ≤ℓ."
  },
  {
    "prompt": "This is a technical discussion about path, dense, temporal in theoretical computer science.",
    "response": "Note that in each undirected realization of D, each temporal path is a dense path in G. Similarly, for directed realizations of D, each temporal path is a dense path in D, where we define dense paths in D analogously to dense paths in G. 6 Recognizing and Realizing Temporal Reachability Graphs We say that a realization λ is frugal if there is no edge e such that the set of labels assigned to e can be replaced by a smaller set while maintaining the property that λ is a realization."
  },
  {
    "prompt": "Explain a concept involving minimal, realization, edge in computational complexity.",
    "response": "We say that a realization λ is minimal on edge e if it is impossible to obtain another realization by replacing λ(e) with a proper subset. A realization is minimal if it is minimal on every edge e. Note that every frugal realization is also minimal. 3 Basic Observations and an Exponential Algorithm We first provide upper and lower bounds for the number of labels per edge in a minimal realization. ▶Lemma 2."
  },
  {
    "prompt": "Explain a concept involving URGR, minimal, Strict in computational complexity.",
    "response": "Let D be an instance of URGR, and let e be a solid edge of G that is not part of a triangle. In each minimal realization of D, e receives at most two labels. Moreover, if D is an instance of any version of URGR besides Any Strict URGR, then in each minimal realization, e receives at most one label. Proof. First, consider Any Strict URGR. Consider a minimal realization and assume the edge e has at least three labels."
  },
  {
    "prompt": "Explain a concept involving label, smallest, largest in computational complexity.",
    "response": "Let Le, Ue, and Me be its smallest label, its largest label, and an arbitrary label in between. Note that no solid edge f ̸= e incident with an endpoint of e can have a label in (Le, Ue) as this would imply a solid edge forming a triangle containing e. Hence, all other edges incident with endpoints of e only have labels ≤Le or ≥Ue."
  },
  {
    "prompt": "Understanding the computational aspects of URGR, proper, original.",
    "response": "Replacing the labels of e by the single label Me maintains exactly the temporal paths containing e that exist before the change, a contradiction to the original realization being minimal. Furthermore, if the original labeling was proper, than the resulting labeling is also proper. For Simple Strict URGR, Simple Non-strict URGR and Happy URGR, it is trivially true that e has at most one label in any realization. Now, consider Any Non-strict URGR and Proper URGR."
  },
  {
    "prompt": "Explain a concept involving label, solid, edge in computational complexity.",
    "response": "Assume that e has at least two labels in a minimal realization, and let Le and Ue be the smallest and largest label of e. Note that no solid edge f ̸= e incident with an endpoint of e can have a label in [Le, Ue]: A label in (Le, Ue) would create a solid edge forming a triangle, f cannot share a label with e in a proper labeling, and f sharing a label with e would create a solid edge forming a triangle in the non-strict case."
  },
  {
    "prompt": "Understanding the computational aspects of labels, incident, endpoints.",
    "response": "Thus, all other edges incident with endpoints of e only have labels < Le or > Ue. Replacing the labels of e by Le maintains exactly the temporal paths containing e that exist before the change, a contradiction to the original realization being minimal. ◀ For general edges, we show a linear upper bound with respect to the number of vertices. ▶Lemma 3. If a graph D = (V, A) is realizable, then each minimal realization for D assigns at most n = |V | labels per edge. Proof."
  },
  {
    "prompt": "Understanding the computational aspects of edge, vertex, labels.",
    "response": "Let e be an edge with more than n labels. We show that at least one can be removed. For each vertex v define αv as the earliest time step in which vertex v can traverse the edge e (if any). Let β be a label of e that is unequal to all αv labels. Then, removing β from e preserves at least one temporal (x, y)-path, if such a path existed before. ◀ Note that this implies that all versions of URGR and DRGR under consideration are in NP."
  },
  {
    "prompt": "Scientific context on URGR, Strict, Theorem and their role in graph theory.",
    "response": "Next, we show that this bound on the number of labels per edge is essentially tight for Any Strict URGR. Thomas Erlebach, Othon Michail, Nils Morawietz 7 ▶Theorem 4. For Any Strict URGR, there is an infinite family of directed graphs B, such that for every D = (V, A) ∈B, where G = (V, E) is the solid graph of D, all of the following properties hold: G is planar. G has a feedback vertex set of size 2 and a feedback edge set of size Θ(n), where n = |V |."
  },
  {
    "prompt": "Explain a concept involving labels, realizable, realization in computational complexity.",
    "response": "D is realizable and there is an edge e ∈E such that every realization of D uses Ω(n) labels on e. Proof. The graphs of solid edges of the reachability graphs in B resemble books with vertices around the sides of their pages. The spine of the book consists of an edge u0u′ 0 joining an upper spine-vertex u0 and a lower spine-vertex u′ 0."
  },
  {
    "prompt": "Scientific context on upper, lower, vertices and their role in graph theory.",
    "response": "Every page has an index 1 ≤i ≤L, where L ≥5 is the total number of pages, and consists of two upper vertices, ui and vi, and two lower vertices, u′ i and v′ i, located at the middle and end of the upper and lower boundaries, respectively. We call ui and u′ i the inner and vi and v′ i the outer vertices of page i."
  },
  {
    "prompt": "Scientific context on page, edges, uivi and their role in graph theory.",
    "response": "The set of edges of page i consists of edges u0ui, uivi, viv′ i, u′ iv′ i, and u′ 0u′ i, around the sides of the page, as well as two additional diagonal edges, u0u′ i and u′ 0ui, between the spine and the inner vertices. We then modify the constructed graph by making pages 1, 2, L −1 and L incomplete: For i = 1, 2, page i does not contain the lower-side vertices u′ i and v′ i, and the diagonal between ui and u′ 0 is a dashed arc (ui, u′ 0) instead of a solid edge."
  },
  {
    "prompt": "Scientific context on page, arc, edge and their role in graph theory.",
    "response": "For i = L −1, L, page i does not contain the upper-side vertices ui and vi, and the diagonal between u′ i and u0 is a dashed arc (u0, u′ i) instead of a solid edge."
  },
  {
    "prompt": "Read and understand this section about vertex, upper, lower from a research paper.",
    "response": "Furthermore, there is a dashed arc from every inner vertex of a page i to every same-side (i.e., upper to upper and lower to lower) inner vertex of a page j if and only if i < j and both vertices exist, and there is a dashed arc to every opposite-side (i.e., upper to lower and lower to upper) inner vertex of a page k if and only if i + 1 < k and the target vertex exists. All remaining ordered pairs of vertices (u, v) are specified as unreachable by D (i.e., (u, v) is not a dashed arc)."
  },
  {
    "prompt": "Understanding the computational aspects of Figure, solid, edges.",
    "response": "See Figure 1(a). Note that n = 4(L −4) + 2 · 4 + 2 = 4L −6 and the solid graph has m = 7(L −4) + 2 · 4 + 1 = 7L −19 edges. u0 uk v′ k u′ k u′ 0 u′ j v′ j v′ i uj vj vi ui vk u′ i i < j < k (a) u0 u′ 0 ui vi u′ i v′ i 1 ≤i ≤L (b) Figure 1 (a) An example of the graph of solid edges and dashed arcs for a D ∈B restricted to three pages. (b) A planar drawing of the solid edges. We start by proving the last property, fixing the edge u0u′ 0 of the spine as the edge required by it."
  },
  {
    "prompt": "This is a technical discussion about labels, edges, distinct in theoretical computer science.",
    "response": "We will show that u0u′ 0 must have at least L −2 = n/4 −1/2 = Θ(n) labels. 8 Recognizing and Realizing Temporal Reachability Graphs We claim that no two edges of a page i can have distinct labels, so edges of page i can be labeled by at most one label, which we denote by li. The two upper edges cannot have distinct labels, otherwise a missing arc between u0 and vi would be realized. The same holds for the two lower edges."
  },
  {
    "prompt": "Understanding the computational aspects of edges, Similarly, viv′.",
    "response": "Similarly, viv′ i and the diagonal edges cannot have a label different from that of the upper and lower edges, otherwise a missing arc would be realized. This implies that the only way to realize the arcs of viv′ i is to label it by li. Edges uivi and u′ iv′ i must also be labeled by li as any other path to realize their arcs would need to use two consecutive edges of page i. For i = 1, 2, both upper edges of page i must be labeled as they are bridges."
  },
  {
    "prompt": "Explain a concept involving labeled, edges, reason in computational complexity.",
    "response": "For i = L −1, L, both lower edges of page i must be labeled for the same reason. For 3 ≤i ≤L −2, at least one of the edges uiu0 and uiu′ 0 must be labeled, and at least one of the edges u′ iu′ 0 and u′ iu0 must be labeled, as the arcs between ui or u′ i and u0 and u′ 0 cannot be realized otherwise. We will show that u0ui and u′ 0u′ i receive a label (which must be equal to li) while the diagonals u0u′ i and u′ 0ui do not receive a label."
  },
  {
    "prompt": "Understanding the computational aspects of label, observe, realized.",
    "response": "First, observe that lj > li must hold for any j > i, otherwise the arc (ui, uj) or (u′ i, u′ j) cannot be realized. Now, assume for a contradiction that there is a page i with 3 ≤i ≤L −2 in which a diagonal receives a label. Consider the smallest such i. Observe that u0u′ i cannot receive label li as this would realize an arc (ui−1, u′ i). Thus, u′ iu′ 0 must receive label li."
  },
  {
    "prompt": "Scientific context on path, realized, temporal and their role in graph theory.",
    "response": "Furthermore, uiu0 must also receive label li as the arc (ui−1, ui) must be realized via the path ui−1, u0, ui; it cannot be realized via a temporal path ending with the edge u′ 0ui as the existence of such a path would imply a temporal path from ui−1 to u′ i, a contradiction. Finally, edge uiu′ 0 cannot receive a label; otherwise, ui can reach both u0 and u′ 0 at time li and therefore reach u′ i+1 at time li+1, a contradiction."
  },
  {
    "prompt": "Understanding the computational aspects of vertices, pages, shows.",
    "response": "This shows that it is impossible that a diagonal edge in page i receives a label. For every two distinct pages i and j such that i < j, the arcs from the inner vertices of i to the same-side inner vertices of j imply that u0ui, u′ 0u′ i and u0uj, u′ 0u′ j must be labeled by li and lj, respectively, and that li < lj as observed above. Thus, the pages must have increasing labels according to the order of their indices."
  },
  {
    "prompt": "This is a technical discussion about page, spine, exists in theoretical computer science.",
    "response": "For every page 1 ≤i ≤L −2, in order to realize the arc (ui, u′ i+2) and/or (u′ i, ui+2) (for every i at least one of these two arcs exists), the spine edge u0u′ 0 must have a label l such that li < l < li+2. Furthermore, that label must be equal to li+1, as li < l < li+1 would create a temporal path from an inner vertex of page i to an opposite-side inner vertex of page i + 1, and li+1 < l < li+2 can be excluded analogously. Thus, the spine edge must receive labels l2, l3, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of page, labels, label.",
    "response": ", lL−1, a total of L −2 distinct labels. The remaining part of the property follows by observing that the labeling assigning label i to all non-diagonal edges of page i, for all 1 ≤i ≤L, and labels 2, 3, 4, . . . , L −2, L −1 to the spine, realizes D. For all 1 < i + 1 < j ≤L, all arcs from the inner vertices of a side of page i to the opposite-side inner vertices of page j are realized by using label i + 1 of the spine. These temporal paths also realize the arcs (ui, u′ 0) for all i = 1, 2, . ."
  },
  {
    "prompt": "Scientific context on arcs, pages, realized and their role in graph theory.",
    "response": ". L −2, (u′ 0, ui) for i = 3, . . . , L −2, (u0, u′ i) for i = 3, . . . , L, and the arcs (u′ i, u0) for i = 3 . . . , L −2. Thus, all solid diagonals in pages 3, . . . , L −2 and the diagonal arcs in the remaining levels are realized. For same-side inner vertices, the temporal paths realizing them go directly through the respective spine-vertex."
  },
  {
    "prompt": "Scientific context on vertices, dashed, arc and their role in graph theory.",
    "response": "As required, no dashed arc between the outer vertices and the rest of the graph is realized as all these paths go through inner vertices, and no two edges incident to an inner vertex use different labels. For all 1 ≤i < j ≤L, no dashed arc from the inner vertices of page j to those of page i is realized because j > i. For the first property we draw the book pages as nested trapezoids with their area increasing by increasing index, and having the spine as their shared basis (see Figure 1(b))."
  },
  {
    "prompt": "Explain a concept involving diagonals, face, drawn in computational complexity.",
    "response": "This gives a planar drawing of all edges but the diagonals. For all 1 ≤i ≤L−1, the diagonals u′ iu0 and ui+1u′ 0 can be drawn on the face between pages i and i + 1 without crossing each Thomas Erlebach, Othon Michail, Nils Morawietz 9 other. Diagonals u1u′ 0 and u′ Lu0 can be drawn on the face formed by page 1 and on the outer face, respectively."
  },
  {
    "prompt": "Read and understand this section about planar, drawing, vertices from a research paper.",
    "response": "This completes a planar drawing of a supergraph of G in which all L pages are complete; the graph G (in which some vertices and edges have been removed from pages 1, 2, L −1, L) thus also admits a planar drawing. The spine vertices form a feedback vertex set of size 2. The union of the diagonal edges and the edges joining the outer vertices of pages 3 to L −2 forms a feedback edge set of size 3(L −4) = 3n/4 −15/2 and every feedback edge set has size at least (7L −19) −(n −1) = 3n/4 −15/2 = Θ(n)."
  },
  {
    "prompt": "Scientific context on holds, property, Theorem and their role in graph theory.",
    "response": "Thus, the second property holds. ◀ We now present single exponential algorithms for all directed and undirected version of RGR under consideration based on our upper bounds on the number of assigned labels per edge/arc. ▶Theorem 5. Each version of URGR and DRGR under consideration can be solved in 2O(|A|) · nO(1) time, where A denotes the arc set of the input graph. Proof. We describe a dynamic program."
  },
  {
    "prompt": "Scientific context on Strict, URGR, program and their role in graph theory.",
    "response": "We state the program only explicitly for Any Strict URGR and afterwards argue how the dynamic program can be adapted to the other problem variants considered. Let D = (V, A) be an instance of Any Strict URGR and let n := |V |. Moreover, let E denote the set of solid edges of D and let m := |E|. Due to Lemma 3, there is a realization for D if and only if there is a realization for D with lifetime at most m · n."
  },
  {
    "prompt": "Scientific context on entry, subset, dynamic and their role in graph theory.",
    "response": "For each subset A′ ⊆A and each i ∈[0, m · n], our dynamic programming table T has an entry T[A′, i]. This entry stores whether there is a temporal graph G with lifetime at most i, such that the strict reachability graph of G is D′ := (V, A′). For i = 0, we set T[∅, 0] = 1 and T[A′, 0] = 0 for each nonempty A′ ⊆A. For each i ∈[1, m · n] and each A′ ⊆A, we set T[A′, i] = ∃A′′ ⊆A′.∃S ⊆E.T[A′ \\ A′′, i −1] ∧(∀(u, v) ∈A′′.∃w.(w = u ∨(u, w) ∈A′ \\ A′′) ∧w reaches v via an edge of S) ∧(∀(u, v) /∈A′."
  },
  {
    "prompt": "Read and understand this section about Intuitively, graph, reachability from a research paper.",
    "response": "̸ ∃w.(w = u ∨(u, w) ∈A′ \\ A′′) ∧w reaches v via an edge of S) Intuitively, the recurrence resembles splitting the reachability of arcs A′ into two sets A′′ and A′ \\ A′′. We recursively check for the existence of a temporal graph G′′ of lifetime at most i −1 with strict reachability graph equal to (V, A′ \\ A′′), and if such a temporal graph exists, we append a new snapshot to the end with edges S."
  },
  {
    "prompt": "Scientific context on realized, introducing, snapshot and their role in graph theory.",
    "response": "The second and the third line of the recurrence then ensure that (i) all arcs of A′ \\ A′′ are realized by introducing this new snapshot i, and that (ii) no arcs outside of A′ are realized by introducing this new snapshot i. We omit the formal correctness proof and proceed with the evaluation of the dynamic program. The graph D is realizable if and only if T[A, n · m] = 1."
  },
  {
    "prompt": "Scientific context on time, table, entries and their role in graph theory.",
    "response": "Since there are 2|A| · n · m table entries and each such entry can be computed in 2|A| · 2|E| · nO(1) ⊆2O(|A|) · nO(1) time, the whole algorithm runs in 2O(|A|) · nO(1) time. This completes the proof for Any Strict URGR. Moreover, note that a corresponding realization (if one exists) can be found in the same time via traceback. If we consider proper labelings, we can modify the recurrence to only allowed to take subsets of edges S ⊆E that are matchings."
  },
  {
    "prompt": "Explain a concept involving modify, edges, non-strict in computational complexity.",
    "response": "If we consider non-strict paths, we can modify the recurrence to ask whether w reaches v via a path using only edges of S instead of only a single edge of S. 10 Recognizing and Realizing Temporal Reachability Graphs If we consider simple labelings, we can modify the dynamic program by a third dimen- sion E′ ⊆E which stores the edges of E we may possibly assign labels to."
  },
  {
    "prompt": "Scientific context on subset, recurrence, call and their role in graph theory.",
    "response": "In the recurrence, we then have to ensure that S is only a subset of E′ and also remove S from the allowed edges in the recursive call, that is, we have to replace T[A′ \\ A′′, i −1] by T[A′ \\ A′′, i −1, E′ \\ S]. For the versions of DRGRunder consideration, we do essentially the same, except that S has to be a subset of A. If we consider proper labelings, instead of picking a matching for S, we must pick a set of arcs that do not induce a path of length more than one."
  },
  {
    "prompt": "Understanding the computational aspects of labelings, case, simple.",
    "response": "If we consider simple labelings, we do essentially the same as in the undirected case. We extend the table by a third dimension that stores the arcs A∗of A we may possibly assign labels to. In the recurrence, we then have to ensure that S is only a subset of A∗. ◀ 4 Solid Bridge Edges: Properties and Splitting Operations In this section, we show several structural results as well as three splitting operations for graphs with bridge edges in the solid graph."
  },
  {
    "prompt": "This is a technical discussion about FPT, URGR, solid in theoretical computer science.",
    "response": "These insights will be important for our FPT algorithm and will also simplify instances of URGR where the solid graph is a tree. Consider a bridge edge e = {u, v} whose deletion splits the solid graph G into connected components Gu and Gv, where Gu contains u and Gv contains v. A dashed arc (a, b) spans e if a ∈V (Gu) and b ∈V (Gv) or vice versa. Two arcs (a, b) and (c, d) span e in the same direction if they span e and either a, c ∈V (Gu) or a, c ∈V (Gv)."
  },
  {
    "prompt": "Read and understand this section about dashed, arcs, direction from a research paper.",
    "response": "If e has a single label in a realization, then it must be the case that the dashed arcs spanning e are “transitive” in the following sense: If dashed arcs (a, b) and (c, d) both span e in the same direction, then (a, d) and (c, b) must also be dashed arcs. This is because for any two temporal paths that pass through e in the same direction, the part of one path up to edge e can be combined with the part of the other path after e. ▶Definition 6."
  },
  {
    "prompt": "This is a technical discussion about edge, bridge, Dub in theoretical computer science.",
    "response": "Consider a bridge edge e = {u, v} whose deletion splits the solid graph G into connected components Gu and Gv. The edge e is a special bridge edge if there exist vertices a in Gu and b in Gv such that Dav = 1, Dub = 1 and Dab = 0 (or if the same condition holds with u and v exchanged). A bridge edge that is not special is called non-special."
  },
  {
    "prompt": "Understanding the computational aspects of bridge, special, edge.",
    "response": "Intuitively, a special bridge edge is a bridge edge e such that there are dashed arcs spanning e in the same direction for which transitivity (as outlined above) is violated. We also refer to special bridge edges as special bridges, special solid edges, or just as special edges. ▶Lemma 7. In each frugal realization of D, every special bridge edge is assigned two labels and every non-special bridge edge is assigned a single label. Proof. Let λ be a frugal realization."
  },
  {
    "prompt": "Explain a concept involving edge, labels, bridge in computational complexity.",
    "response": "Consider a bridge edge e = {u, v}. It is clear that e must receive at least one label in λ, as otherwise the endpoints of e cannot reach each other. As a bridge edge is not contained in a triangle, it receives at most two labels in λ by Lemma 2. It is clear that every special edge must receive two labels as otherwise the violation of transitivity that happens on a special edge would be impossible. Assume that e = {u, v} is a non-special edge that has received two labels x and y with x < y."
  },
  {
    "prompt": "Scientific context on produces, frugal, label and their role in graph theory.",
    "response": "We claim that replacing the double label of e by the new label (x + y)/2 produces a new solution λ′ that also realizes D, a contradiction to λ being frugal. (The possibly half-integral label (x+y)/2 can be made integral in the end by replacing all labels by integers while maintaining their order.) Thomas Erlebach, Othon Michail, Nils Morawietz 11 Consider strict temporal paths first. Note again that the edges f ̸= e incident with endpoints of e can only have labels ≤x or ≥y."
  },
  {
    "prompt": "Explain a concept involving reachability, step, reach in computational complexity.",
    "response": "We consider the reachability from Gu to Gv. (The arguments for the reachability from Gv to Gu are analogous.) Case 1: If every a ∈V (Gu) can reach v in step x in (G, λ), then it can reach v in step (x + y)/2 in (G, λ′). The vertices in V (Gv) that u can reach starting in time step x in λ are precisely the vertices in V (Gv) that can be reached starting in time step (x+y)/2 in λ′, and hence the reachability from Gu to Gv is the same in (G, λ′) as in (G, λ)."
  },
  {
    "prompt": "Read and understand this section about reach, step, time from a research paper.",
    "response": "Case 2: If there exists a ∈V (Gu) that can reach v only in step y in (G, λ), then every vertex b in V (Gv) that can be reached from u in (G, λ) must be reachable starting from u in time step y in (G, λ) (otherwise a would not be able to reach b, making e a special edge). All vertices from V (Gu) that can reach v in (G, λ) can reach v in (G, λ′) in time step (x + y)/2."
  },
  {
    "prompt": "Scientific context on time, starting, step and their role in graph theory.",
    "response": "Furthermore, the vertices in V (Gv) that u can reach starting in time step x in (G, λ) (which are the same as those that can be reached starting at u in time step y in (G, λ)) are precisely the vertices in V (Gv) that can be reached starting at u in time step (x + y)/2 in (G, λ′). Hence, the reachability from V (Gu) to V (Gv) is the same in (G, λ′) as in (G, λ). Repeating such replacements then yields a valid solution in which all non-special edges have a single label."
  },
  {
    "prompt": "Understanding the computational aspects of proper, labeling, URGR.",
    "response": "The same approach works for non-strict temporal graphs (where the labels of edges f ̸= e incident with endpoints of e must be < x or > y in (G, λ)). Furthermore, if the original labeling is proper, then the final labeling is also proper. ◀ Note that this implies the following due to Lemma 2. ▶Corollary 8. Let D be an instance of any version of URGR under consideration besides Any Strict URGR. If the solid graph of D contains a special solid edge, then D is a no-instance."
  },
  {
    "prompt": "Scientific context on share, label, strict and their role in graph theory.",
    "response": "We are now interested in when a pair of bridge edges that share an endpoint must, must not, or may share a label. For the non-strict problem variants (and thus also the happy and proper cases for strict paths), adjacent bridges obviously cannot share a label. The following definition captures the condition for when adjacent bridges must share a label in the other cases. ▶Definition 9. Consider Any Strict URGR or Simple Strict URGR. Let D be an instance of URGR and let G be the solid graph of D."
  },
  {
    "prompt": "This is a technical discussion about denote, set, vertices in theoretical computer science.",
    "response": "Let {u, c} and {v, c} be bridges in G, and let Vu (Vv) denote the set of vertices in the same connected component as u (v) in G −{{u, c}} (G −{{v, c}}). We say that {u, c} and {v, c} are bundled, if (u, v) /∈A and (v, u) /∈A, or at least one of the two edges is special with respect to D[Vu ∪Vv ∪{c}]. Note that the second condition of Definition 9 cannot occur in a yes-instance of Simple Strict URGR. ▶Lemma 10. In every realization, each pair of bundled bridges shares a label. Proof."
  },
  {
    "prompt": "Understanding the computational aspects of bridges, bundled, edges.",
    "response": "Consider bridges {u, c} and {v, c} that are bundled. If (u, v) /∈A and (v, u) /∈A, both edges must receive the same single label. Assume for the rest of the proof that one of the two arcs, say the arc (u, v) is in A (and hence (v, u) /∈A) and at least one of the two edges {u, c} and {v, c} is special. This can only happen in the Any Strict URGR model. The labels on {c, v} must be at least as large as the largest label on {c, u}, as v cannot reach u."
  },
  {
    "prompt": "Read and understand this section about Dab, special, Dub from a research paper.",
    "response": "As at least one of the two edges {u, c} and {v, c} is special in G[Vu ∪Vv ∪{c}] by Definition 9, there must exist vertices a ∈Vu and b ∈Vv such that Dac = Dub = 1 and 12 Recognizing and Realizing Temporal Reachability Graphs Dab = 0 if {u, c} is special or such that Dav = Dcb = 1 and Dab = 0 if {v, c} is special."
  },
  {
    "prompt": "Scientific context on Dvy, Dcy, path and their role in graph theory.",
    "response": "(It cannot be that {u, c} or {v, c} is special because there are vertices x ∈Vv and y ∈Vu with Dxc = Dvy = 1 or Dxu = Dcy = 1 and Dxy = 0; this would contradict the absence of a temporal path from v to u.) If {u, c} and {v, c} did not share a label, any temporal path from a to c could be concatenated with any temporal path from c to b, contradicting Dab = 0. Hence, the two edges must share a label. ◀ Next, we characterize pairs of adjacent bridges that must not share a label in any realization."
  },
  {
    "prompt": "This is a technical discussion about Definition, URGR, instance in theoretical computer science.",
    "response": "▶Definition 11. Let D be an instance of URGR and let G be the solid graph of D. Let {u, c} and {v, c} be bridges in G."
  },
  {
    "prompt": "Explain a concept involving URGR, special, non-special in computational complexity.",
    "response": "We say that {u, c} and {v, c} are separated, if we consider Proper URGR, Happy URGR, or Non-strict URGR, we consider Simple Strict URGR and A contains an arc of {(u, v), (v, u)}, one arc of {(u, v), (v, u)} is contained in A and neither of the bridges {u, c} and {v, c} is special, there are w1, w2 ∈N(c) such that {c, w1} and {c, w2} are also bridges and (u, w1, w2, v) or (v, w1, w2, u) is a path in D, or there is w ∈N(c) such that {w, c} is also a bridge and (u, w, v) (or (v, w, u)) is a path in D such that (i) {u, c} is non-special, (ii) {w, c} is special, or (iii) {v, c} is non-special."
  },
  {
    "prompt": "This is a technical discussion about Lemma, label, bridges in theoretical computer science.",
    "response": "▶Lemma 12. In every frugal realization, each pair of separated bridges shares no label. Proof. If we consider non-strict paths, adjacent bridges cannot share a label because a shared label would imply a triangle containing at least one of these bridges. If we consider proper or happy labeling, no two adjacent edges can share a label."
  },
  {
    "prompt": "Explain a concept involving label, Simple, Strict in computational complexity.",
    "response": "If we consider Simple Strict URGR and A contains, say, the arc (u, v), then the label of {c, v} must be larger than the label of {c, u} because each edge receives at most one label and the two edges must form a temporal path from u to v. If neither of the bridges {u, c} and {v, c} is special and A contains one of the arcs (u, v) or (v, u) (note that it cannot contain both), the same argument as in the previous paragraph shows that the single labels of the two bridges cannot be the same."
  },
  {
    "prompt": "Read and understand this section about case, path, exist from a research paper.",
    "response": "Now consider the case that there exist w1, w2 ∈N(c) such that {c, w1} and {c, w2} are also bridges and (u, w1, w2, v) is a path in D. (The argument for the case that (v, w1, w2, u) is a path in D is analogous.) As the four edges from c to u, w1, w2, v are all bridges, there cannot be any solid edge among the vertices u, w1, w2, v. Thus, there are dashed arcs (u, w1), (w1, w2) and (w2, v). Let x1 = u, x2 = w1, x3 = w2, and x4 = v. Let λ be a frugal realization."
  },
  {
    "prompt": "This is a technical discussion about hxi, ℓxi, min in theoretical computer science.",
    "response": "For i ∈[4], let ℓxi = min λ({c, xi}) and hxi = max λ({c, xi}). We have ℓxi < hxi+1 for i ∈[3] as (xi, xi+1) ∈A and hxi ≤ℓxi+1 as (xi+1, xi) /∈A. Thus, we have hx1 ≤ℓx2 < hx3 ≤ℓx4, showing that hx1 < ℓx4 and hence {c, x1} and {c, x4} cannot share a label. Finally, consider the case that there is w ∈N(c) such that {w, c} is also a bridge and (u, w, v) is a path in D such that (i) {u, c} is non-special, (ii) {w, c} is special, or (iii) {v, c} is non-special."
  },
  {
    "prompt": "This is a technical discussion about analogous, argument, case in theoretical computer science.",
    "response": "(The argument for the case that (v, w, u) is a path in D is analogous.) Again, there cannot be any solid edges among the vertices u, w, v and thus there are dashed arcs (u, w) and (w, v). Let x1 = u, x2 = w and x3 = v and let λ be a frugal realization, and define ℓxi and hxi for i ∈[3] as above. We have ℓx1 ≤hx1 ≤ℓx2 ≤hx2 ≤ℓx3 ≤hx3 and ℓxi < hxi+1 for i ∈[2]. If {u, c} = {x1, c} is non-special, we have hx1 = ℓx1 < hx2 ≤ℓx3. If {w, c} = {x2, c} is special, we have hx1 ≤ℓx2 < hx2 ≤ℓx3."
  },
  {
    "prompt": "Explain a concept involving non-special, Lemmas, Corollary in computational complexity.",
    "response": "If {v, c} = {x3, c} is non-special, we have hx3 = ℓx3 > ℓx2 ≥hx1. In all three cases, we have hx1 < ℓx3, showing that {c, x1} and {c, x3} cannot share a label. ◀ Thomas Erlebach, Othon Michail, Nils Morawietz 13 Lemmas 10 and 12, thus imply the following. ▶Corollary 13. Let e and e′ be adjacent bridges that are both bundled and separated in D. Then D is not realizable."
  },
  {
    "prompt": "Read and understand this section about Subinstances, Splitting, instance from a research paper.",
    "response": "4.1 Splitting Into Subinstances Next, we show that bridge edges in the solid graph can be used to split an instance into smaller subinstances that can be solved independently and have the property that the original instance is realizable if and only if the two subinstances are realizable. First, consider non-special bridges. ▶Lemma 14 (Splitting at a non-special bridge). Consider any variant of URGR."
  },
  {
    "prompt": "Understanding the computational aspects of subinstances, realizable, edge.",
    "response": "Let e = {u, v} be a non-special bridge edge, and let Gu and Gv be the connected components of G resulting from the deletion of e. Then the instance D is realizable if and only if the subinstances induced by V (Gu) ∪{v} and V (Gv) ∪{u} are realizable. Proof. Let Du and Dv denote the subinstances induced by V (Gu) ∪{v} and V (Gv) ∪{u}, respectively. If D is realizable, then the realization λ induces realizations of the two subinstances Du and Dv."
  },
  {
    "prompt": "This is a technical discussion about realizations, label, realizable in theoretical computer science.",
    "response": "If Du and Dv are realizable, their realizations assign a single label to e, and these realizations can be chosen so that e receives the same label in both realizations. Hence, the union of the two realizations is a realization of D. ◀ By applying Lemma 14 repeatedly to a non-special bridge edge that is not pendant until no such edge exists, we obtain subinstances in which all non-special bridge are pendant. Now consider special bridges."
  },
  {
    "prompt": "Explain a concept involving URGR, Strict, Corollary in computational complexity.",
    "response": "Special bridge edges cannot occur in yes-instances of any variant of URGR except Any Strict URGR (see Corollary 8), so we only consider Any Strict URGR in the following. Let edge e = {u, v} be a special bridge with Gu and Gv defined as above."
  },
  {
    "prompt": "Read and understand this section about reach, time, realizable from a research paper.",
    "response": "If the instance is realizable, for every vertex in V (Gu) there are at most three possibilities for which vertices in V (Gv) it can reach, depending on whether it cannot reach v, can reach v at time α, or can reach v only at time β, where α and β with α < β are the labels assigned to e in the realization. The same holds with Gu and Gv exchanged. If this condition is violated, then the instance cannot be realized. The following definition captures this condition."
  },
  {
    "prompt": "This is a technical discussion about analogously, define, vertex in theoretical computer science.",
    "response": "For any vertex w ∈V (Gu), let Ru→v(w) = {x ∈V (Gv) | (w, x) ∈A}, and define Rv→u(x) for x ∈V (Gv) analogously. ▶Definition 15. Let e = {u, v} be a special bridge edge with Gu and Gv defined as above. We say that e is a special bridge edge with plausible reachability if the following conditions hold: If there exist vertices a ∈V (Gu) and b ∈V (Gv) such that Dav = Dub = 1 and Dab = 0, then every vertex w ∈V (Gu) satisfies Ru→v(w) ∈{∅, Ru→v(a), Ru→v(u)} with ∅⊂ Ru→v(a) ⊂Ru→v(u)."
  },
  {
    "prompt": "This is a technical discussion about vertex, satisfies, Lemma in theoretical computer science.",
    "response": "Otherwise, every vertex satisfies Ru→v(w) ∈{∅, Ru→v(u)}. The same condition holds with the roles of Gu and Gv exchanged. ▶Lemma 16 (Splitting at a special bridge). Consider Any Strict URGR. Let e = {u, v} be a special bridge edge with plausible reachability, and let Gu and Gv be the connected components of G resulting from the deletion of e."
  },
  {
    "prompt": "Understanding the computational aspects of Dub, realizable, attach.",
    "response": "Then the instance D is realizable if and only if the two subinstances constructed as follows are realizable: To construct subinstance Du, take the subgraph D′ of D induced by V (Gu) ∪{v} and attach one or two leaves to v as follows: 14 Recognizing and Realizing Temporal Reachability Graphs If there exist vertices a ∈V (Gu) and b ∈V (Gv) such that Dav = Dub = 1 and Dab = 0, then attach a leaf (called out-leaf) z to v with D′ zv = 1 and D′ az = 1 for all a ∈{v} ∪{w ∈V (Gu) | Ru→v(w) = Ru→v(u)}, and all other entries of D′ involving z equal to 0."
  },
  {
    "prompt": "Explain a concept involving Dvd, Dcu, Dcd in computational complexity.",
    "response": "If there exist vertices c ∈V (Gv) and d ∈V (Gu) such that Dcu = Dvd = 1 and Dcd = 0, then attach a leaf (called in-leaf) z′ to v with D′ vz′ = 1 and D′ z′a = 1 for all a ∈{v} ∪Rv→u(c) and D′ z′z = 1 if an out-leaf z has been added, and all other entries of D′ involving z′ equal to 0. Note that at least one of the two conditions above must be satisfied because e is a special bridge edge. The resulting instance D′ is the desired subinstance Du."
  },
  {
    "prompt": "Read and understand this section about analogously, exchanged, constructed from a research paper.",
    "response": "Subinstance Dv is constructed analogously, with the roles of u and v exchanged. Proof. Observe that e is a special bridge edge also in both subinstances Du and Dv. Further- more Du has an out-leaf if and only if Dv has an in-leaf, and vice versa. If D is realizable, then a realization of Du can be obtained as follows: Let λ be a frugal realization of D."
  },
  {
    "prompt": "Read and understand this section about set, restriction, edges from a research paper.",
    "response": "Let λu be the restriction of λ to edges in the subgraph of G induced by V (Gu) ∪{v}, and let α, β with α < β be the two labels assigned to e by λu. If Du has an in-leaf z, set λu(vz) = α. If Du has an out-leaf z′, set λ(vz′) = β. We claim that λu is a realization of Du: It definitely realizes the reachability between all vertices in V (Gu) ∪{v}, so we only need to consider the in- and/or out-leaf attached to v."
  },
  {
    "prompt": "Scientific context on vertices, reach, desired and their role in graph theory.",
    "response": "If v has an out-leaf z, then the vertices in V (Gu) that can reach z are exactly those that can reach v at time α, as desired. If v has an in-leaf z′, then the vertices in V (Gu) that z′ can reach are exactly the vertices that can be reached from v if traversing the edge vu at time β, as desired. Hence, λu is a realization of Du. A realization λv of Dv can be obtained analogously. Now assume that Du and Dv have realizations λu and λv, respectively."
  },
  {
    "prompt": "This is a technical discussion about time, empty, steps in theoretical computer science.",
    "response": "By shifting labels and inserting empty time steps if necessary, we can assume that both labelings assign the same two labels α, β with α < β to uv (by possibly introducing empty time steps in both realizations). Consider Du and λu. If Du has an in-leaf z, the label of the non-special bridge edge vz must equal α, because z must reach some vertices in V (Gu) but not all vertices that v can reach."
  },
  {
    "prompt": "Explain a concept involving reach, out-leaf, vertices in computational complexity.",
    "response": "If Du has an out-leaf z′, the label of the non-special bridge edge vz′ must equal β as not all vertices from V (Gu) that can reach v can reach z′. We make the following modification to λu: If Du has no out-leaf and at least one edge f ̸= e incident with u receives label α, we reduce all labels ≤α on edges of Gu by one. This ensures that all vertices in V (Gu) that can reach v can reach v at time α, without changing any reachabilities."
  },
  {
    "prompt": "Scientific context on reached, in-leaf, incident and their role in graph theory.",
    "response": "If Du has no in-leaf and at least one edge f ̸= e incident with u that has label β, we increase all labels ≥β on edges of Gu by one. This ensures that all vertices in V (Gu) that can be reached from v can still be reached from v if traversing edge vu at time β."
  },
  {
    "prompt": "Understanding the computational aspects of reach, paths, temporal.",
    "response": "Intuitively, these modifications ensure that temporal paths leaving Gu can reach v at time α (if there is no out-leaf) and temporal paths entering Gu can reach all vertices in Gu that v can reach even if these paths reach u only at time β (if there is no in-leaf). We apply the same modification to λv. Now let λ be the union of λu and λv. We claim that λ is a realization of D."
  },
  {
    "prompt": "Read and understand this section about reach, Reachability, Case from a research paper.",
    "response": "Reachability requirements within V (Gu) ∪{v} and V (Gv) ∪{u} are clearly satisfied because λu and λv realize Du and Dv, respectively. Consider an arbitrary a ∈V (Gu). Case 1: Du has no out-leaf. If Ru→v(a) = ∅, then a cannot reach v in λ as it cannot reach v in λu. If Ru→v(a) = Ru→v(u), then a can reach v at time α in λu and can hence reach all vertices in Ru→v(u) in λ. Case 2: Du has an out-leaf z. If Ru→v(a) = ∅, then a cannot reach v in λ as it cannot reach v in λu."
  },
  {
    "prompt": "This is a technical discussion about reach, time, vertices in theoretical computer science.",
    "response": "If Ru→v(a) = Ru→v(u), then a can reach v at time α in λu as otherwise it wouldn’t be able to reach z. Hence, it can reach all vertices in Ru→v(u) Thomas Erlebach, Othon Michail, Nils Morawietz 15 in λ. If ∅̸= Ru→v(a) ⊂Ru→v(u), then a can reach v only at time β in λu as otherwise it would be able to reach z, a contradiction to the definition of Du. Hence, in λ it can reach all vertices in V (Gv) that can be reached after traversing the edge uv from u to v at time β."
  },
  {
    "prompt": "Scientific context on definition, vertices, reachability and their role in graph theory.",
    "response": "By definition of Dv, these are the vertices in V (Gv) that are reachable from the in-leaf z′′ of Dv, and by definition of Dv that set of vertices is Ru→v(c), where c is an arbitrary vertex in V (Gu) with ∅̸= Ru→v(c) ⊂Ru→v(u). As the special edge has plausible reachability, Ru→v(c) = Ru→v(a). Therefore, reachability from Gu to Gv is realized correctly by λ. The arguments for reachability from Gv to Gu are analogous."
  },
  {
    "prompt": "Understanding the computational aspects of Finally, pendant, case.",
    "response": "◀ Finally, we consider the case of two pendant edges that must receive the same single label in any realization and argue that we can remove one of them from the instance provided their reachability requirements from/to the rest of the graph are the same. ▶Lemma 17 (Removal of redundant pendant vertices). Let v and w be two vertices of degree 1 that are both adjacent to the same vertex u in G and satisfy (v, w) /∈A and (w, v) /∈A."
  },
  {
    "prompt": "Read and understand this section about URGR, Non-strict, Proper from a research paper.",
    "response": "For all variants of Non-strict URGR (including Proper URGR and Happy URGR), the instance is a no-instance. For Any Strict URGR and Any Simple URGR, we have that if Dvx = Dwx and Dxv = Dxw hold for all x ∈V \\ {v, w}, then D is realizable if and only if the subinstance D′ resulting from D by deleting v is realizable; otherwise, D is a no-instance. Proof."
  },
  {
    "prompt": "This is a technical discussion about URGR, Non-strict, endpoints in theoretical computer science.",
    "response": "For all variants of Non-strict URGR, it is impossible to have adjacent bridge edges without a temporal path between their endpoints different from the common endpoint. Therefore, consider Any Strict URGR and Any Simple URGR for the remainder of the proof. We know that uv and uw must receive the same single label in any realization and thus must reach the same set of vertices in V \\ {v, w} and be reached from the same set of vertices in V \\ {v, w}."
  },
  {
    "prompt": "Understanding the computational aspects of solution, no-instance, reachability.",
    "response": "If they have different reachability, it is clear that D is a no-instance. If their reachability is the same, the claim can be shown as follows: A solution to D clearly induces a solution to D′. A solution to D′ can be extended into a solution to D by assigning uv the label that uw has received in the solution to D′."
  },
  {
    "prompt": "Explain a concept involving Solid, Graph, Tree in computational complexity.",
    "response": "◀ 5 Algorithms for Instances with a Tree as Solid Graph Since all edges of trees are bridges, we now describe, based on our insights about labels in frugal realizations, an algorithm for instances of URGR where the solid graph is a tree. ▶Lemma 18. If an instance of URGR that has a tree as solid graph is realizable, then there is a frugal realization such that each special bridge receives two consecutive labels. Proof. By Lemma 2, we can assume that no edge has more than two labels."
  },
  {
    "prompt": "Read and understand this section about labels, edge, label from a research paper.",
    "response": "Assume that an edge e has two labels a and b with b > a + 1. Every edge adjacent to e either only has labels ≤a or only has labels ≥b, as an adjacent edge with a label strictly between a and b or with a label ≤a and a label ≥b would create a triangle. Hence, the gap between a and b can be eliminated: In the stars centered at the endpoints of e, shift all labels ≤a by adding b −a −1."
  },
  {
    "prompt": "Explain a concept involving labels, shifts, shift in computational complexity.",
    "response": "Then, propagate the shifts to the remainder of the tree as follows: If the labels on an edge f ̸= e in one of the stars centered at endpoints of e were shifted by b −a + 1, apply that same shift to all labels on all edges in the subtree reachable over f. This modification maintains a valid solution and reduces the number of edges with two labels that have a gap between them by at least 1. Repeating this modification yields the claim of the lemma."
  },
  {
    "prompt": "Scientific context on tree, internal, Recognizing and their role in graph theory.",
    "response": "◀ Let an instance D be given such that the graph of solid edges is a tree. For any internal node of the tree, we refer to the subgraph induced by the node and its neighbors as an 16 Recognizing and Realizing Temporal Reachability Graphs induced star. Because we can apply the splitting rule of Lemma 14 to any internal tree edge that is not special, it suffices to consider instances in which all internal tree edges are special."
  },
  {
    "prompt": "This is a technical discussion about Lemma, tree, apply in theoretical computer science.",
    "response": "Furthermore, we can apply Lemma 17 to reduce the problem to a tree in which there are no two nodes u and w adjacent to the same vertex v such that both (u, w) /∈A and (w, u) /∈A. To see this, note that if neither (u, w) nor (w, u) are in A, then Observation 1 together with Lemma 7 implies that vu and vw are non-special bridges and hence leaves of the tree, so Lemma 17 can indeed be used to eliminate one of u and w."
  },
  {
    "prompt": "Explain a concept involving graph, orientation, instance in computational complexity.",
    "response": "For every induced star in the resulting instance, the auxiliary graph of dashed arcs between its leaves is an orientation of a complete graph. Furthermore, this orientation is acyclic, as a directed cycle on the neighbors u1, u2, . . . , uk of v, with ℓi = min λ(vui) and hi = max λ(vui) for 1 ≤i ≤k, would imply that ℓ1 < h2 ≤ℓ3 ≤h3 ≤· · · ≤ℓk ≤hk ≤ℓ1, a contradiction."
  },
  {
    "prompt": "Understanding the computational aspects of DAG, tournament, realizable.",
    "response": "Therefore, the auxiliary graph of dashed arcs between the leaves of the induced star is a complete DAG (also called an acyclic tournament) if the instance is realizable. The following definition captures the properties of the resulting simplified instances. ▶Definition 19."
  },
  {
    "prompt": "Understanding the computational aspects of DAG, RGR, edge.",
    "response": "An instance of RGR is a simplified tree instance if the solid edges form a tree where every internal edge is special, every pendant edge is non-special, and the auxiliary graph of dashed arcs between the leaves of each induced star is a complete DAG."
  },
  {
    "prompt": "This is a technical discussion about instance, simplified, original in theoretical computer science.",
    "response": "In order to solve the problem on an instance D such that the solid graph of solid edges is a tree, we can split the instance into simplified tree instances in such a way that the original instance is realizable if and only if all the resulting simplified instances are realizable. The total size of the resulting simplified instances is polynomial in the size of the original instance as every edge of the solid graph of the original instance occurs in at most two of the simplified instances."
  },
  {
    "prompt": "Explain a concept involving lemma, time, realization in computational complexity.",
    "response": "Furthermore, the splitting operations can be applied in polynomial time. The discussion can be summarized in the following lemma. ▶Lemma 20. There is a polynomial-time algorithm for the realization problem for instances where the solid edges form a tree if and only if there is a polynomial-time algorithm for the realization problem for simplified tree instances. ▶Observation 21. Let λ be an undirected realization of a directed graph D."
  },
  {
    "prompt": "Understanding the computational aspects of label, edges, common.",
    "response": "No three special bridges sharing one common endpoint share a label. Proof. Let λ be an undirected realization for D. Assume three solid bridges that share a common endpoint have the common label p. There must exist two edges whose labels are p and a label greater than p, or two edges whose labels are p and a label smaller than p. In both cases, there must be a solid edge between the non-shared endpoints of these edges, a contradiction to the assumption that all edges are bridges. ◀ ▶Lemma 22."
  },
  {
    "prompt": "Explain a concept involving uvi, bundled, leaves in computational complexity.",
    "response": "Consider an induced star in a simplified tree instance with center node u and leaves v1, . . . , vk indexed in topological order. If the instance is realizable, the following must hold: If uvi and uvi+2 are bundled for some i ∈[k −2], then uvi and uvi+1 as well as uvi+1 and uvi+2 are also bundled. Proof. If uvi and uvi+2 are bundled, then uvi and uvi+2 must be special, uvi+1 must be non-special, and all three edges share a label γ in any realization."
  },
  {
    "prompt": "Understanding the computational aspects of Vvi, uvi, Definition.",
    "response": "By Definition 9 one of the two edges uvi and uvi+2 must be special in G[Vvi ∪Vvi+2 ∪{u}]. Assume that uvi is special. Thus, there must be a vertex x in Vvi and a vertex y in Vvi+2 such that Dxu = Dviy = 1 and Dxy = 0. But then we also have Dxu = Dvivi+1 = 1 and Dxvi+1 = 0, and hence uvi is also special in G[Vvi ∪Vvi+1 ∪{u}], and thus uvi and uvi+1 are bundled."
  },
  {
    "prompt": "Scientific context on Vvi, Dvi, Duy and their role in graph theory.",
    "response": "Similarly, we have Dvi+1vi+2 = Duy = 1 but Dvi+1y = 0, and hence uvi+2 is also special in G[Vvi+1 ∪Vvi+2 ∪{u}], Thomas Erlebach, Othon Michail, Nils Morawietz 17 and thus uvi+1 and uvi+2 are bundled. The case that uvi+2 is special in G[Vvi ∪Vvi+2 ∪{u}] can be handled analogously. ◀ We now give an algorithm for solving instances whose solid edges form a tree. ▶Theorem 23. There is a polynomial-time algorithm for solving instances of Any Strict URGR for which the solid edges form a tree. Proof."
  },
  {
    "prompt": "This is a technical discussion about Lemma, instances, star in theoretical computer science.",
    "response": "By Lemma 20, it suffices to consider simplified tree instances. In particular, the auxiliary graphs of dashed arcs for all induced stars are complete DAGs. We compute a labeling for every star and merge the labelings in DFS order starting with an arbitrary star. Consider the computation of the labeling for the star centered at a node u. We know that every special edge incident with u receives two consecutive labels and every non-special edge receives a single label."
  },
  {
    "prompt": "This is a technical discussion about DAG, edge, leaves in theoretical computer science.",
    "response": "Furthermore, we know that the dashed arcs between the leaves of the star form a complete DAG. Denote the leaves by v1, v2, . . . , vk in topological order. For 1 ≤i ≤k, call the edge uvi a double edge if it is special and a single edge otherwise. We label the edges in order from uv1 to uvk. Initially, we assign uv1 the label 1 if it is a single edge and the labels 1, 2 if it is a double edge. For i = 2, 3, . . ."
  },
  {
    "prompt": "This is a technical discussion about uvi, edge, double in theoretical computer science.",
    "response": ", k, distinguish the following cases: Case 1: uvi−1 is a double edge labeled a −1, a, uvi is a single edge, and uvi+1 is a single edge or i = k. If there is a vertex w that can reach u via uvi−1 but cannot reach vi (which implies that uvi−1 and uvi are bundled), assign label a to uvi. Otherwise, assign label a + 1 to uvi. Case 2: uvi−1 is a double edge labeled a −1, a, uvi is a single edge, and uvi+1 is a double edge."
  },
  {
    "prompt": "Read and understand this section about uvi, reach, Lemma from a research paper.",
    "response": "If there is a vertex w that can reach u via uvi−1 but cannot reach vi or cannot reach all vertices that u can reach via vi+1 (implying that uvi−1 and uvi are bundled, cf. Lemma 22), assign label a to uvi. Otherwise, assign label a + 1 to uvi. Case 3: uvi−1 is a single edge labeled a, and uvi is a single edge. Label uvi with label a + 1. Case 4: uvi−1 is a single edge labeled a, and uvi is a double edge."
  },
  {
    "prompt": "Scientific context on uvi, assign, labels and their role in graph theory.",
    "response": "If there is a vertex w that can be reached from u via the edge uvi but cannot be reached from vi−1 (implying that uvi−1 and uvi are bundled), assign labels a, a + 1 to uvi. Otherwise, assign labels a + 1, a + 2 to uvi. Case 5: uvi−1 is a double edge labeled a −1, a, and uvi is a double edge. If there is a vertex that can reach u via uvi−1 but cannot reach all vertices that u can reach via uvi (implying that uvi−1 and uvi are bundled), assign labels a, a + 1 to uvi."
  },
  {
    "prompt": "This is a technical discussion about labels, star, assign in theoretical computer science.",
    "response": "Otherwise, assign labels a + 1, a + 2 to uvi. The resulting solutions for the stars can be merged by starting with an arbitrary star and always adding an adjacent star, shifting the labels of that star so that the labels on the edge common to the solution so far and the new star are identical. If the resulting labeling realizes D, we output it. Otherwise, we report that the given instance is a non-instance."
  },
  {
    "prompt": "Understanding the computational aspects of algorithm, correct, prove.",
    "response": "To prove that the algorithm is correct, we show that an arbitrary realization can be transformed into the one produced by the algorithm. Let λ∗be a frugal labeling that realizes D and assigns two consecutive labels to each special bridge edge. Such a labeling exists by Lemma 18. Consider a star S with center u and leaves v1, . . . , vk indexed in topological order. For i < j, it is clear that in λ∗no label of uvj can be smaller than any label of uvi, as vi must not be reachable from vj."
  },
  {
    "prompt": "Explain a concept involving label, edges, labels in computational complexity.",
    "response": "Besides, no two single edges can have the same label. Furthermore, we can assume that there are no gaps in the labels assigned to edges in S, as such gaps could be removed (and the labels of the other stars shifted accordingly). If edges to consecutive leaves do not share a label, we therefore have that the lower label of the second edge equals the 18 Recognizing and Realizing Temporal Reachability Graphs higher label of the first edge plus one."
  },
  {
    "prompt": "This is a technical discussion about edge, uvi, label in theoretical computer science.",
    "response": "The only possibilities for pairs of edges to consecutive leaves to share a label a are: A single edge uvi with label a and a double edge uvi+1 with labels a, a + 1, for some a. A double edge uvi with labels a −1, a and a single edge uvi+1 with label a, for some i. A double edge uvi with labels a −1, a and a double edge uvi+1 with labels a, a + 1. Furthermore, a label a can be shared by edges to at most three consecutive leaves, and by Observation 21 at least one of them must be a single edge."
  },
  {
    "prompt": "This is a technical discussion about edge, DAG, single in theoretical computer science.",
    "response": "It is also impossible that two of them are single edges (as the auxiliary graph is a complete DAG). Furthermore, the single edge must be the middle edge (otherwise it wouldn’t be possible to share the label with both of the double edges), and so the only possibility for three edges to share a label is: A double edge uvi−1 with labels a −1, a. A single edge uvi with label a. A double edge uvi+1 with labels a, a + 1."
  },
  {
    "prompt": "Explain a concept involving reach, vertices, uvj in computational complexity.",
    "response": "Two edges uvi and uvj with i < j must share a label if the edges are bundled, which is the case if there exists a vertex that can reach u via uvi but cannot reach all vertices that u can reach via uvj. If there is no such vertex, assigning disjoint sets of labels to uvi and uvj is valid with respect to the vertices involved as all vertices that can reach u via uvi can then definitely reach all vertices that u can reach via uvj, as desired."
  },
  {
    "prompt": "Read and understand this section about bundled, edges, share from a research paper.",
    "response": "The algorithm ensures that all edges that are bundled (must share labels) do share a label, and assigns disjoint sets of labels to consecutive edges otherwise."
  },
  {
    "prompt": "Explain a concept involving uvi, share, reach in computational complexity.",
    "response": "If λ∗differs from the labeling produced by the algorithm this can only happen if λ∗lets consecutive edges uvi and uvi+1 share a label even though every vertex that can reach u via uvi can reach every vertex that u can reach via uvi+1 and there is also no indirect requirement (stemming from a requirement that uvi and uvi+2 must share a label, or that uvi−1 and uvi+1 must share a label) for uvi and uvi+1 to share a label. In such a case, we can shift all the labels on uvi+1, uvi+2, . . ."
  },
  {
    "prompt": "Scientific context on tree, uvk, propagating and their role in graph theory.",
    "response": ", uvk by +1 (and propagating the shifted labels to the rest of the tree) without affecting feasibility of λ∗. Repeating this operation makes λ∗identical to the labeling produced by the algorithm. ◀ The following lemma implies an alternative method for solving the problem of realizing D in polynomial time in the case where the solid edges form a tree. This method can be generalized to the case where some edges of the tree are pre-labeled, which will be useful later. ▶Lemma 24."
  },
  {
    "prompt": "Understanding the computational aspects of Strict, URGR, realizable.",
    "response": "Let D be an instance of Any Strict URGR on n vertices in which the solid edges form a tree T. Then one can determine in polynomial time a linear program that is feasible if and only if D is realizable. If D is realizable, one can compute a labeling that realizes D from the solution of the LP in polynomial time. Proof."
  },
  {
    "prompt": "This is a technical discussion about path, realizable, check in theoretical computer science.",
    "response": "First, we check conditions that must obviously hold if D is realizable: If (u, v) is an arc, then (u, w) and (w, v) must also be arcs for all internal vertices of the path from u to v in T. If uc and cv are adjacent edges and neither (u, v) nor (v, u) is in A, then uc and cv must be non-special edges (as they must both be assigned the same single label in any realization). Call a non-arc (u, v) /∈A minimal if it is the only arc missing in the direction from u to v on the path from u to v."
  },
  {
    "prompt": "Understanding the computational aspects of edges, non-arc, path.",
    "response": "Consider any minimal non-arc (u, v) such that the path from u to v in T contains at least three edges. Let e1, e2, . . . , ep for some p ≥3 be the edges on the path from u to v. For i ∈[p], let ei = {ui−1, ui}, with ui−1 the endpoint closer to u. Then e1 and ep must be non-special edges and ei for i ∈[2, p −1] must be special edges. The latter part of the claim follows because Du,ui = Dui−1v = 1 but Duv = 0."
  },
  {
    "prompt": "Understanding the computational aspects of special, necessarily, temporal.",
    "response": "If e1 was special, there would necessarily be a temporal path from u to v using the lower labels on ei for i ∈[p −1] and any label on ep. Similarly, if e1 was non-special and ep was special, there Thomas Erlebach, Othon Michail, Nils Morawietz 19 would necessarily be a temporal path from u to v using the unique label on e1 and the higher labels on ei for i ∈[2, p]. Therefore, e1 and ep are non-special."
  },
  {
    "prompt": "Scientific context on conditions, hold, realized and their role in graph theory.",
    "response": "If any of these conditions does not hold, the given instance cannot be realized, and the algorithm can terminate and output an arbitrary infeasible LP. Assume now that the conditions above are met. We show how to formulate the problem of finding the required labels as a linear program (LP) of polynomial size. The LP solution may yield rational numbers as labels, but we can transform the labels to integers in the end. Let δ = 1."
  },
  {
    "prompt": "Scientific context on inequality, variables, express and their role in graph theory.",
    "response": "To express strict inequality between two variables, we will use an inequality constraint that requires one variable to be at least as large as the other variable plus δ. (The choice of δ = 1 is arbitrary, any positive δ would work.) The LP has two non-negative variables ℓe and he for each edge e of T. The values of these variables represent the labels assigned to e, using the convention that ℓe = he represents the case that e has a single label and ℓe < he the case that e has two labels."
  },
  {
    "prompt": "This is a technical discussion about constraint, add, non-special in theoretical computer science.",
    "response": "For every edge e, we therefore add the constraint ℓe = he (1) if e is non-special and the constraint ℓe + δ ≤he (2) if e is special. For every pair of adjacent edges e = {u, c} and e′ = {c, v}, we add constraints as follows: If neither (u, v) nor (v, u) is in A, both edges must be non-special and we add the constraint ℓe = ℓe′ (3) to ensure that their single labels are equal."
  },
  {
    "prompt": "Explain a concept involving path, he′, ℓe′ in computational complexity.",
    "response": "If (u, v) ∈A (and necessarily (v, u) /∈A), we add the constraints ℓe + δ ≤he′ (4) he ≤ℓe′ (5) to ensure there is a temporal path from u to v but none from v to u. Call a non-arc (u, v) /∈A minimal if it is the only arc missing in the direction from u to v on the path from u to v. Consider any minimal non-arc (u, v). Let e1, e2, . . . , ep for some p ≥2 be the edges on the path from u to v. If p = 2, the constraints above already ensure that u cannot reach v, so assume p ≥3."
  },
  {
    "prompt": "Read and understand this section about label, edges, case from a research paper.",
    "response": "As discussed above, e1 and ep must be non-special edges and ei for i ∈[2, p −1] must be special edges in this case. We add the following constraints to ensure that u cannot reach v: hei = ℓei+1, for i ∈[p −1] (6) These constraints ensure that a temporal path from u towards v uses the single label on e1, then the higher label on ei for i ∈[2, p −1], and then cannot traverse ep because the higher label on ep−1 is equal to the single label of ep."
  },
  {
    "prompt": "Read and understand this section about path, arc, Call from a research paper.",
    "response": "Call an arc (u, v) ∈A maximal if there is no arc (x, y) ∈A such that the path from x to y in T contains the path from u to v as a proper subpath. If the path from u to v contains at most one non-special edge e, then u can necessarily reach v as there is a temporal path from u to v that uses the lower label on the special edges before e (or all the way to v if there is no non-special edge e on the path) and the higher label on the edges after e."
  },
  {
    "prompt": "This is a technical discussion about Temporal, Recognizing, Realizing in theoretical computer science.",
    "response": "If the path contains at least two special edges, it suffices to add constraints to ensure, for any 20 Recognizing and Realizing Temporal Reachability Graphs two non-special edges e and e′ such that the edges between e and e′ are all special, that the path with first edge e and final edge e′ contains labels that make it a temporal path. Let e1, e2, . . . , ep with p ≥2 be the edges on the path with first edge e = e1 and final edge e′ = ep, ordered from e to e′."
  },
  {
    "prompt": "Scientific context on ℓei, hei, label and their role in graph theory.",
    "response": "We must exclude the label assignment that satisfies hei = ℓei+1 for all i ∈[p −1], because with that label assignment u can reach the vertex before v only in step hep−1 and thus cannot take the non-special edge ep with label ℓep = hep−1. Therefore, we add the following constraint: p−1 X i=1 (ℓei+1 −hei) ≥δ (7) As we have ℓei+1 −hei ≥0 by (5), this constraint ensures that for at least one i we have ℓei+1 −hei > 0. Let i∗denote such a value of i."
  },
  {
    "prompt": "Explain a concept involving label, path, edge in computational complexity.",
    "response": "Then the path with first edge e and final edge e′ gives a temporal path because we can use the unique label on e, the larger label on edges e2, . . . , ei, the smaller label on edges ei+1, . . . , ep−1, and the unique label on e′. To show that the LP is feasible if and only if D is realizable, we can argue as follows: If λ∗is a labeling that realizes D, it is clear that setting the variables of the LP based on λ∗ is a feasible solution of the LP."
  },
  {
    "prompt": "Understanding the computational aspects of solution, feasible, rational.",
    "response": "For the other direction, assume that the LP has a feasible solution, and let λ be the labeling with rational numbers determined by that feasible solution. The LP constraints were chosen so that u cannot reach v for every minimal non-arc (u, v) and u can reach v for every maximal arc (u, v), where the labels are allowed to be rational numbers. Thus, λ realizes D. Furthermore, the labeling λ can be made integral by sorting all labels and replacing them by integers that maintain their order."
  },
  {
    "prompt": "Read and understand this section about polynomial, time, edges from a research paper.",
    "response": "It is clear that the LP has polynomial size and can be constructed in polynomial time. ◀ We now show that the LP-based approach of Lemma 24 can be extended to a setting where some edges have been pre-labeled. The same approach works for an arbitrary number of edges with pre-labeled labels, but we state it for the special case of two pre-labeled edges as this is what we will require to solve a subproblem that arises in the FPT algorithm in Section 7. ▶Corollary 25."
  },
  {
    "prompt": "This is a technical discussion about Strict, URGR, solid in theoretical computer science.",
    "response": "Let D be an instance of Any Strict URGR on n vertices in which the solid edges form a tree T. Assume that the labels of two solid edges f1 and f2 have been pre-determined (such that f1 receives two labels if it is special and one label otherwise, and the same holds for f2). Assume further that the labels assigned to f1 and f2 are multiples of an integer ≥2n."
  },
  {
    "prompt": "Explain a concept involving pre-labeling, determine, polynomial in computational complexity.",
    "response": "Then one can determine in polynomial time whether there exists a frugal realization for D that agrees with the pre-labeling (and output such a labeling if the answer is yes). Proof. Construct the LP that expresses realizability of D as in the proof of Lemma 24. For the pre-labeled edges f1 and f2 we add equality constraints ensuring that the values of the variables representing their labels are consistent with the pre-labeling."
  },
  {
    "prompt": "Explain a concept involving feasible, labeling, realizes in computational complexity.",
    "response": "If there exists a labeling λ∗that agrees with the pre-labeling and realizes D, then setting the variables of the LP in accordance with those labels constitutes a feasible solution to the LP. Moreover, if the LP has a feasible solution, the values of the variables represent a fractional labeling λ that realizes D, as shown in the proof of Lemma 24."
  },
  {
    "prompt": "Read and understand this section about labels, unchanged, integers from a research paper.",
    "response": "To make the labeling integral, we can sort all labels and replace them by integers that maintain their order and keep the labelings of f1 and f2 unchanged. As the labels on f1 and f2 are multiples of a number ≥2n and we have fewer than 2n distinct labels in λ (the tree T has n −1 edges, and each edge has at most two labels), the gaps between the labels assigned to f1 and f2 contain at least 2n −1 available integers, and this is sufficient even if all labels fall into such a gap."
  },
  {
    "prompt": "Explain a concept involving URGR, Thomas, Erlebach in computational complexity.",
    "response": "◀ Thomas Erlebach, Othon Michail, Nils Morawietz 21 We remark that the variants of URGR different from Any Strict URGR can also be solved in polynomial time as follows if the graph of solid edges is a tree. By Corollary 8, the given instance is a no-instance for these variants of URGR if it contains at least one special bridge, so assume that all solid edges are non-special. Thus, we only need to consider labelings with a single label per edge."
  },
  {
    "prompt": "Explain a concept involving URGR, Lemma, Simple in computational complexity.",
    "response": "For Simple Strict URGR, we construct and solve the LP of Lemma 24. If it is infeasible, the given instance is a no-instance; otherwise, the labeling obtained from the LP solution is a realization using a single label per edge. It remains to consider all non-strict variants of URGR, including Proper URGR and Happy URGR. If there exist two adjacent edges uc and cv such that neither (u, v) nor (v, u) is in A, the given instances is a no-instance for all these variants."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, URGR, solution.",
    "response": "Otherwise, we solve the LP from Lemma 24. If it is infeasible, the given instance has no solution. Otherwise, the labeling obtained from the LP solution is a realization with a single label per edge in which no two adjacent edges receive the same label, and hence that realization constitutes a solution for all variants of Non-strict URGR including Proper URGR and Happy URGR."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, Theorem, alternative.",
    "response": "As an alternative to using the LP-based approach from Lemma 24 to check the existence of the desired realizations in the discussion above, one can also use the combinatorial algorithm from Theorem 23."
  },
  {
    "prompt": "Understanding the computational aspects of Reachability, Undirected, Realizability.",
    "response": "6 Hardness for Undirected Reachability Graph Realizability In this section, we show that all considered versions of Undirected Reachability Graph Realizability are NP-hard, even on a family of instances where each instance has a constant maximum degree and, if it is a yes-instance, can be realized with a constant number of different time labels. ▶Observation 26. Let D be an instance of some version of URGR and let G denote the solid graph of D. Moreover, let v be a vertex of D."
  },
  {
    "prompt": "Explain a concept involving vertex, edge, receives in computational complexity.",
    "response": "For each vertex w ∈NG(v) that has (i) no incoming arc from any other vertex of NG(v) or (ii) no outgoing arc to any other vertex of NG(v), the edge {v, w} receives at least one label in every realization of D. To see this, consider a solid neighbor w of v which has no ingoing arcs form any vertex of NG(v). Then the path (v, w) is the only dense path from v to w, which implies that the edge {v, w} receives at least one label."
  },
  {
    "prompt": "This is a technical discussion about path, Strict, URGR in theoretical computer science.",
    "response": "For a solid neighbor w with no outgoing arcs in NG(v), the path (w, v) is the only dense path from w to v. This then also implies that the edge {v, w} receives at least one label. ▶Theorem 27. Any Strict URGR and Simple Strict URGR are NP-hard on directed graphs of constant maximum degree that have a triangle-free solid graph. Proof. We reduce from 2P2N-3SAT which is known to be NP-hard [23]."
  },
  {
    "prompt": "This is a technical discussion about Input, CNF, negatively in theoretical computer science.",
    "response": "2P2N-3SAT Input: A CNF formula ϕ, where each clause has size at most 3 and each variable occurs exactly twice positively and twice negatively. Question: Is ϕ satisfiable? Let ϕ be an instance of 2P2N-3SAT with variables X and clauses Y := {y1, . . . , y|Y |}. Moreover, let χ: Y →[1, 10] be a coloring of the clauses, such that for each two distinct clauses yi and yj of Y that share a variable, χ(yi) ̸= χ(yj)."
  },
  {
    "prompt": "This is a technical discussion about graph, Reachability, Temporal in theoretical computer science.",
    "response": "Such a coloring χ exists and can be 22 Recognizing and Realizing Temporal Reachability Graphs greedily computed in polynomial time.1 We define a directed graph D = (V, A) in polynomial time, such that there is a simple undirected temporal graph with strict reachability graph equal to D if and only if ϕ is satisfiable. We initialize V and A as empty sets. For each variable x ∈X, we add the following vertices to V : Vx := {x, x, ax, ax, ax,x, bx, bx, bx,x, cx, fx, fx, fx,x, qx, qax, qx, qax}."
  },
  {
    "prompt": "Read and understand this section about Figure, vertices, clause from a research paper.",
    "response": "See Figure 2 for the vertices and arcs of D[Vx]. For each clause yi ∈Y , we add two vertices si and ti to V , that is, a source si and a terminal ti for each clause. Finally, for each two distinct clauses yi and yj from Y that share a variable, we add vertices q(i,j) and q(j,i) to V . Note that these vertices and their incident edges are not shown in Figure 2. Next, we describe the arcs A of D. To this end, we first describe the solid edges E of G. We initialize E as the empty set."
  },
  {
    "prompt": "This is a technical discussion about Figure, solid, add in theoretical computer science.",
    "response": "For each variable x ∈X, we add to E the solid edges between the vertices of Vx that appear in the graph of Figure 2. For each clause yi, we add a solid edge to E between the vertex si and each vertex ℓfor which ℓis a literal of yi. Moreover, for each variable x occurring in clause yi, we add the solid edges {ti, qx}, {ti, qax}, {ti, qx}, {ti, qax}, and {ti, cx} to E."
  },
  {
    "prompt": "Understanding the computational aspects of dashed, add, Finally.",
    "response": "Finally, for each two distinct clauses yi and yj that share a variable, we add the solid edges {si, q(i,j)}, {q(i,j), tj}, {sj, q(j,i)}, and {q(j,i), ti} to E. Next, we describe the dashed arcs of D. For each variable x ∈X, we add to A the dashed arcs between the vertices of Vx as described in the matrix of Figure 2. Let yi be a clause of Y and let x be a variable occurring in yi. We add the dashed arcs (si, cx), (si, ax,x), (si, ax), (x, ti), (x, ti), (ax, ti), (ax, ti), and (bx,x, ti) to A."
  },
  {
    "prompt": "Understanding the computational aspects of add, dashed, arcs.",
    "response": "If x occurs positively in clause yi, we add the dashed arcs (si, ax), (si, bx), (si, fx), and (qx, si) to A. Otherwise, we add the dashed arcs (si, bx), (si, fx) and (qx, si) to A. Let yi and yj be distinct clauses from Y that share a variable and assume without loss of generality that χ(yi) < χ(yj). We add the dashed arcs (si, tj), (sj, ti), and (ti, tj) to A. If yi and yj share a literal, we also add the arc (si, sj) to A. Finally, for each clause yi ∈Y , we add the arc (si, ti) to A."
  },
  {
    "prompt": "Understanding the computational aspects of arcs, Claim, due.",
    "response": "This completes the construction of D. ▷Claim 28. D has a maximum degree of O(1). Moreover, G = (V, E) is triangle-free. Proof. The solid graph induced by the vertices of each variable gadget has constant degree by inspection. Every vertex si has at most 3 + 9 = 12 edges of type {si, ℓ}, {si, q(i,j)}, respectively, at most 13 arcs due to variables occurring in yi, at most 9 arcs due to yi sharing variables with other clauses, and at most 6 arcs due to yi sharing literals with other clauses."
  },
  {
    "prompt": "Understanding the computational aspects of due, edges, clauses.",
    "response": "Every ti has at most 15 edges and 15 arcs due to variables occurring in yi, and at most 9 edges and 18 arcs due to yi sharing variables with other clauses. For the same (constant number of) types of edges and arcs, each of the other vertices of D is related to a variable or literal, which can occur in at most 4 clauses; by inspection, it follows that the degree contributed to each such vertex by each edge or arc type is constant. Thus, D has a maximum degree of O(1)."
  },
  {
    "prompt": "This is a technical discussion about vertices, inspection, triangle in theoretical computer science.",
    "response": "The solid graph induced by the vertices of each variable gadget is triangle-free by inspection. The edge set of G[S i({si} ∪{ti})] is empty, so any triangle must be using at most one source or terminal vertex. Each q(i,j) vertex is adjacent to no other vertex than si and tj, thus cannot be part of a triangle. As no two vertices belonging to different gadgets are adjacent, it remains to show that a source or terminal cannot form a triangle with two vertices belonging to the same gadget."
  },
  {
    "prompt": "Read and understand this section about graph, variable, clauses from a research paper.",
    "response": "A source is only adjacent to the 1 To see this, consider the auxiliary graph HY with vertex set Y and edges between to clauses yi and yj of Y if and only if yi and yj share a variable. The graph HY has a maximum degree of 9, since each clause contains three literals, and each variable occurs in exactly four clauses. Hence, χ is a greedy proper (∆(HY ) + 1)-vertex coloring of HY which exists and can be computed in polynomial time."
  },
  {
    "prompt": "Scientific context on qax, bx,x, ax,x and their role in graph theory.",
    "response": "Thomas Erlebach, Othon Michail, Nils Morawietz 23 si sj bx bx,x bx ax ax,x ax fx fx,x fx qx qx qax qax cx x x tk tℓ 1 1 1 1 1 2 3 12 13 13 14 25 13 14 13 14 α β 13 14 14 15 χ(yi) + 1 ∈[2, 11] χ(yj) + 1 ∈[2, 11] α β α β α β α β si sj tk tℓ x x ax ax ax,x bx bx bx,x cx fx fx fx,x qx qx qax qax si 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 sj 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 tk 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 tℓ 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 x 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 x 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 ax 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 ax 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 ax,x 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 bx 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 bx 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 bx,x 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 cx 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 fx 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 fx 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 fx,x 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 qx 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 qx 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 qax 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 qax 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 Figure 2 The solid graph and adjacency matrix for the variable gadget in the hardness reduction for Any Strict URGR and Simple Strict URGR version on triangle-free graph (Theorem 27)."
  },
  {
    "prompt": "Explain a concept involving red, blue, arcs in computational complexity.",
    "response": "The highlighted cells indicate the dashed arcs. As we show, no realization assigns labels to both the red and the blue edge. If the red (blue) edge receives a label, the two red (blue) cells are realized via paths through the variable gadget Vx. Here, α = χ(yk) + 14 and β = χ(yℓ) + 14, which are both values in [15, 24]."
  },
  {
    "prompt": "Explain a concept involving qax, Recognizing, Realizing in computational complexity.",
    "response": "24 Recognizing and Realizing Temporal Reachability Graphs vertices corresponding to literals and the terminals are only adjacent to vertices qx, qax, qx, qax, and cx, none of which are adjacent to each other. Thus, G = (V, E) is triangle-free. ◁ Next, we show that ϕ is satisfiable if and only if there is an undirected temporal graph with strict reachability graph D."
  },
  {
    "prompt": "Scientific context on graph, temporal, strict and their role in graph theory.",
    "response": "More precisely, we show that if ϕ is satisfiable, then there is a simple undirected temporal graph with strict reachability graph D. (⇐) Let G = (G = (V, E), λ: E →2N) be an undirected temporal graph with strict reachability graph equal to D. We show that ϕ is satisfiable. To this end, we first show that for each variable x ∈X, at most one of the edges of {x, cx} and {x, cx} receives a non-empty set of labels under λ."
  },
  {
    "prompt": "Explain a concept involving Assume, receives, contradiction in computational complexity.",
    "response": "Assume towards a contradiction that there is a variable x ∈X, such that both {x, cx} and {x, cx} receives a non-empty set of labels under λ. We will show that the arc (fx, fx) is not realized. Since (x, x) /∈A and (x, x) /∈A, Observation 1 implies that there is a label α, such that λ({x, cx}) = λ({x, cx}) = {α}. Now, consider the neighborhood of fx (fx). This vertex has only two solid neighbors: x and fx,x (x and fx,x). Moreover, both these neighbors are not joined by a solid edge."
  },
  {
    "prompt": "Read and understand this section about fx,x, Observation, implies from a research paper.",
    "response": "Due to Observation 26, this implies that the edges {x, fx}, {fx, fx,x}, {fx,x, fx}, and {fx, x} all receive at least one label under λ. Since A contains none of the arcs (cx, fx), (fx, cx), (x, fx,x), and (fx,x, x), Observation 1 implies that λ({fx,x, fx}) = λ({fx, x}) = λ({x, cx}) = {α}. Similarly, λ({fx,x, fx}) = λ({fx, x}) = λ({x, cx}) = {α}. Thus, each edge incident with both fx and fx receives the label set {α} under λ."
  },
  {
    "prompt": "Understanding the computational aspects of arc, variable, contradicts.",
    "response": "This contradicts the assumption that the arc (fx, fx) ∈A is realized by the temporal graph G. Consequently, for each variable x ∈X, at most one of the edges of {x, cx} and {x, cx} receives a non-empty set of labels under λ. Based on this observation, we define a truth assignment π of the variables of X that satisfies ϕ. For each variable x ∈X, we set π(x) := true if and only if the edge {x, cx} receives at least one label under λ. Next, we show that π satisfies ϕ. Let yi be a clause of ϕ."
  },
  {
    "prompt": "This is a technical discussion about vertex, show, satisfies in theoretical computer science.",
    "response": "We show that yi is satisfies by π. Since (i) the strict reachability graph of G is equal to D = (V, A) and (ii) A contains the arc (si, ti), there is a strict temporal path P from si to ti in G. Hence, P is a dense path in D. Consider the first internal vertex v of P. Recall that v is a solid neighbor of si. This implies that v is either the vertex corresponding to a literal of the clause yi, or a vertex q(i,j) for some clause yj distinct from yi that shares a variable with yi."
  },
  {
    "prompt": "Explain a concept involving Recall, out-neighbors, literal in computational complexity.",
    "response": "Recall that q(i,j) has only the two out-neighbors si and tj ̸= ti in D. Hence, the dense path P cannot contain q(i,j), since (q(i,j), ti) /∈A. Thus, the first internal vertex of P is a literal ℓof clause yi. Let x denote the variable for which ℓis the literal. We show that the edge {ℓ, cx} receives at least one label under λ. This then implies that π satisfies yi, since by the initial observation, at most one of the edges {x, cx} or {x, cx} received a label under λ."
  },
  {
    "prompt": "This is a technical discussion about ax,x, qaℓ, note in theoretical computer science.",
    "response": "First, note that P cannot contain another source vertex sj ̸= si, since ℓhas no arc towards any out-neighbor of sj in D. Thus, P continues by traversing to some solid neighbor of ℓin Vx. By construction, A contains none of the arcs (ax,x, ti), (bℓ, ti), (si, qℓ), (si, qaℓ). This implies that P contains none of the vertices of {ax,x, bℓ, qℓ, qaℓ}. Consequently, P contains no solid neighbor of aℓbesides ℓ, which also implies that P does not contain aℓ."
  },
  {
    "prompt": "Read and understand this section about definition, solid, neighbor from a research paper.",
    "response": "Hence, by definition of D[Vx], the only solid neighbor of ℓin Vx that can possibly be contained in P is cx. Consequently, P traverses the edge {ℓ, cx}, which implies that {ℓ, cx} receives at least one label under λ. This then implies that π satisfies clause yi. (⇒) Let π be a satisfying assignment of ϕ."
  },
  {
    "prompt": "Explain a concept involving incident, labels, vertex in computational complexity.",
    "response": "We define an undirected temporal graph (G = (V, E′), λ: E′ →N) for some E′ ⊆E as follows: For each clause yi of Y , we set the labels of all edges incident with vertex si to χ(yi)+1 and the labels of all edges incident with vertex ti to χ(yi) + 14. It remains to define the labels of the edges between vertices of Vx for each Thomas Erlebach, Othon Michail, Nils Morawietz 25 variable x ∈X. These labels are depicted in Figure 2."
  },
  {
    "prompt": "Understanding the computational aspects of bx,x, ax,x, fx,x.",
    "response": "Formally, for each variable x ∈X, we set λ({cx, bx,x}) = 1, λ({v, qv}) = 1 for each v ∈{x, x, ax, ax}, λ({bx, bx,x}) = 2, λ({bx, bx,x}) = 3, λ({x, ax}) = 12, λ({ax, ax,x}) = λ({x, ax}) = λ({x, fx}) = λ({fx, fx,x}) = 13, λ({ax, ax,x}) = λ({x, bx}) = λ({x, fx}) = λ({fx, fx,x}) = 14, λ({x, bx}) = 15, and λ({cx, ax,x}) = 25. If x is assigned to true by π, we set λ({x, cx}) = 13 and λ({x, cx}) = ∅. If x is assigned to false by π, we set λ({x, cx}) = ∅and λ({x, cx}) = 14."
  },
  {
    "prompt": "Scientific context on receive, label, edges and their role in graph theory.",
    "response": "This completes the definition of λ. Next, we show that the strict reachability graph of (G, λ) is exactly D. All solid edges that receive a label are realized through the label they receive. Any {x, cx} or {cx, x} edge that does not receive a label is realized by the temporal paths on the smallest squares including the edge (one drawn above and one below it in Figure 2), and these are the only solid edges that might not receive a label."
  },
  {
    "prompt": "This is a technical discussion about edges, incident, receiving in theoretical computer science.",
    "response": "As all edges incident to an si receive the same label χ(yi) + 1 and the same holds for all edges incident to a ti, receiving χ(yi) + 14 in this case, the temporal paths of (G, λ) only use si or tj vertices as endpoints. It follows that none of the arcs that can only be realized by a path using si or ti as an internal vertex, all of which are missing from A, is incorrectly realized. This includes all missing arcs incident to a q(i,j) vertex."
  },
  {
    "prompt": "This is a technical discussion about Figure, present, missing in theoretical computer science.",
    "response": "It also follows that the realization of any (present or missing) arc incident to at least one Vx vertex can be verified by inspection of the solid graph and matrix of Figure 2. It remains to argue about the arcs whose endpoints are both si or tj vertices. Every (si, tj) arc for i ̸= j is realized by the temporal path si, q(i,j), tj using labels χ(yi) + 1, χ(yj) + 14, where χ(yi) + 1 ≤11 < χ(yj) + 14."
  },
  {
    "prompt": "Understanding the computational aspects of arc, realized, present.",
    "response": "Every (si, sj) arc for χ(yi) < χ(yj), present when yi and yj share a literal ℓ, is realized by the temporal path si, ℓ, tj using labels χ(yi) + 1, χ(yj) + 1. Every (ti, tj) arc for χ(yi) < χ(yj), present when yi and yj share a variable x, is realized by the temporal path ti, cx, tj using labels χ(yi) + 14, χ(yj) + 14. Every (si, ti) arc is realized as follows. Let ℓbe a literal of yi which evaluates to true under π, corresponding to a variable x."
  },
  {
    "prompt": "Scientific context on path, labels, realizing and their role in graph theory.",
    "response": "As x is a variable of yi, si and ti are both connected to Vx. If x appears positive in ℓ, then si, x, cx, ti using labels χ(yi)+1, 13, χ(yi)+14 is a realizing temporal path. If x appears negated in ℓ, then si, x, cx, ti using labels χ(yi)+1, 14, χ(yi)+14 is a realizing temporal path."
  },
  {
    "prompt": "Scientific context on path, clauses, share and their role in graph theory.",
    "response": "For clauses yi and yj that share no variable, (si, sj), (ti, tj), and (ti, sj) is not in A and is not realized by (G, λ), because each path between these vertices in G goes trough at least one other source or terminal vertex, which by the initial argument cannot be a temporal path. The same holds for all clauses yi and yj that share a variable but not a literal with respect to the non-arcs (si, sj) and (sj, si)."
  },
  {
    "prompt": "This is a technical discussion about URGR, Theorem, version in theoretical computer science.",
    "response": "◀ By slightly modifying the previous reduction, we can show that also all other version of URGR under consideration are NP-hard. ▶Theorem 29. Each version of URGR under consideration is NP-hard on directed graphs of constant maximum degree. 26 Recognizing and Realizing Temporal Reachability Graphs Proof. Due to Theorem 27, it remains to show the NP-hardness only for (i) Proper Strict URGR, (ii) Happy Strict URGR, and (iii) each version of Non-strict DRGR."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, reduction, instance.",
    "response": "We capture all of these versions in one reduction. We again reduce from 2P2N-3SAT by slightly adapting the reduction from the proof of Theorem 27. Let ϕ be an instance of 2P2N-3SAT with variables X and clauses Y := {y1, . . . , y|Y |}. Moreover, let D′ be the instance of our problem that was constructed in the reduction of Theorem 27 for instance ϕ, and let G′ be the solid graph of D′. We obtain a directed graph D as follows: We initialize D as D′."
  },
  {
    "prompt": "This is a technical discussion about vertex, remove, vertices in theoretical computer science.",
    "response": "First, we remove the vertices fx, fx, and fx,x for each variable x ∈X. Next, for each clause yi ∈Y , we replace the vertex si (ti) by a solid clique Si (Ti). We initialize Si := {si} (Ti := {ti}). Afterwards, we add a twin of si (ti) to Si (Ti), that is, a vertex with the same in- and out-neighborhood as si (ti). Note that each vertex of Si (Ti) has the same solid neighbors as each other vertex of Si (Ti)."
  },
  {
    "prompt": "Explain a concept involving degree, process, max in computational complexity.",
    "response": "We continue this process, until Si (Ti) has an odd size of at least max(9, ri), where ri is the degree of si (ti) in G′. Finally, we make Si (Ti) a clique in G. This completes the construction of D. Since the maximum degree of D′ was a constant and we only added a constant number of twins for each source and terminal vertex, the maximum degree of D is only increased by a constant factor. We now show the correctness of the reduction."
  },
  {
    "prompt": "Read and understand this section about graph, temporal, reachability from a research paper.",
    "response": "To this end, we show that if ϕ is satisfiable, then there is a happy undirected temporal graph with reachability graph D, and if ϕ is not satisfiable, then there is neither a proper undirected temporal graph with reachability graph D nor an undirected temporal graph with non-strict reachability graph D. This then implies that the reduction is correct for all stated versions of our problem."
  },
  {
    "prompt": "Explain a concept involving incident, Suppose, Theorem in computational complexity.",
    "response": "(⇐) Suppose that ϕ is satisfiable and let λ′ be the realization for D′ described in the proof of Theorem 27. Note the only adjacent edge that shared a label under λ′ (i) contained an edge incident with a vertex fℓfor some ℓ∈{x, x | x ∈X} or (ii) are both incident with the same source or incident the same terminal vertex. Since D does not contain any of the fℓ-vertices, the only adjacent edge of G under λ′ are both incident with the same source or incident the same terminal vertex."
  },
  {
    "prompt": "Understanding the computational aspects of note, fℓ-vertex, incident.",
    "response": "Moreover, note that no arc of both D and D′ was realized via temporal paths that go over any fℓ-vertex. To obtain a happy realization for D, we essentially spread all the labeled edges incident with a source si (a terminal ti) to an individual vertex of Si (Ti) each. By definition of Si (Ti), this vertex set contain at least as many vertices each as the number of solid edges incident with si (ti) in G′."
  },
  {
    "prompt": "Explain a concept involving edges, Formally, initialize in computational complexity.",
    "response": "Formally, we initialize a labeling λ of some edges of G as the restriction of λ′ to the common edges of G and G′. Afterwards we iterate over all sources and terminals. For each source si, we take an arbitrary maximal matching M between the solid neighbors of si in G′ and Si. We remove the labels of all edges incident with si from λ (which all had the same label, say αsi) and add the label αsi to all edges of M. We do the same analogously for all terminal vertices. This ensures that λ is proper."
  },
  {
    "prompt": "This is a technical discussion about Finally, Hamiltonian, cycles in theoretical computer science.",
    "response": "Finally, for each si, we take four arbitrary and pairwise edge-disjoint Hamiltonian cycles Cj with j ∈[1, 4]. Such Hamiltonian cycles exist [17], since Si has an odd size of at least 9."
  },
  {
    "prompt": "This is a technical discussion about label, Hamiltonian, αsi in theoretical computer science.",
    "response": "We label the edges of these Hamiltonian cycles as follows: Fix an arbitrary vertex s∗ i ∈Si of Si and label the edges of the Hamiltonian cycles with consecutive labels starting from s∗ i , such that (i) each label of C1 is smaller than each label of C2, (ii) each label of C2 is smaller than αsi, (iii) each label of C3 is larger than αsi, and (iv) each label of C4 is larger than each label of C3."
  },
  {
    "prompt": "Read and understand this section about Erlebach, Othon, Michail from a research paper.",
    "response": "Note that this can be done in a straightforward way but might result in Thomas Erlebach, Othon Michail, Nils Morawietz 27 si sj bx bx,x bx ax ax,x ax qx qx qax qax cx x x . . . . . . Tk Tℓ . . . . . . Si . . . . . ."
  },
  {
    "prompt": "Understanding the computational aspects of qax, Figure, ax,x.",
    "response": "Sj 1 1 1 1 1 2 3 12 13 13 14 25 13 14 α β 14 15 ∈[2, 11] ∈[2, 11] α α α α β β β β ∈Si ∈Sj ∈Tk ∈Tℓ x x ax ax ax,x bx bx bx,x cx qx qx qax qax ∈Si 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 ∈Sj 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 ∈Tk 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 ∈Tℓ 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 x 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 x 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 ax 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 ax 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 ax,x 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 bx 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 bx 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 bx,x 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 cx 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 qx 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 qx 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 qax 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 qax 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 Figure 3 Figure and matrix for the variable gadget in the hardness reduction for the happy version."
  },
  {
    "prompt": "Explain a concept involving depicted, Recognizing, solid in computational complexity.",
    "response": "Only solid edges that receive a label are depicted. 28 Recognizing and Realizing Temporal Reachability Graphs possibly negative labels. The latter is no issue, since we can arbitrarily shift all labels at the same time until they are positive integers, while preserving the same reachabilities."
  },
  {
    "prompt": "This is a technical discussion about vertex, reach, Hamiltonian in theoretical computer science.",
    "response": "Note that the labeling of the four Hamiltonian ensure that (a) prior to time αsi, each vertex of Si can reach each other vertex of Si and (b) after time αsi, each vertex of Si can reach each other vertex of Si. This in particular implies that all arcs of the clique G[Si] are realized. Moreover, it implies that all vertices of Si have the same in- and out-reachabilities, since only edges with label αsi are incident with both a vertex of Si and any other vertex outside of Si."
  },
  {
    "prompt": "Understanding the computational aspects of vertices, respective, single.",
    "response": "We do the same for the terminal vertices and their respective cliques. The resulting labeling is then proper and assigns at most one label to each edge of G. Moreover, λ realizes D, since, intuitively we can merge each source clique Si into a single vertex si and merge each terminal clique Ti into a single vertex ti."
  },
  {
    "prompt": "Understanding the computational aspects of graph, fx,x, realization.",
    "response": "The resulting graph D′′ is then the graph obtained for D′ by removing the vertices of {fx, fx, fx,x | x ∈X}, for which λ′ is a realization, when restricted to the solid edges of D′′. Hence, there is a happy realization for D. (⇒) We show this statement via contraposition. Let G = (G = (V, E), λ: E →2N) be an undirected temporal graph such that (i) λ is a proper labeling and D is the reachability graph of G, or (ii) D is the non-strict reachability graph of G. We show that ϕ is satisfiable."
  },
  {
    "prompt": "Explain a concept involving Theorem, variable, Similar in computational complexity.",
    "response": "Similar to the proof of Theorem 27, we show that for each variable x ∈X, at most one of the edges {x, cx} and {x, cx} receives a label under λ. Let x be a variable of X. Recall that D contains neither of the arcs (x, x) nor (x, x). Assume towards a contradiction that λ({x, cx}) ̸= ∅and λ({x, cx}) ̸= ∅."
  },
  {
    "prompt": "This is a technical discussion about min, max, case in theoretical computer science.",
    "response": "If min λ({x, cx}) < max λ({x, cx}) or max λ({x, cx}) > min λ({x, cx}), the strict and the non-strict reachability graph of G contain at least one arc between x and x; a contradiction to the fact that λ realizes D. Hence, assume that this is not the case. That is λ({x, cx}) = λ({x, cx}) ̸= ∅. Note that this is not possible in the case where λ is a proper labeling. Hence, we only have to consider the case where the non-strict reachability graph of G is D."
  },
  {
    "prompt": "Scientific context on contradiction, non-strict, reachability and their role in graph theory.",
    "response": "Since λ({x, cx}) = λ({x, cx}) ̸= ∅, the non-strict reachability graph of D contains both arcs (x, x) and (x, x); a contradiction. Thus, not both edges {x, cx} and {x, cx} receives a label under λ. The remainder of the proof that ϕ is satisfiable is identical to the one in Theorem 27 and thus omitted. ◀ In all the above reductions, the number of vertices and arcs in the constructed graph D were O(|ϕ|), where ϕ is the size of the respective instance of 2P2N-3SAT."
  },
  {
    "prompt": "This is a technical discussion about ETH, fails, Corollary in theoretical computer science.",
    "response": "Since 2P2N-3SAT cannot be solved in 2o(|ϕ|) · |ϕ|O(1) time, unless the ETH fails [23], this implies the following. ▶Corollary 30. No version of URGR under consideration can be solved in 2o(|V |+|A|) · nO(1) time, unless the ETH fails. Hence, the running time of Theorem 5 can presumably not be improved significantly."
  },
  {
    "prompt": "Explain a concept involving Strict, URGR, Section in computational complexity.",
    "response": "6.1 Parameterized hardness We now strengthen our hardness result for Any Strict URGR and Simple Strict URGR, which will highly motivate the analysis of parameterized algorithms for the parameter ‘feedback edge set number’ of the solid graph, which we consider in Section 7. ▶Theorem 31. Any Strict URGR and Simple Strict URGR are W[2]-hard when parameterized by the feedback vertex set number and the treedepth of the solid graph. Proof."
  },
  {
    "prompt": "Read and understand this section about Set, Cover, hard from a research paper.",
    "response": "We reduce from Set Cover which is W[2]-hard when parameterized by k [9]. Thomas Erlebach, Othon Michail, Nils Morawietz 29 Table 2 A part of the adjacency matrix of the W[2]-hardness reduction."
  },
  {
    "prompt": "This is a technical discussion about vF,u, wF,u, qF,u in theoretical computer science.",
    "response": "u ⊤ vF,u i wF,u i qF,u i ai a′ i bi b′ i ci ⊥i u 1 1 1 0 1 0 1 1 1 0 ⊤ 0 0 0 1 0 0 0 1 1 0 vF,u i 0 0 1 0 1 0 0 0 0 1 wF,u i 1 1 1 1 1 0 1 1 1 0 qF,u i 0 1 1 1 1 0 1 1 1 0 ai 0 0 0 1 0 1 0 0 0 0 a′ i 0 1 0 1 0 1 0 1 1 0 bi 0 1 1 1 0 1 0 1 1 0 b′ i 0 0 0 0 0 0 0 1 1 0 ci 0 1 0 1 0 1 1 0 1 0 ⊥i 0 0 1 0 0 0 0 0 0 0 Set Cover Input: A universe U, a collection F of subsets of U (called hyperedges), and an integer k."
  },
  {
    "prompt": "Scientific context on Question, contained, subset and their role in graph theory.",
    "response": "Question: Is there a subset S ⊆F of size at most k, such that each element u ∈U is contained in at least one hyperedge of S? Let I := (U, F, k) be an instance of Set Cover with F = {F1, . . . , Fr}. Assume without loss of generality that each element of U is contained in at least one hyperedge and that F has size at least k, as otherwise I could be solved trivially."
  },
  {
    "prompt": "Understanding the computational aspects of vertex, vertices, graph.",
    "response": "We obtain a directed graph D = (V, A) with solid graph G = (V, E) as follows: The graphs contain the vertex ⊤, each element u ∈U as a vertex, and for each i ∈[1, k] the vertices of {ai, a′ i, bi, b′ i, ci, ⊥i}. Additionally, for each i ∈[1, k], each F ∈F, and each u ∈F, the graphs contain the vertices wF,u i , vF,u i , and qF,u i . Next, we describe the solid edges of G, that is, the bidirectional arcs of D."
  },
  {
    "prompt": "Read and understand this section about wF,u, Table, edges from a research paper.",
    "response": "See Table 2 for the solid edges and dashed arcs of the main connection gadget of the instance. Let i ∈[1, k]. The graph G contains the edges {ai, a′ i}, {a′ i, ci}, {ci, b′ i}, {b′ i, bi}, and {ci, ⊤}. Moreover, for each F ∈F and each u ∈F, G contains the edges {u, wF,u i }, {wF,u i , ai}, {wF,u i , bi}, {wF,u i , ci}, {wF,u i , vF,u i }, {wF,u i , qF,u i }, {vF,u i , ⊥i}, and {qF,u i , ⊤}. Finally, we describe the dashed arcs of D."
  },
  {
    "prompt": "Explain a concept involving qF,u, arcs, vF,u in computational complexity.",
    "response": "For each i ∈[1, k], D contains the arcs (a′ i, bi), (bi, ai), (bi, ci), (ci, ai), (a′ i, ⊤), (bi, ⊤), and (⊤, b′ i). Let u ∈U. Then, D contains the arc (u, ⊤). Moreover, for each i ∈[1, k], D contains the arcs (u, ai), (u, bi), (u, b′ i), and (u, ci). Additionally, for each hyperedge F ∈F with u ∈F, D contains the arcs (u, vF,u i ), (vF,u i , ai), (bi, vF,u i ), (wF,u i , ⊤), (wF,u i , b′ i), (a′ i, wF,u i ), (qF,u i , vF,u i ), (qF,u i , ai), (qF,u i , bi), (qF,u i , b′ i), and (qF,u i , ci)."
  },
  {
    "prompt": "This is a technical discussion about vertices, dashed, vF,u in theoretical computer science.",
    "response": "The only other dashed arcs in D are between vF,u i -vertices. Let Fx, Fy ∈F with x < y and let ux ∈Fx and uy ∈Fy. For each i ∈[1, k], D contains the arc (vFx,ux i , vFy,uy i ). This completes the construction of D. First, we show the parameter bounds. ▷Claim 32. G has a feedback vertex set of size O(k) and G has treedepth O(k). Proof. Let X := {⊤} ∪{ai, a′ i, bi, b′ i, ci, ⊥i | i ∈[1, k]}. We show that X is a feedback vertex set of G and more further, that each tree in G −X has depth at most 2."
  },
  {
    "prompt": "Scientific context on wF,u, solid, statement and their role in graph theory.",
    "response": "Since |X| = 6k + 1, this then proves the statement. Let u ∈U. The solid neighborhood of u in G is R = {wF,u i | i ∈[1, k], F ∈F, u ∈F}, which is an independent set in G. Now consider the other solid neighbors of the vertices of R in V \\ X. Let wF,u i ∈R. Then, the only two solid neighbors of wF,u i in V \\ X besides u are vF,u i and qF,u i , since ai, bi, and ci are in X. Furthermore, wF,u i is the only solid neighbor of both vF,u i and qF,u i in V \\ X."
  },
  {
    "prompt": "Understanding the computational aspects of Recognizing, Realizing, Temporal.",
    "response": "This implies that G[Vu] is a tree 30 Recognizing and Realizing Temporal Reachability Graphs of depth two (rooted in u), where Vu := {wF,u i , vF,u i , qF,u i | i ∈[1, k], F ∈F, u ∈F} ∪{u}. Moreover, each vertex of V \\ X is contained in exactly one set Vu, since we assumed that each hyperedge is non-empty. This implies that each connected component in G −X is a tree of depth at most 2, which implies the statement."
  },
  {
    "prompt": "Read and understand this section about Set, strict, Cover from a research paper.",
    "response": "◁ We show that I is a yes-instance of Set Cover if and only if D is realizable via strict temporal paths. More precisely, we show that if I is a yes-instance of Set Cover, then there is a simple undirected temporal graph with strict reachability graph D. This then implies the correctness of the reduction for both Any Strict URGR and Simple Strict URGR. (⇐) Let G = (G = (V, E), λ: E →2N) be a undirected temporal graph with strict reachability graph equals to D."
  },
  {
    "prompt": "Read and understand this section about set, wF,u, hitting from a research paper.",
    "response": "We show that there is a hitting set of size at most k for U. To this end, we first analyze for each i ∈[1, k] the structure of the labeling λ with respect to the vertices {ai, a′ i, bi, b′ i, ci, ⊥i} ∪{vF,u i , wF,u i , qF,u i | F ∈F, u ∈F}. Let i ∈[1, k]. For a hyperedge F ∈F, we denote with F(i) the set of vertices {wF,u i | u ∈F}. We show that there is at most one hyperedge F ∈F for which edges between ci and F(i) receive a non-empty set of labels under λ."
  },
  {
    "prompt": "Explain a concept involving receives, non-empty, set in computational complexity.",
    "response": "Assume towards a contradiction that there are two hyperedges Fx ∈F and Fy ∈F with x < y, such that there is at least one edge ex between Fx(i) and ci receives a non-empty set of labels under λ, and there is at least one edge ey between Fy(i) and ci receives a non-empty set of labels under λ. Let wFx,ux i (wFy,uy i ) be the other endpoint of ex (ey) besides ci. Moreover, let z ∈{x, y}. By construction vFz,uz i has only two solid neighbors in G, namely wFx,ux i and ⊥i."
  },
  {
    "prompt": "Read and understand this section about vFz,uz, Observation, solid from a research paper.",
    "response": "This implies that both solid edges incident with vFz,uz i each receive at least one label under λ (see Observation 26). Between these two neighbors, there is no dashed arc in D and no solid edge in G. Due to Observation 1, this implies that there is some αz ∈N, such that λ({⊥i, vFz,uz i }) = λ({vFz,uz i , wFz,uz i }) = {αz}. Similarly, since ez receives at least one label under λ and there is no arc between ci and vFz,uz i , λ(ez) = λ({vFz,uz i , wFz,uz i }) = {αz}. We now show that αx < αy."
  },
  {
    "prompt": "Understanding the computational aspects of vFx,ux, vFy,uy, path.",
    "response": "This is due to the fact that (vFx,ux i , ⊥i, vFy,uy i ) is the only dense (vFx,ux i , vFy,uy i )-path in D and (vFx,ux i , vFy,uy i ) is an arc of D. Concluding, λ(ex) = {αx} and λ(ey) = {αy}, which implies that there is a temporal (wFx,ux i , wFy,uy i )-path under λ. This contradicts the fact that λ realizes D, since (wFx,ux i , wFy,uy i ) is not an arc of D. Consequently, there is at most one hyperedge F ∈F for which edges between ci and F(i) receive a non-empty set of labels under λ."
  },
  {
    "prompt": "This is a technical discussion about set, exists, hyperedge in theoretical computer science.",
    "response": "For each i ∈[1, k], let F ′ i denote the unique hyperedge F ∈F for which edges between ci and F(i) receive a non-empty set of labels under λ, if such a hyperedge F exists. We set S := {F ′ i | i ∈[1, k], F ′ i exists} and show that S is a set cover for U. Let u ∈U. We show that there is a hyperedge in S that contains u. Since λ realizes D and (u, ⊤) is an arc of D, there is a temporal (u, ⊤)-path P under λ. Recall that P is thus a dense path in D."
  },
  {
    "prompt": "Read and understand this section about vF,u, solid, edge from a research paper.",
    "response": "Since {u, ⊤} is not a solid edge of G, P has length at least 3. Let x be the first internal vertex of P, which is a solid neighbor of u in G. By definition, this implies that x = wF,u i for some i ∈[1, k] and some F ∈F with u ∈F. Consider the solid neighbors of x in G besides u. These are the vertices ai, bi, ci, vF,u i , and qF,u i . Since P is a dense (u, ⊤)-path and D contains none of the arcs (ai, ⊤), (vF,u i , ⊤), or (u, vF,u i ), P visits none of the vertices ai, vF,u i , or qF,u i ."
  },
  {
    "prompt": "This is a technical discussion about wF,u, solid, neighbor in theoretical computer science.",
    "response": "Hence, P continues from wF,u i to either bi or ci. Consider the solid neighbors of bi besides wF,u i . These are the vertices of R := {b′ i} ∪({wF ′,u′ i | F ′ ∈F, u′ ∈F ′} \\ {wF,u i }). Since D does not contain the arc (b′ i, ⊤) and contains none of the arcs (wF,u i , r) with r ∈R \\ {b′ i}, the dense path P visits no solid neighbor of bi besides wF,u i . This also implies that P does not visit bi. Thus, the only solid neighbor of wF,u i besides u in P is ci."
  },
  {
    "prompt": "This is a technical discussion about Erlebach, Othon, Michail in theoretical computer science.",
    "response": "Consequently, P Thomas Erlebach, Othon Michail, Nils Morawietz 31 traverses the edge e = {wF,u i , ci}. Thus, e receives at least one label under λ, which implies that F = F ′ i ∈S, that is, at least one hyperedge of S contains u. This implies that S is a set cover of U. (⇒) Let S be a set cover of size at most k for U. Without loss of generality assume that S has size exactly k and let the hyperedges in S be denoted as F ′ i for i ∈[1, k]."
  },
  {
    "prompt": "Read and understand this section about show, labeling, realizes from a research paper.",
    "response": "We show that there is a labeling λ: E →N ∪∅that realizes D. This then implies that there is a simple temporal graph with strict reachability graph equals to D. For each hyperedge Fx ∈F, let αFx := x + 2. Moreover, recall that r := |F|. Hence, αx ∈[3, r + 2]. We define λ as follows: For each u ∈U, we assign label 1 to all edges incident with u. We assign label r + 3 to all edges incident with ⊤."
  },
  {
    "prompt": "Read and understand this section about wF,u, set, vF,u from a research paper.",
    "response": "For each i ∈[1, k], we set λ({ai, a′ i}) := λ({bi, b′ i}) := r+4, λ({a′ i, ci}) := 2, and λ({ci, b′ i}) := r + 5. For each F ∈F, each u ∈U, and each i ∈[1, k], we set λ({wF,u i , vF,u i }) := λ({vF,u i , ⊥i}) := αF , λ({wF,u i , qF,u i }) := 1, λ({wF,u i , ai}) := r + 5, and λ({wF,u i , bi}) := 2. Finally, for each i ∈[1, k] and each u ∈F ′ i (that is, the ith hyperedge of S), we set λ({wF,u i , ci}) := αF ′ i . All other edges of E receive the empty set under λ. We now show that λ realizes D."
  },
  {
    "prompt": "Read and understand this section about label, solid, edges from a research paper.",
    "response": "To this end, we first show that all solid edges are realized. Note that only some solid edges between ci and vertices of {wF,u i | i ∈[1, k], F ∈F, u ∈F} did not receive a label under λ. Let x = wF,u i . Then, there is the temporal paths (x, bi, b′ i, ci) with label sequence (2, r + 4, r + 5) under λ that realizes the arc (x, ci). Similarly, there is the temporal paths (ci, a′ i, ai, x) with label sequence (2, r + 4, r + 5) under λ that realizes the arc (ci, x)."
  },
  {
    "prompt": "Explain a concept involving realized, arcs, solid in computational complexity.",
    "response": "Consequently, each solid edge is realized. Next, we show that exactly the specified dashed arcs are realized. To this end, we first show that all arcs of D are realized. For each i ∈[1, k], the arcs (a′ i, bi), (bi, ci), (ci, ai), (a′ i, ⊤), and (⊤, b′ i) are realized via the unique path between their endpoints in G[{⊤, ai, a′ i, bi, b′ i, ci}]."
  },
  {
    "prompt": "Read and understand this section about wF,u, arbitrary, arc from a research paper.",
    "response": "The arc (bi, ai) is realized via the path (bi, wF,u i , ai) for an arbitrary vertex wF,u i ∈V , and the arc (bi, ⊤) is realized via the path (bi, wF ′ i ,u i , ci, ⊤) for some arbitrary u ∈F ′ i. Note that such a vertex exists, since each hyperedge is non-empty. Let u ∈U. Since S is a set cover, there is some j ∈[1, k], such that u ∈F ′ j. Hence, the arc (u, ⊤) is realized via the path (u, w F ′ j,u j , cj, ⊤). Let F ∈F be a hyperedge that contains u."
  },
  {
    "prompt": "Read and understand this section about path, wF,u, realized from a research paper.",
    "response": "For each i ∈[1, k], the arc (u, ai) is realized via the path (u, wF,u i , ai), and the arcs (u, bi), (u, b′ i), and (u, ci) are realized via subpaths of the path (u, wF,u i , bi, b′ i, ci)."
  },
  {
    "prompt": "Understanding the computational aspects of qF,u, vF,u, wF,u.",
    "response": "Addition- ally, the arcs (u, vF,u i ), (vF,u i , ai), (bi, vF,u i ), (wF,u i , ⊤), (wF,u i , b′ i), (a′ i, wF,u i ), (qF,u i , vF,u i ), (qF,u i , ai), (qF,u i , bi), and (qF,u i , b′ i) are realized via the unique path between their end- points in G[{u, vF,u i , wF,u i , qF,u i , ⊤, ai, a′ i, bi, b′ i}], and the arc (qF,u i , ci) is realized via the path (qF,u i , wF,u i , bi, b′ i, ci). Now consider the arcs in D between vi-vertices. Let Fx, Fy ∈F with x < y and let ux ∈Fx and uy ∈Fy."
  },
  {
    "prompt": "Read and understand this section about vFx,ux, vFy,uy, realized from a research paper.",
    "response": "For each i ∈[1, k], the arc (vFx,ux i , vFy,uy i ) is realized via the path (vFx,ux i , ⊥i, vFy,uy i ) with label sequence (αx, αy) = (x + 2, y + 2). Finally, we show that no non-arc of D is realized. To this end, we analyze the structure of temporal paths under λ. Observe that for each vertex x in R := {⊤}∪U∪{vF,u i | i ∈[1, k], F ∈ F, u ∈F}, λ assigns the same label to all edges incident with x. This implies that no strict temporal path under λ has a vertex of R as an internal vertex."
  },
  {
    "prompt": "This is a technical discussion about wF,u, qF,u, receives in theoretical computer science.",
    "response": "Moreover, (wF,u i , qF,u i , ⊤) is the only temporal path of length at least two has qF,u i as an internal vertex, since {wF,u i , qF,u i } receives label 1 and ⊤∈R. Since (wF,u i , ⊤) ∈A, these paths do not realize non-arcs of D. In particular, no vertex besides wF,u i and ⊤can reach qF,u i ."
  },
  {
    "prompt": "This is a technical discussion about temporal, Recognizing, Realizing in theoretical computer science.",
    "response": "Similarly, the only temporal paths that contain ⊥i as an internal vertex are the paths of the form (vFx,ux i , ⊥i, vFy,uy i ) 32 Recognizing and Realizing Temporal Reachability Graphs with x < y, since each solid neighbor of ⊥i is from R. Since (vFx,ux i , vFy,uy i ) ∈A, these paths do not realize non-arcs of D. Let R′ := R ∪{qF,u i | i ∈[1, k], F ∈F, u ∈F} ∪{⊥i | i ∈[1, k]}."
  },
  {
    "prompt": "Understanding the computational aspects of wF,u, temporal, path.",
    "response": "Next, we show that for each s = wF,u i ∈V , there is no temporal path from s to any vertex t ∈V \\ {wF,u i , vF,u i , qF,u i , ai, bi, a′ i, b′ i, ci, ⊤, u}. Assume towards a contradiction that there is such a temporal path P. Note that P has length at least 2, since there is no solid edge (and thus no labeled edge) between s and t. By the above argumentation, P cannot contain any of the vertices of R′ as internal vertices."
  },
  {
    "prompt": "This is a technical discussion about edge, label, implies in theoretical computer science.",
    "response": "This implies that the P has as first internal vertex s′ one of the solid neighbors of s that is not in R′. That is, s′ ∈{ai, bi, ci}. For s′ = ai, the edge {s, ai} has label r + 5, which is the largest globally assigned label. Hence, P cannot go over ai. For s′ = bi, the edge {s, bi} has label 2, which is the largest label incident with bi besides the one of edge {bi, b′ i}."
  },
  {
    "prompt": "This is a technical discussion about label, edge, neighbor in theoretical computer science.",
    "response": "Since b′ i has only one other solid neighbor (namely ci), P would also have to traverse the edge {b′ i, ci} at time r + 5, which is the largest globally assigned label. Hence, P cannot go over bi. For s′ = ci, the edge {s, ci} has either no label or label αF if F ′ i = F. The only two edges incident with ci with a larger label are the edges towards ⊤(which is in R′) and towards b′ i (for which the edge has label r + 5). In all cases, P cannot reach t; a contradiction."
  },
  {
    "prompt": "Read and understand this section about vertices, wF,u, Similarly from a research paper.",
    "response": "Similarly, only vertices of Z := {wF,u i , vF,u i , qF,u i , ai, bi, a′ i, b′ i, ci, ⊤, u} can reach wF,u i under λ. This in particular implies that each temporal path that contains vertex wFu i contains only vertices of Z. Since G[Z] is a graph on ten vertices, it can easily be verified that no non-arc between the vertices of Z is realized in G[Z]. Hence, if there would be a temporal path under λ that realizes a non-arc, it has to be in G′ := G−{wF,u i | i ∈[1, k], F ∈F, u ∈U}."
  },
  {
    "prompt": "This is a technical discussion about components, iii, Note in theoretical computer science.",
    "response": "Note that the only connected components of G′ are (i) components of size 1 containing a single vertex of U, (ii) stars with center ⊥i for some i ∈ [1, k] with all its solid neighbors (that is, the vertices of {vF,u i | i ∈[1, k], F ∈F, u ∈U}) and (iii) the component G′′ := [{⊤}∪{ai, a′ i, bi, b′ i, ci | i ∈[1, k]}∪{qF,u i | i ∈[1, k], F ∈F, u ∈U}]. In the first type of components, there are trivially no temporal paths realizing non-arcs of D."
  },
  {
    "prompt": "This is a technical discussion about temporal, type, realizing in theoretical computer science.",
    "response": "In the second type of components, there are also no temporal paths realizing non-arcs of D, as we already discussed the temporal paths that go over ⊥i. Thus, consider the last component G′′. Recall that ⊤is in R which implies that no temporal path can pass through ⊤. Hence, if there would be a temporal path under λ that realizes a non-arc, it has to be in G[C ∪{⊤}] for some connected component C of G′′ −{⊤}."
  },
  {
    "prompt": "Read and understand this section about Set, realizes, non-arc from a research paper.",
    "response": "Each such resulting graph has size at most 6 and it can be verified easily that no included temporal path under λ realizes a non-arc. Concluding, λ realizes D. ◀ 7 Parameterizing by the Feedback Edge Set Number In this section we generalize our algorithm on tree instances of URGR to tree-like graphs. A feedback edge set of a graph G is a set F of edges of G, such that G −F is acyclic."
  },
  {
    "prompt": "Scientific context on edge, Theorem, set and their role in graph theory.",
    "response": "We denote by fes the feedback edge set number of the solid graph G of D, that is, the size of the smallest feedback edge set of G. ▶Theorem 33. Each version of URGR can be solved in fesO(fes2) · nO(1) time. Recall that the parameter can presumably not be replaced by a smaller parameter like feedback vertex set number or the treedepth (see Theorem 31). In the remainder, we present the proof of Theorem 33."
  },
  {
    "prompt": "Explain a concept involving URGR, Strict, analogously in computational complexity.",
    "response": "We will only describe the algorithm for the most general version Any Strict URGR, but for all more restrictive versions, all arguments work analogously. Thomas Erlebach, Othon Michail, Nils Morawietz 33 Definitions and Notations. An abstract overview of the basic definitions is given in the following table."
  },
  {
    "prompt": "Scientific context on vertices, graph, set and their role in graph theory.",
    "response": "Object meaning D = (V, A) input graph G = (V, E) solid graph of D F a fixed feedback edge set of G of minimum size X endpoints of the edges of F V ∗ the 2-core of G V ′ := V \\ V ∗ non-core vertices (no vertex of V ′ is in a cycle in G) Tv for some v ∈V ∗ pendant tree with root v Y3 vertices of degree at least 3 in G[V ∗] −F Y2 vertices of V ∗that are adjacent to some vertex of X ∪Y3 X∗:= X ∪Y2 ∪Y3 At least all vertices of degree at least 3 in G[V ∗] and their neighbors Let G = (V, E) be the solid graph and let F be a minimum size feedback edge set of G."
  },
  {
    "prompt": "Explain a concept involving connected, component, components in computational complexity.",
    "response": "We assume without loss of generality that G is connected, as otherwise, we can solve each connected component independently, or detect in polynomial time that D is not realizable if there are dashed arcs between different connected components of the solid graph. Moreover, let X denote the endpoints of the edges of F. Note that |X| ≤2 · |F|. We define the set V ∗ as the (unique) largest subset S of vertices of V that each have at least two neighbors in G[S], that is, V ∗is the 2-core [22] of G."
  },
  {
    "prompt": "Scientific context on vertices, removing, Note and their role in graph theory.",
    "response": "This set can be obtained from V by repeatedly removing degree-1 vertices. Note that V ∗contains all vertices of V that are part of at least one cycle in G (but may also contain vertices that are not part of any cycle). Moreover, we define V ′ := V \\ V ∗. Note that X ⊆V ∗, since F is a minimum-size feedback edge set. Since G is connected, for each vertex v′ ∈V ′, there is a unique vertex v ∈V ∗which has closest distance to v′ among all vertices of V ∗."
  },
  {
    "prompt": "Understanding the computational aspects of vertex, denote, set.",
    "response": "For a vertex v ∈V ∗, denote by Vv the set of all vertices of V ′ for which v is the closest neighbor in V ∗. Note that Vv might be empty. Since v′ is in no cycle, removing v from G would result in v′ ending up in a component that has no vertex of V ∗. Hence, Tv := G[{v} ∪Vv] is a tree for which we call v the root. Moreover, we call Tv the pendant tree of v. Recall that each vertex of V ∗has degree at least 2 in G[V ∗]."
  },
  {
    "prompt": "Scientific context on degree, vertex, leaf and their role in graph theory.",
    "response": "Furthermore, G −F is acyclic and thus G[V ∗] −F is a tree, where each leaf is a vertex of X. Let Y3 denote the set of all vertices of degree at least 3 in G[V ∗] −F, and let Y2 be the neighbors of X ∪Y3 in V ∗\\ (X ∪Y3). Recall that each leaf of G[V ∗] −F is a vertex of X. Moreover, in each tree with ℓleaves, there are O(ℓ) vertices having (i) degree at least 3 or (ii) a neighbor of degree at least 3. This implies that |Y2 ∪Y3| ∈O(|X|). Let X∗:= X ∪Y2 ∪Y3."
  },
  {
    "prompt": "Scientific context on Figure, vertex, section and their role in graph theory.",
    "response": "See Figure 4 for an illustration of these sets of vertices and some of the main definitions used in this section. By the above, |X∗| ∈O(|X|) ⊆O(fes). Recall that each vertex of Y2 has degree exactly 2 in G[V ∗]. Moreover, note that each vertex of V ∗\\ X∗is part of a unique path P where each internal vertex of P is from V ∗\\ X∗and the endpoints of P are from Y2."
  },
  {
    "prompt": "Explain a concept involving acyclic, internal, degree in computational complexity.",
    "response": "For each such path, each internal vertex has degree exactly 2 in G[V ∗] and there are at most |X∗| −1 such paths, since G[V ∗] −F is acyclic. Note that X∗contains all vertices that are part of triangles in G[V ∗] and thus in G. This implies that in each minimal realization of D, each edge incident with at least one vertex of V \\ X∗receives at most two labels (see Lemma 2). We now formally define the above mentioned paths between the vertices of Y2 is a more general way. ▶Definition 34."
  },
  {
    "prompt": "This is a technical discussion about vertex, connector, Figure in theoretical computer science.",
    "response": "Let W with X∗⊆W ⊆V ∗and let P be a path of length at least 2 in G[V ∗] with endpoints a and b in W and all internal vertices from V ∗\\ W. We call P 34 Recognizing and Realizing Temporal Reachability Graphs X vertex Y3 vertex Y2 vertex fes edge V ∗ X∗-connector X∗-connector tree u Tu v Tv X∗:= X ∪Y2 ∪Y3 Figure 4 An illustration of the main definitions used by the FPT algorithm for parameter fes. The grey area contains the vertices of V ∗, that is, the 2-core of G. a W-connector."
  },
  {
    "prompt": "Read and understand this section about W-connector, degree, call from a research paper.",
    "response": "Moreover, let C := G[V (P) ∪S q∈V (P )\\{a,b} Vq]. We call C a W-connector tree and the extension of P. Note that the endpoints of each X∗-connector are from Y2 and thus have degree 2 in G[V ∗]. Moreover, each internal vertex v of a W-connector P has degree exactly 2 in G[V ∗], that is, the only two neighbors of v in G[V ∗] are the predecessor and the successor of v in P. This also implies that each endpoint of a W-connector has degree exactly 2 in G[V ∗]."
  },
  {
    "prompt": "This is a technical discussion about Lemma, W-connector, Observation in theoretical computer science.",
    "response": "Since each edge of P is incident with at least one vertex of V \\ W ⊆V \\ X∗, in each minimal realization of D, each edge of each W-connector receives at most two labels (see Lemma 2). The latter also holds for each W-connector tree. ▶Observation 35. Let W with X∗⊆W ⊆V ∗and let q ∈V \\ (W ∪S w∈W Vw). There is exactly one W-connector tree that contains q. ▶Observation 36. Let W with X∗⊆W ⊆V ∗. Then, there are O(|W| + fes) edges between vertices of W in G and there are O(|W| + fes) W-connectors."
  },
  {
    "prompt": "Explain a concept involving Algorithm, Description, reduction in computational complexity.",
    "response": "Abstract Description of the Algorithm Our algorithm uses two preprocessing steps. In the first step, we will present a polynomial-time reduction rule based on the splitting operations presented in Section 4. With this reduction rule, we will be able to remove vertices from pendant trees that have many vertices. After this reduction rule is applied exhaustively, for each vertex x ∈V ∗, the pendant tree Tx will have O(dx) vertices, where dx denotes the degree of x in G[V ∗]."
  },
  {
    "prompt": "This is a technical discussion about step, set, properties in theoretical computer science.",
    "response": "In the second step, we extend the set X∗to a set W ∗⊆V ∗such that each W ∗-connector has some useful properties (we will define connectors with these properties as nice connectors)."
  },
  {
    "prompt": "Explain a concept involving connector, Erlebach, Othon in computational complexity.",
    "response": "Intuitively, a connector P with extension C is nice if (i) in a realization for D, the arcs Thomas Erlebach, Othon Michail, Nils Morawietz 35 in D[V (C)] can only be realized by temporal paths that are contained in the connector tree C, and (ii) for each arc between a vertex outside of C and a vertex inside of C, we can in polynomial time detect over which (unique) edge of the connector incident with one of its endpoints this arc is realized."
  },
  {
    "prompt": "Explain a concept involving fes, show, time in computational complexity.",
    "response": "As we will show, we can compute such a set W ∗of size O(fes) in polynomial time, or correctly detect that the input graph is not realizable. By the first part, we additionally get that the total number of vertices in pendant trees that have their root in W ∗is O(|W ∗| + fes) = O(fes). The algorithm then works as follows: We iterate over all possible partial labelings λ on the O(fes) edges incident with vertices of W ∗or vertices of pendant trees that have their root in W ∗."
  },
  {
    "prompt": "Understanding the computational aspects of fes, edges, show.",
    "response": "As we will show, we can assume that each of these edges will only receive O(fes) labels in each minimal realization. For each such labeling λ, we need to check whether we can extend the labeling to the so far unlabeled edges, that is, the edges that are part of any W ∗-connector."
  },
  {
    "prompt": "Scientific context on fes, Observation, set and their role in graph theory.",
    "response": "As we will show, we can compute for each such connector P with extension C in polynomial time a set LP of O(1) labelings for the edges of C, such that if λ can be extended to a realization for D, then we can extend λ (independently from the other connectors) by one of the labelings in LP . Since W ∗has size O(fes), there are only O(fes) many W ∗-connectors (see Observation 36) and for each such connector P, the set LP of labelings has constant size."
  },
  {
    "prompt": "This is a technical discussion about fes, combinations, Proposition in theoretical computer science.",
    "response": "Our algorithm thus iterates over all possible O(1)O(fes) = 2O(fes) possible labeling combinations for the connectors and checks whether one of these combinations extends λ to a realization for D. Summarizing, we will show the following. ▶Proposition 37."
  },
  {
    "prompt": "Scientific context on fes, edges, compute and their role in graph theory.",
    "response": "In polynomial time, we can detect that D is not realizable, or compute a set W ∗with X∗⊆W ∗⊆V ∗, such that the set of edges E∗that are (i) incident with at least one vertex of W ∗or (ii) part of some pendant tree with root in W ∗, has size O(fes) and where for each labeling λ of the edges of E∗, we can in 2O(fes) · nO(1) time detect that there is no frugal realization for D that agrees with λ on the labels of all edges of E∗, or compute a labeling that realizes D."
  },
  {
    "prompt": "Scientific context on fes, fesO, time and their role in graph theory.",
    "response": "The running time of this algorithm is then fesO(fes2) · nO(1), since (i) all preprocessing steps run in polynomial time, (ii) we only have to consider fesO(fes2) labelings λ (since we prelabel O(fes) edges with O(fes) labels each), and (iii) for each labeling λ, we only have to check for 2O(fes) possible ways to extend λ to a realization for D. To see the second part, we show that it is sufficient to assign only labels of {i · 2 · n | i ∈O(fes2)} in λ."
  },
  {
    "prompt": "Scientific context on merging, algorithm, describe and their role in graph theory.",
    "response": "To describe our algorithm, we will use the notion of “merging” several partial labelings. This is formally defined as follows. ▶Definition 38. Let E1 and E2 be (not necessarily disjoint) subsets of E. For λ1 : E1 →2N and λ2 : E2 →2N, we define the labeling λ1 ⋉λ2 : E1 ∪E2 →2N by (λ1 ⋉λ2)(e) := λ2(e) if e ∈E2 and (λ1 ⋉λ2)(e) := λ1(e) otherwise. 7.1 Independent structural implication In our algorithm, we will enumerate all reasonable prelabelings of a set of O(fes) edges."
  },
  {
    "prompt": "Read and understand this section about ensure, sufficient, fes from a research paper.",
    "response": "To ensure that this enumeration can be done in FPT-time for fes, we need to ensure that ’few’ labels per edges are sufficient. The following result ensures this property, but is also of independent interest for the understanding of our problem. ▶Theorem 39. Let D be an instance of URGR with solid graph G. In each minimal realization of D, each edge of G receives O(fes) labels. 36 Recognizing and Realizing Temporal Reachability Graphs Proof. Let λ be a realization of D."
  },
  {
    "prompt": "This is a technical discussion about fes, labels, edge in theoretical computer science.",
    "response": "Let F be a feedback edge set of G of size fes. Moreover, let V ∗and X∗be defined as above based on F and recall that |X∗| ∈O(|F|) = O(fes). If each edge receives O(fes) labels, we are done. So assume that there is an edge e that receives ω(fes) labels. We will show that λ is not a minimal realization for D."
  },
  {
    "prompt": "Scientific context on realization, Lemma, endpoints and their role in graph theory.",
    "response": "Note that if not both endpoints of e are in X∗, then e is not part of a triangle in G, which implies that λ is not a minimal realization, since in each such realization, e would receive at most two labels (see Lemma 2). Thus, assume in the following that both endpoints of e are from X∗."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, temporal, path.",
    "response": "Similar to the proof of Lemma 3, we denote for each vertex v ∈V with αv the smallest label assigned to e for which there is a temporal path under λ that starts in v and traverses e at time αv, if such a label exists. Let Le := {αv | v ∈V, αv exists}. Note that if there is at least one label ℓassigned to e that is not contained in Le, then removing label ℓfrom e still preserves at least one temporal path between any two vertices for which a temporal path exists under λ."
  },
  {
    "prompt": "This is a technical discussion about fes, minimal, vertex in theoretical computer science.",
    "response": "Consequently, if such a label ℓexists, we can immediately conclude that λ is not a minimal realization for D. Recall that e receives ω(fes) labels. Hence, to show that λ is not a minimal realization, it suffices to show that Le has size O(fes). To this end, note that each vertex of V is (a) a vertex of X∗, (b) contained in some X∗- connector tree C (excluding the two vertices of C in X∗), or (c) a vertex of some pendant tree rooted in X∗(excluding the root)."
  },
  {
    "prompt": "Scientific context on fes, vertices, contribute and their role in graph theory.",
    "response": "Since X∗has size O(fes), the vertices of X∗contribute O(fes) different labels to Le. Let C be a X∗-connector tree where a and b are the two unique vertices of X∗in C. By definition of connector trees, a and b are both leaves in C."
  },
  {
    "prompt": "Understanding the computational aspects of incident, unique, vertices.",
    "response": "Moreover, since these vertices are the only vertices of X∗in C and both endpoints of e are from X∗, each temporal path starting in C and traversing edge e has to leave the connector tree via the unique edge {a, a′} incident with a in C or via the unique edge {b, b′} incident with b in C. Since both edges are not part of a triangle, they both receive at most two labels each under λ, or λ is not a minimal realization for D (see Lemma 2)."
  },
  {
    "prompt": "Read and understand this section about fes, labels, vertices from a research paper.",
    "response": "Thus, assume that both edges receive at most two labels each. Hence, all vertices of V (C) \\ {a, b} contribute in total at most four labels to Le. Since there are O(fes) X∗-connectors (see Observation 36), all vertices that are contained in X∗-connectors thus contribute O(fes) labels in total to Le. To show that Le has size O(fes), it suffices to show that all vertices that are contained in pendant trees with roots in X∗contribute O(fes) labels in total to Le. Let x ∈X∗."
  },
  {
    "prompt": "Explain a concept involving edge, labels, traverses in computational complexity.",
    "response": "We show that all vertices of Tx contribute O(dx) labels in total to Le, where dx denotes the degree of x in G[V ∗]. Let v ∈Vx and let P be a temporal path from v that traverses the edge e at time αv. Note that P traverses one edge e1 incident with x in Tx and then traverses an edge e2 between x and some other vertex of V ∗. Since e1 is an edge of Tx, e1 is a bridge in G, which implies that max λ(e1) ≤min λ(e2)."
  },
  {
    "prompt": "Explain a concept involving min, time, labels in computational complexity.",
    "response": "Hence, we can assume that P either traverses the edge e2 at time min λ(e2) or at time min(λ(e2) \\ min λ(e2)) (if e2 receives at least two labels). Thus, for each edge between x and some other vertex of V ∗, at most two labels are used that define the αv values for the vertices v ∈Vx. This implies that the vertices of Tx contribute O(dx) labels in total to Le. Note that Observation 36 implies that the sum of degrees of all vertices of X∗in G[V ∗] is O(fes)."
  },
  {
    "prompt": "Scientific context on fes, contribute, labels and their role in graph theory.",
    "response": "Hence, all vertices that are contained in pendant trees with roots in X∗contribute O(fes) labels in total to Le. Consequently, Le has size O(fes). This implies that λ is not a minimal realization, since e receives ω(fes) labels under λ. ◀ 7.2 Dealing with the pendant trees Our goal in this subsection is to prove the following. Thomas Erlebach, Othon Michail, Nils Morawietz 37 ▶Proposition 40."
  },
  {
    "prompt": "This is a technical discussion about vertices, instance, time in theoretical computer science.",
    "response": "In polynomial time, we can (i) detect that D is not realizable or (ii) ob- tain an equivalent instance by (a) adding polynomially many degree-1 vertices to D and (b) removing vertices from D, such that in the resulting instance, in each pendant tree Tv for v ∈V ∗, O(dv) vertices remain, where dv denotes the degree of v in G[V ∗]. For each vertex v ∈V ∗, let Bv denote the vertices of Tv that are adjacent to v in G. By definition of Tv, for each vertex b ∈Bv, the edge {v, b} is a bridge in G."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, bridge, realizable.",
    "response": "We call Bv the bridge vertices of Tv. For each vertex v ∈V ∗, apply the operations behind Lemma 14, Lemma 16, and Lemma 17 to each bridge between v and Bv. These operations might possibly detect that D is not realizable. In this case, report that D is not realizable. Otherwise, each such operation individually splits our instance into at most two instances. Moreover, at most one of these instances contains cycles, since each bridge in Tv has a tree on one side."
  },
  {
    "prompt": "Read and understand this section about realizable, instance, tree from a research paper.",
    "response": "For all resulting tree instances, we check in polynomial time whether they are realizable. If at least one is not realizable, we correctly output that D is not realizable. Otherwise, we only (again) remain with one instance, and this instance contains cycles. In particular, this instance is (besides some possible degree-1 vertices) a subgraph of D[V ∗∪N(V ∗)]. For convenience, assume that D already is the unique resulting instance."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, DAG, complete.",
    "response": "Note that G[{v} ∪Bv] is an induced star with center v, where each edge between v and a vertex of Bv is a bridge in G. Hence, with the same arguments behind Lemma 20, D is not realizable if D[Bv] is not a complete DAG, since the operations behind Lemma 14 and Lemma 17 were applied on all bridges between v and Bv. Hence, if D[Bv] is not a complete DAG, we correctly output that D is not realizable. Otherwise, we continue with the knowledge that D[Bv] is a complete DAG."
  },
  {
    "prompt": "This is a technical discussion about DAG, unique, total in theoretical computer science.",
    "response": "This complete DAG D[Bv] thus implies a unique total order on the vertices of Bv starting from the unique source in D[Bv]. In the following, we denote the ith vertex of this total order of Bv by bi v. Consequently, for each 1 ≤i < j ≤|Bv|, (bi v, bj v) ∈A. Additionally, we check for any two adjacent bridges e and e′ whether they are both bundled and separated. If this is the case for any pair of adjacent bridges, we can correctly detect that D is not realizable due to Corollary 13."
  },
  {
    "prompt": "Explain a concept involving time, separated, property in computational complexity.",
    "response": "Since this property can be checked in polynomial time, we assume in the following that no pair of adjacent bridges is both bundled and separated. We now observe a necessary property for realizable instances. ▶Lemma 41. Let v ∈V ∗and let u ∈V \\ ({v} ∪Vv)."
  },
  {
    "prompt": "Understanding the computational aspects of intervals, realizable, set.",
    "response": "If D is realizable, then the set [1, |Bv|] can be partitioned in three consecutive and pairwise disjoint (possibly empty) intervals I1, I2, and I3, such that (bi v, u) ∈A and (u, bi v) /∈A for each i ∈I1, (bi v, u) /∈A and (u, bi v) /∈A for each i ∈I2, and (u, bi v) ∈A and (bi v, u) /∈A for each i ∈I3. Proof. Since u ∈V \\ ({v} ∪Vv), for each bi v ∈Bv, A contains at most one of the arcs (u, bi v) and (bi v, u). Let λ be a realization of D."
  },
  {
    "prompt": "Read and understand this section about temporal, path, min from a research paper.",
    "response": "If (bi v, u) ∈A for some i ∈[2, |Bv|], then there is a temporal (bi v, u)-path under λ. This temporal path starts by traversing the edge {bi v, v} at time at least min λ({bi v, v}). Since for each j ∈[1, i −1], A contains the arc (bj v, bi v), min λ({bj v, v}) ≤min λ({bi v, v}). Hence, there is also a temporal (bj v, u)-path under λ. This implies that (bj v, u) ∈A."
  },
  {
    "prompt": "Explain a concept involving temporal, interval, exists in computational complexity.",
    "response": "Moreover, this implies that the interval I1 exists as stated and extends from the start of Bv (ending with the largest i for which (bi v, u) ∈A). Similarly, if (u, bi v) ∈A for some i ∈[1, |Bv| −1], then there is a temporal (u, bi v)-path un- der λ. This temporal path ends by traversing the edge {v, bi v} at time at most max λ({v, bi v}). 38 Recognizing and Realizing Temporal Reachability Graphs Since for each j ∈[i + 1, |Bv|], A contains the arc (bi v, bj v), min λ({v, bj v}) ≥max λ({v, bi v})."
  },
  {
    "prompt": "Explain a concept involving temporal, path, implies in computational complexity.",
    "response": "Hence, there is also a temporal (u, bj v)-path under λ. This implies that (u, bj v) ∈A. Moreover, this implies that the interval I3 exists as stated and extends to the end of Bv (starting from the smallest i for which (bi v, u) ∈A). Consequently, the interval I2 contains the remaining vertices of Bv. None of the vertices of I2 can have an arc from or towards u as otherwise, one of the other two intervals could be extended."
  },
  {
    "prompt": "Explain a concept involving Note, pair, time in computational complexity.",
    "response": "◀ Note that this property can be checked for each pair of vertices v and u in polynomial time. Hence, if the property does not hold for some vertex pair, we correctly output that D is not realizable. In the remainder, assume that these intervals exist for each pair of vertices v and u. Moreover, note that they can be computed in polynomial time. We may call the interval I2 the middle interval of u with respect to Tv."
  },
  {
    "prompt": "Explain a concept involving edges, Based, define in computational complexity.",
    "response": "Based on these intervals, we now define for each vertex v ∈V ∗and each neighbor u of v in V ∗a set of O(1) edges in Tv that ’surround’ the edge {u, v}. Intuitively, these edges will be the only edges that can possibly share a label with {u, v} in any realization of G. ▶Definition 42. Let e := {u, v} ⊆V ∗with e ∈E, and let Bv denote the bridge vertices of Tv. Moreover, let I2 denote the middle interval of u with respect to Tv according to Lemma 41."
  },
  {
    "prompt": "Scientific context on edges, interval, index and their role in graph theory.",
    "response": "If Bv ̸= ∅, we define the edges of Tv that surround e as: {{v, bj v} | j ∈[i −2, i + 2] ∩[1, |Bv|]}, if the interval I2 has size at most one, where i ∈ [1, |Bv|] is the smallest index, such that (bj v, u) ∈A for each 1 ≤j < i and (u, bk v) ∈A for each i < k ≤|Bv|, and as ∅, otherwise. In the latter case, we say that the edges {bj v, v} and {bk v, v} block the edge e, where j = min I2 and k = max I2. We now show that this definition matches the previous intuition. ▶Lemma 43."
  },
  {
    "prompt": "Explain a concept involving realization, denote, surround in computational complexity.",
    "response": "Let e := {u, v} ⊆V ∗with e ∈E, and let S denote the edges of Tv that surround e. Then, in each realization of D, e shares no label with any edge of Tv that is incident with v and not in S. Moreover, for each i ∈[1, |Bv| −1] where {v, bi v} and {v, bi+1 v } are not in S, in each frugal realization λ of D, λ(e) ∩[min λ({v, bi v}), max λ({v, bi+1 v })] = ∅. Proof. Recall that each bridge e of G receives at least one label in each realization of D. Let Bv be the bridge vertices of Tv."
  },
  {
    "prompt": "Explain a concept involving DAG, min, Recall in computational complexity.",
    "response": "Recall that D[Bv] is a complete DAG. This implies that for each realization λ for D and for each two bk v and bℓ v from Bv with k < ℓ, min λ({v, bk v}) < max λ({v, bℓ v}) and max λ({v, bk v}) ≤min λ({v, bℓ v}). We distinguish two case. Case 1: S = ∅. That is, there is some i ∈[1, |Bv|−1], such that there is no arc between u and any vertex of {bi v, bi+1 v }. Let λ be a realization for D. We show that λ(e) = ∅. Assume towards a contradiction that λ(e) ̸= ∅."
  },
  {
    "prompt": "Understanding the computational aspects of Observation, implies, arc.",
    "response": "Since there is no arc between u and bi v, Observation 1 implies that there is some α ∈N, such that λ(e) = λ({v, bi v}) = {α}. Since (bi v, bi+1 v ) ∈A, max λ({v, bi+1 v }) > min λ({v, bi v}) = α. Thus, (u, v, bi+1 v ) is a temporal path under λ. This contradicts the fact that λ realizes D, since (u, bi+1 v ) is not an arc of D. Hence, the statement holds if S = ∅. Case 2: S ̸= ∅."
  },
  {
    "prompt": "This is a technical discussion about Assume, edge, contradiction in theoretical computer science.",
    "response": "Assume towards a contradiction that e shares a label with at least one edge {v, bj v} of Tv that is incident with v and does not surround e. Let i ∈[1, |Bv|] be the index over which S was defined. We distinguish two subcases. Case 2.1: j < i. By definition {{v, bℓ v} | ℓ∈{i −2, i −1, i}} ⊆S. Hence, j ≤i −3. By choice of i, (bi−1 v , u) ∈A. Hence, max λ({v, bi−1 v }) ≤min λ(e) ≤max λ(e)."
  },
  {
    "prompt": "Explain a concept involving min, max, Erlebach in computational complexity.",
    "response": "By the initial Thomas Erlebach, Othon Michail, Nils Morawietz 39 argumentation, this implies that max λ({v, bj v}) ≤min λ({v, bi−2 v }) < max λ({v, bi−1 v }) ≤ min λ(e). This contradicts the assumption that e shares a label with {v, bj v}. Case 2.2: j > i. This case can be shown analogously. Consequently, no edge of Tv incident with v outside of S shares a label with e. We now show the second part of the statement."
  },
  {
    "prompt": "This is a technical discussion about min, max, label in theoretical computer science.",
    "response": "That is, let i ∈[1, |Bv| −1] where both ei := {v, bi v} and ei+1 := {v, bi+1 v } are not in S, we show that e has no label from [min λ(ei), max λ(ei+1)]. Assume towards a contradiction that this is not the case. By the above, we know that λ(e) ∩ (λ(ei) ∪λ(ei+1)) = ∅. Moreover, e cannot have a label between min λ(ei) and max λ(ei), as otherwise, a solid edge between u and bi v would be realized. Similarly, e cannot have a label between min λ(ei+1) and max λ(ei+1)."
  },
  {
    "prompt": "Scientific context on remaining, option, receives and their role in graph theory.",
    "response": "Hence, the only remaining option is that e receives a label strictly between max λ(ei) and min λ(ei+1). This then implies that the arcs (bi v, u) and (u, bi+1 v ) are realized. Hence, bi v (bi+1 v ) is from the interval I1 (I3) of u with respect to the pendant tree Tv according to Lemma 41. By definition of S, both ei and ei+1 are in S; a contradiction. ◀ Based on this insight about surrounding edges, we now define another sanity check."
  },
  {
    "prompt": "Understanding the computational aspects of vertices, edge, ensure.",
    "response": "This will then ensure that we can assume some consistent reachabilities between vertices outside of Tv and bridge vertices that are in Tv but that do not surround any external edge. The sanity check reads as follows. ▶Lemma 44. Let v ∈V ∗with |Bv| ≥3 and let i ∈[1, |Bv|−1] such that neither edge {bi v, v} nor edge {bi+1 v , v} is surrounding or blocking any edge incident with v that is outside of Tv."
  },
  {
    "prompt": "Scientific context on Dbivx, Dxbi, exists and their role in graph theory.",
    "response": "If D is realizable, then for each vertex x ∈V \\ ({v} ∪Vv) Dxbiv = Dxbi+1 v (and Dxbi+1 v = Dx outi+1 v , where outi+1 v is the out-leaf of bi+1 v , if it exists), and Dbivx = Dbi+1 v x (and Dbivx = Dini vx, where ini v is the in-leaf of bi v, if it exists). Proof. Let λ be a realization for D and let x ∈V \\ ({v} ∪Vv). Moreover, let α be the smallest label such that there is a temporal (x, v)-path P under λ that reaches v at time α."
  },
  {
    "prompt": "Read and understand this section about edge, min, path from a research paper.",
    "response": "Since x /∈V \\ ({v} ∪Vv), the path P traverses an edge e between v and some other vertex of V ∗at time α. Since neither edge {bi v, v} nor edge {bi+1 v , v} is surrounding e, Lemma 43 implies that α < min λ({bi v, v}) or α > max λ({bi+1 v , v}). If α < min λ({bi v, v}), P can be extended to reach the vertices bi v, bi+1 v , and outi+1 (if the latter exists). In this case, Dxbiv = Dxbi+1 v = Dx outi+1 v = 1."
  },
  {
    "prompt": "Explain a concept involving outi, max, property in computational complexity.",
    "response": "Otherwise, that is, if α > max λ({bi+1 v , v}) , x can reach none of bi v, bi+1 v , and outi+1. In this case, Dxbiv = Dxbi+1 v = Dx outi+1 v = 0. The reachability towards x can be shown analogously. ◀ This property can be checked for each vertex v ∈V ∗and each bi v ∈Vv in polynomial time. If the property does not hold for some vertex pair, we correctly output that D is not realizable. In the remainder, thus assume that the property holds for all vertex pairs."
  },
  {
    "prompt": "Explain a concept involving Based, check, trees in computational complexity.",
    "response": "Based on this sanity check, we now define our final reduction rule to reduce the size of pendant trees. ▶Lemma 45. Let v ∈V ∗and let bi v ∈Bv with 6 ≤i ≤|Bv| −5, such that none of the edges {{v, bj v} | i −5 ≤j ≤i + 5} is surrounding or blocking any edge e incident with v and some other vertex of V ∗."
  },
  {
    "prompt": "Understanding the computational aspects of realizable, out-leaf, removing.",
    "response": "Then, D is realizable if and only if (i) D[V (Tv)] = D[{v} ∪Vv] is realizable and (ii) D′ is realizable, where D′ is the instance obtained from D by removing bi v and its potential in- and out-leaf. Proof. Let D′ be the resulting instance with solid graph G′ after removing bi v and its possible leaf-neighbors form D. Let Dv := D[V (Tv)] = D[{v} ∪Vv]. Moreover, for each j ∈[1, |Bv|], let ei denote the bridge {bi v, v}."
  },
  {
    "prompt": "Scientific context on Recognizing, Realizing, Temporal and their role in graph theory.",
    "response": "40 Recognizing and Realizing Temporal Reachability Graphs The first direction of the statement follows immediately: Let λ be a realization for D. Then, the restriction of λ to the edges of G′ realizes D′, since no path in G between any two vertices of G′ uses vertices that are removed from G. Similarly, the restriction of λ to the edges of Tv realizes Dv, since no path in G between any two vertices of Tv uses vertices outside of Tv. Hence, if D is realizable, then both D′ and Dv are realizable."
  },
  {
    "prompt": "Read and understand this section about realization, direction, frugal from a research paper.",
    "response": "So consider the second direction. Let λ′ be a frugal realization for D′ and let λv be a frugal realization for Dv. We show that there is a realization λ for D. To this end, we will combine both realizations for D′ and Dv. Recall that ei is not an edge that surrounds or blocks any external edge incident with v. Note that for each external edge e incident with v, in both D and D′ the same edges in Tv are surrounding e."
  },
  {
    "prompt": "Scientific context on edge, edges, surrounds and their role in graph theory.",
    "response": "If e is surrounded by at least one edge in Tv under D, then this trivially follows by definition of surrounding edges. Otherwise, if no edge surrounds e under D, then, since ei is not an edge blocking e, D′ still contains both edges that block e in D. Based on these two edges, no edge surrounds e under D′. Due to Lemma 43, this implies that for each external edge e incident with v, e receives no label in [min λ′(ei−5), max λ′(ei+5)]."
  },
  {
    "prompt": "Read and understand this section about special, bridge, edges from a research paper.",
    "response": "Let s = i −3 if ei−3 is a special bridge in D, and let s = i −4, otherwise. Similarly, let t = i + 3 if ei+3 is a special bridge in D, and let t = i + 4, otherwise. The choice of s and t is important to ensure that the edges es−1 (et−1) and es+1 (et+1) are separated under D, D′, and Dv. We will show that this property holds when providing a realization for D. Before we can define a realization for D, we observe several properties about the edges es and et. Let j ∈[1, s] ∪[t, |Bv|]."
  },
  {
    "prompt": "Scientific context on special, separated, Observe and their role in graph theory.",
    "response": "Observe that ej and ei are separated under D. This implies that ej and ei are not bundled under D, since we would have already detected that D is a no-instance if ej and ei are both bundled and separated under D (see Corollary 13). Hence, the removal of bi v and its possible leaves did not make ej non-special, if ej was special in D. In other words, ej is special in D if and only if ej is special in D′. Let j ∈[s, t]."
  },
  {
    "prompt": "Scientific context on special, external, edge and their role in graph theory.",
    "response": "Since ej does not surround any external edge incident with v, ej is special in D if and only if ej is special in Dv. This is due to the fact that ej cannot share a label with any external edge incident with v (see Lemma 43). This in particular implies that es (et) is special in D′ if an only if es (et) is special in Dv. Moreover, max λv(es) < min λv(et), since es and et are separated under Dv. Similarly, max λ′(es) < min λ′(et), since es and et are separated under D′."
  },
  {
    "prompt": "Explain a concept involving assume, loss, generality in computational complexity.",
    "response": "We can thus assume without loss of generality that λ′(es) = λv(es) and λ′(et) = λv(et) (by introducing arbitrarily many empty snapshots in both labelings). Based on this property, we now define a realization λ for D. The labeling λ agrees with λv on all edges incident with at least one vertex of {bj v | j ∈[s + 1, t −1]}, and agrees with λ′ on all other edges. We show that λ realizes D. Let W contain the vertices bj v with j ∈[s+1, t−1] and their possible leaves."
  },
  {
    "prompt": "Read and understand this section about vertices, realized, paths from a research paper.",
    "response": "Since paths between any two vertices of V \\ W only use vertices outside of W, and λ′ realizes D′, exactly the arcs of D between vertices of V \\ W are realized by λ. Similarly, paths between vertices of W only use vertices of W ∪{v}, since all vertices of W are in Tv. Since λ agrees with λv on all edges incident with at least one vertex of W and λv realized Dv, exactly the arcs of D between vertices of W are realized by λ."
  },
  {
    "prompt": "This is a technical discussion about vertices, Dxbs, remains in theoretical computer science.",
    "response": "It thus remains to show that exactly the arcs of D between vertices of W and vertices of V \\ W are realized by λ. Let x ∈V \\ ({v} ∪Vv) and let j ∈[s, t]. Then, since we assumed that the property of Lemma 44 holds (as otherwise we already detected that D is not realizable), Dxbj v = Dxbs−1 v (and Dx outj = Dxbs−1 v if the out-leaf outj of bj v exists). Since λ′ realizes D′, if Dxbs−1 v = 1, then there is a temporal (x, bs−1 v )-path P under λ′ (and thus under λ)."
  },
  {
    "prompt": "This is a technical discussion about temporal, path, max in theoretical computer science.",
    "response": "The path P traverses the edge es−1 at time at most max λ′(es−1) = max λ(es−1) < min λ(ej). This implies Thomas Erlebach, Othon Michail, Nils Morawietz 41 that there is also a temporal (x, bj v)-path (and a temporal (x, outj)-path, if outj exists) under λ. Similarly, if Dxbs−1 v = 0, then there is no temporal (x, bs−1 v )-path under λ′ (and thus also not under λ). In particular, each temporal (x, v)-path under λ reaches v at time at least min λ(es−1)."
  },
  {
    "prompt": "Understanding the computational aspects of temporal, max, path.",
    "response": "As initially discussed, no external edge incident with v has a label in [min λ′(es−1), max λ′(et+1)]. Hence, each temporal (x, v)-path under λ reaches v at time larger than max λ(et+1) > max λ(ej). This implies that there is also no temporal (x, bj v)-path (and no temporal (x, outj)-path, if outj exists) under λ. Hence, the possible arcs from x to bj v and outj are realized by λ if and only if they exist. Similarly, the possible arcs from bj v and inj to x are realized if and only if they exist."
  },
  {
    "prompt": "Explain a concept involving vertices, separated, show in computational complexity.",
    "response": "It thus remains to consider reachability between vertices of W and vertices of Vv \\ W. To analyze this case, we finally show the reason why we chose the specific values for s and t. We show that in both D′ and Dv, es−1 and es+1 are separated. This is due to the fact that (i) if s = i −3, then ei−3 is special which implies that es−1 and es+1 are separated, and (ii) if s = i −4, then ei−3 is non-special which implies that es−1 and es+1 are separated."
  },
  {
    "prompt": "Scientific context on argumentation, realizes, earlier and their role in graph theory.",
    "response": "By the earlier argumentation, es is special under either all or none of the graphs D, D′, and Dv. Thus, each temporal path that traverses first an edge in Tv not incident with a vertex in W and afterwards an edge incident with a vertex of W exists under λv if and only if it exists under λ. This implies that λ realizes exactly the arcs between the vertices of Tv, since λv realizes Dv. In conclusion, λ realizes D."
  },
  {
    "prompt": "This is a technical discussion about Note, size, time in theoretical computer science.",
    "response": "◀ Note that this implies that we can in polynomial time (correctly detect that D is not realizable or) reduce the size of each fixed pendant tree Tx with x ∈V ∗to a size of O(dx), where dx denotes the degree of x in G[V ∗]."
  },
  {
    "prompt": "Explain a concept involving consecutive, surrounding, blocking in computational complexity.",
    "response": "This is due to the fact that each of the O(dx) edges defines at most 5 surrounding or blocking edges in Tx, and we can remove neighbors of x in Tx if there are more than 11 consecutive (with respect to the topological order on Bx) bridges incident with x that are all not surrounding or blocking any external edge. This thus proves Proposition 40."
  },
  {
    "prompt": "This is a technical discussion about Ensuring, fes, set in theoretical computer science.",
    "response": "7.3 Ensuring connectors with nice properties In this section we show how to extend the vertex set X∗to a set W ∗of size O(fes), such that each W ∗-connector has some useful properties that we will use in our algorithm. To define these useful properties, we distinguish between several types of connectors. ▶Definition 46. Let W with X∗⊆W ⊆V ∗and let P be a W-connector with endpoints a and b from V ∗. We say that P is trivial if P has length at most 3."
  },
  {
    "prompt": "Read and understand this section about successor, in,out, connector from a research paper.",
    "response": "Otherwise, let a′ be the successor of a in P and let a′′ be the successor of a′ in P. Similarly, let b′ be the predecessor of b in P and let b′′ be the predecessor of b′ in P. We say that P is an (in,out)-connector if (a, a′′) ∈A and (b′′, b) ∈A. Moreover, P a dense (in,out)-connector, if the path P is a dense path in D and P is an (in,out)-connector. Robust Connectors The first property we consider are robust connectors, which are defined as follows. ▶Definition 47."
  },
  {
    "prompt": "Explain a concept involving W-connector, dense, path in computational complexity.",
    "response": "Let W with X∗⊆W ⊆V ∗and let P be a W-connector with endpoints a and b and extension C. We say that P is robust if there is no dense path between a and b outside of C, that is, if there is no dense path from a to b in D[V \\ (V (C) \\ {a, b})] and there is no dense path from b to a in D[V \\ (V (C) \\ {a, b})]."
  },
  {
    "prompt": "Read and understand this section about Temporal, Reachability, Recognizing from a research paper.",
    "response": "42 Recognizing and Realizing Temporal Reachability Graphs In other words, in a robust connector P, the reachability between the vertices of the extension of P has to be achieved via temporal paths that only use subpaths of the extension C of P. ▶Observation 48. Let W with X∗⊆W ⊆V ∗and let P be a robust W-connector with extension C. Then, for each realization λ∗of D, λ∗ C realizes D[V (C)], where λ∗ C is the restriction of λ∗to the edges of C."
  },
  {
    "prompt": "Scientific context on Observation, robust, directly and their role in graph theory.",
    "response": "This then directly implies the following for robust connectors. ▶Observation 49. Let W with X∗⊆W ⊆V ∗and let P be a robust W-connector. Then for each arc (q, q′) ∈A with q, q′ ∈V (P), the subpath of P from q to q′ is dense or D is not realizable. Additionally, we observe the following hereditary property for robust connectors. ▶Observation 50. Let W and W ′ with X∗⊆W ⊆W ′ ⊆V ∗. Moreover, let P be a W- connector and let P ′ be a W ′-connector such that P ′ is a subpath of P."
  },
  {
    "prompt": "Read and understand this section about robust, connector, Observation from a research paper.",
    "response": "Then, P ′ is robust if P is robust. Finally, since the definition of robust connectors only depends on the non-existence of paths between the endpoints that are outside of the connector, a connector P remains robust, even if adding vertices outside of P to W. Formally, this reads as follows. ▶Observation 51. Let W and W ′ with X∗⊆W ⊆W ′ ⊆V ∗. Moreover, let P be a path in G[V ∗], such that P is both a W-connector and a W ′-connector."
  },
  {
    "prompt": "Understanding the computational aspects of robust, W-connector, connector.",
    "response": "Then, P is a robust W-connector if and only if P is a robust W ′-connector. Based on these properties, we now show that we can find a constant number of vertices of a given connector P to add to W to ensure that each resulting subconnector is robust or trivial. ▶Lemma 52. Let W with X∗⊆W ⊆V ∗and let P be a non-trivial W-connector."
  },
  {
    "prompt": "Read and understand this section about subpaths, subpath, connector from a research paper.",
    "response": "Then, we can compute in polynomial time a set UP ⊆V (P) of size at most 6, such that (i) there are at most two (W ∪UP )-connectors which are subpaths of P and (ii) each (W ∪UP )-connector which is a subpath of P is trivial or robust. Proof. Let a, a′, a′′ be the first three vertices of P and let b′′, b′, b be the last three vertices of P. We distinguish two cases. Case 1: P is not an (in,out)-connector. We set UP := {a′, a′′, b′, b′′}."
  },
  {
    "prompt": "This is a technical discussion about subpath, Assume, connector in theoretical computer science.",
    "response": "Clearly, there is at most one (W ∪UP )-connector which is a subpath of P, namely the subpath P ′ of P from a′′ to b′′. Assume towards a contradiction that P ′ is not robust. This implies that there is a dense path R in D between a′′ and b′′ that uses no edge of P ′. Assume without loss of generality that R goes from a′′ to b′′. Note that the dense path R has to visit both a and b. This implies that (a′′, a) ∈A and (b, b′′) ∈A; a contradiction to the fact that P is not an (in,out)-connector."
  },
  {
    "prompt": "Understanding the computational aspects of robust, Case, in,out.",
    "response": "Consequently, P ′ is robust. Case 2: P is an (in,out)-connector. Without loss of generality assume that (a, a′′) ∈A and (b′′, b) ∈A. Moreover, let pi denote the ith vertex of P when going from a to b. That is, p1 = a and pℓ= b, where ℓdenotes the number of vertices of P. Recall that ℓ≥5, since P is non-trivial. We distinguish two cases. Case 2.1: For each j ∈[3, ℓ−1], (pj, a) /∈A. Note that this implies that (b′′, a) /∈A. Moreover, (a′′, a) /∈A. We set UP := {a′, a′′, b′, b′′}."
  },
  {
    "prompt": "Understanding the computational aspects of subpath, connector, dense.",
    "response": "Clearly, there is at most one (W ∪UP )- connector which is a subpath of P, namely the subpath P ′ of P from a′′ to b′′. Note that Thomas Erlebach, Othon Michail, Nils Morawietz 43 there is no dense path R in D between a′′ and b′′ that uses no edge of P ′, since each such path would contain a and thus cannot be dense. This is due to the fact that (a′′, a) /∈A and (b′′, a) /∈A. Hence, P ′ is robust. Case 2.2: There is some k ∈[3, ℓ−1] with (pk, a) ∈A."
  },
  {
    "prompt": "This is a technical discussion about subpath, smallest, integer in theoretical computer science.",
    "response": "Let j be the smallest integer in [3, ℓ−1] for which (pj, a) ∈A. Hence, (pj−1, a) /∈A. Note that j ̸= 3, since p3 = a′′ and (a, a′′) ∈A (which implies that (a′′, a) /∈A). We set UP := {a′, a′′, b′, b′′, pj−1, pj}. Clearly, there are at most two (W ∪UP )-connectors which are subpaths of P, namely the subpath P1 of P from a′′ to pj−1 and the subpath P2 of P from b′′ to pj. We now show that each such subpath (if it exists) is a trivial or robust connector. First consider P1."
  },
  {
    "prompt": "Read and understand this section about path, exists, dense from a research paper.",
    "response": "Assume that P1 exists and is non-trivial. There is no dense path R1 from a′′ to pj−1 outside of P1, since (i) such a path contains a and pj (in that order) and (ii) (a, pj) /∈A (since (pj, a) ∈A). Similarly, there is no dense path R1 from pj−1 to a′′ outside of P1, since such a path contains a and (pj−1, a) /∈A. Hence, if P1 exists, P1 is trivial or robust. Next, consider P2. Assume that P2 exists and is non-trivial."
  },
  {
    "prompt": "Explain a concept involving path, dense, order in computational complexity.",
    "response": "There is no dense path R2 from pj to b′′ outside of P2, since (i) such a path contains pj−1 and a (in that order) and (ii) (pj−1, a) /∈A. Similarly, there is no dense path R2 from b′′ to pj outside of P2, since such a path contains a and (a, pj) /∈A (since (pj, a) ∈A). Hence, if P2 exists, P2 is trivial or robust. ◀ Nice Connectors We now define the more general property we are interested in. ▶Definition 53. Let W with X∗⊆W ⊆V ∗and let P be a non-trivial W-connector with endpoints a and b."
  },
  {
    "prompt": "Read and understand this section about successor, predecessor, denote from a research paper.",
    "response": "Moreover, let a′ be the successor of a in P, let b′ be the predecessor of b in P, and let C denote the extension of P."
  },
  {
    "prompt": "Understanding the computational aspects of vertex, dense, path.",
    "response": "We say that P is nice if P is robust and if for each vertex q ∈V (P), (i) (q, a) /∈A or (q, b) /∈A and (ii) (a, q) /∈A or (b, q) /∈A, or if P is a dense (in,out)-connector and (i) for each vertex v ∈V \\ V (C) with (v, a) ∈A and (v, b) ∈A, there is no dense path from v to b in D[(V \\ V (C)) ∪{a, b}], and (ii) for each vertex v ∈V \\ V (C) with (a, v) ∈A and (b, v) ∈A, there is no dense path from a to v in D[(V \\ V (C)) ∪{a, b}]. The benefit of nice connectors is the following. ▶Lemma 54."
  },
  {
    "prompt": "Understanding the computational aspects of W-connector, vin, vout.",
    "response": "Let W with X∗⊆W ⊆V ∗and let P be a nice W-connector with endpoints a and b from V ∗. Moreover, let a′ be the successor of a in P, let b′ be the predecessor of b in P, and let C denote the extension of P. Finally, let vin ∈V (C) \\ {a, b} and let vout ∈(V \\ V (C)) ∪{a, b} with (vin, vout) ∈A or (vout, vin) ∈A."
  },
  {
    "prompt": "Understanding the computational aspects of vout, vin, path.",
    "response": "In polynomial time we can detect that D is not realizable, or compute an edge e∗∈{{a, a′}, {b, b′}}, such that if (vin, vout) ∈A (if (vout, vin) ∈A), each dense (vin, vout)-path (dense (vout, vin)-path) contains the edge e∗. Proof. We distinguish two cases. Case 1: P is not a dense (in,out)-connector. Since P is nice, there is at most one c ∈{a, b}, such that A contains arc (vin, c) (arc (c, vin))."
  },
  {
    "prompt": "Explain a concept involving vin, vout, path in computational complexity.",
    "response": "If this holds for neither a nor b, then D is not realizable, since each dense (vin, vout)-path (dense (vout, vin)-path) contains at least one vertex of {a, b}. Otherwise, let c be the unique vertex of {a, b} for which A contains arc (vin, c) (arc (c, vin)). Then, each dense (vin, vout)-path (dense (vout, vin)-path) contains the edge e∗:= {c, c′}. Since all these checks can be performed in polynomial time, the statement follows."
  },
  {
    "prompt": "This is a technical discussion about vout, arcs, Recognizing in theoretical computer science.",
    "response": "44 Recognizing and Realizing Temporal Reachability Graphs Case 2: P is a dense (in,out)-connector. If A contains neither of the arcs (a, vout) or (b, vout) (neither of the arcs (vout, a) or (vout, b)), then D is not realizable, since each dense (vin, vout)-path (dense (vout, vin)-path) contains at least one of a and b. Hence, assume in the following that A contains at least of the arcs (a, vout) or (b, vout) (A contains at least one of the arcs (vout, a) or (vout, b))."
  },
  {
    "prompt": "Read and understand this section about vout, path, dense from a research paper.",
    "response": "If A contains both (a, vout) and (b, vout) (both (vout, a) or (vout, b)), then there is no dense (a, vout)-path (dense (vout, b)-path) in D[(V \\ V (C)) ∪{a, b}], since D is nice. Hence, e∗:= {b, b′} (e∗:= {a, a′}) is contained in each dense (vin, vout)-path (dense (vout, vin)-path) in D. Otherwise, there is a unique vertex c ∈{a, b}, such that (c, vout) ∈A ((vout, c) ∈A). Clearly, the edge e∗:= {c, c′} is contained in each dense (vin, vout)-path (dense (vout, vin)- path) in D."
  },
  {
    "prompt": "Read and understand this section about connector, time, nice from a research paper.",
    "response": "Since all these checks can be performed in polynomial time, the statement follows. ◀ Similar to the robust property, the property of being a nice connector is preserved under addition of vertices that are not in the specific connector. ▶Observation 55. Let W and let W ′ with X∗⊆W ⊆W ′ ⊆V ∗. Moreover, let P be a path in G[V ∗] such that P is both a W-connector and a W ′-connector. Then, P is nice W-connector if and only if P is a nice W ′-connector."
  },
  {
    "prompt": "This is a technical discussion about Finally, robust, connector in theoretical computer science.",
    "response": "Finally, we show that similar to robust connectors, we can compute in polynomial time a set of vertices of a connector P to add to W to ensure that each resulting subconnector is nice or trivial. To this end, recall that due to Observation 48, we can immediately conclude that D is not realizable if D[V (C)] is not realizable for the extension C of a robust connector. Since G[V (C)] is a tree, this property can be checked in polynomial time."
  },
  {
    "prompt": "Explain a concept involving ensure, realizable, Lemma in computational complexity.",
    "response": "Hence, by performing this check whenever we consider the extension C of a robust connector, we ensure that in polynomial time, we can (i) detect that D is not realizable or (ii) ensure that D[V (C)] is realizable. ▶Lemma 56. Let W with X∗⊆W ⊆V ∗and let P be a non-trivial and robust W-connector with extension C."
  },
  {
    "prompt": "Understanding the computational aspects of subpaths, subpath, connector.",
    "response": "If D[V (C)] is realizable, then we can compute in polynomial time a set UP ⊆V (P) of size at most 10, such that (i) there are at most two (W ∪UP )-connectors which are subpaths of P and (ii) each (W ∪UP )-connector which is a subpath of P is trivial or nice. Proof. Let a, a′, a′′ be the first three vertices of P and let b′′, b′, b be the last three vertices of P. Moreover, let C be the extension of P. We distinguish three cases. Case 1: P is a dense (in,out)-connector."
  },
  {
    "prompt": "This is a technical discussion about subpath, connector, set in theoretical computer science.",
    "response": "We set UP := {a′, a′′, b′, b′′}. Let P ′ be the subpath of P from a′′ to b′′. If a′′ = b′′ or {a′′, b′′} ∈E, then there is no subpath of P which is a (W ∪UP )-connector, which implies that UP fulfills the desired properties. Hence, assume that P ′ has length at least 2, which implies that P ′ is the only (W ∪UP )-connector that is a subpath of P. Moreover, note that P ′ is also a dense (in,out)-connector. If P ′ is trivial, UP fulfills the desired properties."
  },
  {
    "prompt": "Explain a concept involving dense, show, non-trivial in computational complexity.",
    "response": "Hence, in the following assume that P ′ in non-trivial. This implies that (b′′, a′′) /∈A, since P ′ is a dense (in,out)-connector. We now show that P ′ is nice. Let C′ be the extension of P ′ and let w ∈V \\ V (C′). First, we show that if (w, a′′) ∈A and (w, b′′) ∈A, then there is no dense (w, b′′)-path in D that avoids the edges of P ′. Afterwards, we show that if (a′′, w) ∈A and (b′′, w) ∈A, then there is no dense (a′′, w)-path in D that avoids the edges of P ′."
  },
  {
    "prompt": "Read and understand this section about path, vertex, Suppose from a research paper.",
    "response": "Suppose that (w, a′′) ∈A and (w, b′′) ∈A. Since P is a dense path in D and P ′ has length at least 4, A does not contain the arc (b′, a′′) (since (a′′, b′) ∈A). Hence, if w ∈{b′} ∪Vb′, Thomas Erlebach, Othon Michail, Nils Morawietz 45 then there is no dense (w, a′′)-path in D, since each such path would contain the vertex b′. For each other vertex of V \\ V (C′), each path in G from w to b′′ either traverses the entire path P ′, or goes over the vertex b."
  },
  {
    "prompt": "This is a technical discussion about vertex, path, due in theoretical computer science.",
    "response": "This is due to the fact that each vertex in each connector has exactly two neighbors in V ∗. Thus, there is no dense (w, b′′)-path in D that avoids the edges of P ′, since such a path would contain the vertex b, but A does not contain the arc (b, b′′) (since (b′′, b) ∈A). Now suppose that (a′′, w) ∈A and (b′′, w) ∈A. First, we show that w /∈{a′} ∪Va′. Since P is a dense path in D and P ′ has length at least 4, A does not contain the arc (b′′, a′) (since (a′, b′′) ∈A)."
  },
  {
    "prompt": "Scientific context on vertex, path, Note and their role in graph theory.",
    "response": "Note that this implies that (b′′, w) /∈A. This is due to the assumption that D[V (C)] is realizable and the fact that each (b′′, w)-path in the tree G[V (C)] contains the vertex a′. Hence, w /∈{a′} ∪Va′. For each other vertex of V \\ V (C′), each path in G from a′′ to w either traverses the entire path P ′, or goes over the vertex a. This is due to the fact that each vertex in each connector has exactly two neighbors in V ∗."
  },
  {
    "prompt": "This is a technical discussion about path, Case, dense in theoretical computer science.",
    "response": "Thus, there is no dense (a′′, w)-path in D that avoids the edges of P ′, since such a path would contain the vertex a, but A does not contain the arc (a′′, a) (since (a, a′′) ∈A). Thus, P ′ is nice, which implies that UP fulfills the desired properties. Case 2: P is not a dense (in,out)-connector. Note that, in this case, we can check in polynomial time, whether P is nice. If this is the case, the set UP := ∅trivially fulfills the desired property."
  },
  {
    "prompt": "Read and understand this section about assume, nice, implies from a research paper.",
    "response": "Hence, assume in the following that P is not nice. This implies that there is some vertex p ∈V (P) with (i) (a, p) ∈A and (b, p) ∈A, or (ii) (p, a) ∈A and (p, b) ∈A. Due to symmetry, assume that (a, p) ∈A and (b, p) ∈A. Moreover, let p′ be the predecessor of p in P. This then implies that the unique (a, p′)-path in P and the unique (b, p)-path in P are dense. Hence, there are at most two subpaths of P that are (W ∪{p, p′})-connectors."
  },
  {
    "prompt": "Understanding the computational aspects of in,out, connectors, dense.",
    "response": "Namely, the subpath P1 from a to p′ and the subpath P2 from b to p. By the above, each of these at most two resulting connectors is trivial or a dense (in,out)-connector. If Pi exists and is non-trivial, let UPi := ∅. Otherwise, let UPi be as defined in the case for dense (in,out)-connectors. Thus, by setting UP := {p, p′} ∪UP1 ∪UP2, we obtain at most two (W ∪UP )-connectors which are subpaths of P and both of these subpaths are trivial or nice."
  },
  {
    "prompt": "Read and understand this section about size, Proposition, Based from a research paper.",
    "response": "Moreover, UP has size at most 10, since UPi has size at most 4 as shown above. ◀ Based on all these results, we now show the main result of this section. ▶Proposition 57. In polynomial time, we can detect that D is a not realizable, or compute a set W ∗with X∗⊆W ∗⊆V ∗of size O(fes), such that each W ∗-connector is nice. Proof. First, we describe how to obtain a set W1 with X∗⊆W1 ⊆V ∗, such that each W1- connector is trivial or robust."
  },
  {
    "prompt": "Read and understand this section about set, Compute, Lemma from a research paper.",
    "response": "Compute the set of X∗-connectors, initialize W1 := X∗, and iterate over each non-trivial X∗-connector P. Compute the set UP of vertices of P according to Lemma 52. Note that the set of W1-connectors and the set of (W1 ∪UP )-connectors differ only by a constant number of subpaths of P. Then, each subpath of P which is a (W1 ∪UP )- connector is trivial or robust due to Lemma 52."
  },
  {
    "prompt": "Understanding the computational aspects of connectors, connector, Observation.",
    "response": "Moreover, for all other (W1 ∪UP )-connectors, the properties of being trivial or being robust are preserved due to Observation 51. Now, add all vertices of UP to W1 and proceed with the next X∗-connector (which is also a W1- connector). Hence, after the iteration over all X∗connectors, the resulting set W1 preserves the stated property that each W1-connector is trivial or robust. Moreover, since UP has constant size for each X∗-connector, and there are O(fes) X∗-connectors, W1 has size O(fes)."
  },
  {
    "prompt": "Scientific context on fes, connector, implies and their role in graph theory.",
    "response": "This also implies that there are O(fes) W1-connectors. Second, we describe how to obtain a set W2 with W1 ⊆W2 ⊆V ∗, such that each W2- connector is trivial or nice. We essentially do the same as in the construction of W1. 46 Recognizing and Realizing Temporal Reachability Graphs Compute the set of W1-connectors, initialize W2 := W1 and iterate over each non-trivial W1- connector P. Note that by definition of W1, P is robust. Let C be the extension of P. Check whether D[V (C)] is realizable."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, set, time.",
    "response": "Note that this can be done in polynomial time, since D[V (C)] is a tree. If this is not the case, then output that D is not realizable. This is correct due to Observation 48. If D[V (C)] is realizable, proceed as follows. Compute the set UP of vertices of P according to Lemma 56. Note that the set of W2-connectors and the set of (W2 ∪UP )-connectors differ only by a constant number of subpaths of P. Then, each subpath of P which is a (W2 ∪UP )-connector is trivial or nice due to Lemma 56."
  },
  {
    "prompt": "This is a technical discussion about Observation, connectors, fes in theoretical computer science.",
    "response": "Moreover, for all other (W2 ∪UP )-connectors, the properties of being trivial or being nice are preserved due to Observation 55. Now, add all vertices of UP to W2 and proceed with the next W1-connector (which is also a W2-connector). Hence, after the iteration over all W1 connectors, the resulting set W2 preserves the stated property that each W2-connector is trivial or nice. Moreover, since UP has constant size for each W1-connector, and there are O(fes) W1-connectors, W2 has size O(fes)."
  },
  {
    "prompt": "Explain a concept involving Finally, vertices, trivial in computational complexity.",
    "response": "Finally, the set W ∗is obtained from W2 by adding all vertices of trivial W2-connectors. Since each trivial connector contains a constant number of vertices, W ∗has size O(fes) and each W ∗-connector is nice due to Observation 55. Note that all operations described run in polynomial time. This thus proves the statement. ◀ 7.4 Computing labelings for prelabeled nice connectors In this subsection, we show the following. ▶Lemma 58. Let W with X∗⊆W ⊆V ∗."
  },
  {
    "prompt": "This is a technical discussion about W-connector, nice, endpoints in theoretical computer science.",
    "response": "Moreover, let P be a nice W-connector with endpoints a and b, let a′ be the successor of a in P, and let b′ be the predecessor of b in P. Moreover, let α1 a, α2 a, α1 b, α2 b ∈{i · 2n | i ∈N} be (not necessarily distinct) natural numbers, and let C denote the extension of P."
  },
  {
    "prompt": "Explain a concept involving labelings, labeling, time in computational complexity.",
    "response": "In polynomial time, we can compute a set LP of O(1) labelings λP of E(C) with λP ({a, a′}) = {α1 a, α2 a} and λP ({b, b′}) = {α1 b, α2 b}, such that if there is a realization λ∗for D with λ∗({a, a′}) = {α1 a, α2 a} and λ∗({b, b′}) = {α1 b, α2 b} and which is minimal on both {a, a′} and {b, b′}, then there is a labeling λP ∈LP for which λ∗⋉λP realizes D. Proof. Without loss of generality assume that α1 a ≤α2 a and α1 b ≤α2 b."
  },
  {
    "prompt": "Scientific context on denote, vertices, Ext and their role in graph theory.",
    "response": "Moreover, let Ext := V \\ (V (C) \\ {a, b}) denote the external vertices of C and let Int := V (C) \\ {a, b} denote the internal vertices of C. To show the statement, we compute a constant number of instances DP of our problem, where for each Di ∈DP , the solid graph Gi of Di is a tree, and Di is a supergraph of D[V (C)]. For each such instance Di, we then check whether there is a labeling λi that realizes Di with λi({a, a′}) = {α1 a, α2 a} and λi({b, b′}) = {α1 b, α2 b}."
  },
  {
    "prompt": "Understanding the computational aspects of realization, bλi, show.",
    "response": "As we will show, if there is a realization λ∗for D with λ∗({a, a′}) = {α1 a, α2 a} and λ∗({b, b′}) = {α1 b, α2 b} which is minimal on both {a, a′} and {b, b′}, then there is some Di ∈DP , such that λ∗⋉bλi realizes D, where bλi is the restriction to the edges of E(C) of an arbitrary realization λi of Di with λi({a, a′}) = {α1 a, α2 a} and λi({b, b′}) = {α1 b, α2 b}."
  },
  {
    "prompt": "Explain a concept involving Int, set, define in computational complexity.",
    "response": "To properly define the instances of DP , we first compute four sets V < c′ , V ≤ c′ , V ≥ c′ , V > c′ of vertices of Int for each vertex c ∈{a, b}. Let c ∈{a, b}. We define the set V < c′ (V > c′ ) as the set of all vertices vin of Int for which (vin, c) ∈A (for which (c, vin) ∈A)."
  },
  {
    "prompt": "Explain a concept involving vout, Int, arc in computational complexity.",
    "response": "Moreover, we define the set V ≤ c′ (V ≥ c′ ) as a smallest non-empty set of vertices of Int, such that there is at least one vertex vout ∈(V \\ V (C)) ∪{a, b} for which V ≤ c′ (V ≥ c′ ) is exactly the set of vertices vin of Int fulfilling (i) (vin, vout) ∈A ((vout, vin) ∈A) and (ii) {c, c′} is the edge of {{a, a′}, {b, b′}} outputted by the algorithm behind Lemma 54 for the arc (vin, vout) (for the arc (vout, vin))."
  },
  {
    "prompt": "Read and understand this section about solid, Erlebach, Othon from a research paper.",
    "response": "Thomas Erlebach, Othon Michail, Nils Morawietz 47 a a′ b b′ α1 a, α2 a α1 b, α2 b f ≤ a′ v≤ a f ≤ b′ v≤ b f ≥ a′ v≥ a f ≥ b′ v≥ b α1 a α2 a α1 b α2 b α1 a α2 a α1 b α2 b Figure 5 An visualization of the solid graph and the labeling for the additional solid edges for the the auxiliary instance DΓ with the consistent type Γ = (J, AJ) with J = {v⊻ c | c ∈{a, b}, ⊻∈{≤, ≥}}. ▷Claim 59. Assume that D is realizable."
  },
  {
    "prompt": "Understanding the computational aspects of vout, vin, arc.",
    "response": "It holds that V ≤ c′ ⊆V < c′ , V ≥ c′ ⊆V > c′ , each of these four sets contains c′, and V < c′ ∩V > c′ = {c′}. Moreover, let vout ∈Ext for which there is at least one vin ∈Int with (vin, vout) ∈A (with (vout, vin) ∈A), such that the edge {c, c′} is the edge of {{a, a′}, {b, b′}} outputted by the algorithm behind Lemma 54 for the arc (vin, vout) (for the arc (vout, vin))."
  },
  {
    "prompt": "Scientific context on vout, arc, Lemma and their role in graph theory.",
    "response": "Then, one of V < c′ or V ≤ c′ (one of V > c′ or V ≥ c′ ) is exactly the set of vertices vin of Int for which {c, c′} is the edge of {{a, a′}, {b, b′}} outputted by the algorithm behind Lemma 54 for the arc (vin, vout) (for the arc (vout, vin)). Proof. First, we show that V ≤ c′ ⊆V < c′ . Assume towards a contradiction that this is not the case and let vin be an arbitrary vertex of V ≤ c′ \\V < c′ ."
  },
  {
    "prompt": "Scientific context on vout, vin, Ext and their role in graph theory.",
    "response": "By definition of V ≤ c′ , there is some vertex vout ∈ Ext such that (vin, vout) ∈A and in each realization for D, each temporal (vin, vout)-path traverses the edge {c, c′}. This then implies that there is a temporal (vin, c)-path, which implies that (vin, c) ∈A. This contradicts the fact that vin /∈V < c′ . Thus, V ≤ c′ ⊆V < c′ . Similarly, we can show that V ≥ c′ ⊆V > c′ . Moreover, note that c′ ∈V < c′ ∩V > c′ , since {c, c′} is a solid edge in G."
  },
  {
    "prompt": "Explain a concept involving vin, definition, vout in computational complexity.",
    "response": "Additionally, there is no vertex vin ̸= c′ that is contained in V < c′ ∩V > c′ , as otherwise, by definition of V < c′ and V > c′ , A contains both arcs (vin, c) and (c, vin), which is not possible by definition of a connector. Now, let vout ∈Ext for which there is at least one vin ∈Int with (vin, vout) ∈A (with (vout, vin) ∈A), such that the edge {c, c′} is the edge of {{a, a′}, {b, b′}} outputted by the algorithm behind Lemma 54 for the arc (vin, vout) (for the arc (vout, vin))."
  },
  {
    "prompt": "Understanding the computational aspects of vout, arc, vin.",
    "response": "We show that one of V < c′ or V ≤ c′ (one of V > c′ or V ≥ c′ ) is exactly the set of vertices vin of Int for which {c, c′} is the edge of {{a, a′}, {b, b′}} outputted by the algorithm behind Lemma 54 for the arc (vin, vout) (for the arc (vout, vin)). Let M denote the set of vertices vin of Int for which {c, c′} is the edge of {{a, a′}, {b, b′}} outputted by the algorithm behind Lemma 54 for the arc (vin, vout) (for the arc (vout, vin))."
  },
  {
    "prompt": "Read and understand this section about proper, subset, Assume from a research paper.",
    "response": "Assume towards a contradiction that M /∈{V < c′ , V ≤ c′ } (M /∈{V > c′ , V ≥ c′ }). Analogously to the proof that V ≤ c′ ⊆V < c′ (V ≥ c′ ⊆V > c′ ), one can show that M ⊆V < c′ (M ⊆V > c′ ). Thus, M is a proper subset of V < c′ (V > c′ ). This implies that V ≤ c′ is a proper subset of V < c′ (V ≥ c′ is a proper subset of V > c′ )."
  },
  {
    "prompt": "Scientific context on distinct, vertices, sets and their role in graph theory.",
    "response": "Hence, the edge {c, c′} needs to be traversed at at least three different time steps to ensure that all three distinct sets V < c′ , V ≤ c′ , and M can reach distinct external vertices (all three distinct sets V > c′ , V ≥ c′ , and M can be reached from distinct external vertices). This contradicts the fact that edge {c, c′} receives at most two labels in 48 Recognizing and Realizing Temporal Reachability Graphs each realization for D."
  },
  {
    "prompt": "Read and understand this section about Note, Claim, polynomial from a research paper.",
    "response": "◁ Note that the properties described in Claim 59 can be checked in polynomial time. Hence, if at least one of the properties does not hold, we can correctly detect in polynomial time that D is not realizable. If this is the case, we define LP as the empty set, which is correct. Hence, assume in the following that all described properties of Claim 59 hold. We now describe how to define the instances of DP . We define four potential vertices J := {v⊻ c | c ∈{a, b}, ⊻∈{≤, ≥}}."
  },
  {
    "prompt": "This is a technical discussion about J,AQ, arcs, potential in theoretical computer science.",
    "response": "Moreover, we define a set of three potential arcs AJ := {(v≤ a , b), (a, v≥ b ), (v≤ a , v≥ b )}. Let Q ⊆J and let AQ ⊆AJ. We call (Q, AQ) a consistent type if Q ∪{a, b} contains all endpoints of the arcs of AQ. Let AQ ⊆AJ. We define the instance D(J,AQ) as follows (see Figure 5): the vertex set of D(J,AQ) is V (C) ∪J ∪{f ⊻ c′ | c ∈{a, b}, ⊻∈{≤, ≥}} and 1. D(J,AQ) contains all arcs from A(V (C)), 2."
  },
  {
    "prompt": "Scientific context on J,AQ, arcs, in,out and their role in graph theory.",
    "response": "if (a, b) is an arc of D (that is, if P is a dense (in,out)-connector), D(J,AQ) contains the arcs (f ≤ a′, b), (a, f ≥ b′ ), and (f ≤ a′, f ≥ b′ ), 3. D(J,AQ) contains the arcs (v⊻ c , c), (c, v⊻ c ), (f ⊻ c′, c′) and (c, f ⊻ c′) (that is, the solid edges {c, v⊻ c } and {c′, f ⊻ c′}) for each c ∈{a, b} and each ⊻∈{≤, ≥}, 4. D(J,AQ) contains the arcs (f ≤ c′ , f ≥ c′ ) and (v≤ c , v≥ c ) for each c ∈{a, b}, 5."
  },
  {
    "prompt": "This is a technical discussion about J,AQ, arcs, arc in theoretical computer science.",
    "response": "D(J,AQ) contains the arcs (c′, v≥ c ), (c, f ≥ c′ ), (v≤ c , c′), and (f ≤ c′ , c) for each c ∈{a, b}, 6. if (v≤ a , b) ∈AQ, D(J,AQ) contains all arcs from {v≤ a , f ≥ a′} × {b, f ≥ b′ }, 7. if (a, v≥ b ) ∈AQ, D(J,AQ) contains all arcs from {a, f ≤ a′} × {v≥ b , f ≤ b′ }, 8. if (v≤ a , v≥ b ) ∈AQ, D(J,AQ) contains all arcs from {v≤ a , f ≥ a′} × {v≥ b , f ≤ b′ }, 9. for each c ∈{a, b}, D(J,AQ) contains the arc (vin, f ≥ c′ ) for each vertex vin ∈V < c′ , 10."
  },
  {
    "prompt": "Read and understand this section about vin, J,AQ, vertex from a research paper.",
    "response": "for each c ∈{a, b}, D(J,AQ) contains the arc (f ≤ c′ , vin) for each vertex vin ∈V > c′ , 11. for each c ∈{a, b}, D(J,AQ) contains the arcs (vin, f ≤ c′ ), (vin, f ≥ c′ ), and (vin, v≥ c ) for each vertex vin ∈V ≤ c′ , and 12. for each c ∈{a, b}, D(J,AQ) contains the arcs (f ≤ c′ , vin), (f ≥ c′ , vin), and (v≤ c , vin) for each vertex vin ∈V ≥ c′ ."
  },
  {
    "prompt": "Scientific context on J,AQ, Claim, vertices and their role in graph theory.",
    "response": "Note that D(J,AQ) has a tree as solid graph, since (i) we did not add any new arcs between vertices of V (C), (ii) we added at most one arc between any two vertices of V (D(J,AQ))\\V (C), and (iii) due to Claim 59, for each c ∈{a, b}, V < c′ ∩V > c′ = {c′}. The latter implies that the solid edges defined in Item 3 are the only solid edge between any vertex of V (D(J,AQ)) \\ V (C) and any vertex of V (C)."
  },
  {
    "prompt": "Read and understand this section about J,AQ, graph, tree from a research paper.",
    "response": "For each consistent type Γ = (Q, AQ), we define the instance DΓ by DΓ := D(J,AQ)[VΓ], where VΓ := V (C) ∪Q ∪{f ≤ c′ | v≥ c ∈Q} ∪{f ≥ c′ | v≤ c ∈Q}. Since D(J,AQ) has a tree as solid graph, this implies that DΓ also has a tree as solid graph. We now define the set LP of labelings for the edges of C. Initially, LP is the empty set. Afterwards, we iterate over each consistent type Γ = (Q, AQ)."
  },
  {
    "prompt": "Understanding the computational aspects of options, pair, check.",
    "response": "For each such pair Γ, we check whether there is a realization λ′ Γ for DΓ with λ′ Γ({a, a′}) = {α1 a, α2 a} and λ′ Γ({b, b′}) = {α1 b, α2 b} which is minimal on both {a, a′} and {b, b′}. If this is the case, we add the labeling λΓ to LP , where λΓ is the restriction of λ′ Γ to the edges of C. This completes the construction of LP . Note that LP has a constant size, since there are at most 24 options for Q and at most 23 options for AQ."
  },
  {
    "prompt": "Read and understand this section about polynomial, Corollary, time from a research paper.",
    "response": "Moreover, each such labeling can be computed in polynomial time, since the initial computation took polynomial time and for each consistent type Γ, we can check in polynomial time whether there is such a desired realization λ′ Γ for DΓ due to Corollary 25."
  },
  {
    "prompt": "Scientific context on Erlebach, Othon, Michail and their role in graph theory.",
    "response": "Thomas Erlebach, Othon Michail, Nils Morawietz 49 It thus remains to show that if there is a realization λ∗for D with λ∗({a, a′}) = {α1 a, α2 a} and λ∗({b, b′}) = {α1 b, α2 b} which is minimal on both {a, a′} and {b, b′}, then there is a labeling λP ∈LP , such that λ∗⋉λP realizes D. Assume that there is a minimum realization λ∗for D with λ∗({a, a′}) = {α1 a, α2 a} and λ∗({b, b′}) = {α1 b, α2 b} which is minimal on both {a, a′} and {b, b′}, as otherwise, the statement trivially holds."
  },
  {
    "prompt": "Understanding the computational aspects of type, consistent, goal.",
    "response": "Our goal is to show that there is a consistent type Γ = (Q, AQ) such that (i) there is a sought realization for DΓ (that is, there is a labeling λΓ ∈LP ) and (ii) λ∗⋉λΓ realizes D. We are now ready to define the consistent type Γ = (Q, AQ) for λ∗. First, we define Q. For each c ∈{a, b} where α1 c < α2 c, we do the following: We add the vertex v≤ c to Q if there is an arc (u, w) ∈A, such that for each temporal (u, w)-path under λ∗, the edge {c, c′} is traversed from c to c′ at time α2 c."
  },
  {
    "prompt": "This is a technical discussion about edge, arc, time in theoretical computer science.",
    "response": "Similarly, we add the vertex v≥ c to Q if there is an arc (u, w) ∈A, such that for each temporal (u, w)-path under λ∗, the edge {c, c′} is traversed from c′ to c at time α2 c. Next, we define AQ. We add the arc (v≤ a , b) to AQ if α1 a < α2 a and there is an arc (u, w) ∈A, such that for each temporal (u, w)-path under λ∗, the edge {a, a′} is traversed from a to a′ at time α2 a and the edge {b, b′} is traversed at a later time."
  },
  {
    "prompt": "Explain a concept involving edge, arc, time in computational complexity.",
    "response": "We add the arc (a, v≥ b ) to AQ if α1 b < α2 b and there is an arc (u, w) ∈A, such that for each temporal (u, w)-path under λ∗, the edge {b, b′} is traversed from b′ to b at time α1 b and the edge {a, a′} is traversed at an earlier time. We add the arc (v≤ a , v≥ b ) to AQ if α1 a < α2 a < α1 b < α2 b and there is an arc (u, w) ∈A, such that for each temporal (u, w)-path under λ∗, the edge {a, a′} is traversed from a to a′ at time α2 a and the edge {b, b′} is traversed from b′ to b at time α1 b."
  },
  {
    "prompt": "Understanding the computational aspects of vout, Claim, path.",
    "response": "This completes the definition of Γ. ▷Claim 60. Let c ∈{a, b}, and let vout ∈Ext such that (vout, c′) ∈A ((c′, vout) ∈A) and each temporal (vout, c′)-path (each temporal (c′, vout)-path) under λ∗traverses the edge {c, c′} from c to c′ (from c′ to c) at time α2 c. Then, V ≥ c′ (V ≤ c′ ) is exactly the set of vertices vin of Int with (vout, vin) ∈A ((vin, vout) ∈A) that can be reached by vout (that can reach vout) only via the edge {c, c′}. Proof."
  },
  {
    "prompt": "Understanding the computational aspects of Int, edge, set.",
    "response": "We show that V ≥ c′ is exactly the set of vertices of Int that vout can reach via any temporal path under λ∗that traverses the edge {c, c′}. (The case for V ≤ c′ can then be shown analogously.) Assume towards a contradiction that this is not the case. Hence, due to Claim 59, V ≥ c′ ̸= V > c′ and V > c′ is exactly the set of vertices of Int that vout can reach via any temporal path under λ∗that traverses the edge {c, c′}."
  },
  {
    "prompt": "Explain a concept involving vout, Ext, Int in computational complexity.",
    "response": "Since V ≥ c′ ̸= V > c′ , there is a vertex vout 2 ∈Ext for which V > c′ is exactly the set of vertices of Int that vout 2 can reach via any temporal path under λ∗that traverses the edge {c, c′}. Since V ≥ c′ is non-empty, there is at least one temporal path that starts at vout 2 and traverses the edge {c, c′} at time at most α2 c from c to c′."
  },
  {
    "prompt": "Read and understand this section about reach, path, paths from a research paper.",
    "response": "Such a path can then be extended to reach all vertices that vout can reach via temporal paths that go over {c, c′}, which would imply V ≥ c′ = V > c′ , a contradiction. ◁ We now show the statement in two steps. ▷Claim 61. There is a realization λ′ Γ for DΓ with λ′ Γ({a, a′}) = {α1 a, α2 a} and λ′ Γ({b, b′}) = {α1 b, α2 b} which is minimal on both {a, a′} and {b, b′}. Proof. Let GΓ be the solid graph of DΓ. We define the a labeling λ′ Γ for the edges of GΓ as follows."
  },
  {
    "prompt": "Explain a concept involving set, Recognizing, Realizing in computational complexity.",
    "response": "For each edge e of E(C), we set λ′ Γ(e) := λ∗(e) and for each c ∈{a, b}, we set λ′ Γ({v≤ c , c}) := α1 c and λ′ Γ({f ≥ c′ , c′}) := α2 c if v≤ c ∈Q and 50 Recognizing and Realizing Temporal Reachability Graphs λ′ Γ({v≥ c , c}) := α2 c and λ′ Γ({f ≤ c′ , c′}) := α1 c if v≥ c ∈Q. This completes the definition of λ′ Γ. Note that λ′ Γ({a, a′}) = {α1 a, α2 a} and λ′ Γ({b, b′}) = {α1 b, α2 b}."
  },
  {
    "prompt": "Explain a concept involving show, edge, statement in computational complexity.",
    "response": "Hence, to show the statement, it remains to show that λ′ Γ realizes DΓ and that for each c ∈{a, b} with α1 c < α2 c, the edge {c, c′} is a special edge under DΓ."
  },
  {
    "prompt": "Understanding the computational aspects of labels, label, pair.",
    "response": "The latter follows by construction of Γ: If for c ∈{a, b}, the realization λ∗of D uses two distinct labels α1 c < α2 c and is minimal on both {a, a′} and {b, b′}, then there is some vertex pair (u, w) with (u, w) ∈A, such that each (u, w)-path under λ∗traverses the edge {c, c′} at time α2 c, as otherwise, the label α2 c can be removed from {c, c′} in λ∗. This then implies that v≤ c ∈Q or v≥ c ∈Q. Due to symmetry assume that v≤ c ∈Q."
  },
  {
    "prompt": "Explain a concept involving arcs, arc, construction in computational complexity.",
    "response": "By construction, DΓ contains the arcs (v≤ c , c′) and (c, f ≥ c′ ) but not the arc (v≤ c , f ≥ c′ ). Consequently, {c, c′} is a special bridge in DΓ, which implies that in each minimum realization for DΓ, edge {c, c′} receives two labels. We now show that λ′ Γ realizes DΓ."
  },
  {
    "prompt": "Understanding the computational aspects of reach, extended, Note.",
    "response": "a) Note that for each c ∈{a, b}, each temporal path under λ′ Γ that reaches c′ at a time step smaller than α1 c can be extended at the end to reach the vertices f ≤ c′ , f ≥ c′ , v≥ c , and c (if they exist) by the subpaths with label sequence (α1 c), (α2 c), (α1 c, α2 c), and (α1 c), respectively, but cannot be extended to reach the vertex v≤ c ."
  },
  {
    "prompt": "Scientific context on Similarly, extended, reach and their role in graph theory.",
    "response": "b) Similarly, each temporal path under λ′ Γ that reaches c′ at a time step exactly α1 c can be extended at the end to reach the vertices f ≥ c′ and c by the respective edges with label α2 c, but cannot be extended to reach f ≤ c′ , v≤ c , or v≥ c ."
  },
  {
    "prompt": "Read and understand this section about edge, smaller-equal, larger-equal from a research paper.",
    "response": "c) All temporal path under λ′ Γ that reaches c′ at a time step larger than α1 c cannot be extended to reach any vertex of f ≤ c′ , f ≥ c′ , v≤ c , v≥ c , and c, since each edge incident with {c, c′} has all labels either smaller-equal α1 c or larger-equal α2 c (as otherwise, {c, c′} would be part of a triangle in G), and each subpath leading to any of the vertices of f ≤ c′ , f ≥ c′ , v≥ c , and c uses an edge first of label at most α2 c."
  },
  {
    "prompt": "Explain a concept involving late, temporal, start in computational complexity.",
    "response": "The similar property also holds for the temporal paths that start from vertex c′ either early or late. d) Each temporal path under λ′ Γ that leaves c′ at a time step larger than α2 c can be extended at the start to begin at any of the vertices f ≤ c′ , f ≥ c′ , v≤ c , and c (if they exist), but cannot be extended to begin at vertex v≥ c ."
  },
  {
    "prompt": "Read and understand this section about Similarly, vertices, extended from a research paper.",
    "response": "e) Similarly, each temporal path under λ′ Γ that leaves c′ at a time step exactly α2 c can be extended at the start to begin with any of the vertices f ≤ c′ and c, but cannot be extended to start from any of the vertices f ≥ c′ , v≤ c , or v≥ c ."
  },
  {
    "prompt": "Read and understand this section about smaller-equal, larger-equal, temporal from a research paper.",
    "response": "f) All temporal path under λ′ Γ that leave c′ at a time step smaller than α2 c cannot be extended at the start to begin from any of the vertices f ≤ c′ , f ≥ c′ , v≤ c , v≥ c , and c, since each edge incident with {c, c′} has all labels either smaller-equal α1 c or larger-equal α2 c (as otherwise, {c, c′} would be part of a triangle in G). We now show that each arc of D′ Γ is realized. To this end, we may implicitly assume that all vertices we talk about exist in D′ Γ."
  },
  {
    "prompt": "Explain a concept involving Item, realizes, Note in computational complexity.",
    "response": "Note that all arcs described in Item 1 are realized since λ∗realizes D, P is a robust connector, and D[V (C)] = DΓ[V (C)]. Since λ∗realizes D, if (a, b) ∈A, then there is a temporal path under λ∗(and thus λ′ Γ) from a to b that only uses edges in C. The subpath R from a′ to b′ of this path thus leaves a′ with a label of at least α2 a and reaches b′ at the latest with a label smaller than α1 b."
  },
  {
    "prompt": "Read and understand this section about Item, simultaneously, argumentation from a research paper.",
    "response": "Hence, by the above argumentation, R can (simultaneously) be extended to start with any of a or f ≤ a′, and end with any of b or f ≥ b′ . Thus, for each arc (u, w) described in Item 2, there is a temporal path from u to w in DΓ under λ′ Γ. Thomas Erlebach, Othon Michail, Nils Morawietz 51 All solid edges described in Item 3 receive a label under λ′ Γ, which implies that also the respective arcs are realized."
  },
  {
    "prompt": "Scientific context on Item, paths, sequence and their role in graph theory.",
    "response": "The arcs of Item 4 are realized by the temporal paths (v≤ c , c, v≥ c ) and (f ≤ c′ , c′, f ≥ c′ ) with label sequence (α1 c, α2 c), since α1 c < α2 c for each c ∈{a, b} with {v≤ c , v≥ c } ⊆Q. Similarly, the arcs of Item 5 are realized by the temporal paths (c′, c, v≥ c ), (c, c′, f ≥ c′ ), (v≤ c , c, c′), and (f ≤ c′ , c′, c), each with label sequence (α1 c, α2 c), since α1 c < α2 c for each c ∈ {a, b} with {v≤ c , v≥ c } ⊆Q."
  },
  {
    "prompt": "Explain a concept involving time, path, edge in computational complexity.",
    "response": "If (v≤ a , b) ∈AQ, then by choice of AQ, there is some vertex pair (u, w) with (u, w) ∈A, such that each temporal (u, v)-path under λ∗traverses the edge {a, a′} from a to a′ at time α2 a and reaches b at a later time step. Consider a subpath R from a′ to b′ of one such (u, v)-path R′. Since R′ traversed the edge {a, a′} at time α2 a, the path R traverses its first edge in time strictly larger than α2 c. Moreover, R′ reaches vertex b′ prior to time step α2 b."
  },
  {
    "prompt": "Scientific context on Item, simultaneously, vertex and their role in graph theory.",
    "response": "Hence, the path R can (simultaneously) be extended to start from any vertex of {v≤ a , f ≥ a′} and end at any vertex of {b, f ≥ b′ }. Thus, all arcs of Item 6 are realized. Similarly, one can show that all arcs of Item 7 are realized, due to the existence of some vertex pair (u, w) with (u, w) ∈A, such that each temporal (u, v)-path under λ∗traverses the edge {b, b′} from b′ to b at time α1 b and visits a at an earlier time step."
  },
  {
    "prompt": "Explain a concept involving edge, time, Item in computational complexity.",
    "response": "Similarly, one can show that all arcs of Item 8 are realized, due to the existence of some vertex pair (u, w) with (u, w) ∈A, such that each temporal (u, v)-path under λ∗traverses the edge {a, a′} from a to a′ at time α2 a and also traverses the edge {b, b′} from b′ to b at time α1 b. By definition, V < c′ are exactly the vertices of Int that have an arc to c in D. Let vin ∈V < c′ . Since (vin, c) ∈A, there is a temporal (vin, c)-path R in G[V (C)] under λ∗and thus λ′ Γ."
  },
  {
    "prompt": "Understanding the computational aspects of Item, realized, arc.",
    "response": "This path thus reaches c′ at a time strictly smaller than α2 c. The subpath from vin to c′ can thus be extended to end in f ≤ c′ by traversing the edge {f ≤ c′ , c′} at time α2 c. Hence, each arc of Item 9 is realized. Similar to the argument that each arc of Item 9 is realized, we can show that each arc of Item 10 is realized."
  },
  {
    "prompt": "This is a technical discussion about vout, Ext, path in theoretical computer science.",
    "response": "Suppose that v≥ c ∈Q.By definition of Q, this implies that there is some vout ∈Ext for which there is some vertex w ∈V with (vout, w) ∈A, such that each (vout, w)-path under λ∗traverses the edge {c, c′} from c to c′ at time α2 c. This implies that (vout, c′) ∈A, and that each temporal (vout, c′)-path under λ∗traverses the edge {c, c′} from c to c′ at time α2 c."
  },
  {
    "prompt": "Scientific context on Claim, Int, edge and their role in graph theory.",
    "response": "Due to Claim 60, V ≥ c′ is exactly the set of vertices vin of Int with (vout, vin) ∈A that can be reached by vout only via the edge {c, c′}. Consequently, all arcs specified in Item 11 are realized, since for each vin ∈V ≥ c′ each temporal (vout, vin)-path under λ∗traverses the edge {c, c′} at time α2 c and thus implies the existence of a temporal (c′, vin)-path starting at a time step strictly larger than α2 c."
  },
  {
    "prompt": "Explain a concept involving Item, realized, arc in computational complexity.",
    "response": "Similar to the proof that each arc of Item 11 is realized, we can show that each arc of Item 12 is realized. Note that only the arcs described above are realized. This is mainly due to the fact that D[V (C)] = DΓ[V (C)]. ◁ This implies that a labeling λΓ for DΓ is contained in LP . We now complete the proof by showing that λ∗⋉λΓ realizes D. ▷Claim 62. The labeling λ∗⋉λΓ realizes D. Proof. First, we observe some properties about the labeling λΓ."
  },
  {
    "prompt": "Understanding the computational aspects of receives, label, edge.",
    "response": "Let λ′ Γ be the realization for DΓ from which we obtained the labeling λΓ. For each c ∈{a, b}, where v≤ c ∈Q, the 52 Recognizing and Realizing Temporal Reachability Graphs edge {v≤ c , c} receives label α1 c and the edge {f ≥ c′ , c′} receives label α2 c. This is due to the fact that λΓ({c, c′}) = {α1 c, α2 c} and A contains the arcs (v≤ c , c′) and (c, f ≥ c′ ) but not the arc (v≤ c , f ≥ c′ )."
  },
  {
    "prompt": "Scientific context on edge, receives, label and their role in graph theory.",
    "response": "Similarly, if v≥ c ∈Q, the edge {v≥ c , c} receives label α2 c and the edge {f ≤ c′ , c′} receives label α1 c. We now show that λ∗⋉λΓ realizes D. First consider the arcs between the vertices of V (C). Since D[V (C)] = DΓ[V (C)] and since P is robust connector (that is, there is no dense path between a and b outside of V (C) \\ {a, b}), λ∗⋉λΓ surely realizes exactly the arcs between the vertices of V (C) in D."
  },
  {
    "prompt": "Read and understand this section about Ext, Int, vin from a research paper.",
    "response": "Next, consider the arcs between one vertex vin of Int = V (C) \\ {a, b} and one vertex vout of Ext \\ {a, b} = V \\ V (C). Let vout ∈Ext \\ {a, b} and let vin ∈Int with (vout, vin) ∈A (the case for (vin, vout) ∈A can be shown analogously). Moreover, let c ∈{a, b} such that the edge {c, c′} is the edge outputted by Lemma 54 for the arc (vout, vin). This implies that each temporal (vout, vin)-path under λ∗traverses the edge {c, c′} from c to c′. We consider two cases."
  },
  {
    "prompt": "Understanding the computational aspects of edge, vin, Case.",
    "response": "Case 1: each temporal (vout, vin)-path under λ∗traverses the edge {c, c′} from c to c′ at time α2 c. Hence, due to Claim 60, V ≥ c′ are exactly the vertices of Int that can be reached from vout over the edge {c, c′}. In particular, vin ∈V ≥ c′ . Moreover, by construction of Q, the vertex v≤ c was added to Q. Since λ′ Γ({v≤ c , c}) = α1 c, each path from v≤ c to c′ reaches vertex c′ at time exactly α2 c under λ′ Γ."
  },
  {
    "prompt": "Understanding the computational aspects of Int, vertices, vout.",
    "response": "Consequently, since (i) V ≥ c′ are exactly the vertices of Int that vertex v≤ c can reach in DΓ and (ii) λ′ Γ realizes DΓ, for exactly the vertices V ≥ c′ , there is a temporal path under λ′ Γ starting at a time step larger than α2 c from c′ to these vertices of Int. Hence, (vout, vin) is realized by λ∗⋉λΓ. Case 2: at least one temporal (vout, vin)-path under λ∗traverses the edge {c, c′} from c to c′ at time α1 c. This case follows similarly with respect to the set V > c′ ."
  },
  {
    "prompt": "Explain a concept involving vout, Ext, Finally in computational complexity.",
    "response": "Finally, consider the arcs between two vertices vout 1 and vout 2 of Ext. Let (vout 1 , vout 2 ) ∈A. If at least one temporal (vout 1 , vout 2 )-path under λ∗does not traverse any edge of C, then this path still exists under λ∗⋉λΓ. Hence, assume in the following that each temporal (vout 1 , vout 2 )- path under λ∗does traverses some edge of C."
  },
  {
    "prompt": "This is a technical discussion about edge, time, traverses in theoretical computer science.",
    "response": "In particular, this implies that P is a dense (in,out)-connector and each such (vout 1 , vout 2 )-path under λ∗traverses the edge {a, a′} from a to a′ at time αi a ∈{α1 a, α2 a} and traverses the edge {b, b′} from b′ to b at time αj b ∈{α1 b, α2 b} with αi a < αj b. We show that there is a temporal (a, b)-path under λΓ that traverses the edge {a, a′} from a to a′ at time αi a and traverses the edge {b, b′} from b′ to b at time αj b."
  },
  {
    "prompt": "Explain a concept involving contained, edge, construction in computational complexity.",
    "response": "By construction of AQ, (a, v≥ b ) is contained in AQ if αj b = α1 b, (v≤ a , b) is contained in AQ if αi a = α2 b, and (v≤ a , v≥ b ) is contained in AQ if αi a = α2 b and αj b = α1 b. Based on these arcs that are contained in AQ and thus in DΓ, and the fact that DΓ is realized by λ′ Γ, there is a temporal (a, b)-path under λΓ that traverses the edge {a, a′} from a to a′ at time αi a and traverses the edge {b, b′} from b′ to b at time αj b."
  },
  {
    "prompt": "Explain a concept involving Note, arcs, realized in computational complexity.",
    "response": "Note that only the arcs described are realized by λ∗⋉λΓ. Hence, λ∗⋉λΓ realizes D. ◁ Consequently, the set LP of labelings of the edges of C fulfills the desired properties and can be computed in polynomial time. ◀ 7.5 The Final Algorithm Our algorithm works as follows by using the main results of the last subsections: Theoretically, these results correctly detect that D is not realizable, or perform some preprocessing."
  },
  {
    "prompt": "Scientific context on algorithm, Erlebach, Othon and their role in graph theory.",
    "response": "In the description of the algorithm, we will implicitly assume that they perform the desired Thomas Erlebach, Othon Michail, Nils Morawietz 53 Algorithm 1 FPT Algorithm for parameter fes. Input : An input-instance D = (V, A) of our undirected problem. Output : Yes if and only if D is a yes-instance of the respective version of the problem. 1 Compute the solid graph G = (V, E) of D. 2 Compute an arbitrary feedback edge set F of minimum size, as well as the set X∗."
  },
  {
    "prompt": "Explain a concept involving Apply, Compute, foreach in computational complexity.",
    "response": "3 Apply all splitting operations and reduction rules. 4 Compute W ∗⊆V ∗according to Proposition 57 and the set C := {P1, . . . , P|C|} of W ∗-connectors. 5 E∗←{e ∈E | e ∩W ∗̸= ∅} ∪S w∈W ∗E(Tw) 6 L ←{i · 2n | i ∈O(fes2)} 7 foreach λ: E∗→2L with |λ(e)| ∈O(fes) for each edge e ∈E∗do 8 foreach P ∈C do 9 Compute the set LP of labelings for the edges of the extension of the connector P that agree with λ on the labels of the first and the last edge of P according to Lemma 58. 10 foreach (λP1, . . ."
  },
  {
    "prompt": "Scientific context on realizable, algorithm, return and their role in graph theory.",
    "response": ", λP|C|) ∈LP1 × · · · × LP|C| do 11 λ∗←λ ⋉λP1 ⋉· · · ⋉λP|C| 12 if λ∗realizes D then return Yes 13 return No preprocessing, as in the case where we detect that D is not realizable, our algorithm can simply output that D is not realizable. A pseudo code of our algorithm is shown in Algorithm 1. First, we apply the operation behind Proposition 40 to ensure that all pendant trees are small with respect to the degree of their root in G[V ∗]."
  },
  {
    "prompt": "Explain a concept involving Proposition, compute, connectors in computational complexity.",
    "response": "Afterwards, compute a set W ∗ according to Proposition 57 and the collection C of all W ∗-connectors. Moreover, let E∗ denote all edges incident with vertices of W ∗and all edges that are part of a pendant tree having its root in W ∗. Note that this includes the first and the last edge of each W ∗- connector. Let L := {i · 2n | i ∈O(fes2)}. Iterate over all possible labelings λ: E∗→2L that assign O(fes) labels to each edge of E∗."
  },
  {
    "prompt": "Read and understand this section about Lemma, Iterate, connector from a research paper.",
    "response": "For each such labeling λ, compute for each W ∗- connector P the set of labelings LP for the edges of the extension of P according to Lemma 58 for {α1 a, α2 a} = λ({a, a′}) and {α1 b, α2 b} = λ({b, b′}). Iterate over all possible combinations of such connector labelings, that is, iterate over each (λ1, . . . , λ|C|) ∈LP1 × · · · × LP|C|. For each such combination, consider the labeling λ∗:= λ ⋉λP1 ⋉· · · ⋉λP|C|. If λ∗realizes D, then correctly output that D is realizable."
  },
  {
    "prompt": "Explain a concept involving fes, size, Recall in computational complexity.",
    "response": "Otherwise, if no considered labeling λ∗realizes D, output that D is not realizable. Recall that W ∗has size O(fes), which implies that E∗has size O(fes) due to Observation 36 and since we assumed that the operation behind Proposition 40 was applied. Moreover, recall that LPi has constant size for each Pi ∈C."
  },
  {
    "prompt": "This is a technical discussion about fes, time, fesO in theoretical computer science.",
    "response": "Since (i) all preprocessing steps run in polynomial time, (ii) there are |L||E∗|·O(fes) ⊆fesO(fes2) possible candidates for λ, and (iii) for each candidate λ, we check O(1)|C| ⊆2O(fes) labelings λ∗, the whole algorithm runs in fesO(fes2) · nO(1) time. It thus remains to prove the correctness. Assume towards a contradiction that the algorithm is not correct. That is, D is realizable but the algorithm does not find a labeling λ∗ that realizes D. We first observe the following."
  },
  {
    "prompt": "Scientific context on Observation, Recognizing, Graphs and their role in graph theory.",
    "response": "54 Recognizing and Realizing Temporal Reachability Graphs ▶Observation 63. If D is realizable, then there is a frugal realization for D, where the labels of the edges of E∗are from L. This follows by two facts: First, in every frugal realization, each of the O(fes) edges in E∗ receives at most O(fes) labels. Hence, in each frugal realization, the set of all labels assigned to edges of E∗has size at most O(fes2)."
  },
  {
    "prompt": "Understanding the computational aspects of time, empty, steps.",
    "response": "Second, since each edge of E \\ E∗receives at most two labels in each minimal realization and there are at most n edges in E \\ E∗(since the feedback edge set F is a subset of E∗and G −F is a tree), by first removing all empty time steps from a frugal realization, and afterwards introducing sufficiently many empty time steps, we can ensure that the labels assigned to E∗are multiples of 2n and more precisely, are from L."
  },
  {
    "prompt": "Explain a concept involving edges, realizable, labels in computational complexity.",
    "response": "Since we assumed that D is realizable, there is, thus, a frugal realization λ′ for D that only assigns labels of L to the edges of E∗. We thus consider the labeling λ which is the restriction of λ′ to the edges of E∗. Now, for each W ∗-connector P, let LP be the corresponding labeling for the edges of the extension of P that agrees with λ′ on the labels of the first and the last edge of P according to Lemma 58."
  },
  {
    "prompt": "This is a technical discussion about Lemma, realizes, implies in theoretical computer science.",
    "response": "Since λ′ realizes D, Lemma 58 implies that there is some λP such that λ′ ⋉λP still realizes D. Since all connectors are pairwise edge-disjoint, there thus exists (λ1, . . . , λ|C|) ∈LP1 ×· · ·×LP|C| for which λ∗:= λ′⋉λ1⋉· · ·⋉λ|C| realizes D. By the fact that λ is the restriction of λ′ to the edges of E∗, λ∗equals λ ⋉λ1 ⋉· · · ⋉λ|C|. This contradicts the assumption that our algorithm does not find a realization λ∗for D. This thus proves Proposition 37 and Theorem 33. Remarks on kernelization."
  },
  {
    "prompt": "This is a technical discussion about nice, preprocessing, steps in theoretical computer science.",
    "response": "All our preprocessing steps run in polynomial time and lead to the set W ∗for which each connector is nice. Moreover, there are only O(fes) edges incident with vertices of W ∗or vertices of pendant trees that have their root in W ∗. Our preprocessing thus nearly achieves a polynomial problem kernel for our problem. Only the size of the connectors prevent this kernelization algorithm so far."
  },
  {
    "prompt": "Understanding the computational aspects of connector, analysis, replace.",
    "response": "We conjecture that (by an even deeper analysis) it is possible to, in polynomial time, replace each connector individually by a connector of constant size. This would then, together with our previous reduction rules imply a kernel for our problem with O(fes) vertices and solid edges and O(fes2) dashed arcs. Based on our exact algorithm (see Theorem 5), this would then also imply a running time of 2O(fes2) · nO(1) by simply solving the kernel via the exact algorithm."
  },
  {
    "prompt": "Explain a concept involving Complexity, Reachability, Graph in computational complexity.",
    "response": "8 The Complexity of Directed Reachability Graph Realizability Finally, we consider the complexity of versions of Directed Reachability Graph Real- izability. As already discussed by Döring [10], each directed graph D can be the strict reachability graph of a simple directed temporal graph (by assigning label 1 to each arc of D). Hence, Any Strict DRGR and Simple Strict DRGR are always yes-instances and can thus be solved in polynomial time."
  },
  {
    "prompt": "Understanding the computational aspects of DRGR, transitive, Strict.",
    "response": "We now show that all DAGs and all transitive graphs are trivial yes-instances for each version of the problem. ▶Lemma 64. An instance D = (V, A) of DRGR is a yes-instance, if we consider Any Strict DRGR or Simple Strict DRGR, D is a DAG, or D is a transitive graph. Thomas Erlebach, Othon Michail, Nils Morawietz 55 Here, a directed graph D = (V, A) is transitive, if for every distinct vertices u, v, w ∈V with (u, v) ∈A and (v, w) ∈A, the arc (u, w) is also in A. Proof."
  },
  {
    "prompt": "Explain a concept involving Döring, arc, label in computational complexity.",
    "response": "The first point was already shown by Döring [10]: By assigning label 1 to every arc of D, each arc is realized and each strict temporal path has length at most one. Hence, this trivial labeling realizes D. For the second point consider an arbitrary topological ordering π of D. One can realize D by a happy labeling λ as follows: Iterate over the topological ordering π from the sinks towards the sources."
  },
  {
    "prompt": "Read and understand this section about outgoing, arcs, vertex from a research paper.",
    "response": "For the ith vertex v of π, label the outgoing arcs B of v via an arbitrary bijection between B and [i · n + 1, i · n + |B|]. Since each vertex has less than n outgoing arcs, this labeling assigns a unique label to each arc. Hence, λ is a happy labeling. Moreover, no temporal path under λ has length more than one, since each path of D of length at least two, the assigned labels are strictly decreasing. This implies that λ realizes D."
  },
  {
    "prompt": "This is a technical discussion about path, arc, point in theoretical computer science.",
    "response": "For the last point consider an arbitrary bijection λ between A and [1, |A|]. Clearly, each arc receives one label and λ is a happy labeling. It remains to show that there is no temporal path between any two vertices u and v for which D does not contain the arc (u, v). Suppose that there is such a temporal path P. Since this path can only use arcs that received a label under λ (that is, since P only uses arcs of D), P is a path in D, which implies that (u, v) is an arc of D."
  },
  {
    "prompt": "Read and understand this section about Lemma, DRGR, Strict from a research paper.",
    "response": "This is due to the fact that D is a transitive graph. Hence, λ realizes D. ◀ Next, we show that on directed graphs that are close to being a DAG, Proper Strict DRGR, Happy Strict DRGR, and each version of Non-strict DRGR under consideration is NP-hard. To this end, we first make the following observations about the possibility of labeling arcs in these settings. Our first observation is a generalization of [10, Lemma 4.1]. ▶Lemma 65."
  },
  {
    "prompt": "Understanding the computational aspects of DRGR, Happy, instance.",
    "response": "Let D be an instance of Proper DRGR, Happy DRGR, or some version of Non-strict DRGR. Moreover, let C be an induces directed cycle of length at least 3 in D. Then, for each realization of D, there is at least one arc of C that receives no label. Proof. Let C = (v1, . . . , vℓ, v1). Assume towards a contradiction that this is not the case and let λ be a proper or non-strict directed realization where each arc of C receives at least one label."
  },
  {
    "prompt": "Explain a concept involving min, max, induced in computational complexity.",
    "response": "Since the cycle is induced, for each i, (vi, vi+2) is not an arc of D. Hence, max λ((vi+1, vi+2)) ≤min λ((vi, vi+1)) for each i ∈[1, ℓ].2 Since λ is a proper or non- strict realization, we even get that max λ((vi+1, vi+2)) < min λ((vi, vi+1)). This leads to a contradiction, since max λ((v2, v3)) < min λ((v1, v2)) ≤max λ((v1, v2)) < min λ((vℓ, v1)) ≤ · · · ≤max λ((v2, v3)). ◀ Let D = (V, A) be a directed graph."
  },
  {
    "prompt": "Scientific context on arc, triangulated, Lemma and their role in graph theory.",
    "response": "We say that an arc (u, v) ∈A is triangulated in D, if there is a vertex x ∈V such that (u, x) ∈A and (x, v) ∈A. ▶Lemma 66. Let D be an instance of any version of DRGR. In each realization of D, each arc (u, v) of D that is not triangulated receives at least one label. Proof. Assume towards a contradiction that there is a directed temporal graph (G, λ) that realizes D such that there is an arc (u, v) of D which is not triangulated in D and receives no label."
  },
  {
    "prompt": "This is a technical discussion about temporal, path, realizes in theoretical computer science.",
    "response": "Since (G, λ) realizes D and D contains the arc (u, v), there is a temporal path P from u to v in (G, λ). Let x be an arbitrary vertex of P distinct from u and v. The existence of the temporal path P implies that there is a temporal path P1 from u to x in (G, λ) and that there is a temporal path P2 from x to v in (G, λ). Since the reachability graph of (G, λ) 2 For convenience, ℓ+ 1 = 1 and ℓ+ 2 = 2."
  },
  {
    "prompt": "Read and understand this section about Recognizing, Realizing, Temporal from a research paper.",
    "response": "56 Recognizing and Realizing Temporal Reachability Graphs is exactly D, this implies that D contains the arcs (u, x) and (x, v). This contradicts the assumption that (u, v) is not triangulated. ◀ Based on these two lemma, we can now show the stated hardness results. ▶Theorem 67. Proper DRGR, Happy DRGR, and all considered versions of Non- strict DRGR are NP-hard on graphs with a constant size feedback arc set."
  },
  {
    "prompt": "This is a technical discussion about DRGR, ETH, time in theoretical computer science.",
    "response": "Moreover, none of these versions of DRGR can be solved in 2o(|V |+|A|) · nO(1) time, unless the ETH fails. Proof. We again reduce from 2P2N-3SAT. Let ϕ be an instance of 2P2N-3SAT with variables X and clauses Y . We construct a directed graph D = (V, A) as follows: For each variable x ∈X, V contain the vertices x, x′, ax, bx, x, x′, ax, and bx. For each clause yi ∈Y , V contains the vertex si. Additionally, V contains the vertices v∗, w∗, ⊤, ⊤′, vS, vL, vL′. Next, we describe the arcs of D."
  },
  {
    "prompt": "Understanding the computational aspects of arcs, vL′, variable.",
    "response": "For each variable x ∈X, A contains the arcs (v∗, x), (x, x′), (x′, x), (x, x′), (x′, w∗), (x, ax), (ax, ⊤), (x, bx), (bx, x′), (x, ⊤), (x′, ⊤), (x′, vL′), (vL, x), and (x, ax), (ax, ⊤), (x, bx), (bx, x′), (x, ⊤), (x′, ⊤), (x′, vL′), (vL, x). Let yi be a clause of Y . For each literal ℓof yi, A contains the arcs (si, ℓ), (si, ℓ′), and (si, bℓ). Additionally, A contains the arcs (si, ⊤) and (vS, si). Finally, A contains the arcs (w∗, v∗), (⊤, ⊤′), (⊤′, vS), (⊤′, vL), and (vL′, vS)."
  },
  {
    "prompt": "This is a technical discussion about Claim, arc, vL′ in theoretical computer science.",
    "response": "This completes the construction of D. ▷Claim 68. The set of arcs {(w∗, v∗), (⊤, ⊤′), (vL′, vS)} is a feedback arc set of D. Proof. Consider the graph D′ obtained by removing these three arcs from D. By construction, for each (u, v) ∈{(w∗, v∗), (⊤, ⊤′), (vL′, vS)}, (u, v) is the only arc going out of u in D and (u, v) is the only arc going into v in D. This implies that u is a sink in D′ and v is a source in D′."
  },
  {
    "prompt": "Explain a concept involving DAG, show, vL′ in computational complexity.",
    "response": "To show that D′ is a DAG it thus remains to show that D′′ := D′ −{w∗, v∗, ⊤, ⊤′, vL′, vS} is a DAG. Note that for each clause yi ∈Y , (vS, si) is the only arc going into si in D. Hence, each vertex of QS := {si | yi ∈Y } is a source in D′′. To show that D′′ is a DAG it thus remains to show that D′′′ := D′′ −QS is a DAG. By construction, D′′′ consists of one connected component Vx for each variable x ∈X, each of which is a DAG itself. Consequently, D′ is a DAG."
  },
  {
    "prompt": "Read and understand this section about set, vL′, satisfying from a research paper.",
    "response": "◁ (⇒) Let π be a satisfying assignment for ϕ. We show that there is a directed happy temporal graph G := (D′, λ) with reachability graph D. We set λ((w∗, v∗)) = 8, λ((⊤, ⊤′)) = 13, λ((⊤′, vL)) = λ((⊤′, vS)) = 12, and λ((vL′, vS)) = 4. For each variable x ∈X, we set λ((v∗, x)) = 7 and λ((x′, w∗)) = 9. Moreover, for each ℓ∈{x, x}, we set λ((ℓ, aℓ)) = 1, λ((aℓ, ⊤)) = λ((ℓ′, ⊤)) = 14, λ((ℓ, bℓ)) = 3, λ((bℓ, ℓ′)) = 15, λ((ℓ′, vL′)) = 5, and λ((vL, ℓ)) = 11."
  },
  {
    "prompt": "Explain a concept involving set, true, false in computational complexity.",
    "response": "If π(x) = true, we set λ((x, x′)) = 6 and λ((x′, x)) = 5. Otherwise, that is, if π(x) = false, we set λ((x′, x)) = 11 and λ((x, x′)) = 10. Finally, for each clause yi ∈Y , we set λ((vS, si)) = 3. Moreover, for each literal ℓof yi, we set λ((si, ℓ)) = 2. All other arcs receive no label. This completes the construction of λ. Note that λ is happy. We now show that the reachability graph of G equals D."
  },
  {
    "prompt": "Read and understand this section about path, temporal, show from a research paper.",
    "response": "To show this, it suffices to show that (i) for each arc (u, v) ∈A that receives no label under λ, there is a temporal (u, v)-path under λ, and (ii) for each non-arc (u, v) /∈A, there is no temporal (u, v)-path under λ. First, we show that all arcs of D are realized. Let (u, v) be an arc of D that receives no label under λ."
  },
  {
    "prompt": "Scientific context on ℓand, variable, clause and their role in graph theory.",
    "response": "By construction of λ, (u, v) has one of four types: there is some variable x ∈X and some ℓ∈{x, x}, such that u = ℓand v = ℓ′, Thomas Erlebach, Othon Michail, Nils Morawietz 57 there is some variable x ∈X and some ℓ∈{x, x}, such that u = ℓand v = ⊤, there is some clause yi ∈Y and some literal ℓcontained in yi, such that u = si and v ∈{ℓ′, bℓ}, or there is some clause yi ∈Y , such that u = si and v = ⊤."
  },
  {
    "prompt": "Read and understand this section about path, arc, realized from a research paper.",
    "response": "If (u, v) is of the first type, the arc is realized by the temporal path (ℓ, bℓ, ℓ′) with la- bels (3, 15). If (u, v) is of the second type, the arc is realized by the temporal path (ℓ, aℓ, ⊤) with labels (1, 14). If (u, v) is of the third type, the arc is realized by the temporal path (si, ℓ, bℓ, ℓ′) with labels (2, 3, 15) (or its subpath). Hence, consider the fourth type and let yi ∈Y . Since π satisfies ϕ, there is some literal ℓof yi, such that ℓis satisfied by π."
  },
  {
    "prompt": "Explain a concept involving arc, realized, received in computational complexity.",
    "response": "This implies that the arc (ℓ, ℓ′) received a label α under λ. By definition, α ∈{6, 10}. Hence, the arc (si, ⊤) is realized by the path (si, ℓ, ℓ′, ⊤) with labels (2, α, 14). Consequently, each arc of D is realized. Next, we show that no non-arc of D is realized. To this end, we analyze the structure of temporal paths of length more than one under λ. Let P be a temporal path of length at least two under λ and let u and v be the endpoints of P. We show that (u, v) is an arc of D."
  },
  {
    "prompt": "This is a technical discussion about arc, vL′, vertex in theoretical computer science.",
    "response": "By definition of λ, for every vertex u ∈{⊤, ⊤′, vS, vL, vL′, v∗, w∗} ∪{si | yi ∈Y }, each arc going into u has a strictly larger label than each arc going out of u. Thus, P contains none of these vertices as an intermediate vertex. In other words, the set of intermediate vertices of P is a subset of {ℓ, ℓ′, aℓ, bℓ| x ∈X, ℓ∈{x, x}}."
  },
  {
    "prompt": "Read and understand this section about arc, label, variable from a research paper.",
    "response": "Moreover, P cannot traverse the arc (v∗, x) for any variable x ∈X, since v∗is not an intermediate vertex of P as each outgoing arc of x receives a strictly smaller label. Similarly, P traverses neither the arc (x′, x) nor the arc (x′, w∗). This implies that there is some variable x ∈X and some literal ℓ∈{x, x}, such that P contains only vertices of Qℓ= {ℓ, ℓ′, aℓ, bℓ, ⊤} ∪Sℓ, where Sℓdenotes the vertices of {si | yi ∈Y, ℓ∈yi}."
  },
  {
    "prompt": "Understanding the computational aspects of DAG, Sℓand, arc.",
    "response": "By construction of D, D[Qℓ] is a DAG with sources Sℓand unique sink ⊤. Moreover, bℓis the only vertex of Qℓthat has no arc towards ⊤. Clearly, P cannot be a temporal path from bℓto ⊤, since each arc going into ⊤has label 13 and the unique arc (bℓ, ℓ′) going out of bℓreceives label 14. Hence, if P is an (u, v)-path with v = ⊤, then (u, v) is an arc of D. Similarly, for each vertex si ∈Sℓ, aℓis the only vertex of Qℓ\\ Sℓfor which (si, aℓ) is not an arc of D."
  },
  {
    "prompt": "Read and understand this section about label, path, arc from a research paper.",
    "response": "Clearly, P cannot be a temporal path from si to aℓ, since each arc going out of si has label 2 and the unique arc (ℓ, aℓ) going into aℓreceives label 1. Hence, if P is a (u, v)-path with u ∈Si, then (u, v) is an arc of D. Thus, in the following, we only have to consider the case where P contains only vertices of {ℓ, ℓ′, aℓ, bℓ}. The only path of length at least two in D[{ℓ, ℓ′, aℓ, bℓ}] is the path (ℓ, bℓ, ℓ′). Thus, the only remaining option for P is to be this unique path."
  },
  {
    "prompt": "Explain a concept involving arc, graph, endpoints in computational complexity.",
    "response": "Since (ℓ, ℓ′) is an arc of D, there is an arc between the endpoints of P in D. Consequently, no non-arc of D is realized, which implies that λ realizes D. (⇐) Let G := (D, λ: A →2N) be a directed temporal graph with (i) proper labeling λ and strict reachability graph equal to D or (ii) non-strict reachability graph equal to D. We show that there is a satisfying assignment π for ϕ. For each variable x ∈X, we set π(x) := ( true λ((x, x′)) ̸= ∅and false otherwise."
  },
  {
    "prompt": "Scientific context on show, satisfies, Lemmas and their role in graph theory.",
    "response": "In the following, we show that π satisfies ϕ. To this end, we first show that λ((x, x′)) = ∅ if π(x) = true, that is, if λ((x, x′)) ̸= ∅. This statement follows form Lemmas 65 and 66 and the fact that C := (v∗, x, x′, x, x′, w∗, v∗) is an induced directed cycle in D for which (x, x′) and (x, x′) are the only triangulated arcs. Hence, if λ((x, x′)) ̸= ∅, then π(x) = false. 58 Recognizing and Realizing Temporal Reachability Graphs Based on this observation, we now show that π satisfies ϕ."
  },
  {
    "prompt": "Explain a concept involving path, arc, temporal in computational complexity.",
    "response": "Let yi be a clause of Y . Since G realizes D and (si, ⊤) is an arc of D, there is a temporal path P from si to ⊤in G. Recall that si has only the out-neighbor ⊤and the out-neighbors {ℓ, ℓ′, bℓ} for each literal ℓof yi. Hence, the temporal path P only visits these vertices, since P is a dense path, that is, since there is an arc from si to each other vertex of P. Moreover, P cannot visit any vertex bℓ, since no such vertex has an arc towards ⊤."
  },
  {
    "prompt": "Understanding the computational aspects of path, arc, implies.",
    "response": "This implies that P is either the path (si, ⊤), or for some literal ℓof yi, P ∈{(si, ℓ, ⊤), (si, ℓ′, ⊤), (si, ℓ, ℓ′, ⊤)}. First, note that the direct arc (si, ⊤) does not receive a label under λ. This is due to Lemmas 65 and 66 and the fact that C := (si, ⊤, ⊤′, vS, si) is an induced directed cycle in D for which (si, ⊤) is the only triangulated arc. This implies that P ̸= (si, ⊤). Hence, there is a literal ℓof yi such that P ∈{(si, ℓ, ⊤), (si, ℓ′, ⊤), (si, ℓ, ℓ′, ⊤)}."
  },
  {
    "prompt": "Explain a concept involving arc, implies, receive in computational complexity.",
    "response": "Similarly to the above, (ℓ, ⊤) does not receive a label under λ, since C := (ℓ, ⊤, ⊤′, vL, ℓ) is an induced directed cycle in D for which (ℓ, ⊤) is the only triangulated arc. This implies that P ̸= (si, ℓ, ⊤). Analogously, (si, ℓ′) does not receive a label under λ, since C := (si, ℓ′, vL′, vS, si) is an induced directed cycle in D for which (si, ℓ′) is the only triangulated arc. This implies that P ̸= (si, ℓ′, ⊤). As a consequence, P = (si, ℓ, ℓ′, ⊤), which implies that λ((ℓ, ℓ′)) ̸= ∅."
  },
  {
    "prompt": "Scientific context on satisfied, clause, variable and their role in graph theory.",
    "response": "If ℓ= x for some variable x ∈X, then π(x) = true by definition of π, which implies that clause yi is satisfied by π. Otherwise, that is, if ℓ= x for some variable x ∈X, then λ((x, x′)) = ∅, since λ((x, x′)) ̸= ∅. Hence π(x) = false by definition of π, which implies that clause yi is satisfied by π. In both cases, clause yi is satisfied by π. Consequently, π satisfies ϕ. ETH lower-bound. Note that D has O(|ϕ|) vertices and arcs."
  },
  {
    "prompt": "Explain a concept involving DRGR, stated, ETH in computational complexity.",
    "response": "Since 2P2N-3SAT cannot be solved in 2o(|ϕ|) · |ϕ|O(1) time, unless the ETH fails [23], this implies the stated running-time lower-bound for the stated versions of DRGR. ◀ This thus settles the complexity of each version of DRGR under consideration. Moreover, an FPT algorithm for any of these versions when parameterized by the size of a smallest feedback arc set is impossible, unless P = NP."
  },
  {
    "prompt": "Explain a concept involving Conclusion, Reachability, Realizability in computational complexity.",
    "response": "9 Conclusion We studied Reachability Graph Realizability and gave for both directed and undirected temporal graphs the complete picture for the classical complexity of all settings, answering this open problem posed by [3] and [10]. For URGR, we additionally showed that the problem can be solved in FPT-time for the feedback edge set number fes of the solid graph."
  },
  {
    "prompt": "Scientific context on URGR, FPT, showed and their role in graph theory.",
    "response": "As we showed, this parameter cannot be replaced by smaller parameters like feedback vertex set number or treedepth of the solid graph, unless FPT = W[2]. There are several directions for future work: First, it would be interesting to see whether (some) versions of URGR admit a polynomial kernel for fes. Another interesting task is to determine whether Proper URGR, Happy URGR, or Non-strict URGR admits an FPT algorithm for feedback vertex set or treedepth."
  },
  {
    "prompt": "Explain a concept involving URGR, Strict, Simple in computational complexity.",
    "response": "This is not excluded by our W[2]-hardness result, since that reduction only worked for Any Strict URGR and Simple Strict URGR. It would also be interesting to see whether Proper URGR, Happy URGR, or Non-strict URGR can be solved in polynomial time on solid graphs that are triangle-free. Finally, one could analyze the parameterized complexity of Directed Reachability Graph Realizability with respect to directed graph parameters."
  },
  {
    "prompt": "Understanding the computational aspects of References, Erlebach, Othon.",
    "response": "Thomas Erlebach, Othon Michail, Nils Morawietz 59 References 1 Eleni C Akrida, Leszek Gąsieniec, George B Mertzios, and Paul G Spirakis. The complexity of optimal design of temporally connected graphs. Theory of Computing Systems, 61:907–944, 2017. 2 Binh-Minh Bui-Xuan, Afonso Ferreira, and Aubin Jarry. Computing shortest, fastest, and foremost journeys in dynamic networks. Int. J. Found. Comput. Sci., 14(2):267–285, 2003. doi:10.1142/S0129054103001728."
  },
  {
    "prompt": "Explain a concept involving Timothée, Corsini, Sarkar in computational complexity.",
    "response": "3 Arnaud Casteigts, Timothée Corsini, and Writika Sarkar. Simple, strict, proper, happy: A study of reachability in temporal graphs. Theor. Comput. Sci., 991:114434, 2024. doi: 10.1016/J.TCS.2024.114434. 4 Arnaud Casteigts, Paola Flocchini, Walter Quattrociocchi, and Nicola Santoro. Time-varying graphs and dynamic networks. International Journal of Parallel, Emergent and Distributed Systems, 27(5):387–408, 2012. 5 Arnaud Casteigts, Nils Morawietz, and Petra Wolf."
  },
  {
    "prompt": "Understanding the computational aspects of MFCS, Distance, transitivity.",
    "response": "Distance to transitivity: New parameters for taming reachability in temporal graphs. In 49th International Symposium on Mathematical Foundations of Computer Science (MFCS 2024), volume 306 of LIPIcs, pages 36:1–36:17. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2024. 6 Esteban Christiann, Eric Sanlaville, and Jason Schoeters. On inefficiently connecting temporal networks. In 3rd Symposium on Algorithmic Foundations of Dynamic Networks (SAND 2024)."
  },
  {
    "prompt": "Read and understand this section about Dagstuhl, Informatik, Pilipczuk from a research paper.",
    "response": "Schloss Dagstuhl–Leibniz-Zentrum für Informatik, 2024. 7 Marek Cygan, Fedor V. Fomin, Lukasz Kowalik, Daniel Lokshtanov, Dániel Marx, Marcin Pilipczuk, Michal Pilipczuk, and Saket Saurabh. Parameterized Algorithms. Springer, 2015. doi:10.1007/978-3-319-21275-3. 8 Argyrios Deligkas and Igor Potapov. Optimizing reachability sets in temporal graphs by delaying. Information and Computation, 285:104890, 2022. 9 Rodney G. Downey and Michael R. Fellows. Fundamentals of Parameterized Complexity."
  },
  {
    "prompt": "Scientific context on Texts, Directed, Computer and their role in graph theory.",
    "response": "Texts in Computer Science. Springer, 2013. doi:10.1007/978-1-4471-5559-1. 10 Michelle Döring. Simple, Strict, Proper, and Directed: Comparing Reachability in Directed and Undirected Temporal Graphs. CoRR, abs/2501.11697, 2025. arXiv:2501.11697. 11 Jessica Enright, Kitty Meeks, George B Mertzios, and Viktor Zamaraev. Deleting edges to restrict the size of an epidemic in temporal networks. Journal of Computer and System Sciences, 119:60–77, 2021. 12 Thomas Erlebach, Nils Morawietz, and Petra Wolf."
  },
  {
    "prompt": "This is a technical discussion about SAND, doi, Parameterized in theoretical computer science.",
    "response": "Parameterized algorithms for multi- label periodic temporal graph realization. In 3rd Symposium on Algorithmic Foundations of Dynamic Networks (SAND 2024), volume 292 of LIPIcs, pages 12:1–12:16. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2024. doi:10.4230/LIPIcs.SAND.2024.12. 13 F. Göbel, J. Orestes Cerdeira, and Henk Jan Veldman. Label-connected graphs and the gossip problem. Discret. Math., 87(1):29–40, 1991. doi:10.1016/0012-365X(91)90068-D."
  },
  {
    "prompt": "Understanding the computational aspects of David, Kempe, Jon.",
    "response": "14 David Kempe, Jon Kleinberg, and Amit Kumar. Connectivity and inference problems for temporal networks. In Proceedings of the thirty-second annual ACM symposium on Theory of computing, pages 504–513, 2000. 15 Nina Klobas, George B Mertzios, Hendrik Molter, and Paul G Spirakis. The complexity of computing optimum labelings for temporal connectivity. Journal of Computer and System Sciences, 146:103564, 2024. 16 Nina Klobas, George B. Mertzios, Hendrik Molter, and Paul G. Spirakis."
  },
  {
    "prompt": "This is a technical discussion about volume, SAND, paths in theoretical computer science.",
    "response": "Temporal graph realization from fastest paths. In 3rd Symposium on Algorithmic Foundations of Dynamic Networks (SAND 2024), volume 292 of LIPIcs, pages 16:1–16:18. Schloss Dagstuhl - Leibniz- Zentrum für Informatik, 2024. doi:10.4230/LIPIcs.SAND.2024.16. 17 Édouard Lucas. Récréations mathématiques, volume 1. Gauthier-Villars, 1882. 18 George B Mertzios, Othon Michail, and Paul G Spirakis. Temporal network optimization subject to connectivity constraints. Algorithmica, 81(4):1416–1449, 2019."
  },
  {
    "prompt": "This is a technical discussion about Recognizing, George, Mertzios in theoretical computer science.",
    "response": "60 Recognizing and Realizing Temporal Reachability Graphs 19 George B. Mertzios, Hendrik Molter, Nils Morawietz, and Paul G. Spirakis. Realizing temporal transportation trees. CoRR, 2025. URL: https://arxiv.org/abs/2403.18513, arXiv:2403.18513. 20 Othon Michail. An introduction to temporal graphs: An algorithmic perspective. Internet Mathematics, 12(4):239–280, 2016. 21 Othon Michail and Paul G Spirakis. Elements of the theory of dynamic networks. Communic- ations of the ACM, 61(2):72–72, 2018."
  },
  {
    "prompt": "This is a technical discussion about Stephen, Seidman, doi in theoretical computer science.",
    "response": "22 Stephen B. Seidman. Network structure and minimum degree. Social Networks, 5(3):269–287, 1983. doi:https://doi.org/10.1016/0378-8733(83)90028-X. 23 Craig A. Tovey. A simplified NP-complete satisfiability problem. Discret. Appl. Math., 8(1):85–89, 1984. doi:10.1016/0166-218X(84)90081-7. 24 Philipp Zschoche, Till Fluschnik, Hendrik Molter, and Rolf Niedermeier. The complexity of finding small separators in temporal graphs. Journal of Computer and System Sciences, 107:72–92, 2020."
  },
  {
    "prompt": "This is a technical discussion about Mar, Range, Avoidance in theoretical computer science.",
    "response": "arXiv:2503.17114v1 [cs.CC] 21 Mar 2025 Range Avoidance in Boolean Circuits via Turan-type Bounds Neha Kuntewar* Jayalal Sarma∗ Abstract Given a circuit C : {0, 1}n →{0, 1}m from a circuit class C, with m > n, ﬁnding a y ∈{0, 1}m such that ∀x ∈{0, 1}n, C(x) ̸= y, is the range avoidance problem (denoted by C-AVOID)."
  },
  {
    "prompt": "This is a technical discussion about Deter, Boolean, time in theoretical computer science.",
    "response": "Deter- ministic polynomial time algorithms (even with access to NP oracles) solving this problem is known to imply explicit constructions of various pseudorandom objects like hard Boolean functions, linear codes, PRGs etc. Deterministic polynomial time algorithms are known for NC0 2-AVOID when m > n, and for NC0 3-AVOID when m ≥ n2 log n, where NC0 k is the class of circuits with bounded fan-in which have constant depth and the output depends on at most k of the input bits."
  },
  {
    "prompt": "Explain a concept involving hand, matrices, range in computational complexity.",
    "response": "On the other hand, it is also known that NC0 3-AVOID when m = n + O \u0000n2/3\u0001 is at least as hard as explicit construction of rigid matrices. In fact, algorithms for solving range avoidance for even NC0 4 circuits implies new circuit lower bounds. In this paper, we propose a new approach to solving range avoidance problem via hyper- graphs."
  },
  {
    "prompt": "Understanding the computational aspects of k-uniform, hypergraph, Turan-type.",
    "response": "We formulate the problem in terms of Turan-type problems in hypergraphs of the following kind - for a ﬁxed k-uniform hypergraph H′, what is the maximum number of edges that can exist in a k-uniform hypergraph H which does not have a sub-hypergraph isomorphic to H′? We ﬁrst demonstrate the applicability of this approach by showing alternate proofs of some of the known results for range avoidance problem using this framework."
  },
  {
    "prompt": "Explain a concept involving MONOTONE, Turan-type, literature in computational complexity.",
    "response": "We then use our approach to show (using several different hypergraph structures for which Turan-type bounds are known in the literature) that there is a constant c such that MONOTONE- NC0 3-AVOID can be solved in deterministic polynomial time when m > cn2. To improve the stretch constraint to linear, we show a new Turan-type theorem for a hypergraph structure (which we call the the loose X 2ℓ-cycles)."
  },
  {
    "prompt": "Understanding the computational aspects of speciﬁcally, connected, uniform.",
    "response": "More speciﬁcally, we prove that Any connected 3- uniform linear hypergraph with m > n edges must contain a loose X 2ℓcycle. Using this, we show that MONOTONE-NC0 3-AVOID can be solved in deterministic polynomial time when m > n, thus improving the known bounds of NC0 3-AVOID for the case of monotone circuits. In contrast, we note that efﬁcient algorithms for solving MONOTONE-NC0 6-AVOID, already implies explicit constructions for rigid matrices."
  },
  {
    "prompt": "Explain a concept involving SYMMETRIC, Building, Chennai in computational complexity.",
    "response": "Building on this further, we show that there is a polynomial time algorithm for SYMMETRIC- NC0 3-AVOID when m > 8n. We also generalize our argument to solve the special case of range avoidance for NC0 k where each output function computed by the circuit, is the majority function on its inputs where m > n2. *Indian Institute of Technology Madras, Chennai, India."
  },
  {
    "prompt": "Understanding the computational aspects of Contents, Introduction, Preliminaries.",
    "response": "Email: neha.kuntewar@gmail.com, jayalal@cse.iitm.ac.in 1 Contents 1 Introduction 2 2 Preliminaries 7 3 Range Avoidance for NC0 k via Fixed Hypergraphs 9 3.1 Polynomial time algorithm for ONE-INTERSECT-MAJ3-AVOID . . . . . . . . . . . . . 13 3.2 Polynomial time Algorithm for ONE-INTERSECT-MAJk-AVOID . . . . . . . . . . . . . 14 4 Polynomial time Algorithm for MONOTONE-NC0 3-AVOID 15 4.1 Reduction from MONOTONE-NC0 3-AVOID to MAJ3-AVOID: . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about Polynomial, Algorithm, time from a research paper.",
    "response": "15 4.2 Polynomial time Algorithm for MAJ3-AVOID . . . . . . . . . . . . . . . . . . . . . . . 16 5 Polynomial time Algorithm for SYMMETRIC-NC0 3-AVOID 21 6 Conclusion 24 A Appendix 27 A.1 Special cases of MONOTONE-AVOID . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 A.2 Alternative Hypergraphs for Solving ONE-INTERSECT-MAJ3-AVOID . . . . . . . . . 28 A.3 Counter Example graphs among the (9, 5)-hypergraphs . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about range, Introduction, Boolean from a research paper.",
    "response": "31 1 Introduction Let C : {0, 1}n →{0, 1}m be a Boolean circuit with {∧, ∨, ¬} gates, with m > n. The range of the function represented by the circuit : Range(C) = {C(x) | x ∈{0, 1}n}. Clearly, ∃y ∈{0, 1}m such that y /∈Range(C). The range avoidance problem (denoted by AVOID) asks, given a circuit C, with m > n, ﬁnd a y /∈Range(C). The AVOID problem (introduced by [KKMP21]) has been shown to have connections to some of the central research questions in circuit lower bounds and pseudorandomness."
  },
  {
    "prompt": "Understanding the computational aspects of FPNP, AVOID, bounds.",
    "response": "In particular, even FPNP algorithm for AVOID is known to imply new circuit lower bounds [Kor22] and new constructions of many other pseudorandom objects [Kor22]. On the algorithms side, AVOID has a trivial ZPPNP algorithm. Indeed, given a circuit C : {0, 1}n →{0, 1}m with m > n, choose y ∈{0, 1}m and use the NP oracle to check if ∃x ∈{0, 1}n such that C(x) = y."
  },
  {
    "prompt": "This is a technical discussion about Range, probability, algorithm in theoretical computer science.",
    "response": "Since m > n, there are at least 1 2 fraction of ys which are outside Range(C), and hence the algorithm succeeds with at least 1 2 probability. Designing a deterministic polynomial time algorithm, with access to an NP oracle (FPNP algorithm) to solve AVOID is a central open problem. Recently, [CHR] obtained the ﬁrst single-valued FS2P algorithm for AVOID which works inﬁnitely often."
  },
  {
    "prompt": "Scientific context on CHR, Subsequently, gave and their role in graph theory.",
    "response": "Subsequently, [Li24] gave an improved FS2P algorithm that works for all n, thus establishing explicit functions in S2E requiring maximum circuit complexity. [CHR] showed an unconditional zero-error pseudodeterministic algorithm with an NP or- acle and one bit of advice that solves AVOID for inﬁnitely many inputs. These results imply 2 pseudo-determinsitic constructions for Ramsey graphs, rigid matrices, pseudo-random genera- tors etc.(See [CHR])."
  },
  {
    "prompt": "Read and understand this section about JLS, AVOID, shows from a research paper.",
    "response": "[ILW23] shows that if there is a deterministic polynomial time algorithm for AVOID then either NP = coNP or there does not exist JLS secure iO. Given the central nature of the problem, it is also meaningful to consider simpler versions ﬁrst : consider C-AVOID to be the restricted version of AVOID where the circuit is guaranteed to be from the class C. For which classes of circuits C do we have an efﬁcient (or FPNP) algorithm for C-AVOID?"
  },
  {
    "prompt": "This is a technical discussion about Ren, Santhanam, Wang in theoretical computer science.",
    "response": "On this frontier, Ren, Santhanam and Wang [RSW22] showed that an FPNP algorithm for C-AVOID implies breakthrough lower bounds even when C restricted to weaker circuit models such as AC0 and NC1 circuits. To go down even further, for every constant k, consider the restricted class of circuits NC0 k where the depth of the circuit is constant and each output bit depends on at most k of the input bits."
  },
  {
    "prompt": "Scientific context on Ren, Santhanam, Wang and their role in graph theory.",
    "response": "In a surprising result, Ren, Santhanam and Wang [RSW22] showed that an FPNP algorithm for NC0 4-AVOID implies FPNP algorithms for NC1-AVOID. Additionally, this will also imply new circuit lower bounds - that there is a family of functions in ENP that requires circuits of depth at least Ω(n1−ǫ). Complementing the above, [RSW22] also exhibited FPNP algorithm for AVOID, when C is re- stricted to De Morgan formulas of size s with m > nω(√s log s)."
  },
  {
    "prompt": "Scientific context on class, FPNP, algorithms and their role in graph theory.",
    "response": "At the low-end regime, [GLW22] showed a polynomial time algorithm for NC0 2 class, where each output bit depends on at most 2 input bits. They show a general template for obtaining FPNP algorithms for restricted classes via hitting set constructions. In particular, they give FPNP algorithms for NC0 k circuits, de Morgan formulas, CNF or DNF, provided the stretch is large enough."
  },
  {
    "prompt": "Understanding the computational aspects of als, log, polynomi.",
    "response": "En route they also give a general method for obtaining the hitting set in polynomial time using approximation degree of polynomi- als. Complementing this further, [GGNS23] designed deterministic polynomial time algorithms for all NC0 k-AVOID for m ≥nk−1 log n . For k = 3, which was the frontier beyond [GLW22], this requires m ≥ n2 log n."
  },
  {
    "prompt": "Read and understand this section about polynomial, deterministic, time from a research paper.",
    "response": "They also showed the reason for lack of progress for NC0 3-AVOID by proving that a deterministic polynomial time algorithm for NC0 3-AVOID , where m = n + O(n2/3) would imply explicit construction of rigid matrices in deterministic polynomial time. This demonstrates the importance of the stretch function in the context of AVOID problem. Our Results: In this paper, we propose a new approach to range avoidance problem for NC0 k circuits via Turan-type extremal problems in hypergraphs."
  },
  {
    "prompt": "Explain a concept involving set, denote, variables in computational complexity.",
    "response": "For an NC0 k circuit C, for each function fi ∈C where i ∈[m], let I(fi) denote the set of input variables that fi depends on. Let HC denote the hypergraph deﬁned as follows: Let V be the set of inputs in C. For 1 ≤i ≤m, deﬁne a hyperedge ei as {xj | j ∈[n], xj ∈I(fi)}. Thus E = {{ei | i ∈[m]}} has exactly m edges, each of size at most k. Let F be the family of functions that appear in the circuit C, and let φ : E →F be a labelling of the edges of the hypergraph H with the corresponding function."
  },
  {
    "prompt": "Read and understand this section about Boolean, functions, induces from a research paper.",
    "response": "Without loss of generality, by assigning colors to Boolean bits, say red R for 0, and blue B for 1, we can interpret these functions as functions from {R, B}k →{R, B}, which induces a color to the hyperedge given any 2-coloring of the vertices of the hypergraph. A 2-coloring of the hyperedges of H, is said to be a φ-coloring if there is a vertex coloring which induces this edge coloring via φ."
  },
  {
    "prompt": "Read and understand this section about theorem, notation, deﬁnition from a research paper.",
    "response": "With this notation, we state the following theorem, which essentially follows from the above deﬁnition (see section 3). 3 Theorem 1.1. Let C : {0, 1}n →{0, 1}m be a circuit and HC be the corresponding hypergraph and let φ be the labelling function. Suppose HC contains a sub-hypergraph H′ (of ﬁxed size) such that there is an edge-coloring of H′ which is not φ-coloring. Then there is a polynomial time algorithm to ﬁnd a string outside the range of C."
  },
  {
    "prompt": "Explain a concept involving main, idea, proof in computational complexity.",
    "response": "The main idea of the above proof is that it sufﬁces to identify an sub-hypergraph H′ in H such that there is an edge-coloring of H′ which is not φ-coloring on H′. If we can ascertain the existence of a copy of H’ by extremal hypergraph theory, then it can be used to solve the AVOID problem."
  },
  {
    "prompt": "Scientific context on Turan-type, family, induced and their role in graph theory.",
    "response": "In particular, this formulates range avoidance problem in terms of Turan-type extremal problems in hypergraphs of the following kind - for a ﬁxed family of hypergraphs H′, what is the maximum number of edges that can exist in any hypergraph that avoids any member of the family H′ as an (induced) subgraph? This is particularly well-studied for k-uniform hypergraph and is denoted by exk(n, H′)."
  },
  {
    "prompt": "This is a technical discussion about Notice, AVOID, circuit in theoretical computer science.",
    "response": "Notice that the family H′ may critically depend on the set of functions F, and the mapping φ, and hence on the circuit C. A natural question is, are there ﬁxed H′ that works for all circuits in the circuit class1 for which we are interested to solve AVOID problem. We ﬁrst demonstrate immediate simple applications of this framework for designing algo- rithms for AVOID in restricted settings."
  },
  {
    "prompt": "Read and understand this section about AVOID, input, designed from a research paper.",
    "response": "As mentioned above, [GLW22] designed a simple iterative algorithm for solving AVOID when the input is restricted to circuits where each output function depends on at most 2 input bits. We show that the same special case can also be solved using our approach. Thus, as a warm up, we derive an alternative proof of the following theorem (originally due to [GLW22]) using our framework."
  },
  {
    "prompt": "This is a technical discussion about AVOID, framework, pro in theoretical computer science.",
    "response": "As second demonstration of our framework, we use tools from extremal graph theory to pro- vide deterministic polynomial time algorithms for AVOID when F contains only ∧and ∨functions that depend on exactly k input bits. We remark that such powerful tools are not needed to solve this special case as the iterative algorithmic idea due to [GLW22] can be extended to this case as well. Nevertheless, we argue that it serves as a demonstration of our technique itself."
  },
  {
    "prompt": "Explain a concept involving proposition, AVOID, state in computational complexity.",
    "response": "We state both of these applications as the following proposition. Proposition 1.2. There is a deterministic polynomial time algorithm for NC0 2-AVOID when m > n. Using the same framework, there is a polynomial time algorithm for solving {ANDk, OR k}-AVOID, for a ﬁxed k. We now demonstrate the main application of this framework. Towards describing the setting of our application, we study the complexity of AVOID in the restricted case of when the circuit is monotone."
  },
  {
    "prompt": "Read and understand this section about AVOID, circuit, monotone from a research paper.",
    "response": "A ﬁrst observation is that by applying DeMorgan’s law we can reduce the AVOID (when m > 2n) in polynomial time to C-AVOID where C is restricted to monotone circuits, where reduction preserves the depth of the circuit and at most doubles the size of the circuit. Hence solving AVOID even for monotone circuits. Proposition 1.3. If m > 2n, AVOID reduces to MONOTONE-AVOID in polynomial time. Proof. Let C : {0, 1}n →{0, 1}m a multi-output circuit with m > 2n which is an instance of AVOID."
  },
  {
    "prompt": "Scientific context on structure, ﬁnd, hypergraph and their role in graph theory.",
    "response": "1We remark that while it may not be easy to ﬁnd the hypergraph structure from the given circuit in general, for the important special cases of the problem mentioned in the previous discussion, just the exhaustive search based on the circuit structure will yield efﬁcient algorithms to ﬁnd the hypergraph structure. 4 We describe how to obtain the circuit C′ : {0, 1}2n →{0, 1}m from C."
  },
  {
    "prompt": "This is a technical discussion about Morgan, circuit, Range in theoretical computer science.",
    "response": "By applying De Morgan’s law to push down the negation gates (with appropriate duplication of each gate), we can construct a circuit D equivalent to circuit C, with all the negations at the leaves. The circuit D has input literals {x1, . . . xn, ¬x1, . . . ¬xn}. Now obtain the circuit C′ by replacing each input variable ¬xi by a new variable x′ i. Observe that Range(C) ⊆Range(C′). Indeed, consider an arbitrary y ∈ Range(C) with a = (a1, . . . an) ∈{0, 1}n such that C(a) = y. Let a′ = (a1, . . ."
  },
  {
    "prompt": "Scientific context on AVOID, circuits, MONOTONE and their role in graph theory.",
    "response": "an, a1, . . . an) ∈ {0, 1}2n. Observe that C′(a′) = y. Since m > 2n, C′ is a valid input instance for MONOTONE- AVOID. Hence, it sufﬁces to solve the range avoidance problem for monotone circuits with m > 2n. Therefore, we can restrict our attention to solving the range avoidance problem for the mono- tone circuits. We also observe the following corollary for the case of NC0 k circuits which are the current boundary for polynomial time algorithms solving AVOID."
  },
  {
    "prompt": "Scientific context on polynomial, time, deterministic and their role in graph theory.",
    "response": "For any k > 0, if m > 2n, NC0 k-AVOID reduces to MONOTONE-NC0 2k-AVOID in deterministic polynomial time. In particular, when m > 2n, NC0 3-AVOID reduces to MONOTONE-NC0 6-AVOID in deterministic polynomial time. In the remaining part of the paper, we use the framework in theorem 1.1 to show polynomial time algorithms for MONOTONE-NC0 3-AVOID and related problems."
  },
  {
    "prompt": "Understanding the computational aspects of Polynomial, Time, Algorithm.",
    "response": "Deterministic Polynomial Time Algorithm for MONOTONE-NC0 3-AVOID with Quadratic stretch: We now show the main technical application of the framework in the case when F contains only MAJ function on k inputs, with the additional constraint that two functions should depend on at most one common input variable. As per the above formulation, this makes the hypergraph to be linear and k-uniform."
  },
  {
    "prompt": "Read and understand this section about AVOID, hypergraphs, Turan-type from a research paper.",
    "response": "In the setting of k-uniform linear hypergraphs, using Turan-type results for speciﬁcally designed graphs in F, we can use theorem 1.1 for deriving polynomial time algorithms for solving AVOID for speciﬁc class of circuits. Turan-type extremal problems for hypergraphs were introduced by [BES73] and bounds are known (see [Kee11] for a survey) for exk(n, H′) for a few hypergraphs H′."
  },
  {
    "prompt": "Read and understand this section about wickets, Turan-type, grid from a research paper.",
    "response": "Bounds are known for when H′ is k × k grid [FR13], wickets [Sol24], fano plane [KS04] (see section 2 for a brief overview of Turan-type extremal problems, and exact deﬁnition of these hypergraphs). We exhibit several different ﬁxed hypergraphs H′ and use them to provide different proofs of the following algorithmic upper bound Theorem 1.4. MONOTONE-NC0 3-AVOID when m > cn2 for any constant c can be solved in deterministic polynomial time."
  },
  {
    "prompt": "Understanding the computational aspects of input, bits, proof.",
    "response": "Our proof relies on a reduction of the problem to a more restricted case of MONOTONE-NC0 3-AVOID where each individual output function is the majority of three input bits. We call this version as MAJ3-AVOID. Notice that for any two functions, there can be at most two input bits that both of them can depend on. By using a combinatorial argument on the circuit, we show how to re- duce MAJ3-AVOID to the case where two functions can depend on at most one common input."
  },
  {
    "prompt": "This is a technical discussion about denote, version, rem in theoretical computer science.",
    "response": "We denote this version as ONE-INTERSECT-MAJ3-AVOID. As mentioned above, we use theo- rem 1.1 and H being k-uniform, and F = {MAJ3} use it to design polynomial time algorithms 5 for ONE-INTERSECT-MAJ3-AVOID where each function is majority on three inputs and two dif- ferent functions depend on at most one common input bit."
  },
  {
    "prompt": "Understanding the computational aspects of lemma, Fano, plane.",
    "response": "We exhibit several different ﬁxed hypergraphs H′ - wickets (lemma 3.7), k-cage (lemma A.4), weak Fano plane (lemma A.6), 3 × 3 grid (lemma 3.9), (k, ℓ)-butterﬂy (lemma A.8), (k, ℓ)-odd kite (lemma A.11) which can be also used to derive polynomial time algorithms for ONE-INTERSECT-MAJ3-AVOID. Extremal bounds are known for exk(n, F) only when F is weak Fano plane, 3 × 3 grid and the wicket."
  },
  {
    "prompt": "Scientific context on theorem, monotone, exk and their role in graph theory.",
    "response": "The best known bounds for exk(n, F) among these is when F is the hypergraph called a wicket (see section 2 for a deﬁnition), and hence we use it in the proof of theorem 1.4. Observing that the above reduction to monotone case also doubles the number of input bits on which each function depends, for NC0 k-AVOID with m > 2n, this implies a reduction to MONOTONE- NC0 2k-AVOID. We use theorem 1.1, with H as the k × k grid (see (lemma 3.9)), we show that: Theorem 1.5."
  },
  {
    "prompt": "Scientific context on time, polynomial, deterministic and their role in graph theory.",
    "response": "There is a deterministic polynomial time algorithm for ONE-INTERSECT-MAJk-AVOID when m > n2. However, unlike the case when k = 3, it is unclear how to reduce MONOTONE-NC0 k-AVOID to MAJk-AVOID, and then further to ONE-INTERSECT-MAJk-AVOID. Indeed, designing polynomial time algorithms for MONOTONE-NC0 6-AVOID itself with m = n + O(n2/3) already leads to explicit construction of rigid matrices [GGNS23], which is an important problem in the area."
  },
  {
    "prompt": "Scientific context on Deterministic, Polynomial, Time and their role in graph theory.",
    "response": "Deterministic Polynomial Time Algorithm for MONOTONE-NC0 3-AVOID with Linear Stretch: Deterministic polynomial time algorithms are known [GGNS23] for NC0 3-AVOID when m ≥ n2 log n and as mentioned above, improving the stretch constraint to m = n + O(n2/3) would imply explicit construction of rigid matrices. We next aim to improve the stretch requirement in the above theorem to linear (in fact, to just n), thus improving the known bounds for the case of MONOTONE-NC0 3-AVOID."
  },
  {
    "prompt": "Scientific context on Turan, theorem, notice and their role in graph theory.",
    "response": "Towards this, notice that the above argument for theorem 1.4 uses the bounds for Turan num- ber from the literature in a blackbox manner. In fact, the quadratic constraints on the stretch function m that we have imposed in theorem 1.4 can be relaxed by using stricter variants of the power-bound conjecture in the context of Turan numbers of linear hypergraphs."
  },
  {
    "prompt": "This is a technical discussion about hypergraph, edges, exists in theoretical computer science.",
    "response": "In particular, [GL21] conjectures that there exists an ǫ such that any linear hypergraph on n vertices having more than ω(n2−ǫ) edges must contain a (u, u −4)-hypergraph2 as a subgraph. The speciﬁc hypergraph of wicket is an example of a (9, 5)-hypergraph. However, there are (9, 5)-hypergraphs which are not suitable for our purpose (See appendix A.3). Hence, we need a stronger variant of this conjecture which insists on having wicket as a subgraph instead of just (9, 5)-subgraphs."
  },
  {
    "prompt": "Explain a concept involving theorem, Speciﬁcally, Turan in computational complexity.",
    "response": "Speciﬁcally, in the case of wickets, which we critically use in our argument for theorem 1.4, [FS24, Sol24] conjectured that the Turan number for 3-uniform hypergraphs avoiding wickets is n2−o(1), and this will directly improve the constraint on m as m = n2−o(1) for theorem 1.4 as well."
  },
  {
    "prompt": "Understanding the computational aspects of linear, Turan, studied.",
    "response": "To go beyond the limitations posed by the above structures for which Turan number bounds are classically studied, we deﬁne a new notion of cycles called X 2ℓ-cycles in 3-uniform linear 2A (u, u−4)-hypergraph in this context is a 3-uniform linear hypergraph which has u−4 edges spanning u vertices. 6 hypergraphs. We ﬁrst show a new Turan-type theorem for such cycles for connected hypergraphs, which might be of independent interest. Theorem 1.6."
  },
  {
    "prompt": "Understanding the computational aspects of loose, connected, linear.",
    "response": "Any connected 3-uniform linear hypergraph with m > n edges must contain a loose X 2ℓ cycle. In the context of ONE-INTERSECT-MAJ3-AVOID where m > n, using the above theorem, we show that the corresponding hypergraph HC contains a loose X 2ℓcycle. Using the framework of theorem 1.1, where we show (lemma 4.5) there exists an edge-coloring of X 2ℓwhich is not MAJ- coloring. Inaddition, we note that although the cycle is not of ﬁxed size, we can ﬁnd it in HC in polynomial time."
  },
  {
    "prompt": "Explain a concept involving theorem, Main, cases in computational complexity.",
    "response": "This gives us the following theorem. Theorem 1.7. (Main Theorem) For m > n, MONOTONE-NC0 3-AVOID can be solved in deterministic polynomial time. Thus, while the applicability of theorem 1.1 seems to impose constraints such as k-uniformity and one-intersection to the cases of AVOID that they can be used to solve, the above theorem indicates that along with other combinatorial reductions to such special cases, it can be still lead to useful bounds for the more general problem."
  },
  {
    "prompt": "This is a technical discussion about Building, symmetric, theorem in theoretical computer science.",
    "response": "Building on the above, we can also prove the following theorem for the case of circuits where each output function is restricted to be symmetric. Theorem 1.8. For m > 8n, SYMMETRIC-NC0 3-AVOID can be solved in deterministic polynomial time. We prove this by showing a polynomial time reduction from SYMMETRIC-NC0 3-AVOID to the MONOTONE-NC0 3-AVOID when m > 8n, which is based on a careful combinatorial analysis of the functions involved."
  },
  {
    "prompt": "Read and understand this section about constant, AVOID, Related from a research paper.",
    "response": "Related Work: More recently, [KPI25] described polynomial time algorithms for solving NC0 k- AVOID when the stretch function is m > ˜Ω(n k 2 − k−2 2(k+2) ) when k is an odd constant, and m > Ω(n k 2 log n) when k is an even constant. This improves the algorithms in [GGNS23] for NC0 k. We note that our algorithm (theorem 1.7) is for k = 3 and works for linear stretch, but in the case of monotone NC0 3-AVOID. Thus, our results are incomparable with [KPI25]."
  },
  {
    "prompt": "Scientific context on Preliminaries, circuit, range and their role in graph theory.",
    "response": "2 Preliminaries We study the range avoidance problem for restricted circuit classes. C-AVOID is the following problem: Given a multi-output circuit C : {0, 1}n →{0, 1}m such that m > n where each output function can be computed by a circuit in class C, ﬁnd a y ∈{0, 1}m which is outside the range of C. In particular, we study the following two problems: MONOTONE-AVOID and SYMMETRIC-AVOID where the circuit class C is restricted to monotone and symmetric functions respectively."
  },
  {
    "prompt": "This is a technical discussion about Hypergraphs, paper, collect in theoretical computer science.",
    "response": "7 Hypergraphs: We collect the preliminaries from theory of hypergraphs that we use in the paper. We will work with hypergraphs G(V, E) where E ⊆2V . A hypergraph is linear if two edges intersect at most one vertex - ∀e1, e2 ∈E, |e1 ∩e2| ≤1. A hypergraph is said to be k-uniform if every edge has exactly k elements from V - ∀e ∈E, |e| = k. We will be working with k-uniform linear hypergraphs, and in particular with k = 3. We will deﬁne some of the hypergraphs that are used in the paper."
  },
  {
    "prompt": "Understanding the computational aspects of vertices, hypergraph, set.",
    "response": "A k × k grid in a hypergraph is a set of k2 vertices {v11, v12, . . . vkk} such that there are edges r1, r2, . . . rk, c1, c2, . . . ck ∈E, corresponding to the vertices in the rows and columns respectively, when the vertices are arranged in the row major order (See ﬁg. 1(a)). A particular special hyper- graph (for k = 3) is called a wicket is the 3 × 3 grid with one edge removed (See ﬁg. 1(b)). A Berge path is deﬁned as v1e1v2 . . . vkekvk+1 where each edge ei contains vertices vi, vi+1."
  },
  {
    "prompt": "Read and understand this section about Berge, path, odd from a research paper.",
    "response": "A Berge path is said to be even if k is even and odd otherwise. We say a hypergraph is connected if there exists a Berge path between any two pair of vertices."
  },
  {
    "prompt": "Explain a concept involving Hypergraphs, Turan-type, Problems in computational complexity.",
    "response": "Turan-type Problems in Hypergraphs: One of the classical extremal Turan-type problems intro- duced in the context of hypergraphs by [BES73] is the following : for a ﬁxed set of k-uniform hyper- graphs F, what is the maximum number of edges that a k-uniform linear hypergraph can have, if it does not contain a subgraph isomorphic to any of the hypergraphs in the collection F of hypergraphs. This number is denoted by exk(n, F)."
  },
  {
    "prompt": "Scientific context on Turan, limn, denoted and their role in graph theory.",
    "response": "The Turan density of F is denoted by π(F) = limn→∞ \u0010\u0000n k \u0001−1exk(n, F) \u0011 . It is known that a k-uniform hypergraph F has π(F) = 0 (also called degenerate) if and only if it is k-partite. The special 3-uniform hypergraph called wicket, denoted by W (see ﬁgure 1(b)) forms an im- portant step in our argument. We will need the following result about 3-uniform linear hyper- graphs due to Solymosi [Sol24]. Proposition 2.1 ( [Sol24])."
  },
  {
    "prompt": "Read and understand this section about Fano, hypergraph, wicket from a research paper.",
    "response": "If a 3-uniform linear hypergraph does not contain a wicket then the number of hyperedges is bounded by o(n2). A slightly weaker result was proven by [GS22] where they showed that ex3(n, W) ≤(1−c)n2 6 . Another standard hypergraph that we will be using is the Fano plane. A Fano plane F is a 3- uniform linear hypergarph which is isomorphic to the hypergraph H(V, E) with vertex set V = [7] and edge set E = {{1, 2, 3}, {3, 4, 5}, {1, 5, 6}, {3, 6, 7}, {2, 5, 7}, {1, 4, 7}, {2, 4, 6}}."
  },
  {
    "prompt": "Understanding the computational aspects of Proposition, shows, result.",
    "response": "The following result shows a bound on ex3(n, F). Proposition 2.2 ( [KS04]). If a 3-uniform linear hypergraph contains more than \u0000n 3 \u0001 − \u0000⌊n/2⌋ 3 \u0001 − \u0000⌈n/2⌉ 3 \u0001 then it must contain a Fano plane F as a sub-hypergraph. Coming to k-uniform hypergraphs, we use the following result about k × k grid Gk, which exhibits a bound for exk(n, {Gk}). Proposition 2.3 ( [FR13]). Let H be a k-uniform linear hypergraph with m > n(n−1) k(k−1) edges. Then H contains a k × k grid Gk as a sub-hypergraph."
  },
  {
    "prompt": "This is a technical discussion about Figure, Fano, Cycles in theoretical computer science.",
    "response": "8 (a) (b) (c) Figure 1: (a) 4 × 4 grid (b) A wicket (c) Fano plane Cycles in Hypergraphs: Various notions of cycles have been explored in the hypergraph setting. [CCGJ18] consider the notion of linear cycles of length ℓ(denoted by Cℓ) which is deﬁned as set of ℓedges such that each pair of adjacent edges ei, ei+1 (modulo ℓ) intersect in exactly one vertex and each pair of non-adjacent edges are disjoint. [CCGJ18] show that for exk(n, C2ℓ) ≤ck,ℓn1+ 1 ℓ and exk(n, C2ℓ+1) ≤c′ k,ℓn1+ 1 ℓ."
  },
  {
    "prompt": "Scientific context on Berge, Bℓwhich, vℓwhere and their role in graph theory.",
    "response": "Another notion of cycles which is well-studied is that of Berge cycle of length ℓ, denoted by Bℓwhich consists of ℓdistinct edges such that each hyperedge ei contains vertices vi, vi+1 where 1 ≤i < ℓand the edge eℓcontains vertices v1, vℓwhere v1, . . . vℓare distinct vertices. [GL12] show that exk(n, B2ℓ) = dk,ℓn1+ 1 ℓand exk(n, B2ℓ+1) = d′ k,ℓn1+ 1 ℓ. X 2ℓ-cycles and Loose X 2ℓ-cycles: We deﬁne a new notion of cycles called X 2ℓ-cycles in 3-uniform linear hypergraphs."
  },
  {
    "prompt": "Read and understand this section about mod, deﬁne, relaxation from a research paper.",
    "response": "We deﬁne X 2ℓto be a relaxation of B2ℓwhere ∃ei, ej such that |ei ∩ej| = 1, i + 1 ≡j mod 2 and |i −j| > 2. We call the (ei, ej)-pair a χ-structure in the cycle. Alterna- tively, we can view X 2ℓusing the lens of Berge path. X 2ℓcan be thought as B2ℓ1 ∪B2ℓ2 ∪χ where B2ℓ1 = u1e1u2 . . . u2ℓ1e2ℓ1u2ℓ1+1, B2ℓ2 = v1e′ 1v2 . . . v2ℓ2e′ 2ℓ2v2ℓ2+1, χ = (e, e′) where e, e′ ̸∈ {e1, . . . e2ℓ1, e′ 1, . . . e′ 2ℓ2} and e = {u1, uℓ1, w}, e′ = {v1, vℓ2, w}."
  },
  {
    "prompt": "Explain a concept involving Berge, walks, denoted in computational complexity.",
    "response": "If we relax the Berge paths to Berge walks (denoted by Pℓ) by allowing repetition of edges and vertices, we get loose X 2ℓcycle. In section 4, we shall prove extremal bounds on this structure and use it to solve range avoidance for restricted circuit classes."
  },
  {
    "prompt": "This is a technical discussion about Range, Avoidance, problem in theoretical computer science.",
    "response": "3 Range Avoidance for NC0 k via Fixed Hypergraphs In this section, we describe the main technical tool of the paper by formulating the range avoidance problem as a way of avoiding certain hypergraph - leading to a Turan-type formulation of the problem. We recall the notation from the introduction: for an NC0 k circuit C = (fi)i∈[m] : {0, 1}n → {0, 1}m. Let HC denote the hypergraph deﬁned as follows: The set of vertices are [n]. For 1 ≤i ≤ m, deﬁne a hyperedge ei as {xj | j ∈[n], xj ∈I(fi)}."
  },
  {
    "prompt": "Explain a concept involving edges, size, coloring in computational complexity.",
    "response": "Thus E = {{ei | i ∈[m]}} has exactly m edges, each of size at most k. Let F be the family of functions that appear in the circuit C, and let φ : E →F be a labelling of the edges of the hypergraph H with the corresponding function. A 2-coloring of the hyperedges of H, is said to be a φ-coloring if there is a vertex coloring which induces this edge coloring via φ. We argue the following: 9 Theorem 1.1."
  },
  {
    "prompt": "Read and understand this section about function, labelling, ﬁxed from a research paper.",
    "response": "Let C : {0, 1}n →{0, 1}m be a circuit and HC be the corresponding hypergraph and let φ be the labelling function. Suppose HC contains a sub-hypergraph H′ (of ﬁxed size) such that there is an edge-coloring of H′ which is not φ-coloring. Then there is a polynomial time algorithm to ﬁnd a string outside the range of C. Proof. Let HC be the hypergraph corresponding to the circuit C. Let H′ be a sub-hypergraph of ﬁxed size ℓin HC."
  },
  {
    "prompt": "Explain a concept involving size, time, note in computational complexity.",
    "response": "We note that since H′ is of ﬁxed size, we can exhaustively search for a copy of H′ in HC in polynomial time. Let Γ : E(H′) →{R, B} be an edge-coloring of H′ which in not a φ-coloring. Let C′ be the circuit corresponding to the hypergraph H′. Let y = y1y2 . . . ym be deﬁned as follows: yi =        ∗, if ei ̸∈E(H′) 0, if ei ∈E(H′) and Γ(ei) = R 1, otherwise where ∗denotes that yi can take any value in {0, 1}. We will show that y ̸∈Range(C)."
  },
  {
    "prompt": "Explain a concept involving Range, function, claim in computational complexity.",
    "response": "Let ∆: {R, B} →{0, 1} be a function such that ∆(R) = 0, ∆(B) = 1. For the sake of brevity, let ∆(x) = ∆(x1, . . . xn) = ∆(x1)∆(x2) . . . ∆(xn). It sufﬁces to show the following claim. Claim 3.1. y ∈Range(C) if and only if ∆(y) is a valid φ-coloring. Proof. Notice that y ∈Range(C) if and only if ∃x ∈{0, 1}n such that C(x) = y. This is equivalent to ∃Π : V →{R, B} such that the corresponding Γ : E →{R, B} is a φ −coloring."
  },
  {
    "prompt": "Scientific context on Range, equivalence, obtained and their role in graph theory.",
    "response": "Indeed, the latter equivalence can be obtained by setting Π, Γ such that Π(vi) = ∆(xi) and Γ(ej) = ∆(yj) for all i ∈[n], j ∈[m]. Since, HC has an edge coloring Γ which is not a φ-coloring, by claim 3.1 we obtain a y ∈{0, 1}m which is outside Range(C). Warm-up 1: Algorithm for NC0 2-AVOID: As mentioned in the introduction, [GLW22] described a polynomial time algorithm for solving the range avoidance problem for NC0 2 circuit."
  },
  {
    "prompt": "Read and understand this section about show, alternate, application from a research paper.",
    "response": "In the following, we show an alternate proof of the same as an application of the framework described above. Proposition 3.2. There is a polynomial time algorithm for NC0 2-AVOID when m > n. Proof. Let C : {0, 1}n →{0, 1}m be the given NC0 2 circuit. [GLW22] observed that there are only four types of NC0 2 functions possible: AND, OR, PARITY and constant functions. We can check the type of function in polynomial time. Suppose there is a constant function f in the circuit."
  },
  {
    "prompt": "Understanding the computational aspects of circuit, function, Wlog.",
    "response": "Wlog let this function be a constant zero function. Then by setting the output of f to 1 and the other output bits arbitrarily we get a string which is outside the range of the circuit. Now we consider the other case when the circuit contains only {∧, ∨, ⊕} gates where inputs may be negated. We consider the graph HC corresponding to the circuit C. Since m > n, the graph HC corresponding to the circuit C indeed contains a cycle Q. Furthermore, we can ﬁnd this cycle in polynomial time."
  },
  {
    "prompt": "Scientific context on φ-coloring, sufﬁces, obtain and their role in graph theory.",
    "response": "Thus, it sufﬁces to obtain an edge-coloring of Q which is not a φ-coloring. 10 We consider the following cases based on the function types of the edges participating in the cycle Q: Case 1: Suppose each edge in Q computes a PARITY of two inputs which may be negated. Con- sider an edge f with endpoints xi, xj. Consider the path P starting at xi and ending at xj obtained by deleting f from Q. Let Γ color all the edges in Q −f to R. Let xi = b ∈{R, B}."
  },
  {
    "prompt": "Explain a concept involving color, function, ﬁxes in computational complexity.",
    "response": "Notice that setting the function to R ﬁxes the color of every other vertex to either b or b depending on the function. In particular, it ﬁxes xj = b′ ∈{b, b}. This ﬁxes the value of the function computed at f, and hence the color of f. Thus, ﬂipping the color of f would produce an edge-coloring which is not achievable. Therefore, the following coloring Γ is a not a φ-coloring."
  },
  {
    "prompt": "Understanding the computational aspects of Case, Suppose, negated.",
    "response": "Γ(e) =        R if e ∈Q −f B if e = f and b′ = b R if e = f and b′ = b Case 2: Suppose there exists an edge e1 in Q which computes AND or OR of two inputs which may be negated. Let the cycle Q be e1e2 . . . eℓe1. Consider the path e1e2 . . . eℓ−1 obtained by deleting the edge eℓfrom Q. Consider the following edge-coloring Γ : Q −eℓ→{R, B} Γ(e) = ( B if e ∈Q −eℓ, φ(e) = ∧ R if e ∈Q −eℓand φ(e) ∈{∨, ⊕} Let e1 ∩eℓ= {xi} and eℓ∩eℓ−1 = {xj}."
  },
  {
    "prompt": "This is a technical discussion about color, ﬁxing, obtain in theoretical computer science.",
    "response": "We would like to show that by coloring the edges of Q −eℓaccording to Γ, we end up ﬁxing the colors of xi, xj thus ﬁxing the color of eℓor we obtain an inconsistency. In the former case, ﬂipping the color of eℓwe obtain an edge- coloring which is not a φ-coloring. It sufﬁces to show that Γ ﬁxes the color of eℓor we obtain an inconsistent coloring for Q −eℓ. We prove this by induction on length of the path. For the base case, wlog let φ(e1) =AND."
  },
  {
    "prompt": "Explain a concept involving color, vertices, incident in computational complexity.",
    "response": "According to our Γ, we color e1 to B, which ﬁxes the color of both vertices incident on e1. By induction hypothesis, let Γ ﬁx the colors of all vertices participating in the path e1e2 . . . ek−1 for k ∈[ℓ]. In the other case when the coloring is inconsistent we are already done. Thus, we would like to argue that Γ ﬁxes the other endpoint y of ek where y ̸∈ek−1."
  },
  {
    "prompt": "Explain a concept involving color, input, Notice in computational complexity.",
    "response": "Notice that, since one input feeding into ek is already ﬁxed Γ gives a way to ﬁx the other input y to b ∈{R, B} by setting the color of ek appropriately or we get an inconsistency at this stage. Thus, inductively either we get an inconsistent coloring (in this case color eℓarbitrarily) or we end up ﬁxing the colors of xi, xj which in turn ﬁxes the color of the edge eℓ. Hence, ﬂipping the color of ℓﬁxed by the above assignment produces an edge-coloring which is not a φ-coloring."
  },
  {
    "prompt": "This is a technical discussion about φ-coloring, case, analysis in theoretical computer science.",
    "response": "By the above case analysis we have an edge-coloring of H′ which is not φ-coloring. By theorem 1.1 we obtain a string outside the range of the circuit in polynomial time. 11 To demonstrate the method further, as another simple application, we show a polynomial time algorithm for solving the range avoidance problem when each output function computes an AND or OR function of k input bits for a ﬁxed k > 0 and satisﬁes the property that any two output functions share at most one input."
  },
  {
    "prompt": "Scientific context on AVOID, Polynomial, time and their role in graph theory.",
    "response": "Warm-up 2: Polynomial time algorithm for {AND, OR}-AVOID: As our next application, we show a polynomial time algorithm for solving the range avoidance problem when each output function computes an AND or OR function of k input bits for a ﬁxed k > 0. Additionally, it satisﬁes the constraint that any two output functions share at most one input. Thus, the hypergraph HC corresponding to C is a k-uniform linear hypergraph. The sub-hypergraphs we will use in this case are k-crown and C∗."
  },
  {
    "prompt": "Understanding the computational aspects of function, application, k-crown.",
    "response": "As our next application, we show a polynomial time algorithm for solving the range avoidance problem when each output function computes an AND or OR function of k input bits for a ﬁxed k. The sub-hypergraphs we will use in this case are k-crown and C∗. A k-crown is a k-uniform linear hypergraph consisting of k disjoint hyperedges {e1, . . . ek} and an additional hyperedge e0 intersecting each hyperedge e1, . . . ek exactly once. C∗is a variant of the k-crown where the edges e1, . . ."
  },
  {
    "prompt": "Read and understand this section about intersect, Proposition, common from a research paper.",
    "response": "ek−2 intersect in a common vertex v ̸∈e0. We will use the following extremal bound on these subhypergraphs Proposition 3.3 ( [ZBW24]). Let H be a k-uniform linear hypergraph with m > k(k−2)(n−s) k−1 where s is the number of vertices with degree at least (k −1)2 + 2. Then H contains a k-crown or C∗. Proposition 3.4. Let C : {0, 1}n →{0, 1}m be a multi-output circuit where m > k(k−2)(n−s) k−1 and each output function computes ANDk or ORk for ﬁxed k."
  },
  {
    "prompt": "Read and understand this section about Additionally, input, output from a research paper.",
    "response": "Additionally, any two output functions share at most one input. Then, there is a polynomial time algorithm for solving AVOID on C. Proof. We will show an edge coloring of the hypergraphs k-crown or C∗which is not a φ-coloring. Γ(e) =              B if e ∈{e1, . . . ek}, φ(e) =AND R if e ∈{e1, . . . ek}, φ(e) =OR B if e = e0, ∃i ∈[k] such that φ(ei) =OR R otherwise By Proposition 3.3 we have that if m > k(k−1)(n−s) k−1 then H contains a C∗or k-crown."
  },
  {
    "prompt": "Scientific context on ﬁnd, time, AVOID and their role in graph theory.",
    "response": "Furthermore, we can ﬁnd these subhypergraphs in polynomial time. By the above argument, we can ﬁnd an edge coloring of C∗and k-crown which is not a φ-coloring. By theorem 1.1 we can ﬁnd a string outside the range of C in polynomial time. We remark that the above special case of AVOID ({AND, OR}-AVOID) can be solved by a di- rect algorithm similar to the algorithm for NC2 0-AVOID due to [GLW22]. Nevertheless the above method is yet another demonstration of our framework for solving AVOID problem."
  },
  {
    "prompt": "Read and understand this section about pergraphs, Polynomial, time from a research paper.",
    "response": "12 3.1 Polynomial time algorithm for ONE-INTERSECT-MAJ3-AVOID In this subsection, we show the ﬁrst application of our formulation of the problem in terms of hy- pergraphs. We apply it to describe a deterministic polynomial time algorithm for ONE-INTERSECT-MAJ3- AVOID. For our purpose, we are interested in a special case of φ-coloring when F = {MAJ}. We deﬁne this formally below: Deﬁnition 3.5 (MAJ-coloring). Let H(V, E) be a k-uniform hypergraph."
  },
  {
    "prompt": "Scientific context on exists, vertex, coloring and their role in graph theory.",
    "response": "We say Γ : E →{R, B} is a MAJ-coloring if there exists a vertex coloring Π : V →{R, B} such that ∀e ∈E(H) we have Γ(e) = B ⇐⇒∃S ⊆e, |S| ≥ j |V | 2 k + 1 such that ∀v ∈S, Π(v) = B. We have the following corollary from theorem 1.1. Corollary 3.6. Let C : {0, 1}n →{0, 1}m be a circuit where each output function computes a MAJ function. Let HC be the corresponding hypergraph. Suppose HC contains a subhypergraph H′ such that there is an edge-coloring of H′ which is not MAJ-coloring."
  },
  {
    "prompt": "Explain a concept involving wicket, hypergraph, polynomial in computational complexity.",
    "response": "Then there is a polynomial time algorithm to ﬁnd a string outside the range of C. Thus it is sufﬁcient to exhibit an explicit ﬁxed size linear 3-uniform hypergraph H′ which satisﬁes the conditions of Corollary 3.6. Hypergraph Structure : Wickets - A wicket is deﬁned as a 3 × 3 grid with one edge removed. By proposition 2.1, if H contains more that o(n2) edges, then it contains a wicket. We will show that there is an edge coloring of a wicket which is not a MAJ-coloring. Lemma 3.7."
  },
  {
    "prompt": "Scientific context on set, edges, MAJ and their role in graph theory.",
    "response": "Let H be a wicket. There exists an edge-coloring Γ : E →{R, B} such that it not a MAJ- coloring. Furthermore, we can ﬁnd this in polynomial time. Proof. Wlog let the edge e removed from the wicket be a column edge. Let E = E1 ∪E2 where E1 be the set of three row edges and E2 denote the set of column edges. We will show that the following Γ : E →{R, B} is not a MAJ-coloring: Γ(e) = ( R if e ∈E1 B if e ∈E2 . Let U be the set of vertices upon which the edges of E2 are incident."
  },
  {
    "prompt": "Understanding the computational aspects of colored, vertices, edges.",
    "response": "For the edges in E1 to be colored R, there must be at least three vertices in U that should be colored R. Similarly, for the edges in E2 to be B, there must be at least 4 vertices in U that should be colored B. Thus, totally there should be at least 7 distinct vertices in U. But |U| = 6, which is a contradiction. Hence, Γ is not a MAJ-coloring. The following is an easy corollary of corollary 3.6 and lemma 3.7. Corollary 3.8."
  },
  {
    "prompt": "Understanding the computational aspects of instance, polynomial, Proof.",
    "response": "Let C : {0, 1}n →{0, 1}m be an instance of ONE-INTERSECT-MAJ3-AVOID with m = Ω(n2). Then we can ﬁnd a y ∈{0, 1}m outside the range of C in polynomial time. 13 Proof. Let HC be the 3-uniform linear hypergraph corresponding to the the circuit C. Since m > o(n2), proposition 2.1 guarantees existence of a wicket in H. Furthermore, we can ﬁnd such wicket in polynomial time by simply checking across all subsets of m which are of size 5. The number of subsets is bounded by \u0000m 5 \u0001 ."
  },
  {
    "prompt": "This is a technical discussion about polynomial, time, corollary in theoretical computer science.",
    "response": "By corollary 3.6 and lemma 3.7 it follows that we can ﬁnd a y outside the range of C in polynomial time. 3.2 Polynomial time Algorithm for ONE-INTERSECT-MAJk-AVOID In this subsection, we show a second application of the framework proposed in theorem 1.1. Let C : {0, 1}n →{0, 1}m be a multi-output circuit such that each output function computes MAJ of k input bits. Let H be a k-uniform hypergraph obtained from C as in the case of theorem 1.1."
  },
  {
    "prompt": "Understanding the computational aspects of variable, k-uniform, hypergraph.",
    "response": "Since every pair of output functions intersect in at most one input variable, we have that H is a linear k-uniform hypergraph. Recall proposition 2.3 which argued that a k-uniform hypergraph that has more than n(n−1) k(k−1) must have a k × k grid contained in it."
  },
  {
    "prompt": "Understanding the computational aspects of theorem, polynomial, show.",
    "response": "We will show that if H contains a k × k- grid then we can solve range avoidance for the corresponding circuit in polynomial time and hence this will imply an algorithm for ONE-INTERSECT-MAJk-AVOID by using the same framework as theorem 1.1. Lemma 3.9. There exists an edge-coloring Γ : E →{R, B} of k × k-grid which is not a MAJ-coloring. Furthermore, we can ﬁnd Γ in polynomial time. Proof. Let H(V, E) be a k×k grid."
  },
  {
    "prompt": "This is a technical discussion about set, edges, rows in theoretical computer science.",
    "response": "Let E = E1∪E2 where E1, E2 are the set of edges corresponding to the rows and columns of the grid respectively. We deﬁne an edge-coloring Γ(e) = ( R if e ∈E1 B otherwise We will show that Γ is not a valid MAJ-coloring. We consider the following cases based on the parity of k: Case 1 Suppose k is odd. For each e ∈E1 to be colored R, at least \u0004 k 2 \u0005 + 1 of its vertices should be colored R."
  },
  {
    "prompt": "Read and understand this section about edges, colored, vertices from a research paper.",
    "response": "Since the edges in E1n are pairwise disjoint, at least k \u0000\u0004 k 2 \u0005 + 1 \u0001 distinct vertices should be colored R. Similarly for edges in E2, we have that at least k \u0000\u0004 k 2 \u0005 + 1 \u0001 vertices in V should be colored B. Hence, there should be at least 2k \u0000\u0004 k 2 \u0005 + 1 \u0001 > k2 vertices in V , which is a contradiction. Case 2 Suppose k is even. By previous argument, for edges in E2 to get color B at least k \u0000k 2 + 1 \u0001 vertices should be colored B."
  },
  {
    "prompt": "Read and understand this section about vertices, contradiction, edges from a research paper.",
    "response": "For edges in E1 at least k \u0000k 2 \u0001 vertices should be colored R. So totally, there are at least k (k + 1) > k2 vertices in V , which is again a contradiction. In either case, we obtain a contradiction. Hence, Γ is not a MAJ-coloring. By proposition 2.3 and lemma 3.2, we have the following result: Theorem 3.10. Let C : {0, 1}n →{0, 1}m be an instance of ONE-INTERSECT-MAJk-AVOID and m > n(n−1) k(k−1) . There is a polynomial time algorithm to ﬁnd a string outside the range of C."
  },
  {
    "prompt": "Explain a concept involving Algorithm, Polynomial, time in computational complexity.",
    "response": "14 4 Polynomial time Algorithm for MONOTONE-NC0 3-AVOID In this section, we describe a deterministic polynomial time algorithm for MONOTONE-NC0 3-AVOID for m = Ω(n2). The algorithm proceeds in three steps. In the ﬁrst step, we give a polynomial time reduction from MONOTONE-NC0 3-AVOID to MAJ3-AVOID. Next, we show a reduction from MAJ3-AVOID to ONE-INTERSECT-MAJ3-AVOID. Finally, we describe a polynomial time algorithm for solving MAJ3-AVOID when m = Ω(n2)."
  },
  {
    "prompt": "Explain a concept involving Theorem, Reduction, show in computational complexity.",
    "response": "4.1 Reduction from MONOTONE-NC0 3-AVOID to MAJ3-AVOID: We show the following reduction: Theorem 4.1. There is a polynomial time reduction from MONOTONE-NC0 3-AVOID to MAJ3-AVOID. Proof. Let C : {0, 1}n →{0, 1}m be a monotone circuit with m > n. We will obtain a circuit C′ : {0, 1}n′ →{0, 1}m′ such that n′ ≤n < m′ ≤m and each output function computes majority of three input bits."
  },
  {
    "prompt": "Read and understand this section about Range, ﬁnd, polynomial from a research paper.",
    "response": "Furthermore, we can ﬁnd a y ̸∈Range(C) from y′ ̸∈Range(C′) in polynomial time by setting the remaining m −m′ many bits of y to arbitrary values. We remark that this proof technique is inspired by [GLW22]. To begin with, note that there are only a few types of monotone functions that depend on 3 bits."
  },
  {
    "prompt": "Explain a concept involving range, string, cases in computational complexity.",
    "response": "For each of these cases we will show a reduction from C : {0, 1}n →{0, 1}m to a smaller circuit C′ : {0, 1}n−1 →{0, 1}m−1 such that a string outside the range of C′ gives a string which is outside the range of C. Let Cj denote the sub-circuit of C that computes the j-th bit of the output. Let Wi := {x ∈{0, 1}3 | |x|1 = i} be the set of inputs with weight i, for i ∈{0, 1, 2, 3}. We describe the reduction for each type of function except when the function is the MAJ3 function."
  },
  {
    "prompt": "Understanding the computational aspects of Case, variables, iteratively.",
    "response": "We iteratively apply this sequence of reduction rules for each j ∈{1, 2 . . . m}, we will end up a circuit where each output bit is the MAJ3 function in terms of input variables. Case 1: C−1 j (0) = ∅: That is Cj computes a constant 1 function. Then setting y1 = 0 and the remaining output bits arbitrarily gives a string outside the range of C. Case 2: C−1 j (0) = W0: Suppose Cj computes OR of three input variables x1, x2, x3."
  },
  {
    "prompt": "Explain a concept involving Range, variables, setting in computational complexity.",
    "response": "Then setting y1 = 0 and all the input variables x1, x2, x3 to 0 we obtain a smaller circuit C′ : {0, 1}n−3 → {0, 1}m−1 such that y′ ̸∈Range(C′) then y = 0y′ is outside the range of C. Case 3: C−1 j (0) = W0 ∪{α} where α ∈W1: Without loss of generality, we assume α = 001. In this case, setting y1 = 0 and the input variables x1, x2 to 0 we obtain a smaller circuit C′ : {0, 1}n−2 →{0, 1}m−1. Same argument applies when α = 100, and α = 010."
  },
  {
    "prompt": "Read and understand this section about Case, generality, loss from a research paper.",
    "response": "Case 4: C−1 j (0) = W0 ∪{α1, α2} where α1, α2 ∈W1: Without loss of generality, let α1 = 001, and α2 = 010. By property of monotone functions, we have that Cj evaluates to 1 on inputs {101, 110, 111}. Then setting y1 = 0 and x1 = 0, yields a circuit C′ : {0, 1}n−1 →{0, 1}m−1. Same argument applies in the case when α1 = 010, α2 = 100 and also in the case when α1 = 001, α2 = 100. 15 Case 5: C−1 j (0) = W0 ∪W1 ∪{α} where α ∈W2: Without loss of generality, assume α = 011."
  },
  {
    "prompt": "Explain a concept involving Case, Setting, smaller in computational complexity.",
    "response": "Setting y1 = 1 and x1 = 1 we get a smaller circuit C′ : {0, 1}n−1 →{0, 1}m−1. The other cases when α = 101 and α = 110 can be handled similarly. Case 6: C−1 j (0) = W0 ∪W1 ∪{α1, α2} where α1, α2 ∈W2: Without loss of generality, assume α1 = 011, α2 = 110. Setting y1 = 1 and x1 = 1 we get a smaller circuit C′ : {0, 1}n−1 →{0, 1}m−1. The same argument applies when W2 \\ {α1, α2} = 110 or 011. Case 7: C−1 j (0) = W0 ∪W1 ∪W2: In this case, Cj computes AND of three input variables x1, x2, x3."
  },
  {
    "prompt": "This is a technical discussion about Range, setting, variables in theoretical computer science.",
    "response": "Then setting y1 = 1 and all the input variables x1, x2, x3 to 1 we obtain a smaller circuit C′ : {0, 1}n−3 →{0, 1}m−1 such that y′ ̸∈Range(C′) then y = 1y′ is outside the range of C. Case 8: C−1 j (0) = W0 ∪W1 ∪W2 ∪W3: That is, Cj computes a constant zero function. Then setting y1 = 1 and the remaining output bits arbitrarily gives a string outside the range of C. It can be veriﬁed that the only function that is not covered in the above cases is when Cj computes MAJ on 3 bits."
  },
  {
    "prompt": "Understanding the computational aspects of bit, function, output.",
    "response": "Note that in each case we eliminate one output bit and at least one input bit. Thus, ﬁnally we are left with a circuit C′ : {0, 1}n′ →{0, 1}m′ where m′ > n′ and each output bit is computed by MAJ3 function. We note that we can check the type of function computed by a circuit by simply evaluating the function on all possible input values. Since the function depends on only three variables there are only 8 input possibilities to check. Hence, overall the reduction can be done in polynomial time."
  },
  {
    "prompt": "Read and understand this section about Algorithm, time, Polynomial from a research paper.",
    "response": "4.2 Polynomial time Algorithm for MAJ3-AVOID We show that there is a deterministic polynomial time algorithm for MAJ3-AVOID. Recall that MAJ3 avoid instance is a circuit C : {0, 1}n →{0, 1}m with constant depth, bounded fan-in and polynomial size, where each output function of the circuit is a majority of exactly three input variables. For each i ∈[m], let fi denote the function corresponding to the i-th output bit, and let I(fi) denote the set of three variables that fi depends on."
  },
  {
    "prompt": "Explain a concept involving steps, deﬁne, demonstrate in computational complexity.",
    "response": "We shall demonstrate working of the algorithm in three steps. For our purpose, we shall deﬁne sub-circuits of C called clusters (denoted by K) as follows: Let O = {f1, . . . fm} be the set of output functions of C. We deﬁne the relation R : O × O as follows: We say (fi, fj) ∈R if ∃some input x that feeds into both fi, fj for i, j ∈[m]. Let R′ be transitive closure of R. Observe that R′ is an equivalence relation. We deﬁne a cluster to be an equivalence class of R′."
  },
  {
    "prompt": "Understanding the computational aspects of Reduction, cluster, Step.",
    "response": "Step 1 : Reduction to a single cluster: The following lemma shows that C must contain a cluster with more outputs than inputs and hence we can concentrate on such a cluster. Lemma 4.2. Let C : {0, 1}n →{0, 1}m be a multi-output circuit with m > n, then there exists a cluster K such that |K| > |I(K)| where I(K) = ∪f∈KI(f). 16 Proof. Let K1, . . . Kt be the clusters such that ∪i∈[t]Ki covers the set of output functions of C. Observe that ∪i∈[t]I(Ki) covers the input set of C."
  },
  {
    "prompt": "Explain a concept involving deﬁnition, cluster, contradiction in computational complexity.",
    "response": "By deﬁnition, ∀i, j ∈[t] such that i ̸= j we have that Ki ∩Kj = ∅and I(Ki) ∩I(Kj) = ∅. Assume for the sake of contradiction that for each cluster Ki, we have |Ki| ≤|I(Ki)|. Then by the above observation, we have that ∪i∈[t]Ki ≤ ∪i∈[t]I(Ki) implying that m < n, which is a contradiction. Let K be a cluster guaranteed by proposition 4.2. Let CK be the sub-circuit of C corresponding to the cluster K. Since, |K| > |I(K)|, there must be a string outside the range of CK."
  },
  {
    "prompt": "This is a technical discussion about Range, Observe, agrees in theoretical computer science.",
    "response": "Observe that if y′ ∈{0, 1}|K| ̸∈Range(CK) then any y ∈{0, 1}m which agrees with y′ is outside Range(C). Hence, it sufﬁces to solve the problem on CK. It is easy to see that if there exist two output functions that intersect in three inputs then we already have a string outside the range. Hence, we consider the cases when they intersect in fewer than three inputs."
  },
  {
    "prompt": "This is a technical discussion about AVOID, Range, problems in theoretical computer science.",
    "response": "Motivated by this, we deﬁne the following problems: TWO-INTERSECT-MAJ3- AVOID (ONE-INTERSECT-MAJ3-AVOID) is the following problem: Given C : {0, 1}n →{0, 1}m such that output function is MAJ3 function and any two functions share at most two (resp. one) inputs, the goal is to ﬁnd y ̸∈Range(C). Step 2 : A reduction to ONE-INTERSECT-MAJ3-AVOID For the rest of the proof CK = C, m′ = m and n′ = n."
  },
  {
    "prompt": "This is a technical discussion about AVOID, time, problem in theoretical computer science.",
    "response": "We show that the there is polynomial time reduction from TWO-INTERSECT-MAJ3- AVOID problem to ONE-INTERSECT-MAJ3-AVOID problem. Finally, in step 3 we will show that that there is a deterministic polynomial time algorithm for ONE-INTERSECT-MAJ3-AVOID prob- lem. Lemma 4.3. There is a polynomial time reduction from TWO-INTERSECT-MAJ3-AVOID to ONE-INTERSECT-MAJ3-AVOID. Proof. Let C : {0, 1}n →{0, 1}m be a circuit such that each output bit is computed by a MAJ3 function where m > n."
  },
  {
    "prompt": "Understanding the computational aspects of step, functions, variables.",
    "response": "By step 1, we know that the set of functions corresponding to C form some cluster K. Hence, we obtain C starting with an arbitrary function in K and iteratively adding a new function f to O, such that I(f) ∩I(O) ̸= ∅. Our algorithm will follow this construction procedure, and eliminate functions that appear in that order by setting input variables. We start with the following claim that helps us to eliminate functions from the circuit by setting input variables."
  },
  {
    "prompt": "Read and understand this section about process, output, claim from a research paper.",
    "response": "Let f1, f2 be two output functions of C such that |I(f1) ∩I(f2)| = 2. Initially O = {f1, f2}, and we iteratively apply the following claim for each function g ∈K \\O that is used to build the cluster by the above process. A variable in I(K) is said to be alive if it is not set to a value in {0, 1} in the below process. Claim 4.4."
  },
  {
    "prompt": "Explain a concept involving variables, setting, consistent in computational complexity.",
    "response": "∀g ∈K \\ O such that |I(g) ∩I(O)| = 2, there is an setting of g ∈{0, 1} and a consistent assignment to some of the input variables, such that the number of variables in I(O) which is alive is at most 1. Proof. We shall prove this by induction on the construction of the cluster. Initially, let O = {f1, f2}, I(O) = {x1, x2, x3, x4}. Without loss of generality, let I(f1) = {x1, x2, x3} and I(f2) = {x2, x3, x4}."
  },
  {
    "prompt": "Understanding the computational aspects of number, variables, observe.",
    "response": "17 We observe that if f1 = 1 and f2 = 0, then x1 = 1, x4 = 0, x3 = ¬x2 and the number of variables in I(O) is exactly 1. By hypothesis, let the number of variables in I(K) which are alive after i steps is at most 1. Now consider a g such that |I(g) ∩I(K)| = 2, we would like to show that there is an assignment of values to g (which also forces setting of input variables) such that in the new cluster, K ∪{g}, the number of variables that are alive is still at most 1."
  },
  {
    "prompt": "Understanding the computational aspects of Case, arise, depending.",
    "response": "The following cases arise depending on the two inputs g shares with the cluster. Let z1, z2 ∈I(g) ∩I(K) and let y = I(g) \\ I(K), b ∈{0, 1}. Case 1: Both z1, z2 are not alive: There are two cases to consider. Suppose z1 = z2 = b. By setting g = b, we can obtain a string which is outside the range of C. The other case is when z1 = b, z2 = b. In this case, we set g = b and y = b. Therefore, the number of variables in the new cluster is still at most 1."
  },
  {
    "prompt": "Scientific context on Case, set, alive and their role in graph theory.",
    "response": "Case 2: Exactly one of z1, z2 is alive: Again two cases arise. Consider the ﬁrst case when z1 = b, z2 = x. Now, we set g = x = y = b eliminating all the variables. The other case is: z1 = b, z2 = ¬x. Similarly, we set g = b, x = b, y = b which eliminates all the variables. Case 3: Both z1, z2 are alive: If z1, z2 that are both alive, then it must be that z1 = x, z2 = ¬x. In this case, we set g = y = 0 so that the number of variables in the cluster is at most 1."
  },
  {
    "prompt": "Scientific context on assignment, consistent, variables and their role in graph theory.",
    "response": "Hence, in all the cases there is an assignment value to g and consistent assignment for variables in I(g) such that there is at most one variable which is alive in I(O). Now, we consider the functions h such that |I(h) ∩I(O)| = 1. Let w := I(h) ∩I(O). If w is assigned to b ∈{0, 1}, then we set h to b and the input variables in I(h) \\ {w} to b. Consider the other case when w is alive. Wlog let w = x. We set the output of h and its other two inputs to x."
  },
  {
    "prompt": "This is a technical discussion about step, case, assigned in theoretical computer science.",
    "response": "In this case we have not yet assigned a ﬁxed 0/1 value to h, which we shall ﬁx in the next step. However, note that in either case the number of alive variables in O is at most 1. After at most n −2 iterations, we would have ﬁxed each of the inputs to one of 0, 1, x or ¬x. This is because in the iterative process, each function that we add to the set O covers at least one new variable. Now consider an f ∈K \\ O, which is guaranteed since m > n. Observe that I(f) ⊆I(O). Let I(f) = {x1, x2, x3}."
  },
  {
    "prompt": "Understanding the computational aspects of Suppose, alive, Case.",
    "response": "The following two cases arise based on the number of alive variables. Case 1: Suppose there is no variable in O which is alive. Then the value of f is already ﬁxed by the inputs I(O). Wlog let this be b ∈{0, 1}. Setting the output of f to b gives a string outside Range(C). Case 2: Suppose there exactly on variable that is alive. Wlog let x1 = x. Case 2(a): Consider the case when x2 = x3 = b for b ∈{0, 1}. Notice that this already forces f to take value b."
  },
  {
    "prompt": "Understanding the computational aspects of Range, functions, Case.",
    "response": "Thus, by setting f to b and the functions outputs of functions that were set to x to b we obtain a y ∈{0, 1}m which is outside Range(C). 18 Case 2(b): Suppose x2 = b, x3 = b. Then setting the output of f to b, we ﬁx the value of x to b. Thus, at this stage there are no variables that are alive in O. Since, |O| ≤n −1 and m > n there exists a g ∈K \\ O such that I(g) ⊆I(O). Since, all inputs are ﬁxed, we can now handle this using case 1. Case 3: Suppose f has two variables that are alive."
  },
  {
    "prompt": "Explain a concept involving Suppose, Case, output in computational complexity.",
    "response": "Let x1 = b where b ∈{0, 1}. Case 3(a): Suppose x2 = x3 = x. By setting the output of f to b we ﬁx the value of x2, x3 to b. This eliminates all the variables from O. Again, this reduces to case 1. Case 3(b): Suppose x2 = x, x3 = x. This ﬁxes the value of f to b. Hence, setting the output of f to b gives a string outside the range. Case 4: Suppose all the three variables of f are alive. Note that since |O | ≤n −1 and m > n there must be at least two more functions outside O."
  },
  {
    "prompt": "Scientific context on functions, problem, satisfy and their role in graph theory.",
    "response": "If any of these functions satisfy the above cases then we have already found a solution to the problem. Therefore, we consider the case when all three functions have all the three variables that are alive. By PHP, there exist two functions at least two of whose inputs are set to x(x). This ﬁxes the output of these functions to x(x). By setting one of outputs to 1 and the other to 0 yields a string outside the range."
  },
  {
    "prompt": "Understanding the computational aspects of scribed, problem, exist.",
    "response": "Thus, if there exist two function in C which have at least two common inputs then as de- scribed above, we already have a solution to the range avoidance problem. Otherwise, we have an instance of ONE-INTERSECT-MAJ3-AVOID. Note that the above steps can be done in polyno- mial time. Hence, it sufﬁces to show a polynomial time algorithm for this case."
  },
  {
    "prompt": "This is a technical discussion about proof, ingredients, ready in theoretical computer science.",
    "response": "Having described all the ingredients of the proof, we are now ready to prove our ﬁrst algorithm for MAJ3-AVOID (which works only for quadratic stretch m > cn2). We apply step 1 to obtain a sub-circuit CK whose outputs form a cluster K. Next we ob- serve that any two functions in CK share at most two input variables since otherwise we can trivially obtain a string outside the range by assigning opposite values to the corresponding out- put bits."
  },
  {
    "prompt": "Explain a concept involving time, polynomial, solving in computational complexity.",
    "response": "By step 2, we reduce our problem to an instance of ONE-INTERSECT-MAJ3-AVOID in polynomial time. Since m > cn2, theorem 3.10 gives a polynomial time algorithm for solving ONE-INTERSECT-MAJ3-AVOID. This gives a polynomial time algorithm for solving MONOTONE-NC0 3-AVOID with quadratic stretch, thus completing the proof of the following theorem from the introduction. Theorem 1.4. MONOTONE-NC0 3-AVOID when m > cn2 for any constant c can be solved in deterministic polynomial time."
  },
  {
    "prompt": "Scientific context on Polynomial, Time, Algorithms and their role in graph theory.",
    "response": "Improved Polynomial Time Algorithms for MONOTONE-NC0 3-AVOID with Linear Stretch: In the previous section, we saw that using wicket as our candidate for forbidden sub-hypergraph, works for quadratic stretch. In order improve this bound on the stretch requirement, we shall use loose a different forbidden sub-hypergraphs which are X 2ℓ-cycles. It sufﬁces to show that the structure has an edge-coloring which is not MAJ-coloring and that the extremal number for the structure is not too large."
  },
  {
    "prompt": "This is a technical discussion about mod, edge-coloring, MAJ-coloring in theoretical computer science.",
    "response": "19 First, we show that there exists an edge-coloring of X 2ℓwhich is not MAJ-coloring. Lemma 4.5. Let H be X 2ℓ-cycle. Then there exists an edge-coloring of H which is not MAJ-coloring. Proof. Let v1e1v2 . . . v2ℓe2ℓv1 be the X 2ℓcycle with (ei, ej) as χ-structure where i ≡0 mod 2, j ≡1 mod 2. Consider the following edge-coloring- Γ(ei) = ( R, if i ≡0 mod 2 B, if i ≡1 mod 2 Let ei ∩ej = {x}. First consider the case when Π(x) = R. This forces the color of the other two endpoints of ej to B."
  },
  {
    "prompt": "Scientific context on Iteratively, mod, iff and their role in graph theory.",
    "response": "Iteratively, we get Π(vj) = B iff j ≡1 mod 2. This implies Γ(ei) = B which is a contradiction. The other case when Π(x) = B is similar. The following is an easy corollary of lemma 4.5. Corollary 4.6. Let H be a loose X 2ℓ-cycle. Then there exists an edge-coloring of H which is not a MAJ- coloring. In the following theorem, we show an extremal bound for the loose C2ℓin a connected 3- uniform linear hypergraph. Theorem 1.6."
  },
  {
    "prompt": "Scientific context on connected, linear, loose and their role in graph theory.",
    "response": "Any connected 3-uniform linear hypergraph with m > n edges must contain a loose X 2ℓ cycle. Proof. Let H a connected 3-uniform linear hypergraph. It sufﬁces to show that there is a loose X 2ℓ cycle in H. Towards this, we construct block graph G corresponding to H as follows: let the set of m hyperedges in H be the vertices of G and add an edge (f, g) if the hyperedges f, g intersect at a vertex. Observe that G is connected."
  },
  {
    "prompt": "Understanding the computational aspects of ﬁnding, walk, edge.",
    "response": "Now we shall argue that ﬁnding a copy of loose X 2ℓcycle in H is equivalent to ﬁnding a subgraph G′ in G where G′ consists of an edge (f, f ′) and distinct odd walks from g to g′ and h to h′ without using edge (f, f ′), where g, h and g′, h′ are neighbors of f, f ′ respectively in G. The equivalence follows from the observation that a walk of length ℓ corresponds to a walk of length ℓ−1 in G. Thus, our task is to show that there must be a G′ in G. Observe that G has m vertices and 3m edges."
  },
  {
    "prompt": "Read and understand this section about path, cycle, show from a research paper.",
    "response": "Hence, G must contain a cycle Q. Let e = (f, f ′) be an edge in Q. Now we would like to show that G \\ e contains G′. We will show the argument for the g to h walk, the other argument is symmetric. Note that G \\ e is still connected. Hence, there must be a path P from g, h. Now P ′ = (f, g)∪P ∪(h, f ′) is a path from f to f ′. If P ′ is an odd length path then we have our desired path from f to f ′ otherwise we shall modify P ′ to obtain a walk of odd length from f to f ′."
  },
  {
    "prompt": "Scientific context on observe, vertex, degree and their role in graph theory.",
    "response": "We observe that the average degree of a vertex in G is 3m n . Hence, there a vertex with minimum degree 3m n . Observe that these vertices form a clique in G. Let f ′′ be one of these functions. Note that f ′′ has degree at least 3m/n > 3 in G. Hence, there must be an odd cycle C′ containing f ′′. Now, there must be a path Q′ from g to f ′′ owing to connectivity of GC. Thus, qQ′f ′′C′f ′′Q′gPg′ is an odd walk from g to g′. Adding edges (f, g), (f ′, g′) to Q′ gives an odd walk from f to f ′."
  },
  {
    "prompt": "Scientific context on similarly, Theorem, Finally and their role in graph theory.",
    "response": "The other walk containing h, h′ can be obtained similarly. Hence there is a copy of G′(loose X 2ℓcycle) in G(H). This completes the proof. 20 Finally, combining all the results above, we shall now prove our ﬁrst algorithm for MONOTONE-NC0 3-AVOID with linear stretch. Theorem 1.7. (Main Theorem) For m > n, MONOTONE-NC0 3-AVOID can be solved in deterministic polynomial time. Proof."
  },
  {
    "prompt": "Explain a concept involving lemma, cluster, solve in computational complexity.",
    "response": "By lemma 4.2 and lemma 4.3, we ﬁrst reduce the circuit to a sub-circuit corresponding to cluster, and then further to the case of ONE-INTERSECT-MAJ3-AVOID. Hence, it sufﬁces to solve ONE-INTERSECT-MAJ3-AVOID. By theorem 1.6 we know that there exists a loose X 2ℓin the hypergraph corresponding to the given circuit. It sufﬁces to show that we can ﬁnd loose X 2ℓin H in polynomial time. By corollary 4.6 and theorem 3.6 we get a polynomial time algorithm to solve MAJ3-AVOID."
  },
  {
    "prompt": "This is a technical discussion about polynomial, ﬁnd, time in theoretical computer science.",
    "response": "Indeed to ﬁnd G′ in the proof of theorem 1.6, we need to ﬁnd the paths P, Q′, and a high degree vertex f ′′, which can be done in polynomial time. Hence, we can ﬁnd the G′ in polynomial time. 5 Polynomial time Algorithm for SYMMETRIC-NC0 3-AVOID A natural restriction of the range avoidance problem would be when all the functions are sym- metric. We call this the SYMMETRIC-AVOID problem. In this section, we use our results for MONOTONE-NC0 3-AVOID to solve SYMMETRIC-NC0 3-AVOID."
  },
  {
    "prompt": "Understanding the computational aspects of function, parity, case.",
    "response": "We look at all possible symmetric functions over three bits and handle each case. We will need the following lemma, which shows that range avoidance problem is easy when all the functions compute parity. Lemma 5.1. Let C : {0, 1}n →{0, 1}m be a circuit such that each output function is a parity function and m > n. Then there is a deterministic polynomial time algorithm to ﬁnd a string outside the range of C. Proof. We note that since each output function computes parity of input bits."
  },
  {
    "prompt": "Scientific context on range, functions, Mij and their role in graph theory.",
    "response": "These are linear functions. We can express this in the form of matrix M of dimensions m × n as follows: Mij = ( 1 if fi depends on variable xj 0 otherwise Let x = (x1, . . . xn). Observe that the Mx is the range of circuit C. Let C1, . . . Cn be the column vectors of M. We have that the column space of Ci is the range of C. Let e1, . . . em be the standard basis vectors. We claim that there exists an ei which is outside the range of C."
  },
  {
    "prompt": "Scientific context on range, string, column-space and their role in graph theory.",
    "response": "Otherwise the column-space covers each m-bit string which is a contradiction since a string outside the range is guaranteed by m > n. We observe that ei ∈Range(C) ⇐⇒ {C1, . . . Cn, ei} is a linearly dependent. Since, we can check whether a set of vectors is linearly independent in polynomial time, we can ﬁnd an ej which is outside the range of C. We can extend the above idea to work with MOD3 functions with different residues."
  },
  {
    "prompt": "Read and understand this section about Step, Algorithm, Set from a research paper.",
    "response": "We deﬁne 21 Algorithm 1 Algorithm for solving MAJ3-AVOID on input C : {0, 1}n →{0, 1}m 1: Step 1: Obtain a cluster K such that |I(K)| < |K| ⊲by observation 4.2 2: Step 2: Reduction from TWO-INTERSECT-MAJ3-AVOID to ONE-INTERSECT-MAJ3-AVOID 3: if f1, f2 ∈K such that I(f1) = {x1, x2, x3} and I(f2) = {x2, x3, x4} then 4: Set f1 = 1, f2 = 0 and x1 = 1, x2 = 0, x4 = ¬x3, O = {f1, f2} 5: repeat 6: Consider g ∈K \\ O such that I(g) ̸= ∅ 7: if |I(g) ∩I(O)| = 2 then 8: Let I(g) = {z1, z2, z3} and z3 = I(g) \\ I(O)."
  },
  {
    "prompt": "Understanding the computational aspects of alive, setting, Range.",
    "response": "Let b ∈{0, 1} 9: if Both z1, z2 are not alive: then 10: If z1 = z2 = b By setting g = b, we obtain y ̸∈Range(C) 11: If z1 = b, z2 = b then set g = b and y = b 12: else if Exactly one of z1, z2 is alive: say z1 = b, z2 = x then 13: Setting g = z3 = b eliminates all the variables 14: else if Both z1, z2 are alive: that is z1 = x, z2 = ¬x then 15: We set g = z3 = 0 so that the number of variables in the cluster is at most 1."
  },
  {
    "prompt": "Read and understand this section about end, Set, Update from a research paper.",
    "response": "16: end if 17: else if I(g) ∩I(O) = {z1} then 18: If z1 = b ∈{0, 1}, then Set g to b and z2 = z3 = b 19: If z1 = x(alive) then set g to x and z2 = z3 = x 20: Update the cluster O = O ∪{g} and I(O) = I(O) ∪I(g) 21: end if 22: until ∃g ̸∈O such that I(g) ∩I(O) ̸= ∅ 23: end if 24: ∃h1, h2, h3 ̸∈O such that I(f) = {w1, w2, w3} ⊆I(O) since |O| ≤n −2 and |I(O)| = n. 25: if ̸ ∃a variable which is alive then 26: Setting the output of h1 to b gives a string outside the range."
  },
  {
    "prompt": "Read and understand this section about Setting, output, Set from a research paper.",
    "response": "27: else if ∃exactly one variable w1 which is alive then 28: if w2 = w3 = b then 29: Setting the output of h1 to b gives a string outside the range. 30: else if w2 = b, w3 = b then 31: Set the output of h1 to b, which ﬁxes w1 to b. 32: We have h2 ̸∈O and I(h2) ⊆I(O). Same as previous case. 33: end if 34: else if h1 has two variables that are alive and w3 = b then 35: if w1 = w2 = x then 36: We set the output of h1 to b. This ﬁxes all input variable(case 1)."
  },
  {
    "prompt": "Understanding the computational aspects of Setting, range, output.",
    "response": "37: else if w1 = x, w2 = x then 38: Setting the output of h1 to b gives a string outside the range. 39: end if 40: else if All three variables are alive then 41: ∃hi, hj whose inputs are set the same. Setting hi to 1 and hj to 0 gives y ̸∈Range(C)."
  },
  {
    "prompt": "Explain a concept involving Step, Find, Lemma in computational complexity.",
    "response": "42: end if 43: Step 3: Polynomial time algorithm for ONE-INTERSECT-MAJ3-AVOID 44: Find a loose X 2ℓin HC ⊲by theorem 1.6 45: Output y ̸∈Range(C) ⊲by corollary 3.6 22 the MODi 3 function for i ∈{0, 1, 2} as follows: MODi 3(x) = ( 1 if x1 + x2 + x3 ≡i mod 3 0 otherwise Lemma 5.2. Let C : {0, 1}n →{0, 1}m be a circuit such that each output function is a MODi 3 function for i ∈{0, 1, 2} and m > n. Then there is a deterministic polynomial time algorithm to ﬁnd a string outside the range of C. Proof."
  },
  {
    "prompt": "Understanding the computational aspects of Mjk, MODi, functions.",
    "response": "We express these linear functions in terms of matrix M with dimensions m × (n + 1) as follows: Mjk =        1 if fj depends on variable xk and k ∈[n] i if fj is an MODi 3 function and k = n + 1 0 otherwise Let C1, . . . Cn+1 be the column vectors of M. Then the column span of {C1, . . . Cn} ⊕3 Cn+1 is the range of C where ⊕3 denotes bitwise MOD3. As earlier let e1, . . . em denote the stan- dard basis vectors."
  },
  {
    "prompt": "This is a technical discussion about range, contradiction, m-bit in theoretical computer science.",
    "response": "One of the ei must be outside the range of C since otherwise every m-bit string is in the range of C, which is a contradiction. We observe that ei ∈Range(C) ⇐⇒ {C1, C2 . . . Cn, ei ⊕3 Cn+1 ⊕3 Cn+1} is linearly dependent. For each of the vectors e1, e2, . . . em we can check the linear dependence in polynomial time. Hence, we can obtain a string outside the range of C in polynomial time."
  },
  {
    "prompt": "Explain a concept involving lemma, polynomial, time in computational complexity.",
    "response": "Equipped with lemma 5.1 and lemma 5.2, we show a polynomial time algorithm for SYMMETRIC-NC0 3-AVOID using a reduction to the MAJ3-AVOID problem. Theorem 5.3. There is a polynomial time algorithm to solve SYMMETRIC-NC0 3-AVOID when m > 8n. Proof. It sufﬁces to show that SYMMETRIC-NC0 3-AVOID reduces to MAJ3-AVOID when m > 8n since we have a polynomial time algorithm to solve MAJ3-AVOID when m > n."
  },
  {
    "prompt": "Scientific context on MODi, functions, modulo and their role in graph theory.",
    "response": "We observe that there are only six types of symmetric functions possible on three bits: the MAJ3 functions, the parity functions(MOD2), the modulo 3 functions with different residues MODi 3 and their respective negations We assume that there are at least 4n functions of the form MAJ3, MOD2, MODi 3. The other case when there are more than 4n negations of these functions can be handled by ﬂipping the output bit values."
  },
  {
    "prompt": "Understanding the computational aspects of Suppose, PHP, Case.",
    "response": "Again, by PHP the following cases arise: Case 1: Suppose there are more than 2n functions of the form MOD2 or MODi 3. By repeated application of PHP we get the further cases: Case 1(a): There are more than n parity functions in the circuit. By lemma 5.1, we can ﬁnd a string outside the range in polynomial time. Case 1(b): Consider the other case when the number of MODi 3 functions is more than n."
  },
  {
    "prompt": "This is a technical discussion about case, functions, polynomial in theoretical computer science.",
    "response": "In this case we can obtain a string which is outside the range of the circuit in polynomial time using lemma 5.2. 23 Case 2: There are more than 2n MAJ3 functions. It sufﬁces to solve the range avoidance problem for the MAJ3-AVOID problem. Thus, we have a polynomial time algorithm to solve SYMMETRIC-NC0 3-AVOID in polynomial time when m > 8n."
  },
  {
    "prompt": "This is a technical discussion about AVOID, Conclusion, Turan-type in theoretical computer science.",
    "response": "6 Conclusion We described the formulation of special cases AVOID in terms of Turan-type problems in k-uniform hypergraphs, and demonstrated some applications to solve monotone versions of AVOID under depth restrictions for the circuit."
  },
  {
    "prompt": "This is a technical discussion about Fano, k-cage, grid in theoretical computer science.",
    "response": "We exhibited several different ﬁxed hypergraphs H′ - k-cage, weak Fano plane, 3 × 3 grid, (3, 3)-butterﬂy, (3, 3)-odd kite which can be also used to derive polynomial time algorithms for ONE-INTERSECT-MAJ3-AVOID which in turn is used to solve MONOTONE-NC0 3-AVOID when the strech is quadratic. Finally, the improvement to linear stretch (theorem 1.7) comes from using loose X 2ℓcycles as our hypergraph. We note that, this is not a ﬁxed sized hypergraph, however, we can ﬁnd it in polynomial time."
  },
  {
    "prompt": "This is a technical discussion about algorithm, time, log in theoretical computer science.",
    "response": "We also obtain a polyno- mial time algorithm for SYMMETRIC-NC0 3-AVOID with linear stretch. Prior to this work, the best known polynomial time algorithm MONOTONE-NC0 3-AVOID and SYMMETRIC-NC0 3-AVOID was via the algorithm for NC0 3-AVOID which requires m > n2 log n [GGNS23]. We extend our framework to solve ONE-INTERSECT-MAJk-AVOID for linear and quadratic stretch respectively. [GGNS23] give a polynomial time algorithm for solving NC0 k-AVOID when m > nk−1 log n ."
  },
  {
    "prompt": "Scientific context on circuits, monotone, Brown and their role in graph theory.",
    "response": "it would be interesting to improve the stretch using the hypergraph framework even for monotone NC0 6 circuits, which would in turn give an improvement for the case of NC0 3-AVOID. References [BES73] WG Brown, P´al Erd¨os, and Vera S´os. Some Extremal Problems on Gamma-graphs. Academic Press, Inc, 1973. [CCGJ18] Claton Collier-Cartaino, Nathan Graber, and Tao Jiang. Linear Tur´an Numbers of Linear Cycles and Cycle-Complete Ramsey Numbers."
  },
  {
    "prompt": "Scientific context on Probability, Combinatorics, Computing and their role in graph theory.",
    "response": "Combinatorics, Probability and Computing, 27(3):358–386, 2018. [CHR] Lijie Chen, Shuichi Hirahara, and Hanlin Ren. Symmetric Exponential Time Requires Near-Maximum Circuit Size, year = 2024. In Proceedings of the 56th Annual ACM Sym- posium on Theory of Computing (STOC 2024), page 1990–1999. [FR13] Zolt´an F¨uredi and Mikl´os Ruszink´o. Uniform hypergraphs containing no grids. Ad- vances in Mathematics, 240:302–324, 2013. [FS24] Jakob F¨uhrer and Jozsef Solymosi. Caps and Wickets."
  },
  {
    "prompt": "Scientific context on Technical, report, Karthik and their role in graph theory.",
    "response": "Technical report, 2024. 24 [GGNS23] Karthik Gajulapalli, Alexander Golovnev, Satyajeet Nagargoje, and Sidhant Saraogi. Range Avoidance for Constant Depth Circuits: Hardness and Algorithms. In Nicole Megow and Adam D. Smith, editors, Approximation, Randomization, and Combinato- rial Optimization. Algorithms and Techniques, APPROX/RANDOM 2023, September 11-13, 2023, Atlanta, Georgia, USA, volume 275 of LIPIcs, pages 65:1–65:18. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2023."
  },
  {
    "prompt": "Explain a concept involving Ervin, Lemons, Gyori in computational complexity.",
    "response": "[GL12] Ervin Gyori and Nathan Lemons. Hypergraphs with no cycle of a given length. Com- binatorics, Probability and Computing, 21(1–2):193–201, 2012. [GL21] W. T. Gowers and J. Long. The length of an s-increasing sequence of r-tuples. Combi- natorics, Probability and Computing, 30(5):686–721, 2021. [GLW22] Venkatesan Guruswami, Xin Lyu, and Xiuhan Wang. Range Avoidance for Low-Depth Circuits and Connections to Pseudorandomness. In Approximation, Randomization, and Combinatorial Optimization."
  },
  {
    "prompt": "Understanding the computational aspects of APPROX, RANDOM, Germany.",
    "response": "Algorithms and Techniques (APPROX/RANDOM 2022), vol- ume 245 of Leibniz International Proceedings in Informatics (LIPIcs), pages 20:1–20:21, Dagstuhl, Germany, 2022. Schloss Dagstuhl – Leibniz-Zentrum f¨ur Informatik. [GS22] Andr´as Gy´arf´as and G´abor N. S´ark¨ozy. The linear Tur´an number of small triple sys- tems or why is the wicket interesting? Discrete Mathematics, 345(11):113025, 2022. [ILW23] Rahul Ilango, Jiatu Li, and R. Ryan Williams."
  },
  {
    "prompt": "This is a technical discussion about Obfuscation, Range, Avoidance in theoretical computer science.",
    "response": "Indistinguishability Obfuscation, Range Avoidance, and Bounded Arithmetic. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing (STOC 2023), page 1076–1089, 2023. [Kee11] Peter Keevash. Hypergraph Tur´an Problems. Surveys in Combinatorics, 06 2011. [KKMP21] Robert Kleinberg, Oliver Korten, Daniel Mitropolsky, and Christos Papadimitriou. To- tal Functions in the Polynomial Hierarchy. In James R."
  },
  {
    "prompt": "Explain a concept involving ITCS, Germany, Innovations in computational complexity.",
    "response": "Lee, editor, 12th Innovations in Theoretical Computer Science Conference (ITCS 2021), volume 185 of Leibniz International Proceedings in Informatics (LIPIcs), pages 44:1–44:18, Dagstuhl, Germany, 2021. Schloss Dagstuhl–Leibniz-Zentrum f¨ur Informatik. [Kor22] Oliver Korten. The Hardest Explicit Construction. In 2021 IEEE 62nd Annual Sympo- sium on Foundations of Computer Science (FOCS), pages 433–444, 2022. [KPI25] Oliver Korten, Toniann Pitassi, and Russell Impagliazzo."
  },
  {
    "prompt": "Understanding the computational aspects of Cell, Probe, Lower.",
    "response": "Stronger Cell Probe Lower Bounds via Local PRGs. Technical Report 30, Electronic Colloquium on Computational Complexity (ECCC), March 2025. [KS04] Peter Keevash and Benny Sudakov. The Tur´an Number Of The Fano Plane. Combina- torica, 25, 03 2004. [Li24] Zeyong Li. Symmetric Exponential Time Requires Near-Maximum Circuit Size: Sim- pliﬁed, Truly Uniform. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing (STOC 2024), 2024."
  },
  {
    "prompt": "This is a technical discussion about Hanlin, Ren, Rahul in theoretical computer science.",
    "response": "25 [RSW22] Hanlin Ren, Rahul Santhanam, and Zhikun Wang. On the Range Avoidance Problem for Circuits. In 63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022, pages 640–650. IEEE, 2022. [Sol24] Jozsef Solymosi. Wickets in 3-uniform hypergraphs. Discrete Mathematics, 347(6):114029, 2024. [ZBW24] Lin-Peng Zhang, Hajo Broersma, and Ligong Wang. A Note on Generalized Crowns in Linear r-graphs. Technical report, 2024."
  },
  {
    "prompt": "Understanding the computational aspects of arxiv, Special, cases.",
    "response": "arxiv : 2401.12339. 26 A Appendix A.1 Special cases of MONOTONE-AVOID We now describe some special cases of the monotone range avoidance problem that are simpler than the general case of AVOID. We consider the depth 1 sub-circuit obtained by viewing circuits as a composition of two circuits. To denote this, for the rest of this discussion, we assume D is a monotone circuit of depth d."
  },
  {
    "prompt": "Read and understand this section about sub-circuit, obtained, depth from a research paper.",
    "response": "For j ≤d, deﬁne Dj to be the sub-circuit of C obtained by removing all the gates and variables below depth j and c Dj be the sub-circuit of C obtained by deleting all the gates above depth j. Let mj denote the number of gates at depth j for circuit C. Hence Dj : {0, 1}mj →{0, 1}m and c Dj : {0, 1}n →{0, 1}mj. By deﬁnition, D = Dj ◦c Dj. We show below that when j ≤2, the problem is easier under some constraints for m1 and m2 respectively. Proposition A.1."
  },
  {
    "prompt": "This is a technical discussion about range, circuit, deterministic in theoretical computer science.",
    "response": "If m > m1 then there is a deterministic polynomial time algorithm that ﬁnds a string outside the range of the circuit. Proof. Since m > m1, we have that D1 is a valid instance of the AVOID problem. Observe that, D1 is a depth 1 monotone circuit. We claim that y ̸∈Range(D1) =⇒y ̸∈Range(D). This follows from the fact that if no input setting x ∈{0, 1}m1 in circuit D1 can produce a y ∈{0, 1}m, then no input assignment of n variables in circuit D can ever produce y."
  },
  {
    "prompt": "Explain a concept involving polynomial, time, range in computational complexity.",
    "response": "We now show how to ﬁnd a y outside the range of D1 in deterministic polynomial time. Hence, when m > m1 there is a polynomial time algorithm for AVOID. Similar to the approach in [GLW22], the idea is that we inductively solve the range avoidance problem for smaller cir- cuits. Consider the ﬁrst output bit y1 (which is a function f1 of the input variables). We consider the following cases based on the type of functions: Case 1: Suppose f is a constant function."
  },
  {
    "prompt": "Explain a concept involving Case, Suppose, variables in computational complexity.",
    "response": "Without loss of generality assume f is the constant zero function. Setting y1 = 1 and the remaining output bits to arbitrary Boolean values, we obtain a string y which is outside the range of D1. Case 2: Suppose f is the ∧of some variables. In this case, we set output bit y1 = 1 and variables feeding into this gate to be 1. Thus, we obtain a smaller circuit D′ 1 : {0, 1}mj−1 →{0, 1}m−1. Case 3: Suppose f is the ∨of some variables. This case can be handled similar to case 2."
  },
  {
    "prompt": "Read and understand this section about obtain, circuit, Setting from a research paper.",
    "response": "Setting output bit y1 = 0 and variables feeding into this gate to be 0, we obtain a smaller circuit D′ 1 : {0, 1}mj−1 →{0, 1}m−1. After n steps, we obtain a circuit D′′ 1 : {0, 1}0 →{0, 1}m−mj . The value of function fmj+1 is ﬁxed by the input assignments. Flipping this value, we obtain a string outside the range of D1. We note that this can be done in O(m) time. Even if m < m1, m is much larger compared to m2, there is FPNP algorithm for MONOTONE- AVOID where s is the size of the circuit."
  },
  {
    "prompt": "Understanding the computational aspects of FPNP, log, Proposition.",
    "response": "Proposition A.2. If m > mω(√m2 log s) 2 there is an FPNP algorithm for MONOTONE-AVOID where s is the size of the circuit. 27 Proof. We show that if m2 < m 2 , then the problem can be reduced in polynomial time to CNF- AVOID. If in addition, m > mω(√m2 log s) 2 , we can use FPNP algorithm due to [GLW22] to solve the CNF-AVOID instance and this implies the above proposition. The reduction is as follows. Let F1, F2 be the set of all ∧gates and all ∨gates respectively in the output layer."
  },
  {
    "prompt": "Understanding the computational aspects of gates, principle, gate.",
    "response": "By pigeon hole principle, we have that either |F1| ≥m 2 or |F2| ≥m 2 . Without loss of generality assume |F1| ≥m/2. Since m2 < m 2 , it sufﬁces to solve the problem for D′ obtained from D2 by eliminating the top ∨gates. Since top layer of D′ contains only ∧gates, we can assume that there are only ∨gates in the second layer of this circuit. Otherwise, we can feed the inputs of this ∧gate directly to the ∧gate above it. Now D2 is a CNF formula."
  },
  {
    "prompt": "Read and understand this section about Solving, Hypergraphs, app from a research paper.",
    "response": "A.2 Alternative Hypergraphs for Solving ONE-INTERSECT-MAJ3-AVOID app:subsec:structures In this subsection, we show other choices of hypergraphs that can be used in place of wickets in the algorithm above for solving ONE-INTERSECT-MAJ3-AVOID. Alternative 1 : k-cage: We deﬁne the graph class as below: Deﬁnition A.3."
  },
  {
    "prompt": "Scientific context on consists, disjoint, k-uniform and their role in graph theory.",
    "response": "A k-cage is a k-uniform linear hypergraph H(V, E) with edge set E = E1 ∪E2 such that E1 consists of pairwsise disjoint edges which intersect exactly at w ∈V and E2 consists of k −1 edges that are pairwise disjoint over V \\ {w}. The picture shown is the 5-cage. We will show that there exists an edge-coloring Γ that is not a MAJ-coloring. Lemma A.4. Let H be a k-cage. There exists an edge coloring Γ : E →{R, B} which is not a MAJ- coloring."
  },
  {
    "prompt": "Explain a concept involving time, ﬁnd, coloring in computational complexity.",
    "response": "Furthermore, we can ﬁnd such a coloring in polynomial time. Proof. Let H be a k-cage with k edge-set E = E1 ∪E2, where E1 is the set of k-edges intersecting in vertex w and E2 is the set of k −1 pairwise disjoint edges over V \\ {w}. We will show t the following Γ is not a MAJ-coloring: Γ(e) = ( B if e ∈E1 R otherwise We consider the following cases based on the parity of k: Case 1: Suppose k is odd."
  },
  {
    "prompt": "Explain a concept involving colored, vertices, Observe in computational complexity.",
    "response": "Observe that for an edge in E2 to be colored R at least \u0004 k 2 \u0005 + 1 of its vertices should be colored R. Since the edges in E2 are pairwise disjoint, we have that there should be at least k \u0000\u0004 k 2 \u0005 + 1 \u0001 vertices in V \\ {w} that are colored R. Similarly, for any edge in e ∈E1 to be colored B, we have that at least \u0004 k 2 \u0005 vertices in e \\ {w} should be colored B."
  },
  {
    "prompt": "Understanding the computational aspects of vertices, colored, intersect.",
    "response": "Since, any two edges in E1 intersect exactly at vertex w, we have that at least \u0000\u0004 k 2 \u0005\u0001 (k −1) vertices in V \\ {w} that should be colored R. Thus, totally there must be at 28 least \u0000\u0004 k 2 \u0005\u0001 (k −1) + \u0000\u0004 k 2 \u0005 + 1 \u0001 (k) = k2 + k −⌊k/2⌋> k(k −1) vertices in V \\ {w}. Since, |V \\ {w}| = k(k −1) we get a contradiction. Case 2: Suppose k is even. As earlier for the edges in E2 to be colored R there must be at least k \u0000k 2 + 1 \u0001 vertices in V \\ {w} that are colored R."
  },
  {
    "prompt": "This is a technical discussion about vertices, colored, odd in theoretical computer science.",
    "response": "Since k is odd, for any edge in e ∈E1 to be colored B, we have that at least k 2 −1 vertices in e \\ {w} should be colored B. Hence, at least \u0000k 2 −1 \u0001 (k −1) vertices in V \\ {w} should be colored R. Thus, totally there must be at least \u0000k 2 + 1 \u0001 (k) + \u0000 k 2 −1 \u0001 (k −1) = k2 −k/2 −1 vertices in V \\ {w} > k(k −1). Since, |V \\ {w}| = k(k −1) we get a contradiction. In either case, we show that Γ is not a MAJ-coloring of H."
  },
  {
    "prompt": "This is a technical discussion about Fano, Weak, plane in theoretical computer science.",
    "response": "Alternative 2 : Weak Fano plane: This structure is similar to the fano plane that was deﬁned in section 2 with one hyperedge removed. Deﬁnition A.5. A weak Fano plane is a 3-uniform linear hypergraph which is isomorphic to the hypergraph H(V, E) with vertex set V = [7] and edge set E = {{1, 2, 3}, {3, 4, 5}, {1, 5, 6}, {3, 6, 7}, {2, 5, 7}, {2, 4, 6}} We will show that there Γ : E →{R, B} which is not a MAJ-coloring. Lemma A.6. Let H be a weak Fano plane hypergraph."
  },
  {
    "prompt": "This is a technical discussion about MAJ-coloring, edge-coloring, Suppose in theoretical computer science.",
    "response": "Then there is an edge-coloring Γ : E →{R, B} which is not a MAJ-coloring. Furthermore, we can ﬁnd this coloring in polynomial time. Proof. Let E1 = {e|1 ∈e}. We will show that the following Γ is not a MAJ-coloring. Γ(e) = ( R if e ∈E1 B otherwise We consider the following cases: Case 1: Suppose Π(1) = B. Then for any edge e ∈E1 to be colored R, it must be that the other two vertices in e should be R. In particular, we have Π(3) = Π(5) = R. This implies Γ({3, 4, 5}) = R which is a contradiction."
  },
  {
    "prompt": "Understanding the computational aspects of Suppose, Case, obtain.",
    "response": "Case 2: Suppose Π(1) = R. Then for an edge e ∈E1 to be colored R there must be a vertex pair {i, j} ∈{{2, 5}, {2, 6)}, {3, 5}, {3, 6}} such that Π(i) = Π(j) = R. We will obtain a contradic- tion in each case: Suppose {i, j} = {2, 5} then Γ({2, 5, 7}) = R which is a contradiction. For {i, j} being one of {2, 6}, {3, 5}, {3, 6} we similarly obtain contradictions for the coloring of the edges {2, 4, 6}, {3, 4, 6}, {3, 7, 6} respectively. Hence, Γ is not a MAJ-coloring of H."
  },
  {
    "prompt": "This is a technical discussion about Alternative, odd, butterﬂy in theoretical computer science.",
    "response": "29 Alternative 3 : (k, ℓ)-odd butterﬂy: A 3-uniform loose cycle of length k, C := u1e1u2 . . . ukeku1 is a hypergraph over the vertex set {u1, . . . , uk} ∪{v1, . . . vk} such that ui, ui+1 ∈ei and ei = (ui, vi, ui+1) where uk+1 = u1. Deﬁnition A.7. A (k, ℓ)-odd butterﬂy is a 3-uniform hypergraph over E = {e1 . . . ek} ∪{f1, . . . fℓ} consisting of two loose cycles C1 := u1e1u2e2 . . . ukeku1 and C2 := u1f1v2f2 . . . vℓfℓu1 where k, ℓ are odd. The ﬁgure demonstrates a (3, 3)-butterﬂy."
  },
  {
    "prompt": "Explain a concept involving coloring, lemma, edge in computational complexity.",
    "response": "In the following lemma we show an edge coloring function Γ which is not a MAJ-coloring. Lemma A.8. Let H be a (k, ℓ)-odd butterﬂy. Then there is an edge coloring Γ : E(H) →{R, B} which is not MAJ-coloring. Furthermore, we can ﬁnd this coloring in polynomial time. Proof. Let C1, C2 be the two loose cycles of order k in H. Let C1 := u1e1u2e2 . . . ukeku1 and C2 := u1f1v2f2 . . . vℓfℓu1. We will show that the following edge-coloring Γ : E →{R, B} is not a MAJ- coloring."
  },
  {
    "prompt": "Explain a concept involving claim, odd, MAJ-coloring in computational complexity.",
    "response": "Γ(ei) = ( R, if i is even B, otherwise Γ(fi) = ( R, if i is odd B, otherwise where i ∈[k]. First, we will show the following claim: Claim A.9. Let Γ : E →{R, B} be a MAJ-coloring under vertex coloring Π : V →{R, B} such that Π(ui) = c and Γ(ei) = c where c ∈{R, B} and c = {R, B} \\ {c}. Then Π(ui+1) = c. Proof. Wlog let Π(ui) = R. Since Γ is a MAJ-coloring satisfying Γ(ei) = B, the other two vertices in ei must be B. In particular we have that Π(ui+1) = B. Suppose Π(u1) = R."
  },
  {
    "prompt": "Understanding the computational aspects of contradiction, claim, odd.",
    "response": "Then by claim A.9 we have that uk = R. We have Γ(ek) = B. This implies Π(u1) = B which a contradiction. Similarly, if Π(u1) = B then we can repeatedly apply the claim A.9 for C2 to obtain a contradiction. Alternative 4 : (k, ℓ)-odd kite: We deﬁne the hypergraph as follows: Deﬁnition A.10. A (k, ℓ)-odd kite is a 3-uniform linear hypergraph with edge set E = {e1, . . . ek} ∪{f1, . . . fk} ∪{e′} consisting of two loose odd cycles C1 = u1e1u2 . . . ukeku1, C2 = u1f1v2f2 . . ."
  },
  {
    "prompt": "Scientific context on odd, kite, MAJ-coloring and their role in graph theory.",
    "response": "fℓ−1ukfℓu1 where ek = fℓ= {u0, u1, uk}, e′ = {u0, ui, vj} such that i ∈{2, . . . k−1}, j ∈{2, 3 . . . ℓ− 1} and i ≡j mod 2. The ﬁgure shows a (3, 3)-odd kite. We will show an edge coloring of (k, ℓ) which is not a valid MAJ-coloring. 30 Lemma A.11. Let H be a (k, ℓ)-odd kite. Then there exists an edge-coloring Γ : E →{R, B} which is not a valid MAJ-coloring. Furthermore, we can ﬁnd such a coloring in polynomial time. Proof."
  },
  {
    "prompt": "Explain a concept involving edge-coloring, odd, show in computational complexity.",
    "response": "We will show that the following edge-coloring Γ is not a valid edge-coloring: Γ(e) =                        B, if e = ei ∈{e1, . . . ek} and i is odd R, if e = ei ∈{e1, . . . ek} and i is even R, if e = fj ∈{f1, . . . fℓ−1} and j is odd B, if e = fj ∈{f1, . . . fℓ−1} and j is even R, if e = e′ = {u0, ui, vj} and i is even B, if e = e′ = {u0, ui, vj} and i is odd We consider the following cases based on the colors of vertices u1, uk: Case 1: Suppose Π(u1) = Π(uk) = R."
  },
  {
    "prompt": "This is a technical discussion about claim, contradiction, Case in theoretical computer science.",
    "response": "Since two of its vertices are colored R, this implies that color of edge ek is R, which is a contradiction. Case 2: Wlog let Π(u1) = R, Π(uk) = B. By repeated application of claim A.9 in cycle C1 we get that Π(uk) = R, which is again a contradiction. The other can be handles similarly by applying claim A.9 on cycle C2. Case 3: Suppose Π(u1) = Π(uk) = B. By repeatedly applying claim A.9 in cycles C1, C2 we get that Π(ui) = B ⇐⇒i ≡0 mod 2 and Π(vj) = B ⇐⇒j ≡0 mod 2."
  },
  {
    "prompt": "This is a technical discussion about contradiction, mod, implies in theoretical computer science.",
    "response": "This implies that Γ(e′) = B ⇐⇒i ≡0 mod 2, which is a contradiction. Since, in either case we obtain a contradiction, Γ is not a valid MAJ-coloring. A.3 Counter Example graphs among the (9, 5)-hypergraphs In continuation with the discussion towards the introduction, we write down the explicit example of a (9, 5)-graph which does not lead to a contradiction as used in our arguments. It is easy to verify that every edge coloring is a MAJ-coloring for this hypergraph."
  },
  {
    "prompt": "Understanding the computational aspects of MONOTONE, solving, explains.",
    "response": "This example explains why the power-bound conjecture in [GL21] is not sufﬁcient to provide us the required improvements for the relation between m and n in our proposed algorithm for solving MAJ3-AVOID and hence MONOTONE- NC0 3-AVOID. 31"
  },
  {
    "prompt": "Explain a concept involving Sum, Adaptive, Enumeration in computational complexity.",
    "response": "1 Beyond Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-2n/2 Enumeration Jesus Salas Independent Researcher jesus.salas@gmail|microsoft.com https://orcid.org/0009-0007-6411-2270 Note This is a preliminary, extended version posted on arXiv. A shorter version may appear in a conference. We welcome peer reviews, suggestions, or alternative analyses."
  },
  {
    "prompt": "This is a technical discussion about Subset, Sum, problem in theoretical computer science.",
    "response": "Abstract The Subset Sum problem, which asks whether a given set of n integers contains a subset summing to a specified target t, is a fundamental NP-complete problem arising in cryptography, combinato- rial optimization, and beyond. The classical meet-in-the-middle (MIM) algorithm of Horowitz and Sahni achieves a worst-case time complexity of e O(2n/2), which remains the best-known deterministic bound."
  },
  {
    "prompt": "Understanding the computational aspects of sums, full, instances.",
    "response": "Yet many instances exhibit abundant collisions in partial sums, so the true difficulty is often governed by the number of unique subset sums (U) rather than the full 2n subsets. We present a structure-aware, adaptive solver that deterministically enumerates only the distinct sums, pruning duplicates on the fly and thus operating in time proportional to e O(U) whenever U ≪2n."
  },
  {
    "prompt": "This is a technical discussion about MIM, enumerator, meet-in in theoretical computer science.",
    "response": "Its core is a unique-subset-sums enumerator combined with a double meet-in- the-middle strategy and lightweight dynamic programming, thereby avoiding the classical MIM’s expensive merging phase. Additionally, we introduce a combinatorial tree compression technique that ensures strictly sub-2n/2 enumeration even on unstructured instances, effectively reducing the exponent by a small but nontrivial constant factor."
  },
  {
    "prompt": "Scientific context on operation, producing, solutions and their role in graph theory.",
    "response": "Our solver supports anytime and online operation, producing intermediate solutions (partial expansions) early and adapting seamlessly to newly added elements. Both theoretical analysis and empirical evidence show that for structured inputs—such as those with small doubling constants, high additive energy, or significant additive redundancy—our method can significantly outperform classical approaches, often approaching near-dynamic programming efficiency."
  },
  {
    "prompt": "Scientific context on bound, MIM, worst and their role in graph theory.",
    "response": "Even in the worst- case regime, it never exceeds the e O(2n/2) bound, and its compression-based pruning guarantees a genuine constant-factor speedup over naive MIM enumerations. We conclude by discussing how this instance-specific adaptivity refines the Subset Sum complexity landscape, and how our unified, output-sensitive framework could inspire further advances in adaptive exponential-time algorithms."
  },
  {
    "prompt": "Read and understand this section about Subset, Sum, algorithms from a research paper.",
    "response": "2012 ACM Subject Classification NP-complete problems, Design and analysis of algorithms Keywords and phrases Subset Sum, NP-Completeness, Adaptive Algorithms, Additive Combina- torics, Exponential Time Algorithms, Instance Hardness 1 Introduction The Subset Sum problem is a classical NP-complete problem that asks whether a given set S of n integers contains a subset whose elements sum to a specified target t."
  },
  {
    "prompt": "Understanding the computational aspects of Mar, Traditionally, arXiv.",
    "response": "Traditionally, arXiv:2503.20162v1 [cs.DS] 26 Mar 2025 2 worst-case complexity is measured in terms of n and the total number of potential subsets, i.e., 2n. However, in many practical scenarios, the input exhibits significant additive struc- ture—captured by parameters such as the doubling constant or additive energy—which implies that many subsets yield the same sum."
  },
  {
    "prompt": "This is a technical discussion about cases, defined, subset in theoretical computer science.",
    "response": "In such cases, the true computational challenge is determined not by n alone but by the effective search space U, defined as the number of distinct subset sums in Σ(S). The seminal meet-in-the-middle algorithm of Horowitz and Sahni [1] achieves a worst-case running time of e O(2n/2) by dividing S into two lists and enumerating all subset sums for each."
  },
  {
    "prompt": "Explain a concept involving number, sums, subset in computational complexity.",
    "response": "Although this bound remains tight in theory, empirical studies and recent research [2, 3] suggest that the practical difficulty of an instance is better characterized by the number of unique subset sums rather than the total number of subsets. For example, when the input set has a low doubling constant or high additive energy, many subset sums coincide, reducing U and, consequently, the work required to solve the instance."
  },
  {
    "prompt": "Understanding the computational aspects of Knapsack, Subset, Sum.",
    "response": "A significant body of work has analyzed the empirical hardness of NP-complete problems, including Subset Sum and related variants such as k-Sum or Knapsack. Early investigations by [4] illustrated how “phase transitions” can yield exceptionally hard instance families, even though many randomly generated instances are surprisingly easy in practice."
  },
  {
    "prompt": "Read and understand this section about log, Subset, Sum from a research paper.",
    "response": "Subsequent works [5, 6, 7] have shown that the density of Subset Sum instances (often measured by n/ log(max ai)) strongly influences typical-case runtimes. In particular, so-called “low-density” or “medium-density” inputs can often be solved faster than the classical 2n/2 meet-in-the- middle bound would suggest."
  },
  {
    "prompt": "Scientific context on Subset, Sum, Recent and their role in graph theory.",
    "response": "Recent studies in cryptographic settings [8] further document large empirical speedups over the worst-case analysis, confirming that the actual difficulty of Subset Sum hinges dramatically on its structural properties and input distributions. Randomization can also help in practice. Several contributions have aimed to improve the time complexity for solving Subset Sum. In particular, randomized algorithms introduced in [8, 2] leverage the additive structure to reduce the effective search space."
  },
  {
    "prompt": "This is a technical discussion about speedups, solution, randomized in theoretical computer science.",
    "response": "Although these randomized methods often yield practical speedups, they introduce an element of uncertainty and do not guarantee an exact solution. These empirical findings reinforce our structure-sensitive perspective: they demonstrate that for many real or random-like instances (whether sparse or dense), the effective search space may be significantly smaller than 2n."
  },
  {
    "prompt": "Scientific context on Subset, Sum, advances and their role in graph theory.",
    "response": "Despite these advances, a universal method that deterministically exploits the combi- natorial structure of Subset Sum across all instances remains elusive. The challenge arises from the diversity of inputs: on the one hand, collision-rich (or small-range) instances can be tackled effectively by pseudopolynomial DP or meet-in-the-middle; on the other hand, sparse sets with minimal collisions may require different techniques (e.g., sparse convolution)."
  },
  {
    "prompt": "Scientific context on Freiman, Theorem, constants and their role in graph theory.",
    "response": "Moreover, although results such as Freiman’s Theorem provide strong bounds for sets with small doubling constants, they do not extend uniformly to all instances. Motivated by these observations, we propose a structure-aware framework that leverages both the combinatorial properties of the enumeration process and the underlying additive combinatorics of the Subset Sum problem."
  },
  {
    "prompt": "Explain a concept involving effectively, subset, sum in computational complexity.",
    "response": "Our approach centers on a novel unique subset sum enumerator that generates only the distinct subset sums, effectively parameterizing the runtime by the effective search space U. While traditional worst-case analyses focus on the 2n subsets, practical instance hardness is better captured by U."
  },
  {
    "prompt": "Scientific context on contributions, structure, instance and their role in graph theory.",
    "response": "By exploiting this insight, our adaptive, deterministic solver efficiently adjusts its behavior to the input’s structure—whether the instance is dense or sparse—thereby setting the stage for the detailed 3 exposition of our contributions in the subsequent sections. 2 Key Contributions In this work, we present a novel adaptive and structure-aware solver for the Subset Sum problem that dynamically adjusts to the instance structure."
  },
  {
    "prompt": "Scientific context on Subset, Unique, Enumeration and their role in graph theory.",
    "response": "The key contributions of our approach are: Unique Subset Sums Enumeration: We introduce an enumeration strategy that generates each unique subset sum exactly once, eliminating redundant computations and dynamically pruning branches (§5). Double Meet-in-the-Middle Optimization: Our solver leverages a double meet-in- the-middle approach to efficiently combine solutions from two halves, avoiding expensive sorting and merging phases (§6)."
  },
  {
    "prompt": "Explain a concept involving enumeration, Optimization, Tree in computational complexity.",
    "response": "Combinatorial Tree Compression Optimization: We propose a lightweight struc- tural optimization that provably reduces the effective exponent in all cases, ensuring a strictly sub-2n/2 enumeration (§7). Anytime and Online Behavior: The solver supports incremental computation and allows seamless updates when new elements are introduced, making it suitable for real-time applications (§8)."
  },
  {
    "prompt": "Scientific context on Structure, Adaptivity, constants and their role in graph theory.",
    "response": "Adaptivity to Instance Structure: We analyze how our method naturally adapts to structured instances—those with small doubling constants, high additive energy, or significant redundancy—often matching near-dynamic programming performance (§10- §15)."
  },
  {
    "prompt": "Understanding the computational aspects of Algorithms, complexity, Implications.",
    "response": "Implications for Exponential-Time Algorithms: We discuss how our approach refines the complexity landscape of NP-complete problems by focusing on instance-specific hardness rather than worst-case complexity, and we outline directions for future work (§16). The remainder of the paper is organized as follows. In Section 3, we introduce the necessary background and notation. Section 4 reviews related work on Subset Sum and its variants."
  },
  {
    "prompt": "Explain a concept involving Section, enumerator, approach in computational complexity.",
    "response": "In Section 5, we describe our unique subset sums enumerator, which forms the cornerstone of our approach. Section 6 details our double meet-in-the-middle optimization, and Section 7 presents our combinatorial tree compression technique to further reduce the enumeration cost. Next, Section 8 outlines our divide-and-conquer strategy that enables anytime operation, and Section 9 provides the additive-combinatorial underpinning to our method."
  },
  {
    "prompt": "Explain a concept involving Section, hardness, method in computational complexity.",
    "response": "Section 10 then analyzes the adaptive time complexity and instance hardness. In Section 11, we present runtime experiments that demonstrate the adaptivity of our method, while Section 12 summarizes the overall runtime when all optimizations are combined. Section 13 then provides a comparative table of existing Subset Sum solvers, including our approach. Finally, Section 14 highlights the key insights and benefits of our framework, Section 15 briefly discusses the emerging dynamism of our method."
  },
  {
    "prompt": "Explain a concept involving Section, Sum, explores in computational complexity.",
    "response": "Section 16 explores broader implications and future directions, and Section 17 concludes the paper. 3 Preliminaries In this section, we introduce the basic concepts, notations, and parameters that form the foundation for our analysis and algorithm. 4 3.1 Basic Definitions Let S ⊂Z be a nonempty finite set of n integers. The classical Subset Sum Problem is defined as follows: ▶Definition 1 (Subset Sum Problem)."
  },
  {
    "prompt": "Read and understand this section about Definition, satisfying, determine from a research paper.",
    "response": "Given a set S of n integers and a target t satisfying 1 ≤t ≤ X x∈S x, determine whether there exists a subset S′ ⊆S such that X x∈S′ x = t. ▶Definition 2 (Asymptotic Notation). We use the notation e O(f(n)) to denote an upper bound that suppresses polylogarithmic factors. That is, a function g(n) is in e O(f(n)) if there exist constants c > 0 and k ≥0 such that for all sufficiently large n, g(n) ≤c f(n) · (log n)k. ▶Definition 3 (Density)."
  },
  {
    "prompt": "Understanding the computational aspects of Subset, Sum, define.",
    "response": "We define the density of a Subset Sum instance S = {a1, a2, . . . , an} as d(S) = n log2(max S), which measures the ratio between the number of elements and the bit-length of the largest element. A higher density indicates that many elements are packed into a relatively small numerical range, often leading to more additive collisions and a smaller effective search space. ▶Definition 4 (k-Permutations and k-Subsets)."
  },
  {
    "prompt": "Explain a concept involving process, set, ordered in computational complexity.",
    "response": "For a set S, a k-permutation is an ordered selection of k distinct elements from S representing a partial solution in the enumeration process. When order is not significant, the same collection is referred to as a k-subset. In our algorithm, we generate k-permutations incrementally, and upon verifying that their aggregated sum is unique, we interpret them as k-subsets. ▶Definition 5 (Combinatorial Topology)."
  },
  {
    "prompt": "Explain a concept involving combinatorial, subsets, topology in computational complexity.",
    "response": "The combinatorial topology of an instance S refers to the inherent structure of the combinatorial tree formed by all possible subsets of S. This topology is determined by the additive relationships within S, and remains fixed for a given static instance. It dictates how distinct k-permutations may converge to the same subset sum. 3.2 Unique Subset Sums Although there are 2n possible subsets of S, many of these subsets yield the same sum when the input set S exhibits additive structure."
  },
  {
    "prompt": "Scientific context on define, set, unique and their role in graph theory.",
    "response": "We define the set of all unique subset sums as: Σ(S) = ( X x∈S′ x : S′ ⊆S ) . The effective search space is then given by: U = |Σ(S)|. In many practical instances, U is significantly smaller than 2n, and our algorithm explicitly exploits this fact by parameterizing the running time in terms of U."
  },
  {
    "prompt": "Explain a concept involving Additive, Structure, Doubling in computational complexity.",
    "response": "5 3.3 Additive Structure Measures The following measures quantify the degree of additive structure in the set S: 3.3.0.1 Doubling Constant The doubling constant of S is defined by C = |S + S| |S| , where S + S = {a + b | a, b ∈S}. A low doubling constant indicates that many pairwise sums overlap, which in turn suggests that the effective number U of unique subset sums is reduced. 3.3.0.2 Additive Energy The additive energy E(S) is a measure of the number of additive collisions in S."
  },
  {
    "prompt": "Read and understand this section about defined, number, solutions from a research paper.",
    "response": "It is defined as the number of solutions in S to a + b = c + d, with a, b, c, d ∈S. High additive energy implies that many subsets yield the same sum, again reflecting a small U. 3.4 Extended Structural Parameters Beyond the doubling constant C and the additive energy E(S), we will also make use of three additional parameters to quantify the structure or redundancy in S. 3.4.0.1 Linearity Factor λ."
  },
  {
    "prompt": "This is a technical discussion about relationships, factor, near-linear in theoretical computer science.",
    "response": "We define a linearity factor λ ∈[0, 1] to capture the extent to which the elements of S exhibit near-linear (e.g., arithmetic-progression-like) relationships. A smaller λ indicates that the set resembles a linear or nearly linear structure, often leading to higher collision rates among subset sums. 3.4.0.2 Clustering Factor γ. We define a clustering factor γ ∈[0, 1] to indicate how tightly the distinct sums in Σ(S) cluster."
  },
  {
    "prompt": "Read and understand this section about effective, Smaller, numerically from a research paper.",
    "response": "Smaller γ implies that many subset sums overlap or collide numerically, thus reducing the effective search space U. 3.4.0.3 Duplicate Measure δ. We let δ represent the effective number of duplicate elements in S. Repeated elements often cause multiple branches in our enumerator to converge onto the same partial sum, effectively merging those branches rather than keeping them distinct. Consequently, having more duplicates reduces the effective search space U."
  },
  {
    "prompt": "Explain a concept involving parameters, capture, aspects in computational complexity.",
    "response": "These three parameters (λ, γ, and δ) all capture different aspects of additive redundancy in S. In particular, they complement the more classical measures C and E(S) by providing additional insight into how and why the number of unique subset sums U might remain well below 2n. 6 Remark on “Input-Specific” Hardness."
  },
  {
    "prompt": "Understanding the computational aspects of input-specific, structure-aware, complexity.",
    "response": "Throughout this paper, when we discuss “input-specific” or “structure-aware” complexity, we are referring to significant subfamilies of instances that exhibit certain structural properties (e.g., high collision rates, small doubling constants, or additive redundancy). These properties can cause the number of distinct subset sums, U = |Σ(S)|, to remain far below the naive 2n, making the instance practically easier than worst-case bounds would suggest."
  },
  {
    "prompt": "Scientific context on NP-completeness, distribution, pruning and their role in graph theory.",
    "response": "This perspective does not contradict NP-completeness; it simply underscores that many real or structured inputs deviate markedly from the fully unstructured worst-case distribution, thus giving the solver room for substantial pruning. 3.5 Discussion In our analysis and experiments, we primarily measure an instance’s effective hardness by the number of unique subset sums U = |Σ(S)|."
  },
  {
    "prompt": "Understanding the computational aspects of possibilities, Subset, additive.",
    "response": "While the worst-case scenario for Subset Sum is governed by 2n possibilities, many practical instances exhibit significant additive structure—captured initially by the doubling constant C and additive energy E(S), but more finely parameterized by the linearity factor λ, clustering factor γ, and duplicate measure δ. Low Doubling / High Additive Energy: Such instances exhibit a substantial overlap among partial sums, thereby reducing U."
  },
  {
    "prompt": "Understanding the computational aspects of Factor, sums, Linearity.",
    "response": "Linearity Factor λ: If S is nearly linear or progression-like, many subsets collapse onto the same sums, again decreasing U. Clustering Factor γ: Even when S is not strictly linear, the subset sums can be tightly bunched numerically, indicating heavy overlap in the resulting sums. Duplicate Measure δ: Repeated elements cause multiple enumerator paths to merge onto the same partial sums—instead of branching off uniquely. Thus, duplicates can significantly reduce the overall search space U."
  },
  {
    "prompt": "Understanding the computational aspects of parameters, captures, facet.",
    "response": "Each of these parameters captures a different facet of additive redundancy. In essence, they all explain why, in many structured instances, the effective search space U can be dramatically smaller than 2n. Our algorithm takes advantage of these forms of redundancy by enumerating only distinct sums and pruning entire branches once collisions are detected. However, in unstructured instances, U remains close to 2n, so our method gracefully reverts to near-worst-case performance."
  },
  {
    "prompt": "Explain a concept involving strategy, unique, subset-sums in computational complexity.",
    "response": "The subsequent sections detail how this perspective guides the design of our unique- subset-sums enumerator and our overall double meet-in-the-middle strategy. Implementation and Input Assumptions. For simplicity of exposition, we assume throughout that the input set S consists of nonnegative integers."
  },
  {
    "prompt": "Read and understand this section about negative, omits, easily from a research paper.",
    "response": "Our enumerator and collision-pruning techniques readily apply even if S contains the integer 0 (this simply creates an immediate collision at sum 0 for any subset that chooses or omits 0), or if it contains negative integers (the memoization structure can store negative partial sums just as easily). In the worst case, storing all unique partial sums still requires e O(2n/2) space, although collision-rich inputs typically reduce memory usage sig- nificantly."
  },
  {
    "prompt": "Scientific context on tests, large, cryptographically and their role in graph theory.",
    "response": "We leave fully parallel implementations and extended empirical tests—particularly for large (cryptographically sized) n—as future work. 7 4 Prior Work The Subset Sum problem has attracted extensive research due to its NP-completeness and practical significance. Early approaches, such as Bellman’s dynamic programming algorithm [9], offered pseudopolynomial-time solutions with time complexity O(n t) (where t is the target sum)."
  },
  {
    "prompt": "Read and understand this section about time, large, Koiliaris from a research paper.",
    "response": "Over time, several refined DP-based techniques have been proposed to mitigate this cost when t remains large. For instance, Koiliaris and Xu [10] introduced a faster pseudopolynomial-time algorithm running in e O(t√n) by leveraging FFT-based convolutions for partial-sum computations."
  },
  {
    "prompt": "This is a technical discussion about Bringmann, Subset, Sum in theoretical computer science.",
    "response": "More recently, Bringmann [11] developed a near- linear pseudopolynomial algorithm for Subset Sum under specific parameter regimes, achieving further speedups in cases where the numeric range is not excessively large. Nevertheless, once t grows beyond a moderate threshold, these DP-based methods become infeasible in practice because their complexity remains tied to the numerical size of the target."
  },
  {
    "prompt": "Scientific context on Sahni, Horowitz, algorithm and their role in graph theory.",
    "response": "A major breakthrough was achieved with the meet-in-the-middle algorithm of Horowitz and Sahni [1], which achieves a worst-case time complexity of e O(2n/2) by splitting the input set into two parts and enumerating all subset sums for each half."
  },
  {
    "prompt": "Explain a concept involving enumeration, Subset, empirical in computational complexity.",
    "response": "However, while this bound remains tight for pathological inputs, empirical evidence and more recent theoretical insights [11, 12, 13] reveal that practical Subset Sum hardness is more accurately governed by collisions in partial sums rather than the full 2n enumeration. Exploiting Additive Structure A key observation in recent work is that many inputs exhibit significant additive structure, which naturally leads to a high degree of such collisions."
  },
  {
    "prompt": "Scientific context on energy, GAP, doubling and their role in graph theory.",
    "response": "This structure can be quantified by measures such as the doubling constant and additive energy. For instance, Freiman’s Theorem [14] guarantees that sets with small doubling can be embedded in a low-dimensional generalized arithmetic progression (GAP), implying that the effective search space U is dramatically smaller than 2n when the input is highly structured."
  },
  {
    "prompt": "This is a technical discussion about Randomized, approaches, techniques in theoretical computer science.",
    "response": "Randomized Algorithms Randomized approaches [8, 2] have been proposed to exploit additive structure by using collision-based pruning techniques. These algorithms navigate the combinatorial tree of subset sums probabilistically, effectively reducing the number of distinct paths that must be examined. While these methods often yield substantial practical speedups, they inherently introduce uncertainty and do not always guarantee an exact solution."
  },
  {
    "prompt": "Understanding the computational aspects of Determinism, Challenges, techniques.",
    "response": "Challenges and the Need for Determinism Despite the improvements provided by randomized techniques, these methods generally do not extend well to all instances, especially sparse inputs where collisions are infrequent. Moreover, existing deterministic methods (such as the classical meet-in-the-middle approach) do not fully exploit the underlying additive structure of the input."
  },
  {
    "prompt": "Understanding the computational aspects of universal, deterministic, properties.",
    "response": "There remains a gap in the literature for a universal, deterministic method that adapts to the input’s additive properties. Relation to Instance Complexity An important theoretical angle for analyzing how NP-complete problems can be signifi- cantly easier on “structured” inputs is the notion of instance complexity studied by Orponen, 8 Ko, Schöning, and Watanabe [15]."
  },
  {
    "prompt": "Understanding the computational aspects of Informally, special-case, bound.",
    "response": "Informally, instance complexity measures the size of a “special-case program” that decides whether a particular input x belongs to the language A, within a given time bound, while never misclassifying any other inputs (it may answer “don’t know” on them). They prove that although NP-hard sets still have infinitely many “intrinsically hard” instances, many classes of instances admit specialized programs of lower complexity."
  },
  {
    "prompt": "Read and understand this section about Subset, Sum, worst-case from a research paper.",
    "response": "Their results help explain why algorithms exploiting collisions or other structural properties can solve large subfamilies of Subset Sum instances much faster than the worst-case 2n/2. Our Contribution Recent research [11, 2] has indicated that parameterizing Subset Sum in terms of U (or related structural measures) can yield exponential speedups in practical cases. However, prior work has largely relied on randomized methods or specific assumptions on the input structure."
  },
  {
    "prompt": "Read and understand this section about deterministic, structure-aware, parameter from a research paper.",
    "response": "Our work fills this gap by introducing a deterministic, structure-aware framework that leverages the effective search space U as a parameter, thereby providing a unified approach that adapts seamlessly to both dense and sparse instances. The collision-driven pruning in our unique subset-sums enumerator does more than simply reduce enumeration overhead; it also offers a real-time lens into the structure of the input."
  },
  {
    "prompt": "Read and understand this section about Specifically, sums, enumerator from a research paper.",
    "response": "Specifically, the enumerator’s memoization table tracks how often newly formed partial sums coincide with previously encountered sums."
  },
  {
    "prompt": "Scientific context on High, Clustered, Repetitions and their role in graph theory.",
    "response": "When collisions appear frequently and early, this strongly indicates that the input set S exhibits nontrivial additive or redundant structure, for example: A small doubling constant (|S + S| ≤C|S|), High additive energy (a + b = c + d collisions), Clustered or near-linear arrangements of elements, Repetitions or duplicates that force multiple branches onto the same sums."
  },
  {
    "prompt": "Explain a concept involving sparse, enumerator, preprocessing in computational complexity.",
    "response": "Without any additional preprocessing, the enumerator naturally adapts to collisions as they occur, effectively exploiting the input’s underlying structure (whether dense, sparse, or in-between) and automatically adjusting its work to reflect the true hardness U = |Σ(S)|. 5 Unique Subset Sums Enumerator A naive solution to the Subset Sum problem would exhaustively enumerate all 2n subsets of S and check whether any of them sum to the target t."
  },
  {
    "prompt": "Understanding the computational aspects of sums, practice, structure.",
    "response": "In practice, however, when the input set S exhibits significant additive structure, many of these 2n subsets produce redundant sums. The true challenge lies in generating the unique subset sums—that is, the effective search space of distinct sums defined by Σ(S) = ( X x∈S′ x : S′ ⊆S ) . We denote the size of this set by U = |Σ(S)|. In many instances, U is dramatically smaller than 2n."
  },
  {
    "prompt": "Understanding the computational aspects of data, sums, process.",
    "response": "9 Our novel enumeration model provides access to all generated data (i.e., the subsets of S and their sums) throughout the process. To implement the enumerator, we designed a novel permutation-generating algorithm. Conventional permutation generators output one full permutation per step (for example, by swapping two elements from the previous permutation). In contrast, our approach generates all possible permutations of S one column at a time (i.e., left-to-right)."
  },
  {
    "prompt": "Scientific context on INPUT, step, Initial and their role in graph theory.",
    "response": "At each step, the algorithm emits the k-permutations (or k-subsets) of increasing cardinality according to the following process: Initial step: Start with an INPUT list containing the empty k-permutation, {}. For each step: Extend each entry in the INPUT by one column by appending every element from S not already included, and add these expanded k-permutations to the OUTPUT. Once all entries in INPUT have been processed, set INPUT ←OUTPUT and repeat until no new k-permutations are generated."
  },
  {
    "prompt": "Explain a concept involving Input, Output, Column in computational complexity.",
    "response": "For example, consider the set S = {1, 2, 3, 4, 5}: Column Expansion Sub-process 1 Input = {{}} =⇒ Output = {{1}, {2}, {3}, {4}, {5}} Input ←Output ——— Column Expansion Sub-process 2 Input = {{1}, {2}, {3}, {4}, {5}} =⇒ Output = {{1, 2}, {1, 3}, {1, 4}, {1, 5}, {2, 1}, {2, 3}, . ."
  },
  {
    "prompt": "Read and understand this section about Input, Output, Column from a research paper.",
    "response": ".} Input ←Output ——— Column Expansion Sub-process N (up to i ≤n 2 ) Input = Output[i −1] =⇒ Output[i] Since permutations in isolation do not directly solve the Subset Sum problem, our algorithm tracks the sum of each expanded k-permutation during the column expansion process. Before adding an expanded k-permutation to the OUTPUT, the algorithm checks whether its sum has already been encountered in the memoization table; if so, the k- permutation is pruned."
  },
  {
    "prompt": "Read and understand this section about tree, yielding, directly from a research paper.",
    "response": "This pruning mechanism eliminates enumerating redundant branches in the combinatorial tree, yielding an optimal algorithm for generating the unique subset sums from S and, hence, directly determining U. Once this process completes, all possible k-permutations for k ≤n 2 are generated. We restrict the enumeration to k ≤n 2 because any subset with more than n 2 elements can be obtained by computing its complement with respect to S."
  },
  {
    "prompt": "Read and understand this section about compu, tational, usage from a research paper.",
    "response": "This strategy eliminates redundant enumeration and significantly reduces both compu- tational overhead and memory usage, while ensuring that all unique subset sums—i.e., all elements of Σ(S) (and hence the effective parameter U) are captured. 10 For the complete pseudocode of our unique subsets sum enumerator algorithm, please refer to Appendix A."
  },
  {
    "prompt": "Read and understand this section about k-subset, entity, Combinatorial from a research paper.",
    "response": "5.1 k-Permutations and k-Subsets and the Combinatorial Tree Structure In this paper, we use the terms k-permutation and k-subset interchangeably to refer to the same entity, with the interpretation depending on the phase of the algorithm: When an entity appears in the INPUT or OUTPUT lists during any column expansion sub-process, it is treated as a k-subset."
  },
  {
    "prompt": "Scientific context on subset, unique, constraint and their role in graph theory.",
    "response": "In this context, it represents a subset of k elements that sums to a unique value and has passed the Unique Subset Sum constraint, which is the primary focus of our solver. When the entity serves as a prefix for generating (k + 1)-permutations during the column expansion, it is viewed as a k-permutation. Just before validation against the Unique Subset Sum constraint, the expanded (k + 1)- permutation is reinterpreted as a k-subset; if it satisfies the constraint, it is then added to the OUTPUT."
  },
  {
    "prompt": "This is a technical discussion about solver, duality, clarifies in theoretical computer science.",
    "response": "This duality clarifies the logic behind both the enumerator and the solver. k-permutations allow us to efficiently traverse and query the combinatorial tree structure during candidate generation, while k-subsets provide the precise representations needed to verify unique subset sums. This dual treatment is central to our approach, ensuring that the enumeration process remains both dynamic and efficient. 5.2 Seeding support An important feature of our enumerator is its support for seeding."
  },
  {
    "prompt": "Understanding the computational aspects of sums, elements, k-subsets.",
    "response": "If the memoization table is pre-populated with unique subset sums (i.e., elements of Σ(S)) and their corresponding k-subsets, the enumerator can seamlessly resume processing from that point. This capability allows intermediate results from previous cycles to be preserved and re-utilized, thereby reducing redundant computation and enabling an anytime (or incremental) behavior."
  },
  {
    "prompt": "This is a technical discussion about Behavior, FFT, convolution in theoretical computer science.",
    "response": "5.3 Convolution-Like Behavior in k-Permutation Enumeration Although our unique subset sum enumerator does not perform convolution in the strict numerical sense (e.g., via the FFT), it exhibits behavior that is conceptually analogous to convolution. Traditional convolution combines two sequences A and B to produce a new sequence: A + B = {a + b | a ∈A, b ∈B}. In our approach, the enumeration process generates k-permutations, which represent all possible orderings of k-subsets of the input set S."
  },
  {
    "prompt": "Read and understand this section about Partial, convolution, Aggregation from a research paper.",
    "response": "This process has several key characteristics that resemble convolution: Aggregation of Partial Sums: As the algorithm extends partial k-permutations by appending new elements, it aggregates the resulting partial sums in a manner analogous to convolution summing overlapping portions of signals. This aggregation naturally leads to the identification of collisions—i.e., multiple k-permutations yielding the same subset sum."
  },
  {
    "prompt": "Understanding the computational aspects of Tree, Combinatorial, Propagation.",
    "response": "11 Propagation Through the Combinatorial Tree: Different orderings of the elements propagate the partial sums through various branches of the combinatorial tree. Im- portantly, many distinct paths converge on the same k-subset sum, since the order of elements does not affect the numerical total. This convergence mirrors how convolution combines contributions from shifted versions of a sequence, ensuring that all possible paths leading to the same sum are effectively merged."
  },
  {
    "prompt": "Scientific context on Branches, Combination, Implicit and their role in graph theory.",
    "response": "Implicit Combination of Branches: The self-similarity and structural properties of k-permutations ensure that once a particular subset sum is computed, further redundant branches that would yield the same sum are pruned. Much like convolution naturally consolidates overlapping components, our method consolidates identical subset sums, thereby reducing the effective search space."
  },
  {
    "prompt": "This is a technical discussion about convolution, numerically, permutation in theoretical computer science.",
    "response": "Thus, while our algorithm does not explicitly implement FFT-based numerical convolution, it performs a topological convolution whose effects are expressed numerically: as the k- permutation enumeration process sweeps through the combinatorial space, it aggregates partial solutions, causing many distinct orderings to converge to the same unique subset sum."
  },
  {
    "prompt": "Explain a concept involving Instance, Litmus, Test in computational complexity.",
    "response": "5.4 A 4-Column Litmus Test for Instance Hardness and Splitting One practical way to gauge how “collision-rich” a Subset Sum instance is—and thus how large or small its effective search space U might be—is to run our enumerator only up to the 4th column. Concretely, this amounts to enumerating all subsets of size ≤4 from some subset of the input and measuring how often their sums collide. Polynomial-Time Approximation of U."
  },
  {
    "prompt": "Read and understand this section about test, input, enumerate from a research paper.",
    "response": "If we apply this 4-column test to one half of the input (of size n/2), we must enumerate 4 X k=0 \u0012n/2 k \u0013 ≈ O \u0000(n/2)4\u0001 = O(n4). Hence, it is a polynomial-time procedure. The fraction of collisions among these 4-subsets can serve as a proxy for how dense or structured the instance is. If collisions already appear frequently in subsets as small as size 4, then the entire half will likely have a smaller U overall. If collisions remain rare, that half may be near-worst-case. Per-Split vs."
  },
  {
    "prompt": "Explain a concept involving Testing, Whole-Set, split in computational complexity.",
    "response": "Whole-Set Testing. In a meet-in-the-middle setting, it is natural to apply this partial expansion on each split ℓ0 and ℓ1 separately to approximate U0 and U1 (the collision rate in each half). Alternatively, one can: (i) Test on the entire set of size n: enumerating 4-subsets up to \u0000n 4 \u0001 , still O(n4), to measure collisions globally before deciding how to split."
  },
  {
    "prompt": "Scientific context on half, Balance, splits and their role in graph theory.",
    "response": "(ii) Balance or refine the splits: if one half shows a much higher collision rate than the other, you might reassign a few elements from the sparser half to the denser half to improve overall performance of the double meet-in-the-middle. Thus, the 4-column test not only approximates the instance’s hardness in polynomial time but can also guide how we partition or balance the input for the main solver."
  },
  {
    "prompt": "Scientific context on Correctness, step, information and their role in graph theory.",
    "response": "It is a light preliminary step (only O(n4) time) that can significantly improve the final exponential search by exploiting early collision information. 12 5.5 Correctness and Completeness of Enumerating Unique Sums ▶Theorem 6 (Enumerator Correctness). Let E be the enumerator described in Section 5, which expands partial solutions one element at a time, pruning a branch whenever the new sum collides with a previously recorded sum. Then: 1."
  },
  {
    "prompt": "Understanding the computational aspects of Completeness, sum, Uniqueness.",
    "response": "(Completeness) For every subset S′ ⊆S, its sum P x∈S′ x eventually appears in the enumerator’s memoization table (i.e., no unique sum is ever missed). 2. (Uniqueness) Each distinct sum in Σ(S) is generated exactly once (the enumerator never duplicates the same sum). Proof. We prove each property separately. (1) Completeness. Let S′ = {xi1, xi2, . . . , xik} be any subset of S, written such that i1 < i2 < · · · < ik."
  },
  {
    "prompt": "Understanding the computational aspects of enumerator, stage, sum.",
    "response": "We argue by induction on k that the enumerator generates the partial sum P x∈S′ x at some stage. Base Case (k = 0): If S′ = ∅, then its sum is 0. By construction, the enumerator initializes with the empty subset (sum 0) in its memoization structure. Inductive Step: Suppose for all subsets of size at most k, the enumerator eventually includes their sums. Consider a subset S′ of size k + 1 with elements indexed in ascending order."
  },
  {
    "prompt": "Scientific context on xik, ordering, subset and their role in graph theory.",
    "response": "Let S′′ = S′ \\ {xik+1} be the subset formed by removing the last element in that ordering. Then S′′ is of size k and by the inductive hypothesis, P x∈S′′ x is already in the enumerator’s memo. When the algorithm attempts to expand S′′ by adding new elements, it eventually considers adding xik+1. If at that moment the enumerator sees that P x∈S′′ x + xik+1 is not yet in the memo, it inserts it, thereby generating the sum of S′."
  },
  {
    "prompt": "Understanding the computational aspects of xik, eventual, expansion.",
    "response": "Even if the enumerator visits other elements prior to xik+1, the eventual column-by-column (or step-by-step) expansion ensures that every unchecked element is tried. Collisions only prune repeated sums, never blocking a new one. Thus, by induction, every subset sum P x∈S′ x eventually appears. (2) Uniqueness. Assume for contradiction that the enumerator inserts the same sum s twice into its memoization table."
  },
  {
    "prompt": "Explain a concept involving process, enumerator, subsets in computational complexity.",
    "response": "Let A and B be two different subsets with P A = P B = s, and suppose they appear at different times in the enumerator’s process. Without loss of generality, let A be discovered first. The enumerator maintains a hash set or table storing all sums seen so far. Hence, by the time it considers generating B, it checks whether s is already in the table. Since A already placed s there, B’s creation would be pruned (the branch is skipped). This contradicts the assumption that B was inserted."
  },
  {
    "prompt": "Explain a concept involving Enumeration, sums, distinct in computational complexity.",
    "response": "Therefore, each distinct sum s enters the memo exactly once. Combining the two parts yields that the enumerator collects all unique sums in Σ(S) exactly once. ◀ 5.6 Bounded-Size Enumeration for Complements ▶Lemma 7 (Half-Subset Enumeration Preserves Completeness). Suppose the enumerator is restricted to enumerating only subsets of size at most ⌊n/4⌋in one half of the input (with n/2 13 elements). Let Umax be the number of unique sums enumerated by that half."
  },
  {
    "prompt": "Scientific context on subset, complement, size and their role in graph theory.",
    "response": "Then for any subset S′ in that half of size > n/4, we can represent S′ as the complement of some smaller subset. Hence the enumerator (plus complement logic) still captures all sums P x∈S′ x. Proof Sketch. Let ℓbe the list of n/2 elements in one half. If |S′| > n 4 , its complement S∗⊂ℓhas size |ℓ| −|S′| < n 4 . By definition, P x∈S′ x + P y∈S∗y = P z∈ℓz."
  },
  {
    "prompt": "Understanding the computational aspects of sum, pointer, records.",
    "response": "Once the enumerator records the sum P y∈S∗y, we store a pointer (or a simple record) that S∗’s complement yields the sum of S′ which is P x∈S′ x. Thus, no sums arising from subsets larger than n/4 are missed. ◀ 5.7 Implementation Invariants In a practical implementation, we typically maintain: A queue/stack of partial k-subsets (or k-permutations), A memoization hash table, Memo, mapping each sum σ to (i) a boolean or usage counter, and (ii) a pointer to the k-subset that generated σ."
  },
  {
    "prompt": "Understanding the computational aspects of Memo, enumeration, show.",
    "response": "One can show that throughout the enumeration, for each partial sum σ in Memo: σ = X x∈X x for some unique subset X ⊆S. Whenever we consider adding a new element x to X, we do a hash-table lookup on σ + x. This ensures collisions are detected immediately. Because a previously generated σ + x can never be inserted again, the data structure remains consistent until the entire half-subset search finishes. — Remarks."
  },
  {
    "prompt": "Read and understand this section about approach, sum, results from a research paper.",
    "response": "These results ensure that the column-by-column approach is both correct and never repeats the same sum. Combined with meet-in-the-middle and complement usage (see Sections 6–7), this gives a time complexity of e O(2n/2) in the worst case, but can be significantly lower when collisions are frequent."
  },
  {
    "prompt": "Read and understand this section about BFS-like, Note, Approaches from a research paper.",
    "response": "5.8 Brief Note on BFS-Based Approaches Although our method’s column-by-column enumeration might appear “BFS-like” at first glance, we stress that it diverges from standard BFS-based subset enumerations in critical ways. Classical BFS methods—including those used in certain meet-in-the-middle merges such as Schroeppel–Shamir [23]—expand subsets in discrete levels (e.g., all subsets of size k before moving to size k + 1)."
  },
  {
    "prompt": "Scientific context on BFS, checks, duplicates and their role in graph theory.",
    "response": "Even when BFS integrates real-time collision checks (using hash sets to prune duplicates), it still follows a layer-oriented expansion that carries all partial sums from that layer in memory before proceeding. Our Enumerator vs. BFS. Column-by-Column vs. Layer-by-Layer: Rather than enumerating by subset cardinalities, our solver expands “one new element at a time” in a topological manner."
  },
  {
    "prompt": "Read and understand this section about layer, bulk, enumerator from a research paper.",
    "response": "The enumerator prunes each branch immediately upon collision without waiting for a layer to complete or merging to occur in bulk. This avoids ephemeral duplicate states that BFS might generate in a single layer. 14 Adaptive Scheduling and Seeding: We allow partial k-permutations to be deferred to a future cycle if their expansions appear unpromising (see §8). This fine-grained scheduling is not commonly present in BFS-based merges, where expansions are typically uniform within a layer."
  },
  {
    "prompt": "Scientific context on Convolution, collapses, Topological and their role in graph theory.",
    "response": "Numerical and Topological Convolution: Our enumerator includes a unique combination of lightweight DP checks, forced collisions, and the generation of k-permutations in a manner that “collapses” repeated sums much earlier. By contrast, BFS enumerations typically store or process many partial sums in a given level, possibly discarding duplicates only after a layer-wide pass."
  },
  {
    "prompt": "Scientific context on logic, distinct, intermixes and their role in graph theory.",
    "response": "Although some BFS-based techniques can prune collisions on the fly too, the core dataflow in our method remains distinct: it intermixes meet-in-the-middle logic, dynamic pruning, and an anytime scheduling of expansions. Hence, we do not directly compare run times against a purely BFS-based subset enumeration—the algorithmic philosophies differ substantially."
  },
  {
    "prompt": "Scientific context on BFS, analogy, layers and their role in graph theory.",
    "response": "(As an analogy, BFS enumerates by cardinality layers, while we expand by columns and partial-sum uniqueness in a topological-numerical tree.) Why a Head-to-Head BFS Comparison is Nontrivial. We acknowledge that, in principle, one might code a BFS approach to attempt similar collision checks. Yet ensuring that BFS can replicate our incremental scheduling, column expansions, combinatorial-tree-short-circuits, and real-time integration with meet-in-the- middle is not straightforward."
  },
  {
    "prompt": "This is a technical discussion about BFS, Introducing, schema in theoretical computer science.",
    "response": "Introducing these features into a level-based BFS would effectively transform it into a different enumeration schema—one close to our method. Therefore, while some BFS improvements (e.g., Schroeppel–Shamir’s memory optimizations) overlap with our goals, the two techniques differ enough in design principle that we focus primarily on standard MIM, DP, and additive-structure solvers for our main comparisons (§13)."
  },
  {
    "prompt": "Explain a concept involving Subset, Sum, Sums in computational complexity.",
    "response": "6 Subset Sum Solver with Double Meet-in-the-Middle Optimization Having described our foundational Unique Subset Sums enumerator—which generates k-subsets (for k ≤n 2 per split) and produces only the distinct subset sums (of effective size U = |Σ(S)|)—we now integrate it into a complete Subset Sum solver. We begin by splitting the input set S into two lists, ℓ0 and ℓ1."
  },
  {
    "prompt": "Explain a concept involving sums, split, reducing in computational complexity.",
    "response": "For each split, we run our enumerator to generate all unique subset sums, reducing the effective search space from 2n/2 potential sums in each half down to U0 and U1 distinct sums (with U0, U1 ≤U). In many practical instances, U is substantially smaller than the worst-case bound. Crucially, we do not need to wait until each enumerator completes in order to check for a viable solution."
  },
  {
    "prompt": "Read and understand this section about split, strategy, logic from a research paper.",
    "response": "Instead, as soon as a new unique subset sum is produced in one split, we combine it in real time with sums (and complement logic) from the other split, using a double meet-in-the-middle strategy. On average, verifying each new partial sum requires requires a constant number of lookups to see whether it forms a solution with a counterpart from the other split—whether by checking a direct complement or merging two partial sums."
  },
  {
    "prompt": "This is a technical discussion about sum, Lemma, procedure in theoretical computer science.",
    "response": "This on-the-fly procedure is supported by the following lemmas: Lemma (1): Let A ⊂ℓx and B ⊂ℓy. Then sum(A ∪B) = t ⇐⇒ t −sum(A) = sum(B). 15 Lemma (2): For each split independently, a unique subset sum corresponding to the complement of a subset (denoted Ac) yields a valid solution if sum(Ac) = sum(ℓ) −t. Lemma (3): For two splits, if the complements of subsets from both splits (denoted Ac and Bc) satisfy sum(Ac ∪Bc) = sum(S) −t, then a valid solution is found."
  },
  {
    "prompt": "Scientific context on Similarly, subset, sums and their role in graph theory.",
    "response": "Lemma (4): Similarly, if a subset from one split and the complement of a subset from the other (denoted Bc) satisfy sum(A ∪Bc) = t, then a valid solution is obtained. These lemmas cover all ways to combine sums from the two splits. By working with the unique subset sums (of effective size U) rather than all 2n/2 candidate sums, the number of candidate combinations is dramatically reduced in structured instances, directly reflecting the efficiency gained by our enumerator."
  },
  {
    "prompt": "Explain a concept involving merge, time, half in computational complexity.",
    "response": "By merging complementary sums from each half, our method verifies candidate solutions in constant time on average, thereby avoiding the costly sort/merge/list-scan step of the classical approach. For the complete pseudocode of our double meet-in-the-middle algorithm, please refer to Appendix A. 6.1 Time Complexity Analysis Splitting the input S into two lists ℓ0 and ℓ1 takes O(n) time."
  },
  {
    "prompt": "Scientific context on split, enumerator, enumerate and their role in graph theory.",
    "response": "For each split, we enumerate all unique subset sums using our modified enumerator (which generates k-subsets for k ≤n 2 )."
  },
  {
    "prompt": "Read and understand this section about Lemma, case, verification from a research paper.",
    "response": "In the worst case, the total number of k-subsets generated per split is bounded by n/2 X k=0 \u0012n/2 k \u0013 ≤2 \u0012n/2 n/4 \u0013 Since both splits are processed independently and the solution verification (using Lemma (1) and related checks) incurs only a logarithmic cost per k-subset, the overall time complexity of the solver is e O \u0012\u0012n/2 n/4 \u0013\u0013 ≈e O \u00122n/2 √n \u0013 ≈e O \u0010 2n/2\u0011 which in structured instances effectively becomes e O(U), where U ≪2n/2."
  },
  {
    "prompt": "Read and understand this section about sums, methods, algorithm from a research paper.",
    "response": "Thus, by parameterizing the algorithm in terms of U (the effective number of unique subset sums), we obtain a refined understanding of instance hardness and a significant practical improvement over classical methods. For the complete pseudocode of our double meet-in-the-middle algorithm, please refer to Appendix A."
  },
  {
    "prompt": "This is a technical discussion about MIM, Compression, Combinatorial in theoretical computer science.",
    "response": "16 7 Meet-in-the-Middle Speed-Up via Combinatorial Tree Compression Meet-in-the-middle (MIM) is a classical approach to Subset Sum (and related problems) that splits the input set S of size n into two lists of size ≈n/2 each. It then enumerates all 2n/2 possible subsets in each half, producing two lists of partial sums, and merges them in time e O(2n/2) [1]."
  },
  {
    "prompt": "This is a technical discussion about factor, correctness, describe in theoretical computer science.",
    "response": "Below, we describe a simple yet effective technique that reduces the exponent by a noticeable constant factor, while preserving correctness. Crucially, we emphasize that this method does not actually modify the problem instance. Instead, it reshapes the combinatorial enumeration tree so that a significant portion of the naive meet-in-the-middle expansions are collapsed (i.e. pruned) earlier. Classical MIM Recap. Recall that in the standard MIM algorithm: 1."
  },
  {
    "prompt": "Understanding the computational aspects of subsets, half, sums.",
    "response": "We split S into two disjoint subsets ℓ0 and ℓ1, each of size n/2. 2. For each half, we enumerate all 2n/2 subsets and record their sums. 3. One list is sorted or hashed; we then look for complementary sums from the other list to see if the target t can be matched. This yields a worst-case time bound on the order of e O(2n/2) and a space usage of O(2n/2). Combinatorial Tree Compression (“Injected Collisions”). We can shrink the exponent by about 0.415 by “forcing” a duplicate inside each half."
  },
  {
    "prompt": "Scientific context on replace, common, describe and their role in graph theory.",
    "response": "A common way to describe this is: in each half ℓi, replace the smallest element by the second-smallest element. However, a clearer interpretation is that we do not truly alter the set S; rather, we inject a short-circuit into the enumeration logic so that many subset sums from ℓi collide earlier. Hence, large portions of the search tree are recognized as duplicates and pruned. Formally, let x0, x1 be the two smallest elements in half ℓ0 (so x0 ≤x1)."
  },
  {
    "prompt": "Read and understand this section about tag, duplicate-forcing, arises from a research paper.",
    "response": "We then tag x0 as a “duplicate-forcing element” that always merges into the branch for x1 if a collision arises. An analogous choice is made in the other half ℓ1. When carrying out MIM, each partial sum also keeps track of a “duplicate-usage flag.” If two partial sums differ only in which copy of x1 they used, they are merged into a single node. Resulting Exponential Reduction. Consider two identical elements, say x0 and x1, in one half of the split."
  },
  {
    "prompt": "This is a technical discussion about contributes, include, elements in theoretical computer science.",
    "response": "Normally, when considering whether to include these two elements, there are four possible choices: 1. Include neither: contributes 0. 2. Include only x0: contributes x0. 3. Include only x1: contributes x1. 4. Include both: contributes x0 + x1. If x0 and x1 are treated as distinct, these four options yield four different sums."
  },
  {
    "prompt": "Understanding the computational aspects of included, cases, force.",
    "response": "However, if we force x0 to behave as a duplicate of x1 — that is, if we merge the cases where only x0 is included with those where only x1 is included — then the two middle cases produce the same sum. As a result, the four possibilities collapse into only three unique outcomes. 17 In effect, for every block of 4 potential combinations, we obtain only 3 unique sums. This is a reduction by a factor of 3 4 (i.e., we retain 75% of the original count)."
  },
  {
    "prompt": "Scientific context on half, sums, yields and their role in graph theory.",
    "response": "When applied uniformly to an entire half that originally yields 2n/2 sums, the new count becomes 0.75 × 2n/2 We can express the multiplier 0.75 as: 0.75 = 2−log2(4/3) where log2 \u00124 3 \u0013 ≈0.415 Thus, the total number of unique subset sums in that half becomes: 2n/2−0.415 This means that, while the worst-case complexity remains exponential, the effective exponent is reduced by about 0.415 for each half of the split."
  },
  {
    "prompt": "This is a technical discussion about terms, produced, unique in theoretical computer science.",
    "response": "In practical terms, if one half originally produced 2n/2 unique sums, forcing a duplicate lowers that number to approximately 2n/2−0.415, leading to a significant constant-factor speedup in the overall algorithm. Key Clarification: Not Modifying the Instance. Despite the phrase “force a duplicate,” we do not literally change any element in the original set S that we want to solve."
  },
  {
    "prompt": "Read and understand this section about subsets, including, enumerator from a research paper.",
    "response": "Instead, we slightly reorganize the enumerator so that subsets including what was x0 always merge with subsets including x1 (and similarly in ℓ1). Hence, it is more accurate to say we inject collisions into the enumerator’s search tree rather than rewriting the input. The target remains the same, and correctness is preserved with a small amount of extra bookkeeping (e.g., a boolean flag to track the “collision merges”)."
  },
  {
    "prompt": "Scientific context on Numerical, effort, sums and their role in graph theory.",
    "response": "Numerical checks comparing sums from ℓ0 and ℓ1 then require only constant additional effort. Further Enhancements As a practical matter, one injected collision in the format of forced duplicate per split is often enough to cut the unique sums in each half by roughly 25%, thereby reducing the exponent from n 2 to about n 2 −0.415."
  },
  {
    "prompt": "Read and understand this section about extra, constrained, split from a research paper.",
    "response": "If memory is especially constrained, one can try forcing two elements per split, potentially cutting unique sums by an additional factor of 0.75 for each extra duplicate. However, each extra duplicate also increases the meet-step overhead (since you must check more “usage-flag” combinations)."
  },
  {
    "prompt": "This is a technical discussion about principle, subsets, overhead in theoretical computer science.",
    "response": "In principle, the idea can be extended further or combined with other collision-based heuristics to prune even more subsets, but each additional forced duplicate must balance enumeration savings against added overhead. Hence the “one or two forced duplicates per half” approach is usually a sweet spot, though more sophisticated or specialized schemas can offer further memory and time savings if carefully managed. Half-Subset Enumeration Plus Injected Collisions."
  },
  {
    "prompt": "Understanding the computational aspects of subsets, half, default.",
    "response": "By default, our enumerator lists only those subsets of size at most n/4 in each half (relying on complements for larger subsets). This alone cuts naive meet-in-the-middle enumeration by a factor of 2, leaving 2 n/2−1 expansions per half instead of 2n/2. 18 Injection a collision by forcing one duplicate in each split (that is, exactly one forced duplicate per half) reduces the overall enumeration cost by an additional 25% factor, i.e. multiply by 0.75."
  },
  {
    "prompt": "Read and understand this section about combining, yields, practice from a research paper.",
    "response": "Hence, combining with half-subset enumeration yields 0.5 × 0.75 = 0.375 so we end up enumerating only about 37.5% of the naive 2n/2 expansions in practice. If we force two duplicates in each split, we observe another 25% compound reduction ( 0.752 = 0.5625), giving 0.5 × 0.752 = 0.28125 or about 28% of the naive expansions."
  },
  {
    "prompt": "Explain a concept involving Guaranteed, Pruning, Universality in computational complexity.",
    "response": "Universality and Guaranteed Pruning A final point is that this combinatorial tree compression via the forced-duplicate method applies uniformly to all input instances, whether highly structured or not; it integrates seamlessly into our enumerator with no special-case logic or side effects. In essence, once duplicates are forced, every instance behaves as though it had some intrinsic collision structure, because large parts of the combinatorial tree merge early."
  },
  {
    "prompt": "Scientific context on Divide, Conquer, Running and their role in graph theory.",
    "response": "8 Divide and Conquer for Anytime Running Time In this section, we present a divide-and-conquer variant of our baseline solver without the combinatorial tree compression optimization. This choice lets us illustrate the incremental and “resume” behavior more transparently. Our goal is to show how the algorithm can progressively explore the problem space—whose effective size is determined by U = |Σ(S)| (the number of unique subset sums)—in an anytime fashion."
  },
  {
    "prompt": "Read and understand this section about enumerator, enumeration, variant from a research paper.",
    "response": "This variant also highlights how we leverage the seeding and branch enumeration features of our enumerator. 8.1 Overview of the Optimization Recall that our basic enumeration process expands k-permutations column by column to generate all feasible unique subset sums for each split, thereby ultimately producing a set of distinct sums of size U, which is typically much smaller than the full 2n possibilities."
  },
  {
    "prompt": "Understanding the computational aspects of INPUT, sums, expansion.",
    "response": "In addition to this standard expansion, our enumerator supports seeding: if the initial INPUT contains partially developed k-permutations, their accumulated sums (which contribute to the overall U) are added to the memoization structure, and the enumeration resumes from these prefixes. This seeding capability ensures that intermediate results are retained and re-utilized, thereby connecting successive enumeration cycles while avoiding redundant work over the same unique sums."
  },
  {
    "prompt": "Understanding the computational aspects of Figure, k-subsets, shows.",
    "response": "Figure 1 shows the number of k-subsets generated in each column expansion sub-process for an input instance with n = 48. In this case, the process is executed n/4 = 12 times, yielding a total of 8 388 607 k-subsets per split, which corresponds to the expected 2n/2 2 in the worst case. The memoization structure holds only half of the distinct subset sums, reflecting the true effective search space. 19 Figure 1 Counts of k-subsets enumerated per column expansion (input n = 48)."
  },
  {
    "prompt": "Explain a concept involving Figure, multi, ple in computational complexity.",
    "response": "Our improved approach modifies this behavior by slicing the enumeration into multi- ple cycles rather than performing one long cycle. Each cycle handles a different slice of the problem—updating and extending the memoized unique subset sums (up to size U) incrementally—as illustrated in Figure 2 for the same n = 48 instance. Figure 2 Run-time graphs for n = 48 with optimization enabled."
  },
  {
    "prompt": "Understanding the computational aspects of cycle, panel, expansion.",
    "response": "The left panel shows the most computationally expensive column expansion in each cycle, while the right panel illustrates the number of subsets enumerated in each cycle. 8.2 The Look-Ahead and Rescheduling Mechanism Our optimization uses a look-ahead strategy to decide whether to postpone (reschedule) the expansion of a k-permutation to a later cycle, this strategy is customizable, the chosen mechanism for our experiments works as follows: 1."
  },
  {
    "prompt": "Understanding the computational aspects of SUM, column, block.",
    "response": "For each k-permutation about to expand a new column, we examine a block of candidate elements. The size of this block is determined by look-ahead =          0, if 2n > SUM(S), \u0004 n 16 \u0005 +    n 32 + 1, if n 32 > 1, 0, otherwise, otherwise. In very dense instances—i.e., when the total number of possible subset combinations 2n exceeds the available numerical space SUM(S) (the sum of all elements in S)—the search space is saturated and the look-ahead is set to 0."
  },
  {
    "prompt": "Read and understand this section about scales, shown, look-ahead from a research paper.",
    "response": "Otherwise, the look-ahead scales with n as shown above. 20 2. During the i-th column expansion of the current cycle, we consider a block of candidate elements spanning from the i-th position up to the (i + look-ahead)-th position. For each candidate element in this block that is not already included in the current k-permutation, we compute the tentative new subset sum obtained by including that element. 3."
  },
  {
    "prompt": "Scientific context on current, cycle, k-permutation and their role in graph theory.",
    "response": "If this tentative sum has already been recorded in our memoization table (i.e., it is not unique among the U values), we do not expand the current k-permutation in the current cycle. Instead, we determine a future cycle for expansion based on the candidate element’s position within the look-ahead block, futureCycle = currentCycle + (position of the candidate element in the block) and then reschedule the k-permutation to be expanded in that future cycle."
  },
  {
    "prompt": "Scientific context on contributing, early, process and their role in graph theory.",
    "response": "This look-ahead and rescheduling mechanism forces the algorithm to prioritize the expansion of those k-permutations that are most likely to yield new unique subset sums (i.e., contributing to U) early in the process. In contrast, those that are rescheduled—due to diminished potential from redundancy—produce fewer new sums when eventually expanded."
  },
  {
    "prompt": "Scientific context on Note, continuity, cycle and their role in graph theory.",
    "response": "Note that the seeding mechanism ensures that any previously computed unique subset sums are retained and reintroduced in subsequent cycles, thus maintaining continuity. The chosen look-ahead formula and rescheduling strategy ensure that each cycle processes roughly half of the deferred work from the previous cycle, resulting in a geometric decay in the workload."
  },
  {
    "prompt": "Explain a concept involving elements, expansion, practice in computational complexity.",
    "response": "In practice, the look-ahead mechanism is exhausted for each k-permutation as it nears its maximum expansion (approximately n/4 elements), at which point no further rescheduling is required and all pending k-permutations complete their expansion."
  },
  {
    "prompt": "Read and understand this section about work, cycle, cycles from a research paper.",
    "response": "Formally, if we denote by W0 the work (i.e., the number of unique subset sums) performed in the first cycle, then the work in cycle i is approximately Wi ≈W0 2i Thus, the total work across all cycles is the sum of a geometric series: Wtotal = ∞ X i=0 Wi ≈ ∞ X i=0 W0 2i = 2W0 Although the series terminates after a finite number of cycles, this analysis shows that the dominant contribution comes from the early cycles and that the work halves from one cycle to the next."
  },
  {
    "prompt": "Explain a concept involving Anytime, Online, Usage in computational complexity.",
    "response": "8.3 A Brief Example for Anytime/Online Usage To illustrate the incremental power of this approach, suppose you run the solver for two cycles on a split of 16 elements. You can pause the solver and observe the partial unique subset sums (i.e., a portion of Σ(S) with effective size U), or even insert a new element xnew into the same split."
  },
  {
    "prompt": "Understanding the computational aspects of data, unique, k-subsets.",
    "response": "Since the enumerator retains all memoized data (unique sums and its corresponding k-subsets), the arrival of xnew spawns new branches from the relevant prefixes without requiring a full rebuild. This process exemplifies the anytime property: at each cycle boundary, you have a partial solution that may already solve simpler targets or indicate that no solution exists under certain conditions."
  },
  {
    "prompt": "This is a technical discussion about Anytime, Time, Analysis in theoretical computer science.",
    "response": "21 8.4 Experiments and Time Analysis for the Anytime Runtime We conducted a series of experiments on worst-case input instances to demonstrate the anytime behavior of the algorithm. These experiments were run on a system with 64GB RAM and a 12th Gen i7-12700 processor. Figure 3 illustrates how the Unique Subset Sums generation process evolves for input instances of lengths 32, 40, 48, and 56."
  },
  {
    "prompt": "Understanding the computational aspects of panel, cycle, left.",
    "response": "In each graph, the left panel displays the most computationally expensive column expansion in each cycle, while the right panel illustrates the number of subsets enumerated in each cycle. In all cases, the first enumeration cycle generates the most k-subsets, which confirms our expectation from the prioritization logic described earlier—following a geometric series. Figure 3 Anytime run-time graphs for input instances of lengths 32, 40, 48, and 56."
  },
  {
    "prompt": "This is a technical discussion about Intuition, Slicing, Mechanism in theoretical computer science.",
    "response": "8.5 Intuition Behind the Slicing Mechanism A key observation is that the structure of the generated k-permutations exhibits a fractal- like behavior. When visualized by the ordinal positions of the elements, the structure remains consistent as the input size increases; only additional layers are added as the length of the input increases with additional elements. This structural property allows our look-ahead approach to scale efficiently when handled at the proper granularity level."
  },
  {
    "prompt": "Read and understand this section about Time, Solution, Initial from a research paper.",
    "response": "22 8.6 Initial Solution Time Complexity and Improvement Over Time Estimation The worst-case initial solution time complexity for input instances with 2n/2 Unique Subset Sums per split—when the chosen look-ahead and rescheduling mechanism is applied—is given by: O \u0012 poly(n) · \u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0013 ≈e O \u0012\u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0013 and the work per cycle decreases progressively according to: Wi ≈W0 2i where i is the iteration index and the maximum number of expansion cycles is n/4 9 Additive-Combinatorial Underpinnings In this section, we elaborate on how classical results from additive combinatorics underpin the “effective search space” parameter U = |Σ(S)|."
  },
  {
    "prompt": "Understanding the computational aspects of small, doubling, Subset.",
    "response": "In particular, phenomena such as small doubling, high additive energy, duplicate elements, and specialized sequences explain why many real or “structured” instances of Subset Sum exhibit U ≪2n, thereby enabling substantial pruning in our algorithm. 9.1 Small Doubling and Freiman-Type Embeddings A set A ⊂Z (or R) is said to have small doubling if |A + A| ≤C |A| for some constant C. Equivalently, A + A is not much larger than A itself."
  },
  {
    "prompt": "This is a technical discussion about Tao, GAP, Freiman in theoretical computer science.",
    "response": "A fundamental result by Freiman [14] (see also Tao [17] and Tao–Vu [18]) shows that such sets can be embedded into a low-dimensional generalized arithmetic progression (GAP). Roughly stated: ▶Theorem 8 (Freiman’s Theorem, informal). If A ⊂Z satisfies |A + A| ≤C |A|, then there exists a low-dimensional generalized arithmetic progression P such that A ⊆P and |P| ≤α |A| for some constant α. The dimension depends on C."
  },
  {
    "prompt": "Scientific context on explode, combinatorially, addition and their role in graph theory.",
    "response": "Such low-dimensional progressions do not “explode” combinatorially under addition—many partial sums coincide. Consequently, if a set S has small doubling, the total number of distinct subset sums |Σ(S)| can be far below the worst-case 2n bound."
  },
  {
    "prompt": "This is a technical discussion about GAP, size, Freiman in theoretical computer science.",
    "response": "In fact, by applying Freiman’s theorem more directly, any set S ⊂Z of size n with |S + S| ≤K|S| is contained in a generalized arithmetic progression (GAP) of dimension OK(1) and size at most f(K) n; hence, |Σ(S)| ≤nOK(1), which is exponentially smaller than 2n. In our solver (see Section 5), each collision in the combinatorial tree prunes branches that would otherwise enumerate duplicate sums, reducing the naive 2n/2 partial sums to an effective count on the order of |Σ(S)|."
  },
  {
    "prompt": "Explain a concept involving Additive, Energy, Pruning in computational complexity.",
    "response": "9.2 Additive Energy and Collision-Based Pruning An alternative measure of additive structure is the additive energy E(A) of a finite set A, defined by E(A) = {(a, b, c, d) ∈A4 : a + b = c + d} 23 A high additive energy indicates that many pairs (a, b) and (c, d) yield the same sum a + b = c + d."
  },
  {
    "prompt": "Understanding the computational aspects of rich, sums, strong.",
    "response": "This strong overlap again suggests fewer unique sums, so the set A is “collision- rich.” Results like the Balog–Szemerédi–Gowers theorem ([18]) imply that if E(A) ≥|A|3 K , then a large subset of A can be embedded in a small-doubling configuration. Even when a set does not exhibit small doubling prima facie, high additive energy thus forces many collisions in partial sums."
  },
  {
    "prompt": "Understanding the computational aspects of context, branch, energy.",
    "response": "In our context, whenever the enumerator attempts to extend a partial sum that has already been generated, it detects a duplicate and prunes that branch. Hence, large additive energy—and thus frequent collisions—dramatically reduces the effective number of expansions needed. Our enumerator never re-lists an already-encountered sum. High energy =⇒more collisions =⇒far fewer newly generated sums. For more on bounding E(S) and its structural implications, we refer to [19, 20] and references therein."
  },
  {
    "prompt": "Read and understand this section about Specialized, Sequences, Duplicates from a research paper.",
    "response": "9.3 Duplicates and Specialized Sequences Beyond small doubling and high additive energy, other specific properties further diminish |Σ(S)|. Two prominent examples are duplicate elements and specialized sequences that exhibit near-linear or progression-like behavior. 9.3.0.1 Duplicates. When S contains duplicates, many different subsets produce the same sum because repeated values contribute identically."
  },
  {
    "prompt": "Explain a concept involving Sequences, reducing, sums in computational complexity.",
    "response": "This redundancy effectively prunes the search space, reducing the effective number of unique sums. 9.3.0.2 Specialized Sequences. Certain highly structured sequences—such as arithmetic progressions, quadratic sequences, or Fibonacci-like sequences—tend to produce overlapping subset sums, which significantly reduce the effective search space U. For example, a proper arithmetic progression P of length L satisfies (see [18]) |Σ(P)| ≤O(L2) a dramatic reduction from the worst-case 2L."
  },
  {
    "prompt": "Scientific context on contrast, geometric, progressions and their role in graph theory.",
    "response": "In contrast, geometric progressions (with a common ratio of at least 2) typically yield distinct subset sums due to their exponential growth. Ultimately, it is these near-linear or progression-like structures that yield heavy sum collisions, decreasing U and reducing enumeration cost."
  },
  {
    "prompt": "Understanding the computational aspects of Container, Methods, contained.",
    "response": "9.4 Container Methods Recent advances in additive combinatorics, particularly hypergraph container methods [21, 22], show that sets with small doubling or high additive energy can be “contained” in families of structured sets (e.g., small GAPs). These techniques yield tight bounds on how many sets of a given size possess such properties, reinforcing the idea that collision-rich instances are relatively common in certain distributions."
  },
  {
    "prompt": "This is a technical discussion about possibilities, reasoning, continu in theoretical computer science.",
    "response": "24 Our algorithm’s dynamic pruning is reminiscent of container-based reasoning: by continu- ally merging branches that yield identical partial sums, we restrict the search to roughly e O(U) expansions rather than enumerating 2n/2 possibilities. Container-based insights thereby help explain why such structure-driven collisions systematically reduce the runtime in practice."
  },
  {
    "prompt": "This is a technical discussion about Additive, Unified, Perspective in theoretical computer science.",
    "response": "9.5 A Unified Perspective on Additive Structure Classical measures like small doubling or high additive energy capture global overlap in sums. Specialized sequences and duplicates drive collisions through more explicit local or combinatorial patterns. We introduce additional parameters, such as the linearity factor λ, clustering factor γ, and duplicate measure δ, primarily to track how redundant the subset sums become: Linearity Factor λ: Gauges how near-linear or AP-like the elements in S are."
  },
  {
    "prompt": "Scientific context on Factor, Clustering, tightly and their role in graph theory.",
    "response": "Clustering Factor γ: Indicates how tightly |Σ(S)| clusters in numerical value (many sums coincide or lie close together so subsequent sum expansions collide). Duplicate Measure δ: Quantifies the effective number of repeated elements, leading to merged branches in enumeration. A low doubling constant or high additive energy is effectively another manifestation of these redundancies—they all cause collisions among the partial sums."
  },
  {
    "prompt": "Scientific context on Crucially, linearity, collisions and their role in graph theory.",
    "response": "Crucially, linearity alone does not guarantee collisions (some arithmetic progressions have large step sizes and yield distinct sums), but dense linear structures do force many overlaps. Consequently, these parameters must be viewed as markers of additive redundancy, dictating how quickly or frequently the enumerator merges branches that share a sum."
  },
  {
    "prompt": "Read and understand this section about Implications, Algorithm, etc. from a research paper.",
    "response": "9.6 Implications for Our Algorithm Whenever S exhibits one or more of these additive structures (small doubling, high energy, large duplicates, near-linear patterns, etc.), it naturally produces overlapping sums, driving down the effective count of unique partial sums U = |Σ(S)|. Our solver’s collision-based pruning directly leverages this overlap: 1. Fewer Explicit Expansions. Each newly formed partial sum is checked against a memo of sums already encountered, pruning branches on any collision."
  },
  {
    "prompt": "This is a technical discussion about expansions, regimes, Adaptive in theoretical computer science.",
    "response": "Consequently, the algorithm invests only e O(U) expansions, often far below 2n/2 in structured regimes. 2. Adaptive to Structure. While “worst-case” dissociative sets see no collisions (leading to the classical 2n/2 behavior), typical or real-world instances often exhibit small doubling, high energy, or near-linearity. The algorithm adapts automatically to those structural properties, pruning aggressively and reducing runtime."
  },
  {
    "prompt": "This is a technical discussion about collision-rich, Sum, perspective in theoretical computer science.",
    "response": "Hence, from a theoretical perspective, these additive-combinatorics results justify why many Subset Sum instances do not suffer exponential blow-up: they reside in a “collision-rich” region of the input space. From a practical standpoint, detecting such collisions on the fly (rather than enumerating all subsets and only then filtering duplicates) is precisely what our unique-subset-sums enumerator does."
  },
  {
    "prompt": "This is a technical discussion about dynamic, structure-sensitive, observe in theoretical computer science.",
    "response": "This dynamic, structure-sensitive approach underlies the significant empirical speedups we observe. 25 10 Adaptive Time Complexity and Instance Hardness Analysis A key advantage of our algorithm is its inherent adaptivity: its effective time complexity depends not only on the number of elements n and their bit-length w but also on the structure of the input as captured by U = |Σ(S)|."
  },
  {
    "prompt": "Read and understand this section about approach, input, interplay from a research paper.",
    "response": "In our approach, the interplay between dynamic programming and combinatorial enumeration naturally adjusts the computational effort based on the intrinsic characteristics of the input. 10.1 Key Factors Influencing Instance Classification and Hardness Let n denote the number of elements in the set S, and let w = log2(max S) be the bit- length of the largest element."
  },
  {
    "prompt": "Scientific context on factor, additive, structure and their role in graph theory.",
    "response": "In addition to the classical measures of additive structure—such as the doubling constant C and additive energy E(S)—we also consider the extended structural parameters introduced earlier: the linearity factor λ, the clustering factor γ, and the duplicate measure δ. These parameters collectively capture the degree of additive redundancy in S and offer a finer-grained understanding of how collisions among subset sums occur."
  },
  {
    "prompt": "Understanding the computational aspects of Dense, instances, dancy.",
    "response": "We can broadly classify instances as follows: Dense Instances: When w is relatively small compared to n (i.e., the elements are confined to a narrow numerical range) and the extended parameters indicate high redun- dancy (for example, low λ and γ or high δ), many subsets yield identical sums. In these cases, the effective search space U = |Σ(S)| is dramatically smaller than 2n."
  },
  {
    "prompt": "This is a technical discussion about Instances, Sparse, structure in theoretical computer science.",
    "response": "Sparse Instances: When w is large and the input exhibits little additive structure, the number of unique subset sums U approaches its worst-case bound. Mixed Instances: When S contains regions with differing structural properties, the overall behavior is a blend of the above cases. For mixed instances, the input is partitioned at least into two subsets: SD (the dense part) with nD elements and effective bit-length wD, leading to a smaller UD."
  },
  {
    "prompt": "Read and understand this section about part, dense, elements from a research paper.",
    "response": "SS (the sparse part) with nS elements and effective bit-length wS, where US is closer to 2n/2. Naturally, nD + nS = n. 10.2 Time Complexity in Different Regimes The algorithm’s running time adapts to the structure of the instance, which can be broadly categorized into dense, sparse, and mixed regimes. 10.2.1 Dense Regime When the instance is dense, our solver behaves much like a dynamic programming approach on a relatively small numerical range."
  },
  {
    "prompt": "Explain a concept involving sums, early, resulting in computational complexity.",
    "response": "Because collisions occur early, each column expansion prunes many redundant sums, resulting in a small effective numerical range of unique subset sums (i.e., U ≪2n/2). Furthermore, since we only need to expand up to n 4 elements in each split, the running time is typically bounded by Tdense(n) = O \u0010 n 4 · w \u0011 = O(n w) 26 once collisions saturate the partial sums."
  },
  {
    "prompt": "Understanding the computational aspects of Sparse, Regimes, range.",
    "response": "10.2.2 Sparse Regimes When the elements span a large numerical range (so w is comparable to n), collisions are rare, and the algorithm tends to enumerate close to the full meet-in-the-middle bound of e O \u00002n/2\u0001 (i.e., U approaches 2n/2)."
  },
  {
    "prompt": "Read and understand this section about Tsparse, duplicates, sparse from a research paper.",
    "response": "Nonetheless, duplicates and some inherent structure can still reduce the actual branching factor: Unstructured: In the absence of duplicates and additional structure, the solver reverts to near-worst-case time: Tsparse(n) = e O \u00002n/2\u0001 Structured : In the presence of duplicates, additional structure, and/or clustering the solver effectively reduces the number of unique sums, so that Tsparse_structured,dups,additive,cluster(n, δ, λ, γ) ≈ e O \u0010 2 \u0010 n/2−c δ \u0011 ·λ·γ\u0011 10.2.3 Mixed Regime In mixed instances, where the input contains both dense and sparse segments, the overall running time can be approximated as a weighted sum: Tmixed,dups,additive,cluster(n, δ, λ, γ) ≈fD · O(nD wD) + fS · e O \u0010 2 \u0010 n/2−c δ \u0011 ·λ·γ\u0011 where fD and fS are the fractions of the instance exhibiting dense and sparse behavior, respectively, and the effective number of unique sums U in the dense segment is much smaller than in the sparse segment."
  },
  {
    "prompt": "Understanding the computational aspects of Topological, Target, Numerical.",
    "response": "10.3 Topological vs. Numerical Target Perspective In a classical sense, one might guess that Σ(S)/2 is the hardest target because enumerating all subsets up to that value can be combinatorially explosive. However, our column-by-column enumerator is structured so that each level of expansion focuses on all subsets (permutations) of size k before moving on to size k + 1."
  },
  {
    "prompt": "Explain a concept involving half, partial, discovered in computational complexity.",
    "response": "Consequently, partial sums near Σ(S)/2 may be discovered (and pruned, if duplicated) relatively early—often via complementary checks around half of ℓ0 and half of ℓ1. By contrast, a target t that requires (for instance) the maximum k-subset expansions (i.e. near k = n/4 per split) is “topologically deeper” in the enumerator’s tree. The solver only finalizes those largest subsets after it has exhausted all smaller-k expansions."
  },
  {
    "prompt": "Scientific context on standpoint, enumerator, hardest and their role in graph theory.",
    "response": "Hence, from the enumerator’s standpoint, the hardest target is not numerically about being near Σ(S)/2, but rather about being reached only by large or late-expanding subsets. 10.4 Output-Sensitive Perspective A noteworthy aspect of our approach is its output-sensitive nature. In many applications, it is not enough to merely decide whether a target sum t exists; one may need to produce 27 all distinct subset sums (or a representative subset of them)."
  },
  {
    "prompt": "Understanding the computational aspects of distinct, enumerated, sums.",
    "response": "Because our enumeration process generates each new distinct sum exactly once, the total running time is effectively proportional to the number U of unique sums that are actually enumerated. Formally, if enumerating or listing these distinct sums themselves is viewed as part of the required output, then a runtime of O(U) (or e O(U)) is necessarily optimal from an output-sensitive standpoint. That is, one cannot do better than linear time in the size of the output generated."
  },
  {
    "prompt": "Explain a concept involving Adaptive, Experiments, Runtime in computational complexity.",
    "response": "11 Adaptive Runtime Experiments We perform experiments to demonstrate how the cost of generating U depends on various structural properties, and how the adaptive enumeration model self-adjusts during computation to optimally construct the specific combinatorial tree of the input instance. All experiments were executed using the non-anytime version of the algorithm to simplify the runtime analysis to a single cycle."
  },
  {
    "prompt": "This is a technical discussion about Additionally, instance, solution in theoretical computer science.",
    "response": "Additionally, solution finding was disabled to simulate the worst-case scenario—a no-solution instance that requires exhaustive exploration of the problem space. 11.1 Dense Instances In this experiment, we generate a worst-case instance with n = 48 and w = 48 and verify that it indeed produces U = 2n/2 unique subset sums. In this baseline instance, all elements are distinct and sufficiently spread out so that no additional additive structure is exploited."
  },
  {
    "prompt": "This is a technical discussion about experiment, long, ensure in theoretical computer science.",
    "response": "To ensure the accuracy of the experiment, every element is confirmed to be exactly w bits long. This precaution avoids creating a mixed instance, where certain regions of the input might be denser than others. Figure 4 illustrates the effect of density on the enumeration process. We identify a threshold at approximately w ≈32 (corresponding to a density of 1.5), below which density begins to influence the total number of unique subset sums progressively."
  },
  {
    "prompt": "Read and understand this section about bits, effectively, density from a research paper.",
    "response": "We then reduce the bit-length of the elements by 4 bits at each step (effectively shifting the numbers right) to generate instances with higher density. This setup demonstrates the progressive runtime dynamism of the enumerator as the effective search space U is reduced. Figure 4 Counts of k-subsets enumerated per column expansion for a (input n = 48) input instance with elements of varying bit-length from w=32 to w=20 decreasing in steps of 4 bits."
  },
  {
    "prompt": "Scientific context on Subset, Unique, Sums and their role in graph theory.",
    "response": "28 Scenario Density Unique Subset Sums Diff with Previous baseline (n=48/w=48) 1.0 8 388 607 – n=48/w=32 1.50 8,382,135 99.92% n=48/w=28 1.71 8,311,785 99.16% n=48/w=24 2.00 7,061,331 84.96% n=48/w=20 2.40 2,072,581 29.35% n=48/w=16 3.00 227,034 10.95% Table 1 Comparison of Unique Subset Sums and Density for n = 48 with varying bit-length w. Transition Phase Analysis and Time Complexity Approximation. Let w = log2 \u0010 max x∈S x \u0011 be the bit-length of the elements in the input set S."
  },
  {
    "prompt": "Read and understand this section about scenario, exploited, classical from a research paper.",
    "response": "In the worst-case scenario—when no additive structure is exploited—the classical meet-in-the-middle algorithm generates roughly 2n/2 distinct subset sums."
  },
  {
    "prompt": "Scientific context on small, number, elements and their role in graph theory.",
    "response": "However, if the elements in S are small (i.e., w is small), then the total sum of all elements is upper bounded by X x∈S x = O \u0010 n · 2w\u0011 By the pigeonhole principle, even if we consider all possible subsets, the number of distinct subset sums (i.e., the effective search space) is at most U = O \u0010 n · 2w\u0011 Thus, when the input is dense (i.e., when the range of possible sums is much smaller than the number of candidate subsets), many different k-subsets yield the same sum."
  },
  {
    "prompt": "Explain a concept involving Taking, words, phenomenon in computational complexity.",
    "response": "This phenomenon occurs when n · 2w ≪2n/2 Taking logarithms of both sides gives w < n 2 −log2 n In other words, if the bit-length w of the input elements satisfies w < n 2 −log2 n then the effective search space U is bounded by U = O \u0010 n · 2w\u0011 which is exponentially smaller than 2n/2."
  },
  {
    "prompt": "Explain a concept involving threshold, number, enumerator in computational complexity.",
    "response": "This threshold marks the transition phase of the enumerator: when w is below this threshold, the algorithm benefits from a high degree of collisions (i.e., many k-subsets yield 29 the same sum), resulting in a dramatic reduction in the number of branches that must be explored as the density increases. Furthermore, since our enumeration model implements a double meet-in-the-middle strategy, the maximum number of iterations is n/4 (as observed in our experimental runtime graphs)."
  },
  {
    "prompt": "Understanding the computational aspects of Instances, Dense, Extremely.",
    "response": "Consequently, the overall time complexity of the algorithm improves to roughly O \u0010n 4 · 2w\u0011 ≈O \u0010 n · 2w\u0011 which is exponentially better than the worst-case bound when w < n 2 −log2 n. 11.2 Extremely Dense Instances In this experiment, we process several instances with n = 100 and varying w values (16, 20, and 24), with d > 3.5. In extremely dense instances, the maximum average expansion per element is 2w, and the maximum number of column expansions is n/4."
  },
  {
    "prompt": "Understanding the computational aspects of Counts, Figure, input.",
    "response": "Figure 5 Counts of k-subsets enumerated per column expansion (input n = 100 with w = 16, 20, 24). In extremely dense instances, the pigeonhole principle forces so many collisions that the enumeration must expand through all required columns, resulting in a runtime dominated by these dense expansions. This outcome is precisely what one would expect from the inherent column-by-column dynamics of our approach."
  },
  {
    "prompt": "Read and understand this section about Duplicates, Figure, Elements from a research paper.",
    "response": "11.3 Duplicates Elements Figure 6 illustrates the effect of duplicates during enumeration, we start with a worst-case instance of n=48 with 2n/2 unique subset sums (no duplicates) and progressively introduce duplicates, up to 4 per split, duplicating always a different element. 30 Figure 6 Counts of k-subsets enumerated per column expansion (input n = 48)."
  },
  {
    "prompt": "Read and understand this section about subset, previous, unique from a research paper.",
    "response": "We can observe how the most expensive enumeration matches our previous results for worst-case instances with n=48, and how the introduction of a new duplicate reduces the total unique subset sums from the input instance by a 75% from the previous run consistently. Scenario Unique Subset Sums Diff."
  },
  {
    "prompt": "Read and understand this section about Duplicates, Table, Previous from a research paper.",
    "response": "with Previous No Duplicates 8 388 607 – 1 Duplicate 6 291 455 75% 2 Duplicates 4 718 591 75% 3 Duplicates 3 538 943 75% 4 Duplicates 2 654 207 75% Table 2 Number of unique subset sums for various levels of duplication, and the percentage difference from the previous row."
  },
  {
    "prompt": "Explain a concept involving Approximate, Time, Complexity in computational complexity.",
    "response": "Duplicates Approximate Time Complexity Analysis In the worst-case input instances (with no duplicates and no additional additive structure properties) the number of candidate unique subset sums are U = 2n/2, where n is the input length. Suppose that for each duplicate encountered, the number of effective unique subset sums is reduced by a factor of 0.75 as demonstrated by the experiment. This gives rise to the recurrence T(U, δ + 1) = 0.75 T(U, δ) with the base case T(U, 0) = 2n/2."
  },
  {
    "prompt": "Explain a concept involving number, factor, complexity in computational complexity.",
    "response": "Solving this recurrence, we obtain T(U, δ) = 2n/2 · (0.75)δ Since 0.75 = 2−log2(4/3) and log2(4/3) ≈0.415, 31 we can express the reduction factor as (0.75)δ = 2−0.415 δ Thus, the overall time complexity becomes T(U, δ) = 2n/2−0.415 δ This shows that, with δ duplicates, the effective number of branches is reduced by a factor of approximately 2−0.415 δ, leading to a final complexity of eΘ \u0010 2n/2−0.415 δ\u0011 when a varying number of elements have a single duplicate present on its corresponding split."
  },
  {
    "prompt": "Explain a concept involving Additive, Structure, elements in computational complexity.",
    "response": "11.4 Additive Structure In this experiment, we process an instance with n = 48 and introduce arithmetic progressions to observe the effect of additive structure on the number of unique subset sums. First, we insert one arithmetic progression (a sequence) of 3 elements and then extend it to 4 elements. Next, we introduce a disjoint progression to form two sequences—first with 3 elements each, and then extend both sequences to 4 elements each."
  },
  {
    "prompt": "Explain a concept involving split, length, sums in computational complexity.",
    "response": "This experimental design allows us to verify the effect of having one versus two sequences per split, as well as the impact of increasing the sequence length, on the reduction of unique subset sums. Figure 7 Counts of k-subsets enumerated per column expansion (input n = 48) when the input instance includes 1 or 2 arithmetic progressions of 3 and 4 elements."
  },
  {
    "prompt": "Read and understand this section about seq, length, Subset from a research paper.",
    "response": "Scenario Unique Subset Sums Diff with Previous No sequences (2n/2) 8 388 607 – 1 seq – 3 length 7 340 031 87.50% 1 seq – 4 length 5 767 167 78.57% No sequences (2n/2) 8 388 607 – 2 seq – 3 length 6 422 527 76.56% 2 seq – 4 length 3 964 927 61.73% Table 3 Unique Subset Sums and percentage difference for the one and two sequence cases. 32 We observe that the presence of two sequences reduces the number of unique subset sums more than a single sequence does."
  },
  {
    "prompt": "Scientific context on case, extending, approximately and their role in graph theory.",
    "response": "Furthermore, extending the sequence length by one element results in an additional reduction of approximately 9% for the one-sequence case and about 15% for the two-sequence case. This demonstrates that the effect of additive structure compounds, thereby reducing the total number of unique subset sums."
  },
  {
    "prompt": "Understanding the computational aspects of sequences, Fibonacci-like, Additional.",
    "response": "Additional experiments with other types of sequences (e.g., arithmetic progressions, quadratic sequences, and certain Fibonacci-like sequences) confirm that structured sequences with near-linear or progression-like behavior can significantly reduce the total number of unique subset sums below 2n/2.."
  },
  {
    "prompt": "Read and understand this section about Summary, input, structural from a research paper.",
    "response": "11.5 Summary The conducted experiments demonstrates that adaptive behavior of our algorithm is directly tied to the effective search space U which is dependent on the structural characteristics of the input instance. In particular, the overall runtime improves when the input instance exhibits structural characteristics that reduce U relative to the worst-case bound."
  },
  {
    "prompt": "This is a technical discussion about Dense, range, small in theoretical computer science.",
    "response": "For example, when the instance is: Dense: When the elements are confined to a narrow numerical range (i.e., small bit- length w), many subset sums collide, yielding a significantly reduced U and near-dynamic programming performance (O(n w)). Redundant (Duplicates): The presence of duplicate elements inherently reduces the number of distinct subset sums by eliminating redundant branches."
  },
  {
    "prompt": "Read and understand this section about Linearity, Clustering, Structured from a research paper.",
    "response": "Structured (Linearity and Clustering): When the elements exhibit near-linear relationships or are tightly clustered, the resulting additive structure forces many k- subsets to produce the same sum, thereby reducing U. In contrast, in sparse instances where the elements are widely spread and lack significant structure, U remains close to the worst-case 2n/2, and the algorithm reverts to near-worst- case performance."
  },
  {
    "prompt": "Scientific context on hardness, behaviors, instance and their role in graph theory.",
    "response": "For mixed instances, the overall runtime reflects a combination of these behaviors. This refined understanding of instance hardness—captured by U and influenced by factors such as duplicates, linearity, and clustering—enables our solver to gracefully adapt to the specific characteristics of the input without requiring explicit control flow adjustments, specialized pre-processing, or prior knowledge of the instance’s structure. Concluding Remark on Adaptive Hardness."
  },
  {
    "prompt": "Scientific context on additive, structure, doubling and their role in graph theory.",
    "response": "Our experimental and theoretical analysis confirms that the effective search space U = |Σ(S)| is intricately governed by the input’s additive structure—whether measured through small doubling, high additive energy, specialized sequence patterns, or duplicate redundancy. These combinatorial properties directly dictate the number of unique subset sums that our algorithm must process, and thus, they are the key to its adaptive pruning strategy and overall speedup."
  },
  {
    "prompt": "Read and understand this section about input, worst-case, instances from a research paper.",
    "response": "This unified perspective not only explains the observed transition from worst-case to near-dynamic programming performance in structured instances but also provides a clear roadmap for further optimizing NP-complete solvers by focusing on the inherent structure of the input. 33 12 Combining All the Optimizations Our algorithm integrates several key techniques to reduce the effective work required for solving Subset Sum, even in worst-case instances."
  },
  {
    "prompt": "Understanding the computational aspects of Incorporating, Optimizations, Stirling.",
    "response": "We start with an original time complexity bound based on the enumeration of unique subset sums: T(n) = O \u0010 poly(n) · \u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0011 which, after suppressing polylogarithmic factors, can be written as T(n) ≈e O \u0010\u0012n/2 −⌊n/16⌋ n/4 −⌊n/32⌋ \u0013\u0011 Using Stirling’s approximation (and ignoring the floor functions for large n), we set: N = n 2 −n 16 = 7n 16 and K = n 4 −n 32 = 7n 32 Since K/N = 1 2, the binomial coefficient is approximated by \u0012N K \u0013 ≈e O \u0010 2N·H(1/2)\u0011 = e O \u0010 2N\u0011 = e O \u0010 27n/16\u0011 Incorporating Additional Optimizations 1."
  },
  {
    "prompt": "Scientific context on Trick, Complement, half and their role in graph theory.",
    "response": "Complement Trick: Instead of enumerating all subsets in each half (which would be 2n/2), we generate only subsets up to size n/4, since any larger subset is the complement of a smaller one. This effectively reduces the count by a factor of 2, so that in each half we have about 27n/16 2 2. Collision Injection: By forcing one duplicate per half, we introduce a collision that further reduces the number of unique subset sums by a factor of approximately 0.75."
  },
  {
    "prompt": "Read and understand this section about constant, Combining, trick from a research paper.",
    "response": "Combining with the complement trick, the effective number of enumerated subsets becomes: \u001227n/16 2 \u0013 × 0.75 ≈27n/16−1.415 where the constant 1.415 arises from the logarithm in base 2 of the constant factor 2 × 1 0.75−1 ≈2.67 (since 2−1.415 ≈0.375). 3. Anytime (Cyclic) Processing: The enumeration is organized into approximately n/4 cycles, where each subsequent cycle performs roughly half the work of the previous one."
  },
  {
    "prompt": "Understanding the computational aspects of term, geometric, decay.",
    "response": "This geometric decay sums to only a constant multiple of the work done in the first cycle and thus does not change the exponential term."
  },
  {
    "prompt": "Understanding the computational aspects of Time, Worst-Case, Speedup.",
    "response": "Overall Worst-Case Time Complexity and Speedup In the worst-case, the classical meet-in-the-middle algorithm runs in time: TMIM(n) = e O \u0010 2n/2\u0011 = e O \u0010 28n/16\u0011 34 By combining the complement trick, collision injection, and cyclic processing, our algorithm achieves an effective worst-case time complexity of: Tours(n) ≈e O \u0010 27n/16−1.415\u0011 Thus, the speedup factor relative to the classical approach is: 28n/16 27n/16−1.415 = 2 n 16 +1.415 This means that even in the worst-case, where the effective search space is nearly 2n/2, our optimizations yield an exponential speedup of approximately 2n/16 (with an additional constant factor of roughly 21.415), or better if the combinatorial tree is compressed more."
  },
  {
    "prompt": "Understanding the computational aspects of subsets, worst-case, algorithm.",
    "response": "Although our algorithm still operates in exponential time in the worst-case, the combined techniques ensure that it always enumerates strictly fewer than 2n/2 subsets."
  },
  {
    "prompt": "This is a technical discussion about Table, Anytime, Online in theoretical computer science.",
    "response": "Table 4 summarizes the effects of the discussed optimizations: Technique Multiplicative Factor Exponent Reduction (bits) Effective Complexity Classical MIM 1 0 e O(2n/2) Half-Subset Enumeration 1/2 1 e O(2n/2−1) Collision Injection 0.75 log2(1/0.75) ≈0.415 e O(2n/2−1.415) Anytime/Online – n/16 + 1.415 e O \u0010 27n/16−1.415\u0011 Table 4 Summary of how each optimization step reduces the effective time complexity exponent."
  },
  {
    "prompt": "Read and understand this section about Comparison, Table, Existing from a research paper.",
    "response": "13 Comparison with Existing Solvers Table 5 summarizes how traditional state-of-the-art approaches compare with the proposed adaptive, structure-aware solver. 35 Algorithm / Ap- proach Worst-Case Time Memory Structure Use? Anytime- Online?"
  },
  {
    "prompt": "Explain a concept involving Bellman, Partial, Shamir in computational complexity.",
    "response": "Key Charac- teristics Limitations Dynamic Pro- gramming (Bellman)[9] O(n · t) (pseudo- polynomial) O(t) Minimal No Exact if t is small; conceptu- ally simple Infeasible if t large Koiliaris–Xu (2017)[10] e O(t√n) O(t) Partial (fast convolution in DP) No Pseudopolynomial FFT-based method for Subset Sum Performance tied to t; not fully structure- adaptive Schroeppel– Shamir (1979)[23] e O(2n/2) e O(2n/4) None No Reduced mem- ory vs."
  },
  {
    "prompt": "Scientific context on Horowitz, Sahni, classi and their role in graph theory.",
    "response": "classi- cal MIM (BFS merge of partial sums) Still 2n/2 in time; no struc- tural adaptivity; more complex to implement Classical Meet- in-the-Middle (Horowitz & Sahni)[1] e O(2n/2) e O(2n/2) None No Deterministic splitting and merging Worst-case exponential; sorting/merging overhead Randomized Methods (e.g."
  },
  {
    "prompt": "Scientific context on Graham, Joux, Partial and their role in graph theory.",
    "response": "Howgrave– Graham & Joux)[8, 2] Often sub-2n/2 on average Similar to MIM (e O(2n/2)) Partial (prob- abilistic colli- sions) No Often faster in practice; ex- ploits collisions probabilistically No exact guaran- tee; performance variability Recent De- terministic Improvements (e.g."
  },
  {
    "prompt": "Understanding the computational aspects of worst, Varies, Proposed.",
    "response": "Bringmann, Nederlof et al.)[11, 12] e O(2n/2) Varies (often exponential) Some structure usage No Improved bounds in certain regimes Still 2n/2 worst- case; less dy- namic adaptiv- ity Adaptive, Structure- Aware Solver (Proposed) e O(2n/2) worst- case; effectively e O(U) in struc- tured cases Reduced vs."
  },
  {
    "prompt": "Explain a concept involving dupli, cations, MIM in computational complexity.",
    "response": "MIM; e O(U) if U ≪2n/2 Yes (collision pruning, dupli- cations, etc.) Yes (incremen- tal & online) Dynamic collision- pruning; enumerates only distinct sums; double MIM + DP Exponential if no struc- ture; but real speedups in practice Table 5 Comparison of various Subset Sum solvers with the proposed adaptive, structure-aware approach."
  },
  {
    "prompt": "Explain a concept involving Key, Insights, Benefits in computational complexity.",
    "response": "14 Key Insights and Benefits In addition to the core complexity improvements and adaptive features described in previous sections, our algorithm exhibits several further properties that enhance its practical and theoretical appeal. Many of these benefits stem from our focus on the effective search space—namely, the unique subset sums, whose total number is U = |Σ(S)|. We summarize these properties below."
  },
  {
    "prompt": "Scientific context on Incremental, process, Anytime and their role in graph theory.",
    "response": "14.1 Anytime and Incremental Behavior Our algorithm’s slicing and rescheduling techniques enable it to produce intermediate results during the enumeration process. This anytime behavior is particularly valuable in 36 time-critical or resource-constrained environments, as it allows the algorithm to be interrupted at any point while still providing partial, useful outputs and an exact solution upon full execution."
  },
  {
    "prompt": "This is a technical discussion about sums, exploration, ensures in theoretical computer science.",
    "response": "The incremental nature of the combinatorial tree exploration, which tracks only unique subset sums (i.e., up to U distinct sums), ensures that progress is steadily made toward a solution. 14.2 Online Updates as a New Branch Beyond the anytime behavior described in Section 8, our solver can also operate online, handling newly arrived elements without discarding previous partial enumerations."
  },
  {
    "prompt": "Explain a concept involving branch, xnew, practice in computational complexity.",
    "response": "In practice, adding a new element xnew to one of the splits (ℓ0 or ℓ1) is treated exactly like introducing a new “branch” in the enumerator: 1. We record xnew in the relevant split’s data structure and update the total sum of that split accordingly. 2. The algorithm’s standard column-expansion process (including any look-ahead or slicing rules) then applies to xnew just as it would to any other element."
  },
  {
    "prompt": "Understanding the computational aspects of sums, unique, xnew.",
    "response": "Consequently, xnew will contribute to new k-permutations, potentially introducing additional unique subset sums. Importantly, the memoization mechanism continues to track all previously generated unique sums (i.e., the current state of U), ensuring that no work is lost. Because we do not discard or rebuild the enumerations performed so far, all prior informa- tion—including the unique subset sums—remains intact."
  },
  {
    "prompt": "Read and understand this section about start, branches, undergoing from a research paper.",
    "response": "The new element’s presence is seamlessly incorporated as if it had been present from the start, with only the new branches (and the corresponding updates to U) undergoing the enumeration steps. 14.3 Adaptive Prioritization and Governability A distinctive benefit of our anytime exploration is that it renders the search process highly governable."
  },
  {
    "prompt": "Scientific context on search, branches, cycle and their role in graph theory.",
    "response": "Since the algorithm operates in configurable discrete cycles—each producing a set of candidate branches that contribute to U—we can reassess the search state after every cycle. Once a cycle completes, the scheduler can reorganize the rescheduled branches by prioritizing those likely to yield new unique subset sums, pruning those that are redundant (i.e., those whose sums already exist in Σ(S)), and even deliberately triggering collisions to block unpromising regions of the search space."
  },
  {
    "prompt": "Read and understand this section about time, enhancing, efficiency from a research paper.",
    "response": "This adaptive prioritization not only guarantees the delivery of partial solutions at any interruption point but also enables the solver to be controlled and fine-tuned in real time, enhancing overall efficiency. 14.4 Early Capture of Imbalanced Solutions Any solution that requires an imbalanced distribution of elements between the two splits is typically detected earlier in the enumeration process."
  },
  {
    "prompt": "This is a technical discussion about split, elements, algorithm in theoretical computer science.",
    "response": "In our algorithm, the worst-case scenario arises when ≈n/4 elements are required from at least one split. In contrast, if a valid solution exists that uses fewer than n/4 elements from one split, it will be found in an earlier cycle. The more imbalanced the solution (i.e., the greater the difference in the number of elements chosen from each split), the sooner its corresponding unique subset sum is generated, leading to faster detection of a solution."
  },
  {
    "prompt": "Explain a concept involving Integration, Potential, techniques in computational complexity.",
    "response": "37 14.5 Potential for Integration with Other Heuristics The structural design of our algorithm facilitates the integration of additional heuristics or approximation techniques. In scenarios where an exact solution is not strictly necessary, heuristic methods may be incorporated into the combinatorial tree exploration or the dynamic programming phase to further accelerate computation."
  },
  {
    "prompt": "This is a technical discussion about speed, broadening, framework in theoretical computer science.",
    "response": "By leveraging the reduced effective search space U, such hybrid approaches can balance exactness with speed, broadening the practical utility of our framework. 14.6 Approximation via Bit Clearing and Rounding Error Bound One further advantage of our approach is the ability to trade a small amount of accuracy for significant efficiency gains."
  },
  {
    "prompt": "Scientific context on target, bit-length, effectively and their role in graph theory.",
    "response": "By clearing a fixed number of bits from the input values and the target, we reduce the effective bit-length, effectively pushing the problem into a denser regime where collisions (and hence a smaller U) occur more frequently. Although this introduces a bounded rounding error, it substantially reduces the computational load without compromising overall accuracy. 14.7 Parallelizability The inherent structure of our algorithm naturally lends itself to parallel processing."
  },
  {
    "prompt": "Scientific context on independent, approach, splits and their role in graph theory.",
    "response": "The division of the input into two independent splits (via the double meet-in-the-middle approach) and the independent processing of different slices of the combinatorial tree create multiple subproblems that can be handled concurrently. As each subproblem is focused on generating a portion of the unique subset sums (contributing to U), the workload can be distributed efficiently across multiple cores or nodes, further reducing practical running times."
  },
  {
    "prompt": "Understanding the computational aspects of Recasting, subset, sum.",
    "response": "14.8 Framework for Recasting Other Problems The approach of constructing a combinatorial tree for unique subset sum generation, combined with complement-based matching and dynamic programming integration, provides a versatile framework. This framework is not limited to the Subset Sum problem; it can be generalized to tackle other NP-hard combinatorial problems that exhibit similar structural properties."
  },
  {
    "prompt": "Understanding the computational aspects of worst-case, space, bound.",
    "response": "By focusing on the effective search space U rather than the worst-case 2n bound, our method paves the way for new algorithmic paradigms that are both structure-aware and practically efficient. 14.9 Space Complexity While our algorithm retains the same worst-case time complexity as the classical meet-in- the-middle approach, i.e., e O \u0012\u0012n/2 n/4 \u0013\u0013 ≈e O \u0010 2n/2\u0011 in practice it requires less than half the memory in the worst-case."
  },
  {
    "prompt": "This is a technical discussion about sums, focusing, unique in theoretical computer science.",
    "response": "By focusing on storing only the unique subset sums (of effective size U), the memory footprint is significantly reduced in all instances. 38 15 Emergent Dynamism and Instance Hardness A feature of our algorithm is its ability to adapt dynamically to the structure of the input by focusing on the effective search space—namely, the unique subset sums (U = |Σ(S)|)."
  },
  {
    "prompt": "Explain a concept involving tree, unique, Instance in computational complexity.",
    "response": "Without requiring any global knowledge of the complete combinatorial tree, the algorithm incrementally extends partial k-permutations only when the resulting k-subset sum is unique, thereby keeping U as small as possible. 15.1 Instance Hardness Classification The effective complexity of an instance is determined by the number of unique subset sums U, rather than by the total number of subsets."
  },
  {
    "prompt": "Read and understand this section about Hard, Instances, worst-case from a research paper.",
    "response": "In practice: Hard Instances are those for which U is close to the worst-case 2n/2, necessitating nearly full exploration. However, when the combinatorial tree is compressed, by injecting collisions in the format of one or two forced duplicates (as in Section 7), even these “worst-case” inputs are forcibly compressed below 2n/2 unique sums, ensuring that the effective enumeration remains strictly sub-2n/2."
  },
  {
    "prompt": "Explain a concept involving duplicates, near-linearity, tering in computational complexity.",
    "response": "sub-2n/2 Instances exhibit inherent structure (e.g., duplicates, near-linearity, or clus- tering) that naturally produces far fewer distinct sums, allowing extensive pruning even without explicit modifications."
  },
  {
    "prompt": "Read and understand this section about short, arises, structure from a research paper.",
    "response": "In short, while the naive worst-case exponential cost eO(2n/2) arises when an input lacks any detectable structure, many real instances—and indeed any instance combinatorial tree augmented with injected collisions—demonstrate considerably smaller U, and thus a significantly reduced effective search space. 15.2 Scaling Invariance A notable property is that the exploration cost, as measured by U, remains invariant under uniform scaling of the input elements and the target."
  },
  {
    "prompt": "Understanding the computational aspects of factor, remains, unchanged.",
    "response": "Thus, if the input is scaled by a constant factor, the relative structure—and hence the effective U—remains unchanged. This highlights that instance hardness is governed by combinatorial structure rather than absolute numerical values. 16 Implications for NP-Complete Problems and Future Directions This adaptive, structure-aware approach to Subset Sum has implications that extend beyond a single solver."
  },
  {
    "prompt": "Explain a concept involving problems, hardness, improving in computational complexity.",
    "response": "It sheds light on both the nature of instance-specific hardness and potential lines of attack for (or barriers to) improving worst-case bounds in NP-complete problems. 16.1 Broader Significance for NP-Complete Problems Intrinsic Hardness: The minimal computation cost needed to expand the unique subset sums (i.e., to increase U) provides an instance-specific measure of hardness."
  },
  {
    "prompt": "Read and understand this section about blanket, worst-case, input from a research paper.",
    "response": "This effectively shifts the perspective from a blanket 2n (or 2n/2) worst-case assumption to the actual number of distinct sums that materialize for a given input. Reframing Open Problems: Improving upon the eO(2n/2) bound (in a deterministic sense) would entail reducing the “effective search space” U as well as compressing the 39 process needed to enumerate or detect those sums."
  },
  {
    "prompt": "This is a technical discussion about large, growth, collision-driven in theoretical computer science.",
    "response": "The collision-driven enumeration approach clarifies that if U truly remains large, even an optimal pruning strategy still faces exponential growth."
  },
  {
    "prompt": "Understanding the computational aspects of Techniques, practice, challenge.",
    "response": "(Techniques such as our forced-duplicate method illustrate that such compression is feasible in practice, though generalizing these methods to broader settings remains an open research challenge.) Guiding Future Research: By focusing on the effective search space U, future work might explore hybrid strategies, potentially merging collision-based pruning with other algorithmic or combinatorial optimizations."
  },
  {
    "prompt": "Understanding the computational aspects of Discussion, heuristics, partial.",
    "response": "This instance-centric perspective could inform new heuristics, partial dynamic programming methods, or specialized branch-and-bound approaches in broader NP-hard problems. 16.2 Discussion, Lower-Bound Perspectives, and Future Directions While our primary focus is on designing a practical and adaptive solver, the underlying unique-subset-sums enumerator has additional theoretical ramifications: Potential for Real-Time Structural Insights."
  },
  {
    "prompt": "Understanding the computational aspects of sum, rate, effectively.",
    "response": "Our method prunes branches immediately when a newly formed partial sum collides with a previously generated sum, effectively measuring the collision rate in real time. This rate correlates directly with key structural parameters, such as the additive energy and doubling constant of S. A high early collision rate indicates substantial redundancy or near-linearity within the set, while infrequent collisions suggest that the instance is approaching the worst- case 2n/2 behavior."
  },
  {
    "prompt": "Explain a concept involving Section, hardness, runs in computational complexity.",
    "response": "Moreover, the litmus test described in Section 5.4 runs in polynomial time, offering an efficient means to approximate U and assess the intrinsic structural hardness of the input instance. Instance-Focused Hardness. Our approach underpins an instance-based notion of hardness: even though Subset Sum is NP-complete, the difficulty for any particular input is driven by the effective number of distinct sums U = |Σ(S)| rather than by n alone."
  },
  {
    "prompt": "Explain a concept involving statement, bust, metric in computational complexity.",
    "response": "This shifts the complexity lens from a purely worst-case statement (2n/2 or bust) to an adaptive metric. We thus envision new analyses of why certain inputs remain Ω(2n/2) even for an “optimal” collision-pruning algorithm. These analyses could reinforce conditional lower bounds: if even a collision-driven enumerator must enumerate nearly 2n/2 partial sums in a particular input family, that family embodies a near-worst-case instance for all algorithms of this style."
  },
  {
    "prompt": "Explain a concept involving Paths, Lower, Bounds in computational complexity.",
    "response": "Possible Paths to Lower Bounds. Although unconditional lower bounds for Subset Sum remain elusive, our enumerator— being near-optimal in enumerating unique sums—could serve as a framework for showing that no deterministic approach can do better on carefully constructed input families. If such a family provably forces Σ(S) to be large, then even an anytime, collision-aware search must handle an exponential cascade of distinct partial sums."
  },
  {
    "prompt": "This is a technical discussion about fine-grained, instance-based, hardness in theoretical computer science.",
    "response": "This could support new “fine-grained” or “instance-based” hardness proofs, tying enumerator-level arguments directly to structured combinatorial lower-bound research. While fully closing the gap to a strong unconditional bound is beyond current methods, we believe this line of investigation will shed fresh light on why certain Subset Sum instances remain genuinely hard despite the many structural shortcuts discussed in this paper. 40 Future Work."
  },
  {
    "prompt": "This is a technical discussion about Integrate, Real-time, detection in theoretical computer science.",
    "response": "Real-time structure detection: Integrate collision-rate tracking and partial-sum distribution plots to classify inputs automatically. Broader NP-hard problems: Adapt our enumerator to other combinatorial domains where partial states often coincide, e.g., knapsack variants, integer linear programs with repeated coefficients, or specialized SAT instances with symmetrical clauses."
  },
  {
    "prompt": "Scientific context on Investigate, lower-bound, Refined and their role in graph theory.",
    "response": "Refined lower-bound frameworks: Investigate families of sets that remain intractable for any collision-based algorithm, thus supporting stronger lower-bound arguments in a fine-grained or parameterized sense. Altogether, we view the enumerator as not only a powerful practical tool but also a lens for examining deeper additive-structure phenomena and for guiding future complexity-theoretic lines of inquiry."
  },
  {
    "prompt": "Understanding the computational aspects of Conclusions, Sahni, Horowitz.",
    "response": "17 Conclusions We have investigated a novel solver for the Subset Sum problem, improving on the classical meet-in-the-middle algorithm of Horowitz and Sahni [1] in several concrete ways. Our approach is built around the observation that, in practice, the true computational challenge lies in generating only the unique subset sums (i.e., the effective search space of size U = |Σ(S)|), which is typically much smaller than the worst-case 2n possibilities."
  },
  {
    "prompt": "Understanding the computational aspects of Adaptive, Specifically, Instances.",
    "response": "Specifically: Adaptive Behavior Across Instances. As analyzed in Section 10, our solver adapts smoothly to the structure of the input. In dense instances—where high collision rates yield a small U—the algorithm achieves near-dynamic programming performance, while in sparse instances it gracefully reverts to near-worst-case behavior. This instance-dependent adaptivity, measured effectively by U, leads to significant practical speedups on many inputs. Reduced Enumeration Per Split."
  },
  {
    "prompt": "Scientific context on larger, subsets, complements and their role in graph theory.",
    "response": "By enumerating only subsets up to size n/4 in each half, the algorithm systematically captures all “larger” subsets via arithmetic complements. This strategy effectively reduces the number of explicit enumerations—generating roughly 2n/2−1 subsets per half instead of 2n/2—and limits the unique sums produced to U, the much smaller effective space. Elimination of Sorting/Merging Phases."
  },
  {
    "prompt": "Read and understand this section about subset, lookups, sums from a research paper.",
    "response": "Rather than creating two massive lists of subset sums that require sorting (with a cost of e O(n 2n/2)), our method immediately verifies each newly generated subset sum via constant-time hash lookups. This immediate checking directly leverages the fact that only new unique sums (contributing to U) are kept, thus eliminating the expensive sorting step. Anytime and Online Capabilities."
  },
  {
    "prompt": "Scientific context on retained, allowing, slicing and their role in graph theory.",
    "response": "The slicing and rescheduling mechanism ensures that intermediate unique subset sums are retained, allowing the algorithm to be paused or updated online without discarding the work already done. New elements can be incor- porated seamlessly, with the memoization structure continuously tracking the evolving set Σ(S) (and hence U). Adaptive Approximation."
  },
  {
    "prompt": "This is a technical discussion about effective, number, target in theoretical computer science.",
    "response": "By clearing a fixed number of significant bits from both the input values and the target, the algorithm reduces the effective numerical resolution, thereby decreasing the effective U through increased collisions. This controlled approxi- mation introduces only a bounded additive error (within ±2N−1, N=number of cleared 41 bits) while substantially reducing computational workload. Governing the Search Process."
  },
  {
    "prompt": "Understanding the computational aspects of anytime, incremental, design.",
    "response": "Thanks to its anytime, incremental design, the algorithm provides rich intermediate data that can be used to govern subsequent search decisions. After each cycle, the scheduler can dynamically reorganize branches—pruning those that do not yield new unique subset sums (i.e., that do not increase U) and focusing on promising regions of the search space. This fine-grained governability enhances efficiency in later cycles."
  },
  {
    "prompt": "This is a technical discussion about algorithm, sums, central in theoretical computer science.",
    "response": "A central feature of our work is the emerging dynamism of the algorithm—its ability to dynamically adjust to different input types by focusing on the effective unique subset sums (U). This adaptability is crucial for handling real-world problems, where data can vary widely in terms of density, structure, and complexity."
  },
  {
    "prompt": "Read and understand this section about learns, essentially, making from a research paper.",
    "response": "The algorithm essentially “learns” the structure of the input and tailors its exploration accordingly, making it more efficient and capable of tackling previously intractable instances. This adaptive nature directly translates to our algorithm’s practical impact: while the worst-case theoretical complexity remains e O(2n/2), our method achieves significant improvements in real-world applications by exploiting structural properties that reduce U."
  },
  {
    "prompt": "Understanding the computational aspects of theoretical, design, demonstrating.",
    "response": "This work thus bridges theoretical insights from additive combinatorics with practical algorithm design, demonstrating that the most efficient solutions to NP-complete problems may emerge not from universally reducing worst-case complexity but by adapting the solution approach to the problem’s inherent characteristics. Broader Theoretical Implications. Beyond these direct algorithmic contributions, our collision-based enumerator sheds light on deeper instance-focused hardness questions."
  },
  {
    "prompt": "Read and understand this section about table, time, collision from a research paper.",
    "response": "Because each newly formed partial sum is immediately compared against a memoization table, the enumerator effectively tracks the collision rate of sums in real time. High collision rates reveal hidden additive structure—such as small doubling constants or significant repetition—while sparse collisions indicate behavior approaching the worst-case 2n/2 scenario."
  },
  {
    "prompt": "Understanding the computational aspects of Section, step, fact.",
    "response": "Consequently, the enumerator can diagnose or classify inputs by their additive complexity without a separate preprocessing step; in fact, the litmus test described in Section 5.4 runs in polynomial time, offering an efficient approximation of U and a clear measure of an instance’s intrinsic hardness."
  },
  {
    "prompt": "Read and understand this section about optimal, perspective, collision-driven from a research paper.",
    "response": "From a theoretical perspective, an “optimal” collision-driven enumerator that produces every distinct sum exactly once can serve as a litmus test for potential lower bounds: if even such an approach must explore Ω(2n/2) partial sums for certain constructed families, it strongly implies that no faster deterministic method exists for those instances."
  },
  {
    "prompt": "This is a technical discussion about Sum, arguments, reinforcing in theoretical computer science.",
    "response": "Such analyses could bolster instance-based or conditional lower-bound arguments, reinforcing the notion that some Subset Sum instances remain inherently hard despite any structural advantages. Combinatorial Tree Compression and “Hard” Instances. Even for “unstructured” or worst-case inputs—those that otherwise require enumerating up to 2n/2 subsets per half—two straightforward optimizations guarantee a strictly sub-2n/2 enumeration."
  },
  {
    "prompt": "Scientific context on split, complements, enumerate and their role in graph theory.",
    "response": "First, we enumerate only half the subsets per split (those up to size n/4, with the others taken as complements). This alone multiplies the naïve 2n/2 by a factor of 0.5. Second, we force duplicates in each half, which empirically prunes an additional 25% (factor 0.75) per duplicate in each split. Thus, with one duplicate per split, the total search drops to 0.5 × 0.75 = 0.375 42 meaning only about 37.5% of the naïve enumeration remains."
  },
  {
    "prompt": "Scientific context on split, forced, Directions and their role in graph theory.",
    "response": "For two forced duplicates per split, it becomes 0.5 × 0.752 ≈0.28 and so on. Hence, a modest injection of collisions ensures a real constant-factor speedup for every instance, with each additional duplicate further reducing the exponent (up to the point where overhead from extra checks outweighs the benefit). 17.1 Open Directions and Future Research Our work opens several promising avenues for further exploration and enhancement."
  },
  {
    "prompt": "Explain a concept involving Tighter, Bounds, interesting in computational complexity.",
    "response": "In addition to the directions discussed above, we identify the following research paths as particularly interesting: Tighter Parameter Bounds. Investigate improved theoretical bounds for the effective search space U using advanced techniques from additive combinatorics. Refining the analysis of parameters such as the doubling constant C and additive energy E(S) may yield a more precise characterization of instance hardness. Hybrid Randomized-Deterministic Algorithms."
  },
  {
    "prompt": "Understanding the computational aspects of Explore, components, Distributed.",
    "response": "Explore the design of algorithms that blend deterministic guarantees with randomized components. Such hybrid methods could leverage the best aspects of both paradigms, achieving improved average-case performance while still ensuring exact solutions. Parallel and Distributed Implementations. While our approach is inherently parallelizable, further exploration of parallel and distributed algorithms is warranted."
  },
  {
    "prompt": "Scientific context on Problems, Investigating, balancing and their role in graph theory.",
    "response": "Investigating dynamic load balancing, distributed hash table implementations, and fault tolerance mechanisms could lead to additional speedups on modern high-performance computing platforms. Generalization to Other NP-Hard Problems. The framework of unique-sum enumeration, combined with complement-based matching and dynamic slicing, may be adapted to other NP-hard combinatorial problems (e.g., knapsack variants, partition, scheduling, or cryptanalysis)."
  },
  {
    "prompt": "Scientific context on Structures, Data, Studying and their role in graph theory.",
    "response": "Studying these generalizations could pave the way for a broader class of structure-aware exact solvers. Improved Data Structures. Explore the integration of advanced data structures—such as succinct data structures, cache-efficient hash tables, or probabilistic filters—to further reduce memory usage and accelerate lookups during the enumeration and dynamic programming phases. Hybrid Heuristic Strategies."
  },
  {
    "prompt": "Understanding the computational aspects of Investigate, greedy, techniques.",
    "response": "Investigate the potential of incorporating additional heuristic methods (e.g., greedy or local search techniques) into the unified method framework. Such strategies might further reduce the number of cycles required to compress the search space in practice. Refined Complexity Classification. Study the implications of our adaptive approach on the complexity landscape of NP-complete problems."
  },
  {
    "prompt": "This is a technical discussion about investigate, hardness, Remarks in theoretical computer science.",
    "response": "In particular, investigate whether leveraging the concept of the effective search space U and real-time collision metrics can yield conditional lower bounds or a refined classification of problem hardness. Final Remarks."
  },
  {
    "prompt": "Scientific context on barrier, pruning, strategy and their role in graph theory.",
    "response": "Our solver demonstrates that even without breaking the fundamental e O(2n/2) barrier, a careful recombination of enumerator slicing, dynamic collision-based 43 pruning, partial dynamic programming, and a double meet-in-the-middle strategy, com- bined with a highly effective enumeration model, can yield impressive practical gains."
  },
  {
    "prompt": "Read and understand this section about subset, reflected, anytime from a research paper.",
    "response": "By automatically adapting to the structure of the input—reflected in the effective number of unique subset sums U—and offering anytime/online capabilities, our method presents a versatile improvement over classical approaches. We hope these ideas not only accelerate practical Subset Sum instances but also inspire deeper investigation into input structures, leading to refined classifications of instance hardness and novel insights into lower bounds for NP-complete problems."
  },
  {
    "prompt": "Scientific context on References, Ellis, Sahni and their role in graph theory.",
    "response": "44 References 1 Ellis Horowitz and Sartaj Sahni. Computing partitions with applications to the knapsack problem. Journal of the ACM, 21(2):277–292, 1974. 2 Karl Bringmann and Vasileios Nakos. Top-k-convolution and the quest for near-linear output-sensitive subset sum. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, page 982–995. Association for Computing Machinery, 2020. 3 Tim Randolph and Karol Węgrzycki."
  },
  {
    "prompt": "Scientific context on Integer, Doubling, Programming and their role in graph theory.",
    "response": "Parameterized Algorithms on Integer Sets with Small Doubling: Integer Programming, Subset Sum and k-SUM. In 32nd Annual European Symposium on Algorithms (ESA 2024), volume 308 of Leibniz International Proceedings in Informatics (LIPIcs), pages 96:1–96:19, 2024. 4 C. Cheeseman, B. Kanefsky, and W. M. Taylor. Where the REALLY hard problems are. In Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI), pages 331–337, 1991. 5 J. C. Lagarias and A. M. Odlyzko."
  },
  {
    "prompt": "Understanding the computational aspects of subset, sum, Joux.",
    "response": "Solving low-density subset sum problems. J. ACM, 32(1):229–246, 1985. 6 M. J. Coster, A. Joux, B. A. Lamacchia, A. M. Odlyzko, C.-P. Schnorr, and J. Stern. Improved low-density subset sum algorithms. Computational Complexity, 2:111–128, 1992. 7 A. Flaxman and B. Przydatek. Solving medium-density subset sum problems in expected polynomial time. In STACS 2005, volume 3404 of LNCS, pages 305–314. Springer, 2005. 8 Nick Howgrave-Graham and Antoine Joux. New generic algorithms for hard knapsacks."
  },
  {
    "prompt": "Read and understand this section about EUROCRYPT, Gilbert, Advances from a research paper.",
    "response": "In Henri Gilbert, editor, Advances in Cryptology – EUROCRYPT 2010, pages 235–256. Springer Berlin Heidelberg, 2010. 9 Richard Bellman. The theory of dynamic programming. Bulleting of the American Mathe- matical Society, 60(6):503–515, November 1954. 10 Konstantinos Koiliaris and Chao Xu. A faster pseudopolynomial time algorithm for subset sum. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’17, pages 1062–1072, Philadelphia, PA, USA, 2017."
  },
  {
    "prompt": "Read and understand this section about Mathematics, Industrial, Applied from a research paper.",
    "response": "Society for Industrial and Applied Mathematics. 11 Karl Bringmann. A near-linear pseudopolynomial time algorithm for subset sum. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1073–1084. SIAM, 2017. 12 Jesper Nederlof and Karol Węgrzycki. Improving schroeppel and shamir’s algorithm for subset sum via orthogonal vectors. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1670–1683. ACM, 2021."
  },
  {
    "prompt": "Explain a concept involving Amir, Abboud, Karl in computational complexity.",
    "response": "13 Amir Abboud, Karl Bringmann, Danny Hermelin, and Dvir Shabtay. Seth-based lower bounds for subset sum and bicriteria path. In ACM Transactions on Algorithms (TALG), volume 18, pages 1–22, 2023. 14 Gregory A. Freiman. On the addition of finite sets. Doklady Akademii Nauk, 158:1038–1041, 1964. 15 Pekka Orponen, Ker-I Ko, Uwe Schöning, and Osamu Watanabe. Instance complexity. Journal of the ACM, 41(1):96–121, 1994. 16 G. A. Freiman. The addition of finite sets. i."
  },
  {
    "prompt": "Understanding the computational aspects of Zavedenii, Vysshikh, Uchebnykh.",
    "response": "Izvestiya Vysshikh Uchebnykh Zavedenii. Matematika, (6):202–213, 1959. 17 Terence Tao. Structure and randomness in combinatorics, 2007. 18 Terence Tao and Van Vu. Additive Combinatorics. Cambridge University Press, 2006. 19 Imre Z. Ruzsa. Sumsets and structure. In Combinatorial Number Theory and Additive Group Theory, pages 87–210. Birkhäuser, 2009. 20 Giorgis Petridis. New proofs of plünnecke-type estimates for product sets in groups. Combinatorica, 32(6):721–733, 2012. Published 2012/12/01."
  },
  {
    "prompt": "Understanding the computational aspects of Saxton, Samotij, Morris.",
    "response": "45 21 R. Morris, W. Samotij, and D. Saxton. An asymmetric container lemma and the structure of graphs with no induced 4-cycle. ArXiv preprint arXiv:1806.03706, 2018. 22 David Saxton and Andrew Thomason. Hypergraph containers. Inventiones mathematicae, 201(3):925–992, January 2015. 23 Richard Schroeppel and Adi Shamir. A o(2n/2,s = o(2n/4)) algorithm for certain np-complete problems. SIAM Journal on Computing, 10(3):456–464, 1981."
  },
  {
    "prompt": "Understanding the computational aspects of sum, Input, Subset.",
    "response": "46 A Unique Subset sums Enumerator Algorithm Algorithm 1 Unique Subset sums enumerator 1: struct kPerm(sum, elements) { ▷holds a k-permutation instance 2: sum ←0 ▷sum of elements 3: elements[n] ←{0, 0, ...} ▷elements of this k-permutation 4: } 5: 6: S ←{...} 7: n ←len(S) 8: MemoizedSums ←{} 9: uniqueSubsetSumDictionary ←{} 10: 11: Input ←{kP erm(0, {})} 12: 13: procedure Enumerate 14: for i ←0 to n/2 do ▷for all columns 15: Output ←{} ▷Clean the Output 16: for all current ∈Input do ▷for all k-permutations on Input 17: for k ←0 to n −1 do ▷for all elements in S 18: if current.elements[k] = 0 then ▷if element is not included 19: sum ←current.sum + S[k] ▷expand 20: if MemoizedSums.lookup(sum) = 0 then ▷not found?"
  },
  {
    "prompt": "This is a technical discussion about expanded, sum, Output in theoretical computer science.",
    "response": "21: MemoizedSums.add(sum) ←1 ▷set as found 22: expanded ←new kP erm(sum, current.elements) ▷copy current 23: expanded.elements[k] ←1 ▷flag new element as included 24: Output.add(expanded) ▷to the Output list 25: uniqueSubsetSumDictionary.Add(sum, expanded) ▷track k-subsets 26: Input ←Output ▷prepare Input for next column expansion 47 Algorithm 2 Subset Sum Solver with Double Meet-in-the-Middle Optimization 1: struct kPerm(sum, elements, split) { 2: elements[n] ←{0, 0, . ."
  },
  {
    "prompt": "Understanding the computational aspects of current.split, sum, currentSplitSum.",
    "response": ".} ▷indicator vector for inclusion 3: split ←−1 ▷split membership (0 for ℓ0, 1 for ℓ1) 4: sum ←0 ▷sum of elements 5: } 6: ℓ0 ←{} ▷left split 7: ℓ1 ←{} ▷right split 8: MemoizedSumsL0 ←{} ▷Unique sums from ℓ0 9: MemoizedSumsL1 ←{} ▷Unique sums from ℓ1 10: allKSubsetsL0 ←{} ▷k-subsets from ℓ0 11: allKSubsetsL1 ←{} ▷k-subsets from ℓ1 12: SumL0 ←0 ▷Total sum of elements in ℓ0 13: SumL1 ←0 ▷Total sum of elements in ℓ1 14: procedure Initialize(S) 15: for k ←0 to n −1 do 16: if k mod 2 = 0 then 17: ℓ0.add(S[k]) 18: SumL0 ←SumL0 + S[k] 19: else 20: ℓ1.add(S[k]) 21: SumL1 ←SumL1 + S[k] 22: Input.Add(kPerm(0, {}, 0)) ▷empty k-permutation for left split 23: Input.Add(kPerm(0, {}, 1)) ▷empty k-permutation for right split 24: procedure Solver(S, target) 25: Initialize(S) 26: for i ←0 to n 4 −1 do ▷iterate for each column expansion (up to n/2 per split) 27: Output ←{} 28: for all current ∈Input do 29: splitElements ←ℓ0 if current.split = 0, ℓ1 otherwise 30: MemoizedSums ← n MemoizedSumsL0, if current.split = 0 MemoizedSumsL1, if current.split = 1 31: allKSubsets ← n allKSubsetsL0, if current.split = 0 allKSubsetsL1, if current.split = 1 32: theOtherKSubsets ← n allKSubsetsL1, if current.split = 0 allKSubsetsL0, if current.split = 1 33: theOtherMemoizedSums ← n MemoizedSumsL1, if current.split = 0 MemoizedSumsL0, if current.split = 1 34: currentSplitSum ← n SumL0, if current.split = 0 SumL1, if current.split = 1 35: for k ←1 to len(splitElements) do 36: if current.elements[k] = 0 then ▷if element S[k] not yet included 37: sum ←current.sum + splitElements[k] 38: if MemoizedSums.lookup(sum) = 0 then 39: MemoizedSums.add(sum) ←1 40: expanded ←new kP erm(sum, current.elements, current.split) 41: expanded.elements[k] ←1 42: Output.add(expanded) 43: allKSubsets.add(sum, expanded) ▷Compute and store the complementary subset sum: 44: MemoizedSums.add(currentSplitSum −sum) ←1 45: allKSubsets.add(currentSplitSum −sum, expanded) 46: Check(expanded, target, theOtherKSubsets, theOtherMemoizedSums, currentSplitSum) 47: Input ←Output 48: print No solution found’ 49: procedure Check(kSubset, target, theOtherKSubsets, theOtherMemoizedSums, currentSplitSum) 50: if kSubset.sum = target then ▷current sum = t?"
  },
  {
    "prompt": "Scientific context on STOP, print, kSubset and their role in graph theory.",
    "response": "51: print “Subset from a single Split : ” + kSubset 52: STOP 53: if (currentSplitSum −kSubset.sum) = target then ▷k-subsetc = t? 54: print “Subset found on a single Split using a complement subset:” + kSubset 55: STOP 56: if theOtherMemoizedSums[ target −kSubset.sum ] = 1 then 57: theOther ←theOtherKSubsets[target −kSubset.sum] 58: if theOther.sum = target −kSubset.sum then ▷is theOther B or Bc?"
  },
  {
    "prompt": "Understanding the computational aspects of composedSubset, STOP, kSubset.Combine.",
    "response": "59: composedSubset ←kSubset.Combine(theOther) ▷Σ(A ∈ℓx) + Σ(B ∈ℓy) = t 60: print “(A+B) subsets sum found using both splits:” + composedSubset 61: else 62: composedSubset ←kSubset.Combine(theOther.Complement()) ▷Σ(A ∈ℓx) + Σ(Bc ∈ℓy) = t 63: print “(A+B’) subsets sum found using both splits:” + composedSubset 64: STOP 65: if theOtherMemoizedSums[ target −(currentSplitSum −kSubset.sum) ] = 1 then 66: composedSubset ← kSubset.Combine(theOtherKSubsets[ target − (currentSplitSum − kSubset.sum) ]) 67: print “(A’+B’) subsets sum found using both splits:” + composedSubset.Complement() 68: STOP 48 Algorithm 3 Divide-and-Conquer Procedure (Anytime) 1: 2: [...] 3: // The following queues must be processed in order until depleted 4: schedulingQueues ←{{P erm(0, {}, 0), KP erm(0, {}, 1)}, {}, ..., {}} 5: lookAhead ←n/16 ▷Empirically determined 6: [...] 7: 8: procedure ExpandOrSchedule(kP erm, splitElements, splitMemoizedSums, currentCycle) 9: mustExpand ←T rue 10: kIndex ←kP erm.LastDeferralIndex + 1 11: currentStep ←0 12: while currentStep < lookAhead AND mustExpand = T rue do 13: kSubsetSum ←kP erm.sum + splitElements[kIndex] 14: if kIndex < len(splitElements) AND kP erm.elements[kIndex] = 0 then ▷Element not yet included 15: if splitMemoizedSums.lookup(kSubsetSum) = 1 then ▷Sum already found?"
  },
  {
    "prompt": "Read and understand this section about Input, currentStep, kIndex from a research paper.",
    "response": "16: schedulingQueues[currentCycle + currentStep].Add(kP erm) ▷Reschedule 17: kP erm.LastDeferralIndex ←kIndex ▷Update continuation index 18: mustExpand ←F alse ▷Drop from current enumeration 19: kIndex ←kIndex + 1 ▷Next element in look-ahead block 20: currentStep ←currentStep + 1 ▷Increment step count 21: return mustExpand 22: 23: // Changes required in the Solver procedure: 24: 25: procedure Solver(S, target) 26: Initialize(S) 27: currentIteration ←0 28: while len(Input) > 0 do ▷Enable multiple iterations 29: if currentIteration > 0 then ▷After the first iteration 30: Input ←futureIterations[currentIteration −1] ▷Use the schedule array 31: for i ←0 to (n/4) −1 do ▷For the required columns in the split 32: Output ←{} 33: for all current ∈Input do 34: [...] 35: expand ←true 36: expand ←ExpandOrSchedule(current, splitElements, currentIteration, MemoizedSums) 37: if expand = true then 38: [...] ▷Proceed with normal expansion 39: currentIteration ←currentIteration + 1 Code Availability The complete source code implementing the algorithms described in this paper will available at https://github.com/jesus-p-salas/subset-sum-solver soon."
  },
  {
    "prompt": "Understanding the computational aspects of OnDev-LCT, Thwala, Minh.",
    "response": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards Federated Learning Chu Myaet Thwala, Minh N. H. Nguyenb, Ye Lin Tuna, Seong Tae Kima, My T."
  },
  {
    "prompt": "This is a technical discussion about Engineering, University, Florida in theoretical computer science.",
    "response": "Thaic, Choong Seon Honga,∗ aDepartment of Computer Science and Engineering, Kyung Hee University, Yongin-si, Gyeonggi-do 17104, South Korea bVietnam - Korea University of Information and Communication Technology, Danang, Vietnam cDepartment of Computer and Information Science and Engineering, University of Florida, Gainesville, Florida 32611, USA Abstract Federated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple edge devices while preserving privacy."
  },
  {
    "prompt": "Read and understand this section about learning, success, hinges from a research paper.",
    "response": "The success of FL hinges on the efficiency of participating models and their ability to handle the unique challenges of distributed learning. While several variants of Vision Transformer (ViT) have shown great potential as alternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher compu- tational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL."
  },
  {
    "prompt": "This is a technical discussion about devices, bandwidth, size in theoretical computer science.",
    "response": "Since client devices in FL typically have limited computing resources and communication bandwidth, models intended for such devices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data distributions encountered in FL. To address these challenges, we propose OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks with limited training data and resources."
  },
  {
    "prompt": "Read and understand this section about MHSA, LCT, features from a research paper.",
    "response": "Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global repre- sentations of images."
  },
  {
    "prompt": "This is a technical discussion about Extensive, demands, making in theoretical computer science.",
    "response": "Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks. Keywords: convolutional transformer, lightweight, computer vision, federated learning, data heterogeneity 1."
  },
  {
    "prompt": "Explain a concept involving Recent, Introduction, strides in computational complexity.",
    "response": "Introduction Recent trends in deep learning enable artificial intelligence to make significant strides, even surpassing humans in many tasks. When it comes to image understanding, convolutional neural networks (CNNs) have become the de facto among nu- merous types of deep neural architectures."
  },
  {
    "prompt": "Explain a concept involving ageNet, Deng, COCO in computational complexity.",
    "response": "Along with the tremendous size of publicly available image datasets, i.e., Im- ageNet (Deng et al., 2009) and Microsoft COCO (Lin et al., 2014), deep CNNs like ResNet (He et al., 2016) and its variants have been dominating the field of computer vision for decades."
  },
  {
    "prompt": "Scientific context on Howard, Sandler, Han and their role in graph theory.",
    "response": "Evidently, lightweight CNNs, such as MobileNets (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019) and their extensions (Han et al., 2020; Li et al., 2021b), have achieved state-of-the-art performance for many on-device vision tasks since image-specific inductive biases (Lenc and Vedaldi, 2015; Mitchell et al., 2017) allow them to learn visual representations and recognize complex patterns with fewer parameters."
  },
  {
    "prompt": "Understanding the computational aspects of CNNs, Zuo, Chu.",
    "response": "CNNs, however, are spatially local (Zuo et al., 2015), making them ∗Corresponding author Email addresses: chumyaet@khu.ac.kr (Chu Myaet Thwal), nhnminh@vku.udn.vn (Minh N. H. Nguyen), yelintun@khu.ac.kr (Ye Lin Tun), st.kim@khu.ac.kr (Seong Tae Kim), mythai@cise.ufl.edu (My T."
  },
  {
    "prompt": "Read and understand this section about Accuracy, Figure, MACs from a research paper.",
    "response": "Thai), cshong@khu.ac.kr (Choong Seon Hong) 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 MACs (×109) 60 65 70 75 80 85 90 Accuracy (%) 0.21M 0.31M 0.51M 0.91M 0.28M 0.48M 0.42M 0.45M 1.21M 1.23M 0.27M 0.47M 0.67M 0.21M 0.72M OnDev-LCT CCT ViT ResNet MobileNetv2 Figure 1: Performance comparison on CIFAR-10 dataset in the central- ized scenario. The comparison is performed among small model variants, i.e., #Params < 1.3M, while constraining the computational budget within 0.1 × 109 MACs."
  },
  {
    "prompt": "Explain a concept involving size, bubble, area in computational complexity.",
    "response": "Each bubble’s area is proportional to the model size. hard to capture global representations, i.e., contextual depen- dencies between different image regions. Parallel to the widespread use of CNNs in computer vision, transformer architectures (Vaswani et al., 2017), which exploit the multi-head self-attention mechanism, have emerged to be- come an integral part of natural language processing (NLP)."
  },
  {
    "prompt": "Read and understand this section about Jan, arXiv, cs.CV from a research paper.",
    "response": "In response to the dominant success of transformers in large- scale language modeling, attention-based architectures have re- Preprint submitted to Neural Networks January 23, 2024 arXiv:2401.11652v1 [cs.CV] 22 Jan 2024 cently piqued a growing interest in computer vision."
  },
  {
    "prompt": "This is a technical discussion about Wang, Bello, Parmar in theoretical computer science.",
    "response": "Attention- embedded CNN models (Wang et al., 2017; Hu et al., 2018; Bello et al., 2019), transformer-based networks (Parmar et al., 2018; Dosovitskiy et al., 2021; Touvron et al., 2021), and most recently, hybrid architectures of convolutional transform- ers (Wu et al., 2021a; Hassani et al., 2021; Mehta and Rastegari, 2022; Graham et al., 2021; Chen et al., 2022b) have shown their potential in a wide range of vision tasks."
  },
  {
    "prompt": "Scientific context on Transformer, ifications, Dosovitskiy and their role in graph theory.",
    "response": "Vision Transformer (ViT) (Dosovitskiy et al., 2021), adapting the encoder design of NLP transformer (Vaswani et al., 2017) with minimum mod- ifications, shows its superior performance over cutting-edge CNNs in several benchmark tasks. Hence, ViT becomes more prevalent in computer vision for its ability to learn global rep- resentations through attention mechanisms."
  },
  {
    "prompt": "Read and understand this section about Conversely, ters, perfor from a research paper.",
    "response": "Conversely, the general trend towards improving the perfor- mance of transformers is to increase the number of parame- ters, which demands a vast amount of computational resources. Thus, the unprecedented size of ViT hinders its applications on resource-constrained devices."
  },
  {
    "prompt": "Understanding the computational aspects of Dosovitskiy, ViT, pre.",
    "response": "Moreover, ViT requires pre- training on large-scale image datasets, i.e., ImageNet-21K or JFT-300M, to learn generalizable visual representations since it lacks some inductive biases inherent in CNNs (Dosovitskiy et al., 2021), such as translation equivariance and locality."
  },
  {
    "prompt": "Scientific context on Hassani, Mehta, Raste and their role in graph theory.",
    "response": "Re- cent studies focus on combining the strengths of CNNs with transformers and have become acknowledged for their high- performing hybrid architectures of convolutional transform- ers (Wu et al., 2021a; Hassani et al., 2021; Mehta and Raste- gari, 2022; Graham et al., 2021; Chen et al., 2022b)."
  },
  {
    "prompt": "Read and understand this section about techniques, demanding, regimes from a research paper.",
    "response": "However, these hybrid models still have a huge number of parameters or require extensive data augmentation techniques, demanding higher computational resources for training to perform at the same level as CNNs in low-data regimes. On the other hand, for many domains including science and medical research (Varoquaux and Cheplygina, 2022; Ghassemi et al., 2020), it is costly to get a large quantity of annotated data with the size of ImageNet (Deng et al., 2009) for training."
  },
  {
    "prompt": "This is a technical discussion about col, lenging, data in theoretical computer science.",
    "response": "Due to the privacy-sensitive nature of user data in such domains, col- lecting and centralizing the training data also turn out to be chal- lenging. Federated learning (FL) is a distributed model training strategy that guarantees user privacy without the need for data collection. FL enables multiple client devices to collaboratively train their local models without exposing the sensitive data of each client."
  },
  {
    "prompt": "Read and understand this section about ing, FedAvg, averag from a research paper.",
    "response": "The vanilla FL algorithm called federated averag- ing (FedAvg) (McMahan et al., 2017) learns a shared global model by iteratively averaging the local model updates of par- ticipating clients. A successful FL implementation can yield promising results for on-device applications, especially in cer- tain domains where data privacy and confidentiality are crucial."
  },
  {
    "prompt": "This is a technical discussion about Nonetheless, challenges, including in theoretical computer science.",
    "response": "Nonetheless, FL introduces several core challenges, including communication bottlenecks and data heterogeneity (Li et al., 2020a), which need to be considered before it can be applied in real-world distributed systems. Moreover, since client devices can only offer a limited amount of computing resources, models intended for those devices should be lightweight and capable of on-device applications."
  },
  {
    "prompt": "Understanding the computational aspects of lightweight, Convolutional, result.",
    "response": "As a result, it demands an open research question: “Whether a powerful lightweight vision transformer can be efficiently designed for resource-constrained devices?”. In light of this demand, we propose OnDev-LCT: On-Device Lightweight Convolutional Transformers for vision tasks, in- spired by the idea of employing an early convolutional stem in transformers. We introduce image-specific inductive priors to our models by incorporating a convolutional tokenizer be- fore the transformer encoder."
  },
  {
    "prompt": "Explain a concept involving LCT, Specifically, images in computational complexity.",
    "response": "Specifically, we leverage effi- cient depthwise separable convolutions in our LCT tokenizer for extracting local features from images. The multi-head self- attention (MHSA) mechanism in our LCT encoder implicitly enables our models to learn global representations of images. Hence, benefiting from both convolutions and attention mech- anisms, we design our OnDev-LCTs for on-device vision tasks with limited training data and resources."
  },
  {
    "prompt": "Read and understand this section about models, formance, bench from a research paper.",
    "response": "We analyze the per- formance of our models on various image classification bench- marks in comparison with popular lightweight vision models in the centralized scenario. We further explore our OnDev- LCT models in FL scenarios to empower client devices for vi- sion tasks while coping with data heterogeneity, resource con- straints, and user privacy (Li et al., 2020a)."
  },
  {
    "prompt": "Read and understand this section about aug, mentations, computa from a research paper.",
    "response": "Since data aug- mentations during local training can incur additional computa- tion overhead for resource-constrained FL devices, we compare our models with the baselines in FL scenarios without applying data augmentation techniques. Our empirical analysis indicates that OnDev-LCTs significantly outperform the other baseline models in both centralized and FL scenarios under various data heterogeneity settings."
  },
  {
    "prompt": "Understanding the computational aspects of Additionally, dis, cussions.",
    "response": "Additionally, we provide detailed dis- cussions based on our observations from various performance analyses to demonstrate the efficiency and effectiveness of our OnDev-LCTs. Figure 1 depicts the image classification per- formance on the CIFAR-10 (Krizhevsky et al., 2009) dataset for different model families with variants of comparable sizes, i.e., #Params < 1.3M (millions) and multiply-accumulate or MACs < 0.1G (billions), in the centralized scenario."
  },
  {
    "prompt": "Scientific context on Compared, baselines, demands and their role in graph theory.",
    "response": "Compared to the other baselines, we can observe that our OnDev-LCTs can achieve better accuracy with fewer parameters and lower computational demands. 2. Related Works In this section, we highlight the evolution of prior research efforts in computer vision. Then, we give an overview of how FL can be applied for on-device vision applications. Convolutional neural networks."
  },
  {
    "prompt": "This is a technical discussion about Krizhevsky, Deng, record-breaking in theoretical computer science.",
    "response": "Since AlexNet’s (Krizhevsky et al., 2012) record-breaking achievement on ImageNet (Deng et al., 2009), CNN architectures have gained a great deal of traction in computer vision. As the general trend towards better accuracy is to develop deeper and wider networks (Li et al., 2021c), deep CNNs (He et al., 2016; Simonyan and Zisser- man, 2014; Szegedy et al., 2015) that can extract complex fea- tures from images have become popular for vision applications."
  },
  {
    "prompt": "Read and understand this section about Wang, attention, residual from a research paper.",
    "response": "The residual attention networks proposed by Wang et al. (2017) build attention-aware feature maps to provide different repre- sentations of focused image patches as they embed the power of attention mechanisms in CNNs for better classification. How- ever, these improvements in accuracy do not always result in more efficient networks in terms of computation, speed, and 2 size (Khan et al., 2020)."
  },
  {
    "prompt": "Understanding the computational aspects of Varoquaux, Cheplygina, Parekh.",
    "response": "Real-world applications, such as med- ical imaging (Varoquaux and Cheplygina, 2022), self-driving cars (Parekh et al., 2022), robotics (Roy et al., 2021; Neuman et al., 2022), and augmented reality (AR) (Mutis and Ambekar, 2020) require edge devices to run vision tasks promptly un- der limited resources."
  },
  {
    "prompt": "Understanding the computational aspects of Howard, Sandler, Han.",
    "response": "In response to these challenges, several lightweight CNNs (Howard et al., 2017; Sandler et al., 2018; Howard et al., 2019; Han et al., 2020; Li et al., 2021b; Hu et al., 2018; Iandola et al., 2016; Park et al., 2020) have been devel- oped as their flexible and simple-to-train nature may easily re- place deep CNN backbones while minimizing the network size and improving latency."
  },
  {
    "prompt": "Scientific context on Zuo, CNNs, drawback and their role in graph theory.",
    "response": "Yet, CNNs have one major drawback: as they are spatially local, it makes them hard to extract com- plex global interactions among the spatial features of images on a broader range (Zuo et al., 2015; LeCun et al., 2015). Transformers in vision. Dosovitskiy et al. (2021) introduced Vision Transformer (ViT) as the first notable example of using a pure transformer backbone for computer vision tasks."
  },
  {
    "prompt": "Explain a concept involving data, lacking, insufficient in computational complexity.",
    "response": "How- ever, lacking some inductive priors makes it difficult for ViT to enhance its generalization ability for out-of-distribution data samples when training data is insufficient. Hence, transform- ers are data-hungry, demanding extremely large pre-training datasets, such as ImageNet-21K or JFT-300M, to compete with their convolutional counterparts (Khan et al., 2021). This has led to an explosion in model and dataset sizes, limiting the use of ViT among practitioners in low-data regimes."
  },
  {
    "prompt": "Explain a concept involving con, text, transformers in computational complexity.",
    "response": "In the con- text of lightweight and efficient vision transformers, parallel lines of research have emerged, exploring different strategies to accomplish high-performance on-device vision tasks."
  },
  {
    "prompt": "Scientific context on Touvron, Lin, Zhu and their role in graph theory.",
    "response": "Re- searchers have delved into techniques such as knowledge dis- tillation (Touvron et al., 2021), quantization (Li et al., 2022; Lin et al., 2022), pruning (Zhu et al., 2021; Fang et al., 2023), and architectural modifications (Mehta and Rastegari, 2022; Graham et al., 2021) to enhance the efficiency of transformers on vision tasks."
  },
  {
    "prompt": "Read and understand this section about Data, Transformer, Touvron from a research paper.",
    "response": "In an effort to improve upon ViT, the Data- efficient image Transformer (DeiT) (Touvron et al., 2021) in- troduced a transformer-specific distillation strategy that does not require a large quantity of training data. For a given pa- rameter constraint, DeiT still requires huge computational re- sources, with its performance slightly below that of Efficient- Nets (Tan and Le, 2019) with equivalent sizes."
  },
  {
    "prompt": "Scientific context on els, Menghani, addition and their role in graph theory.",
    "response": "In addition, extensive data augmentation techniques and powerful teacher models are prerequisites for the best performance of DeiT mod- els. Nonetheless, models for real-world vision applications should be lightweight and efficient so that they can be deftly run on resource-constrained devices (Menghani, 2023)."
  },
  {
    "prompt": "Explain a concept involving quantization, vision, approaches in computational complexity.",
    "response": "As one of the most powerful model compression approaches, quantization has gained considerable attention due to its po- tential in reducing the computation cost and memory over- head while striving to maintain the performance of vision trans- formers. Quantization involves converting the full-precision or floating-point values (i.e., 32 bits) of weights and activations to integers with lower bit-width, thus accelerating the process- ing of vision transformers in resource-constrained settings."
  },
  {
    "prompt": "Read and understand this section about Liu, QAT, PTQ from a research paper.",
    "response": "To this point, several works have been studied for adapting vi- sion transformers to two distinct families of quantization meth- ods: quantization-aware training (QAT) (Li et al., 2022; Liu et al., 2023) and post-training quantization (PTQ) (Liu et al., 2021; Lin et al., 2022; Yuan et al., 2022; Ding et al., 2022), depending on the trade-off between achieving better accuracy and simpler deployment."
  },
  {
    "prompt": "Explain a concept involving prun, Blalock, Frankle in computational complexity.",
    "response": "With an intuitive idea of compress- ing the model without significant performance sacrifice, prun- ing (Blalock et al., 2020; Frankle et al., 2021) has also proved to be highly effective and practical for accelerating vision trans- formers to varying degrees."
  },
  {
    "prompt": "Understanding the computational aspects of Researchers, var, ious.",
    "response": "Researchers have explored var- ious pruning techniques aimed at removing specific compo- nents of vision transformers, such as attention heads or indi- vidual weights, to create more streamlined variants suitable for resource-constrained devices."
  },
  {
    "prompt": "Explain a concept involving Dong, Hassibi, Sanh in computational complexity.",
    "response": "For instance, unstructured pruning (Dong et al., 2017; LeCun et al., 1989; Hassibi et al., 1993; Sanh et al., 2020) targets individual weights, providing more flexibility in model compression, but it often requires special hardware accelerators or software for model accelera- tion (Han et al., 2016)."
  },
  {
    "prompt": "Read and understand this section about Zhu, Xiang, Yang from a research paper.",
    "response": "Conversely, structured pruning (Zhu et al., 2021; Yu and Xiang, 2023; Yu et al., 2022; Yang et al., 2023; Fang et al., 2023) has gained great interest due to its hardware-friendly nature and ability to reduce inference over- head by removing entire components of vision transformers, such as multi-head self-attention layers, token embeddings, or neurons."
  },
  {
    "prompt": "Understanding the computational aspects of model, size, Nonetheless.",
    "response": "Nonetheless, the choice of the most suitable pruning technique for a particular task depends on several factors in- cluding the target model size, the desired performance level, and the available computational resources, in order to strike a balance between reducing the model size and retaining essential representations and prediction capabilities. Convolutional transformers."
  },
  {
    "prompt": "Read and understand this section about Convolutional, vision, architec from a research paper.",
    "response": "In the context of architec- tural modifications, the Convolutional vision Transformer (CvT) (Wu et al., 2021a) introduced convolutions into ViT for improving its performance and efficiency on vision tasks. A convolutional token embedding is added prior to each hierar- chical stage of CvT transformers, along with a convolutional projection layer, which replaces the position-wise projection of multi-head self-attention in the transformer encoders. Hassani et al."
  },
  {
    "prompt": "Explain a concept involving trans, vision, dispelled in computational complexity.",
    "response": "(2021) dispelled the myth of “data-hungry vision trans- formers” by presenting ViT-Lite with smaller patch sizing for patch-based image tokenization of the original ViT. The same authors introduced a sequential pooling method (SeqPool) in Compact Vision Transformer (CVT) (Hassani et al., 2021) to eliminate the need for a classification token from vision trans- formers."
  },
  {
    "prompt": "Explain a concept involving CCT, Compact, Transformer in computational complexity.",
    "response": "Coupling the strengths of CNNs and ViT, Compact Convolutional Transformer (CCT) (Hassani et al., 2021) and MobileViT (Mehta and Rastegari, 2022) have been proposed as lightweight hybrid models to improve the flexibility of trans- formers in vision tasks. CCT replaced the patch-based tok- enization of ViT with convolutional tokenization that preserves the local spatial relationships between convolutional patches."
  },
  {
    "prompt": "Understanding the computational aspects of Client, Server, MobileViT.",
    "response": "MobileViT introduced a lightweight ViT model by replacing the local processing in convolutions with transformers to learn global representations of images. Thus, in addition to the as- pect of ViT in capturing long-range feature dependencies pro- vided by the attention mechanism, convolution and pooling op- erations of CNNs come up with the translation equivariance and 3 4 3 3 2 1 1 Client 1 Server Client k 1. Server distributes a global model. 2. Client trains a local model. 3."
  },
  {
    "prompt": "This is a technical discussion about model, server, Local in theoretical computer science.",
    "response": "Client sends model updates back to server. 4. Server aggregates model updates into a new global model. 2 Global model On-Device Local model On-Device Local model Figure 2: The vanilla federated averaging (FedAvg) framework. A typical four-step procedure is iterated until convergence. locality, allowing these hybrid models to work better on smaller datasets. To this end, these models desire high-level data aug- mentation techniques to justify their optimal performance in vi- sion tasks."
  },
  {
    "prompt": "Scientific context on CNN, Likewise, large and their role in graph theory.",
    "response": "Likewise, large computational resources are still es- sential for them to beat mobile-friendly CNN models. Recently, Graham et al. (2021) proposed a hybrid neural net- work named LeViT for faster inference of image classification by designing a multi-stage transformer in ConvNet’s clothing. Convolutional blocks of LeViT are built as pyramids for de- creasing the resolution of feature maps with an increasing num- ber of channels."
  },
  {
    "prompt": "Scientific context on learned, per-head, tention and their role in graph theory.",
    "response": "The authors replaced the ViT’s positional embedding with a learned, per-head translation-invariant at- tention bias and redesigned attention-MLP blocks to improve the network capacity. LeViT models are trained following the DeiT’s distillation-driven training with two classification heads; thus, they require a powerful pre-trained teacher model, i.e., RegNetY-16GF, and strong computational resources, i.e., 32 GPUs, to perform 1000 training epochs in 3 to 5 days."
  },
  {
    "prompt": "Scientific context on Chen, Mobile-Former, Howard and their role in graph theory.",
    "response": "Mobile-Former proposed by Chen et al. (2022b) parallelizes MobileNet (Howard et al., 2019) and transformer with a bidi- rectional cross-attention bridge in between. In contrast to the computationally efficient parallel design, Mobile-Former intro- duces additional parameters due to its parameter-heavy classi- fication head."
  },
  {
    "prompt": "Understanding the computational aspects of SDTA, encoder, Maaz.",
    "response": "The EdgeNeXt (Maaz et al., 2022), with its in- novative split depthwise transpose attention (SDTA) encoder, demonstrates remarkable potential for mobile devices through its stage-wise design of hybrid architecture. EdgeNeXt variants surpass previous hybrid models in both accuracy and model size while significantly reducing the computational overhead."
  },
  {
    "prompt": "Scientific context on Rastegari, proposed, Mehta and their role in graph theory.",
    "response": "Mehta and Rastegari (2023) proposed MobileViTv2, an im- proved version of the original MobileViT, to alleviate the main efficiency bottleneck caused by the multi-headed self-attention. The authors introduced a separable self-attention method with linear complexity, making MobileViTv2 suitable for deploy- ment on resource-constrained devices."
  },
  {
    "prompt": "Explain a concept involving Vision, Performer, Lin in computational complexity.",
    "response": "Jeevan and Sethi (2022) adapted linear attention mechanisms to Vision Transformer (ViT) (Dosovitskiy et al., 2021), resulting in the creation of Vision X-formers (ViXs), where X ∈{Performer (P), Lin- former (L), Nystr¨omformer (N)} (Choromanski et al., 2020; Wang et al., 2020c; Xiong et al., 2021). This adaptation led to a remarkable reduction of up to seven times in GPU memory requirements."
  },
  {
    "prompt": "This is a technical discussion about Additionally, con, volutional in theoretical computer science.",
    "response": "Additionally, the authors introduced inductive priors by replacing the initial linear embedding layer with con- volutional layers in ViXs, resulting in a substantial improve- ment in classification accuracy without increasing the model size. Drawing our inspiration from the key concepts presented in those prior works, we aim to develop high-performing, lightweight convolutional transformers for vision applications on devices with limited resources in low-data regimes. Federated learning."
  },
  {
    "prompt": "Understanding the computational aspects of Figure, FedAvg, Federated.",
    "response": "Federated averaging (FedAvg) (McMahan et al., 2017) is the earliest and most widely used FL baseline that executes an iterative model averaging process via a typ- ical four-step procedure, as illustrated in Figure 2. However, FedAvg suffers from performance deterioration when dealing with distribution shifts in training data across multiple clients."
  },
  {
    "prompt": "Scientific context on Step, Figure, Wang and their role in graph theory.",
    "response": "Various works have been proposed in an attempt to enhance the performance of FedAvg for tackling the data heterogeneity by focusing on either two aspects: improvements in local training, i.e., Step 2 of Figure 2 (Li et al., 2020b; Karimireddy et al., 2020; Li et al., 2021a) or improvements in server aggregation, i.e., Step 4 of Figure 2 (Wang et al., 2020a; Hsu et al., 2019; Wang et al., 2020b)."
  },
  {
    "prompt": "Explain a concept involving presents, models, recent in computational complexity.",
    "response": "A recent study on ViT-FL (Qu et al., 2022) presents a new perspective on utilizing ViT in federated vision models. Their empirical analysis demonstrates that “transform- ers are highly robust to distribution shifts in data”. However, the performance of ViT-FL heavily relies on the pre-training dataset, or ViT requires a huge amount of training data to out- perform CNNs when trained from scratch."
  },
  {
    "prompt": "Scientific context on domain, specific, system and their role in graph theory.",
    "response": "Often, it is hard to readily obtain a well-pre-trained ViT model for various domain- specific tasks in a practical distributed system. Moreover, the unprecedented size of ViT (e.g., ≈6M for ViT-Tiny or ≈22M for ViT-S (Qu et al., 2022; Touvron et al., 2022)) may intro- duce hurdles to communication bottlenecks when exchanging the model updates for aggregation."
  },
  {
    "prompt": "Read and understand this section about study, settings, Preliminaries from a research paper.",
    "response": "Hence, in this study, we aim to tackle those challenges by deploying our OnDev-LCTs in FL scenarios under various data heterogeneity settings. 3. Preliminaries In this section, we describe the preliminaries and motivations behind the development of our OnDev-LCT models. Patch-based tokenization."
  },
  {
    "prompt": "Understanding the computational aspects of Doso, number, patches.",
    "response": "In the standard ViT model (Doso- vitskiy et al., 2021), a 2D input image x is split into multiple non-overlapping fixed-size patches: x ∈RH×W×C ⇒xp ∈RN×(P2C), where C is the number of channels, N = HW P2 is the number of patches with (H, W) and (P, P) for the dimensions of original image and patches, respectively. Image patches are flattened into a sequence of 1D vectors (aka tokens), which are then lin- early projected into D-dimensional space: [x1 pE; x2 pE; . . ."
  },
  {
    "prompt": "Read and understand this section about vector, Dog, projection from a research paper.",
    "response": "; xN p E], where E ∈R(P2C)×D is a projection vector. For the purpose 4 ReLU V K Q BN ReLU Linear Classifier Class Cat Dog ..."
  },
  {
    "prompt": "Understanding the computational aspects of Linear, Pwise, Bottleneck.",
    "response": "Linear Depthwise Separable Convolution LayerNorm Multi-Head Self-Attention LayerNorm Dense Dropout MLP block x 2 stride = 2 Linear Bottleneck Residual Linear Bottleneck LCT Encoder Input Image Reshaped Tokenized Patches stride = 1 Linear Bottleneck 3 x 3 Conv2D LCT Tokenizer Reshaping LCT Encoder Sequence Pooling BN BN Linear Depthwise Separable Convolution stride = s 3 x 3 Dwise Conv2D BN ReLU x L M x Expand 1 x 1 Pwise Conv2D Shrink 1 x 1 Pwise Conv2D x R Figure 3: An overview of On-Device Lightweight Convolutional Transformer (OnDev-LCT)."
  },
  {
    "prompt": "Understanding the computational aspects of LCT, row, tokenizer.",
    "response": "The middle row depicts the architecture of an LCT tokenizer, visualizing its core components in the bottom row. The structure of an LCT encoder is illustrated on the right. of classification, an extra learnable xclass token is prepended to the patch embeddings. Since this ViT’s patch-based tokeniza- tion step ignores spatial inductive biases, information along the boundary regions of the input image is lost (Khan et al., 2021)."
  },
  {
    "prompt": "Explain a concept involving Epos, quence, xclass in computational complexity.",
    "response": "Thus, a positional vector Epos ∈R(N+1)×D is encoded to the se- quence, resulting in an input embedding z0 to the transformer encoder as: z0 = [xclass; x1 pE; x2 pE; . . . ; xN p E] + Epos. Although this positional encoding makes it easier to learn com- plex features through visual representations, ViT still requires a large scale of training data to generalize well. Convolutional tokenization."
  },
  {
    "prompt": "Scientific context on Agarap, Hassani, acti and their role in graph theory.",
    "response": "A convolutional tokenization block introduced by CCT (Hassani et al., 2021) consists of a single standard convolution with a ReLU (Agarap, 2018) acti- vation followed by a MaxPool operation. An input image x is tokenized into convolutional patches and reshaped into an input embedding z0 to the transformer encoder as: z0 = MaxPool(ReLU(Conv2D(x))). This convolutional tokenization step makes CCT flexible enough to remove the positional encoding as well by injecting inductive biases into the model."
  },
  {
    "prompt": "Explain a concept involving Sabour, Xinyi, Chen in computational complexity.",
    "response": "However, as the MaxPool op- eration reduces the spatial resolution of the input image, all the relevant information cannot be preserved well (Sabour et al., 2017; Xinyi and Chen, 2019). Moreover, having multiple con- volutional tokenization blocks may speed up the loss of all spa- tial information, which in turn hurts the model performance. Transformer encoder. In the original ViT (Dosovitskiy et al., 2021), each encoder, i.e., l = 1, 2, . . ."
  },
  {
    "prompt": "Explain a concept involving sending, encoder, MHSA in computational complexity.",
    "response": ", L, is stacked on top of one another, sending its output zl to the subsequent encoder. Each encoder contains two main components, such as a multi- head self-attention (MHSA) block and a 2-layer multi-layer perceptron (MLP) block, with a LayerNorm (LN) and residual connections in between. The last encoder outputs a sequence zL of image representations. Sequence pooling."
  },
  {
    "prompt": "Read and understand this section about sequence, LCT, CVT from a research paper.",
    "response": "An attention-based sequence pooling (Se- qPool) in CVT (Hassani et al., 2021) maps the last encoder’s output sequence to the classifier by pooling the relevant infor- mation across different parts of the sequence and eliminates the need for an extra learnable class token. 4. OnDev-LCT Architecture In this section, we present the architecture of our OnDev- LCT by explaining key components in detail. It includes three main parts: LCT tokenizer, LCT encoder, and a classifier."
  },
  {
    "prompt": "Read and understand this section about Figure, LCT, overview from a research paper.",
    "response": "An overview of our OnDev-LCT is illustrated in Figure 3. LCT tokenizer. Inspired by the idea of leveraging a convolu- tional stem for image tokenization, we introduce spatial induc- tive priors into our models by replacing the entire patch-based tokenization of ViT (Dosovitskiy et al., 2021) with an LCT to- kenizer. Given an input image x, our LCT tokenizer encodes it into convolutional patches before forwarding it to the LCT encoder."
  },
  {
    "prompt": "Scientific context on operations, ReLU, Agarap and their role in graph theory.",
    "response": "Our tokenizer starts with M convolution operations: xm = ReLU(BN(Conv2D(x))), m = 1, 2, . . . , M (1) where Conv2D represents a standard 3×3 convolution followed by a BatchNorm (BN) with ReLU (Agarap, 2018) activation."
  },
  {
    "prompt": "This is a technical discussion about Linear, convolution, layer in theoretical computer science.",
    "response": "After the final convolution operation, the output xM is sent to a block of Linear Depthwise Separable Convolution: x′ 0 = LinearDWS(xM), (2) which is composed of a layer of 3 × 3 depthwise convolution followed by a layer of 1 × 1 pointwise convolution: x′ 0,dw = ReLU(BN(DepthwiseConv2D(xM))), x′ 0 = Linear(BN(PointwiseConv2D(x′ 0,dw)))."
  },
  {
    "prompt": "Scientific context on channel, channels, linear and their role in graph theory.",
    "response": "(3) 5 DepthwiseConv2D extracts features through spatial filtering per input channel, while PointwiseConv2D is used to project chan- nels output from the depthwise layer by a linear combination of those channels. Each convolution operation is followed by BatchNorm with a ReLU for depthwise convolution and a Lin- ear activation for pointwise convolution. The output feature map is forwarded through R Residual Linear Bottleneck blocks: x′ r = LinearBottleneck(x′ r−1) + x′ r−1, r = 1, 2, . . ."
  },
  {
    "prompt": "Explain a concept involving Bottleneck, DWS, feature in computational complexity.",
    "response": ", R (4) Each Bottleneck block first shrinks the feature map via a 1 × 1 convolution. DWS convolution is then applied to remap the output to the same dimension as the input feature map: x′ r,shrink = ReLU(BN(PointwiseConv2D(x′ r−1))), x′ r = LinearDWS(x′ r,shrink). (5) Here, instead of using several MaxPool operations, we im- plement DWS convolutions at the heart of our OnDev-LCT to maintain the spatial information of the input image while build- ing up better representations."
  },
  {
    "prompt": "This is a technical discussion about tions, DWS, convo in theoretical computer science.",
    "response": "The amount of computations can also be drastically reduced as DWS breaks the standard convo- lution operation, i.e., channel-wise and spatial-wise computa- tions, into two separate layers. Moreover, the Bottleneck struc- ture reduces the number of parameters and computation costs while preventing information loss of using non-linear functions, such as ReLU (Agarap, 2018))."
  },
  {
    "prompt": "Read and understand this section about LCT, encoder, blocks from a research paper.",
    "response": "The idea of residual blocks is to make our model as thin as possible by increasing the depth with fewer parameters while improving the ability to effectively train a lightweight vision transformer. At the end of the LCT tokenization step, encoded patches are reshaped to be processed through L blocks of LCT encoder: x′ R ∈RHR×WR×CR ⇒z0 ∈RHW×D, (6) where CR = D is the dimension of the input embedding z0. ViT-based LCT encoder."
  },
  {
    "prompt": "Read and understand this section about Dosovitskiy, LCT, MHSA from a research paper.",
    "response": "We construct our LCT encoder by following the encoder design of the original ViT (Dosovitskiy et al., 2021), which comprises MHSA and MLP blocks: z′ l = MHSA(LN(zl−1)) + zl−1, l = 1, 2, . . . , L (7) zl = MLP(LN(z′ l)) + z′ l, l = 1, 2, . . . , L (8) The MHSA block in the encoder receives the input embedding z0 in the form of three parameters Query (Q), Key (K), and Value (V)."
  },
  {
    "prompt": "Explain a concept involving Query, Keys, begins in computational complexity.",
    "response": "Self-attention (SA) begins by computing the dot products of the Query with all Keys and applying a softmax function to obtain the attention weights on the Values. Multi- head enables the multiple calculations of self-attention in par- allel with h independent attention heads: MHSA(Q, K, V) = Concat(SA1, SA2, . . . , SAh)."
  },
  {
    "prompt": "Understanding the computational aspects of MHSA, capture, information.",
    "response": "(9) With the multi-head self-attention (MHSA), our model can capture long-range positional information as each head can pay attention to different parts of the input embedding and capture different contextual information by calculating the associations between image patches in a unique way. The MLP block con- tains two layers with GELU (Hendrycks and Gimpel, 2016) ac- tivation and dropouts."
  },
  {
    "prompt": "This is a technical discussion about LayerNorm, chang, ing in theoretical computer science.",
    "response": "LayerNorm (LN) is applied before each component of the encoder to prevent its values from chang- ing abruptly, improving the training performance. A residual connection is also applied to route the information between the components. Then, the final output sequence zL is forwarded to the sequence pooling (SeqPool) layer. Classifier."
  },
  {
    "prompt": "This is a technical discussion about Hassani, layer, sequence in theoretical computer science.",
    "response": "We apply SeqPool (Hassani et al., 2021) over the output sequence zL to eliminate the use of learnable class token: z = σ(FC(LN(zL))) × zL, (10) where LayerNorm (LN) is applied before passing the sequence to the fully connected (FC) layer, then attention weights are added by a softmax function σ."
  },
  {
    "prompt": "Understanding the computational aspects of information, output, con.",
    "response": "As the output sequence con- tains relevant information across different parts of the input embedding, preserving this information via attention weights can improve the model performance with no additional param- eters compared to the class token and thus significantly reduce the computation overhead. Eventually, the output z is sent to a single-layer classifier: y = FC(z), (11) where y is the final classification output of our OnDev-LCT. 5."
  },
  {
    "prompt": "Explain a concept involving Experiments, section, OnDev in computational complexity.",
    "response": "Experiments In this section, we analyze the performance of our OnDev- LCTs over existing lightweight vision models by conducting extensive experiments on multiple image datasets in both cen- tralized and federated learning scenarios. Datasets. We conduct extensive experiments on six image classification datasets: CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), MNIST (Deng, 2012), Fashion-MNIST (Xiao et al., 2017), EMNIST-Balanced (Cohen et al., 2017), and FEMNIST (Caldas et al., 2018)."
  },
  {
    "prompt": "Explain a concept involving regimes, experiments, study in computational complexity.",
    "response": "Since our study focuses on edge devices with limited resources for tackling vision tasks in low-data regimes, we do not use additional pre-training data for all experiments. CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) datasets consist of 50,000 training samples and 10,000 test samples of 32 × 32 RGB images in 10 and 100 classes, respectively."
  },
  {
    "prompt": "This is a technical discussion about MNIST, Deng, samples in theoretical computer science.",
    "response": "MNIST (Deng, 2012) and Fashion- MNIST (Xiao et al., 2017) datasets contain 60,000 training samples and 10,000 test samples of 28 × 28 single-channel im- ages in 10 classes. EMNIST-Balanced (Cohen et al., 2017) contains 112,800 training samples and 18,800 test samples of 28 × 28 single-channel handwritten character images in 47 bal- anced classes."
  },
  {
    "prompt": "Read and understand this section about data, training, sets from a research paper.",
    "response": "We use the original training and test sets of each dataset for the centralized scenario while dividing the training sets into 10 and 50 client data partitions for the FL scenarios. To control the level of data heterogeneity for each partition, we adjust the concentration parameter β of the Dirichlet dis- tribution (Ferguson, 1973) from {0.1, 0.5, 5}, where smaller β corresponds to a higher degree of data heterogeneity."
  },
  {
    "prompt": "Understanding the computational aspects of Client, clients, Class.",
    "response": "Using 6 0 2 4 6 8 10 Client ID 0 2 4 6 8 10 Class ID 0 1000 2000 3000 4000 (a) For 10 clients (β = 0.1) 0 2 4 6 8 10 Client ID 0 2 4 6 8 10 Class ID 0 500 1000 1500 2000 2500 (b) For 10 clients (β = 0.5) 0 2 4 6 8 10 Client ID 0 2 4 6 8 10 Class ID 200 400 600 800 1000 1200 (c) For 10 clients (β = 5) 0 10 20 30 40 50 Client ID 0 2 4 6 8 10 Class ID 0 500 1000 1500 2000 (d) For 50 clients (β = 0.1) 0 10 20 30 40 50 Client ID 0 2 4 6 8 10 Class ID 0 100 200 300 400 500 600 700 (e) For 50 clients (β = 0.5) 0 10 20 30 40 50 Client ID 0 2 4 6 8 10 Class ID 50 100 150 200 250 (f) For 50 clients (β = 5) Figure 4: Detailed data partitions on CIFAR-10 dataset using Dirichlet distribution."
  },
  {
    "prompt": "This is a technical discussion about Smaller, heterogeneity, corresponds in theoretical computer science.",
    "response": "Smaller β corresponds to higher data heterogeneity. Best viewed in color."
  },
  {
    "prompt": "Explain a concept involving Model, Trans, Params in computational complexity.",
    "response": "Model #Trans #Convs Patch Pos Emb SeqPool #Params ViT-Lite-1/16 1 ✗ 16 × 16 ✓ ✗ 0.25M ViT-Lite-2/16 2 ✗ 16 × 16 ✓ ✗ 0.27M ViT-Lite-1/8 1 ✗ 8 × 8 ✓ ✗ 0.42M ViT-Lite-2/8 2 ✗ 8 × 8 ✓ ✗ 0.45M ViT-Lite-1/4 1 ✗ 4 × 4 ✓ ✗ 1.21M ViT-Lite-2/4 2 ✗ 4 × 4 ✓ ✗ 1.23M CCT-2/2 2 2 ✗ ✓ ✓ 0.28M CCT-4/2 4 2 ✗ ✓ ✓ 0.48M OnDev-LCT-1/1 1 1 ✗ ✗ ✓ 0.21M OnDev-LCT-2/1 2 1 ✗ ✗ ✓ 0.31M OnDev-LCT-4/1 4 1 ✗ ✗ ✓ 0.51M OnDev-LCT-8/1 8 1 ✗ ✗ ✓ 0.91M OnDev-LCT-1/3 1 3 ✗ ✗ ✓ 0.25M OnDev-LCT-2/3 2 3 ✗ ✗ ✓ 0.35M OnDev-LCT-4/3 4 3 ✗ ✗ ✓ 0.55M OnDev-LCT-8/3 8 3 ✗ ✗ ✓ 0.95M Table 1: Detailed architecture of transformer-based model variants."
  },
  {
    "prompt": "Scientific context on Hassani, follow, implementation and their role in graph theory.",
    "response": "We follow the implementation of Hassani et al. (2021) for ViT-Lite variants. this partitioning strategy, each FL client may have relatively few or no data samples in certain classes. Figure 4 visualizes the data distribution of 10 and 50 FL clients for the CIFAR- 10 dataset. Additionally, we evaluate the effectiveness of our OnDev-LCTs in realistic non-IID settings by using the Fed- erated Extended MNIST (FEMNIST) dataset provided by the LEAF benchmark (Caldas et al., 2018)."
  },
  {
    "prompt": "Understanding the computational aspects of EMNIST, MNIST, Cohen.",
    "response": "The FEMNIST dataset is commonly used in the FL literature, as it is designed to repli- cate practical FL scenarios for multi-class (i.e., 62 classes) im- age classification by naturally partitioning the total of 805,263 data samples in Extended MNIST (EMNIST) (Cohen et al., 2017) dataset into 3,550 clients based on the writer of the digit or character."
  },
  {
    "prompt": "Explain a concept involving Caldas, Chen, Shi in computational complexity.",
    "response": "To align with prior FL works (Caldas et al., 2018; Wu et al., 2021b; Chen et al., 2022a; Shi et al., 2022), we sub- sample 5% of the total data (approximately 180 clients) and split it into 80% for training and 20% for testing per client. Implementation details. We implement our experiments using the TensorFlow (Abadi et al., 2015) framework. All experi- ments are conducted on a single NVIDIA RTX 3080 GPU with 10 GB memory."
  },
  {
    "prompt": "Read and understand this section about sample, round, clients from a research paper.",
    "response": "We consider two client sampling strategies for experiments in FL scenarios (except for the FEMNIST dataset): (1) we sample 10 clients where all the clients participate in ev- ery FL round, (2) we sample 50 clients and randomly choose 10 clients to participate in each FL round."
  },
  {
    "prompt": "This is a technical discussion about scenario, Krizhevsky, rounds in theoretical computer science.",
    "response": "For CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009), we train each model for 200 epochs in centralized experiments and conduct 60 commu- nication rounds for the FL scenario with 10 clients and 100 rounds for the FL scenario with 50 clients, with each client training for 5 local epochs."
  },
  {
    "prompt": "Explain a concept involving Adam, model, CIFAR in computational complexity.",
    "response": "For other datasets, except for FEM- NIST, we train each model for 50 epochs in centralized exper- iments and conduct 30 communication rounds for the FL sce- Model Family CIFAR 10 CIFAR 100 MNIST Fashion MNIST EMNIST Balanced FEMNIST Optimizer Type ResNet 5e−3 5e−3 5e−3 5e−3 5e−3 1e−3 Adam MobileNetv2 5e−3 5e−3 3e−3 5e−3 5e−3 3e−3 Adam ViT-Lite 1e−3 1e−3 1e−3 1e−3 1e−3 1e−3 AdamW CCT 1e−3 1e−3 1e−3 1e−3 1e−3 1e−3 AdamW OnDev-LCT 3e−3 5e−3 1e−3 3e−3 3e−3 1e−3 Adam Table 2: Learning rate values and optimizer types for each model family."
  },
  {
    "prompt": "This is a technical discussion about local, Models, decay in theoretical computer science.",
    "response": "Models with AdamW use the default values of β1 = 0.9, β2 = 0.999, and weight decay = 1e−4. nario with 10 clients and 60 rounds for the FL scenario with 50 clients, with each client training for 3 local epochs. For FL ex- periments with the FEMNIST dataset, we conduct 120 commu- nication rounds, with each client training for 3 local epochs per round."
  },
  {
    "prompt": "Understanding the computational aspects of participate, clients, round.",
    "response": "During the communication rounds, we randomly select C clients to participate in training, considering three different settings of C ∈{5, 10, 2 ∼10}, which simulate common sce- narios where clients may be offline or have limited connectivity to participate in every round. The setting 2 ∼10 represents the number of clients selected in each round, varying from 2 clients to 10 clients. We conduct 5 separate runs for each cen- tralized experiment and report the best accuracy values."
  },
  {
    "prompt": "This is a technical discussion about LCT, reported, separate in theoretical computer science.",
    "response": "For FL, reported values are the best median values from the last 5 FL rounds of 3 separate runs. Model variants and baselines. We explore several variants of OnDev-LCT by tuning the number of LCT encoders and the number of standard convolutions in the LCT tokenizer. For instance, “OnDev-LCT-2/3” specifies a variant with 2 LCT encoders and 3 standard convolutions. We compare the per- formance of our OnDev-LCTs with existing lightweight vi- sion models as baselines."
  },
  {
    "prompt": "Scientific context on ResNet-r, Sandler, CIFAR and their role in graph theory.",
    "response": "For pure CNN baselines, we imple- ment small-sized variants of ResNet (He et al., 2016) and Mo- bileNetv2 (Sandler et al., 2018), which are specially designed for CIFAR, denoted by “ResNet-r” where r is the number of layers and “MobileNetv2/α” where α is the width multiplier, respectively."
  },
  {
    "prompt": "Read and understand this section about Hassani, Dosovitskiy, eral from a research paper.",
    "response": "Following (Hassani et al., 2021), we design sev- eral “ViT-Lite” model variants with pure backbones of the vi- sion transformer (Dosovitskiy et al., 2021) by varying numbers of transformer layers and patch sizes. We also implement small “CCT” variants (Hassani et al., 2021) (only with 3 × 3 convolu- tion kernels), denoted with the number of transformer encoders followed by the number of convolutional blocks."
  },
  {
    "prompt": "Scientific context on transformer-based, attention, heads and their role in graph theory.",
    "response": "For all the transformer-based models, we set the embedding dimension as 128 and use 4 attention heads. Table 1 shows the detailed ar- chitecture of transformer-based model variants. Hyperparameter selection. We perform manual hyperparame- ter tuning for each model family and apply the best settings."
  },
  {
    "prompt": "Read and understand this section about Transformers, Convolutional, MNIST from a research paper.",
    "response": "7 Model CIFAR-10 CIFAR-100 MNIST Fashion MNIST EMNIST Balanced #Params MACs Convolutional Neural Networks (Designed for CIFAR) ResNet-20 82.30 42.62 99.49 92.45 88.38 0.27M 0.04G ResNet-32 82.68 46.75 99.50 92.82 88.18 0.47M 0.07G ResNet-44 83.53 48.79 99.58 92.46 88.07 0.67M 0.10G ResNet-56 83.03 48.14 99.50 92.48 88.14 0.86M 0.13G MobileNetv2/0.2 71.79 37.99 99.06 90.16 85.42 0.21M <0.01G MobileNetv2/0.5 77.10 41.71 99.11 89.70 87.66 0.72M <0.01G MobileNetv2/0.75 80.10 44.37 99.16 90.13 86.75 1.39M <0.01G MobileNetv2/1.0 80.50 44.51 99.34 90.53 87.49 2.27M 0.01G Lite Vision Transformers ViT-Lite-1/16 53.71 25.24 97.66 87.37 82.02 0.25M <0.01G ViT-Lite-2/16 53.74 25.21 97.66 87.67 82.23 0.27M <0.01G ViT-Lite-1/8 61.67 32.51 98.04 87.01 83.35 0.42M 0.01G ViT-Lite-2/8 62.60 33.46 97.97 87.30 83.35 0.45M 0.02G ViT-Lite-1/4 68.66 38.43 98.63 90.25 86.22 1.21M 0.04G ViT-Lite-2/4 71.03 41.98 98.65 90.70 86.05 1.23M 0.07G Compact Convolutional Transformers CCT-2/2 79.71 50.75 99.17 91.37 88.93 0.28M 0.03G CCT-4/2 80.92 53.23 99.20 91.73 89.41 0.48M 0.05G On-Device Lightweight Convolutional Transformers (Ours) OnDev-LCT-1/1 84.55 57.69 99.46 92.73 89.52 0.21M 0.03G OnDev-LCT-2/1 86.27 59.17 99.39 92.50 89.18 0.31M 0.04G OnDev-LCT-4/1 86.61 61.36 99.53 92.70 89.39 0.51M 0.05G OnDev-LCT-8/1 86.64 62.62 99.49 92.59 89.55 0.91M 0.08G OnDev-LCT-1/3 85.73 57.66 99.41 92.72 88.96 0.25M 0.07G OnDev-LCT-2/3 86.04 60.26 99.46 93.31 89.60 0.35M 0.08G OnDev-LCT-4/3 87.03 61.95 99.38 92.60 89.49 0.55M 0.09G OnDev-LCT-8/3 87.65 62.91 99.28 92.82 89.45 0.95M 0.12G Table 3: Comparison on image classification performance in the centralized setting."
  },
  {
    "prompt": "Understanding the computational aspects of runs, Cohen, reported.",
    "response": "All reported accuracy (%) values are the best of 5 runs. All experiments are conducted without applying data augmentation or learning rate schedulers. The best accuracy is marked in bold. The best entry of each model family is underlined. In the centralized scenario, the training batch size is set to 256 for EMNIST-Balanced (Cohen et al., 2017) and 128 for others. In FL experiments, we reduce the batch size by half, i.e., 128 for EMNIST-Balanced (Cohen et al., 2017) and 64 for the rest."
  },
  {
    "prompt": "Understanding the computational aspects of FEMNIST, dataset, Lite.",
    "response": "It is important to note that, we set the training batch size to 16 for all experiments with the FEMNIST dataset. We use Adam (Kingma and Ba, 2014) optimizer for OnDev- LCTs and CNN baselines (He et al., 2016; Sandler et al., 2018) while AdamW (Loshchilov and Hutter, 2017a) with weight de- cay = 1e−4 is used for CCT (Hassani et al., 2021) and ViT- Lite (Dosovitskiy et al., 2021) variants. We manually tune the learning rate and use the best value for each model family."
  },
  {
    "prompt": "Explain a concept involving ble, learning, rate in computational complexity.",
    "response": "The selected learning rate values and optimizers are shown in Ta- ble 2. Since we are focusing on a fair comparison of the model performance, we train all the models from scratch without pre- training or applying data augmentation techniques and learning rate schedulers for all experiments by default. These techniques could challenge the fairness of evaluation and provide sensitive results for tuning hyperparameters, especially in FL scenarios."
  },
  {
    "prompt": "Understanding the computational aspects of scenario, label, dur.",
    "response": "However, label smoothing with a probability of 0.1 is used dur- ing the training of all model variants. Comparison in centralized scenario. Table 3 reports image classification performance of our OnDev-LCTs compared to existing lightweight vision models in a centralized scenario. The results show that our OnDev-LCT variants significantly outperform the other baselines while having lower computa- tional demands."
  },
  {
    "prompt": "Understanding the computational aspects of smallest, parameters, model.",
    "response": "Even our smallest OnDev-LCT-1/1 with 0.21M parameters can perform better in most cases compared to other model variants. Our larger variant OnDev-LCT-8/3 further im- proves the performance, especially on CIFAR-10 and CIFAR- 100 (Krizhevsky et al., 2009), achieving 4.62% and 14.77% better than ResNet-56 (He et al., 2016), which has a compa- rable model size and computational demands."
  },
  {
    "prompt": "This is a technical discussion about Hinton, CIFAR, Krizhevsky in theoretical computer science.",
    "response": "Figure 5 shows t-SNE (van der Maaten and Hinton, 2008) visualizations of feature embeddings learned by each model on the CIFAR- 10 (Krizhevsky et al., 2009) test data. Visualizations show that features learned by our proposed OnDev-LCT model are more divergent and separable than those learned by other baselines. Comparison in FL scenarios."
  },
  {
    "prompt": "Read and understand this section about Figure, Class, constrain from a research paper.",
    "response": "We conduct experiments in FL scenarios with small variants of each model family, constrain- ing the number of parameters to 1M (millions) and the com- 8 Class ID 0 1 2 3 4 5 6 7 8 9 (a) OnDev-LCT-1/1 (b) CCT-2/2 (c) ViT-1/8 (d) ResNet-20 (e) MobileNetv2/0.2 Figure 5: t-SNE visualizations of feature embeddings on CIFAR-10 test set learned by centralized models. Best viewed in color."
  },
  {
    "prompt": "Explain a concept involving clients, Table, Comparison in computational complexity.",
    "response": "Model FedAvg with 10 clients CIFAR-10 CIFAR-100 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 67.55 75.43 77.72 27.89 37.08 37.53 ResNet-32 71.55 78.05 80.39 31.34 41.33 42.68 ResNet-44 72.56 77.73 80.55 32.85 43.60 44.81 MobileNetv2/0.2 56.55 67.07 69.44 21.52 30.57 31.67 MobileNetv2/0.5 60.50 72.35 74.27 23.40 33.28 32.58 ViT-Lite-1/8 48.53 58.05 59.70 17.85 27.60 31.37 ViT-Lite-2/8 49.03 58.57 60.02 17.99 28.20 32.32 CCT-2/2 66.45 74.86 78.42 35.21 47.99 50.50 CCT-4/2 66.38 75.36 78.55 39.44 50.43 52.76 OnDev-LCT-1/1 76.02 82.25 84.24 42.64 55.23 58.07 OnDev-LCT-2/1 76.85 82.81 84.95 44.83 56.74 59.53 OnDev-LCT-4/1 78.10 84.01 86.12 47.51 58.85 61.79 OnDev-LCT-8/1 79.70 84.74 86.77 49.75 59.71 62.42 Table 4: Comparison on image classification performance in FL with 10 clients under different data heterogeneity settings."
  },
  {
    "prompt": "Understanding the computational aspects of runs, accuracy, reported.",
    "response": "All reported accuracy (%) values are the best of 3 runs. Three best accuracy values are marked in bold. putation (i.e., MACs) to 0.1G (billions). For ViT-Lite (Has- sani et al., 2021) models, instead of choosing the smallest vari- ants, we choose larger variants with smaller patch sizes that can achieve better accuracy in centralized experiments."
  },
  {
    "prompt": "This is a technical discussion about Table, Krizhevsky, heterogeneity in theoretical computer science.",
    "response": "Table 4 and Table 5 provide the image classification performance of our OnDev-LCTs on CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) datasets compared to the other baselines under various data heterogeneity settings of FL. We can observe that our OnDev-LCTs significantly outperform other baselines in all data heterogeneity settings."
  },
  {
    "prompt": "Understanding the computational aspects of Table, sce, nario.",
    "response": "From Table 4 with a 10-client sce- nario, most of the transformer-based model variants can achieve comparable or only slightly lower results than the centralized performance under less data heterogeneity settings with β val- ues of 5 and 0.5. For CNNs, we can observe a significant gap between their centralized and FL performance, especially for the highest data heterogeneity setting with β = 0.1."
  },
  {
    "prompt": "Scientific context on data, heterogeneity, Remark and their role in graph theory.",
    "response": "Remark- ably, all the existing lightweight baseline models suffer from data heterogeneity in FL, especially the ViT-Lite model vari- ants. Meanwhile, our OnDev-LCT models achieve the best performance and are more robust to data heterogeneity across clients than the other baselines. Similarly, in Table 5 with a 50-client scenario, we can observe that our OnDev-LCTs out- perform the other baselines in all degrees of data heterogeneity."
  },
  {
    "prompt": "Read and understand this section about LCT, MHSA, innova from a research paper.",
    "response": "The reason behind this capability stems from the innova- tive combination of convolutions in our LCT tokenizer and the MHSA mechanism in our LCT encoder, which enables our OnDev-LCTs to strike a balance between local feature extrac- tion and global context representation."
  },
  {
    "prompt": "Understanding the computational aspects of clients, Table, Comparison.",
    "response": "The strength of our LCT tokenizer lies in its efficient depthwise separable convolutions Model FedAvg with 50 clients CIFAR-10 CIFAR-100 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 53.44 69.62 74.84 26.64 33.89 34.93 ResNet-32 55.79 70.50 76.17 26.27 36.89 38.02 ResNet-44 55.78 71.08 76.10 28.91 36.44 39.07 MobileNetv2/0.2 44.30 55.97 60.84 13.66 16.15 18.06 MobileNetv2/0.5 46.59 62.36 63.29 12.19 18.52 18.83 ViT-Lite-1/8 36.22 52.94 57.05 14.02 24.68 28.06 ViT-Lite-2/8 36.85 53.13 57.28 14.44 25.39 28.22 CCT-2/2 55.67 68.87 73.45 33.51 42.17 45.59 CCT-4/2 56.05 69.82 74.52 35.26 43.68 46.95 OnDev-LCT-1/1 66.18 76.93 79.71 32.69 49.34 53.83 OnDev-LCT-2/1 68.25 78.20 80.55 35.66 52.10 54.71 OnDev-LCT-4/1 67.24 77.68 80.87 38.82 52.64 56.15 OnDev-LCT-8/1 68.44 78.28 82.42 42.23 54.97 58.18 Table 5: Comparison on image classification performance in FL with 50 clients under different data heterogeneity settings."
  },
  {
    "prompt": "Understanding the computational aspects of runs, accuracy, reported.",
    "response": "All reported accuracy (%) values are the best of 3 runs. Three best accuracy values are marked in bold. and residual linear bottleneck blocks, as these convolutions cap- ture relevant spatial information efficiently while significantly reducing the computational burden and model size. By effec- tively extracting local features, our LCT tokenizer can capture essential details in image representations, thus enhancing the overall accuracy of the model."
  },
  {
    "prompt": "Read and understand this section about MHSA, LCT, mecha from a research paper.",
    "response": "Moreover, the MHSA mecha- nism in our LCT encoder facilitates learning global representa- tions of images, making OnDev-LCTs better at recognizing un- seen patterns and more adaptive to new and diverse data. This adaptability is crucial in practical applications where the model may encounter images from different sources or domains. In the context of FL, models are trained locally on individual devices with non-IID data distribution, and their updates are aggregated on the server."
  },
  {
    "prompt": "Explain a concept involving lenges, necks, chal in computational complexity.",
    "response": "This process often introduces chal- lenges, such as data heterogeneity and communication bottle- necks. However, the lightweight nature of our OnDev-LCT ar- chitecture allows for more efficient communication of model updates between client devices and the central server, mitigat- ing the impact of communication constraints."
  },
  {
    "prompt": "This is a technical discussion about tegration, convo, lutional in theoretical computer science.",
    "response": "The advantage of OnDev-LCT can be attributed to its unique design and the in- tegration of image-specific inductive biases through the convo- lutional tokenization step before the transformer encoder. The incorporation of residual linear bottleneck blocks in the LCT tokenizer enhances the model’s ability to adapt to local varia- tions in data distribution, making it more resilient to non-IID data in the FL setting."
  },
  {
    "prompt": "Read and understand this section about choices, parameters, constraints from a research paper.",
    "response": "By leveraging these design choices, our OnDev-LCTs can learn more generalizable representations with fewer parameters, which is particularly beneficial in FL scenarios with limited data and communication constraints. As 9 Class ID 0 1 2 3 4 5 6 7 8 9 (a) OnDev-LCT-1/1 (b) CCT-2/2 (c) ViT-Lite-1/8 (d) ResNet-20 (e) MobileNetv2/0.2 Figure 6: t-SNE visualizations of feature embeddings on CIFAR-10 by FedAvg global models (10 clients with β = 0.5). Best viewed in color."
  },
  {
    "prompt": "Read and understand this section about Figure, Class, t-SNE from a research paper.",
    "response": "Class ID 0 1 2 3 4 5 6 7 8 9 (a) OnDev-LCT-1/1 (b) CCT-2/2 (c) ViT-1/8 (d) ResNet-20 (e) MobileNetv2/0.2 Figure 7: t-SNE visualizations of feature embeddings on CIFAR-10 by FedAvg global models (50 clients with β = 0.5). Best viewed in color. Model No."
  },
  {
    "prompt": "Scientific context on Table, Comparison, IID and their role in graph theory.",
    "response": "of selected clients (C) 5 10 2∼10 ResNet-20 78.94 81.57 78.29 ResNet-32 79.82 82.23 79.27 ResNet-44 80.01 82.45 80.20 MobileNetv2/0.2 66.72 68.68 64.93 MobileNetv2/0.5 66.70 74.60 69.16 ViT-Lite-1/8 59.75 62.06 59.57 ViT-Lite-2/8 60.48 63.48 59.58 CCT-2/2 78.58 80.71 78.78 CCT-4/2 78.60 81.28 79.95 OnDev-LCT-1/1 77.86 80.68 76.67 OnDev-LCT-2/1 79.57 81.43 76.56 OnDev-LCT-4/1 79.84 81.84 78.76 OnDev-LCT-8/1 80.33 82.02 80.06 Table 6: Comparison on image classification performance in realistic non- IID FL settings using the FEMNIST dataset from LEAF benchmark."
  },
  {
    "prompt": "This is a technical discussion about runs, ported, accuracy in theoretical computer science.",
    "response": "All re- ported accuracy (%) values are the best of 3 runs. Three best accuracy values are marked in bold. a result, the proposed OnDev-LCT models demonstrate higher performance and robustness compared to other baselines under various data heterogeneity settings of FL. However, the scope and applicability of OnDev-LCT are not confined to FL scenar- ios and, the flexibility and efficient nature of OnDev-LCT holds great potential in emerging fields like edge AI and on-device AI."
  },
  {
    "prompt": "Scientific context on device, domains, clients and their role in graph theory.",
    "response": "It is important to emphasize that the superior performance of the proposed OnDev-LCT highlights its potential value and broader applicability in FL and other resource-constrained on- device vision tasks across various domains. Figure 6 and Fig- ure 7 show the t-SNE (van der Maaten and Hinton, 2008) visu- alizations of feature embeddings learned by each global model under FedAvg setting (β = 0.5) with 10 clients and 50 clients on the CIFAR-10 (Krizhevsky et al., 2009) test data."
  },
  {
    "prompt": "Read and understand this section about scenarios, Cal, OnDev from a research paper.",
    "response": "Comparison in realistic FL scenarios. In this section, we con- ducted a performance comparison of our proposed OnDev- LCTs with other baseline models using the FEMNIST (Cal- das et al., 2018) dataset. The results, as presented in Table 6, demonstrate that convolutional transformers, such as OnDev- LCTs and CCTs (Hassani et al., 2021), achieve competitive per- formance on par with ResNet variants (He et al., 2016)."
  },
  {
    "prompt": "Understanding the computational aspects of Lite, Hassani, FEMNIST.",
    "response": "ViT- Lite (Hassani et al., 2021) models, while being lightweight, show comparatively weaker performance, due to their limited ability to efficiently capture both local and global features, which are crucial for accurate image classification in realis- tic FL scenarios like FEMNIST. The accuracy of MobileNetv2 variants (Sandler et al., 2018) may not have reached its peak during the conducted training epochs, implying the potential for increased accuracy with further training."
  },
  {
    "prompt": "This is a technical discussion about Nonetheless, formance, lected in theoretical computer science.",
    "response": "Nonetheless, our proposed OnDev-LCTs consistently show their impressive per- formance across various scenarios with different numbers of se- lected clients participating in each training round, thus demon- strating the effectiveness of our model design for on-device vi- sion tasks in FL scenarios."
  },
  {
    "prompt": "Explain a concept involving lutions, con, convo in computational complexity.",
    "response": "The integration of efficient convo- lutions, self-attention mechanisms, and image-specific induc- tive biases empowers our models to strike a balance between local feature extraction and global context representation, con- tributing to their superior performance and efficiency. These compelling findings highlight the potential of our OnDev-LCT architecture as a practical solution for on-device vision tasks in FL with limited data and computational resources. 6."
  },
  {
    "prompt": "This is a technical discussion about Results, Additional, transformers in theoretical computer science.",
    "response": "Additional Results We present additional results for the performance analysis of our proposed architecture in this section. Comparative analysis with low-bit vision transformers. As an orthogonal approach to our work, we explore the existing lit- erature on popular quantized, aka low-bit vision transformers, emphasizing their contributions to the domain of on-device im- age classification tasks. Liu et al."
  },
  {
    "prompt": "This is a technical discussion about PTQ, 32-32, FEMNIST in theoretical computer science.",
    "response": "(2021) present VT-PTQ, a mixed-precision post-training quantization scheme, which in- troduces a ranking loss to the quantization objective to keep 10 Model Quantization method #bit(W-A) CIFAR-10 FEMNIST (C = 10) Inference time (s) #Params Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 Q-ViT QAT 4-4 55.91 37.89 43.02 45.73 59.48 3.50 22M MinMax PTQ 8-8 55.05 31.04 45.27 49.56 37.99 2.42 22M EMA PTQ 8-8 52.86 29.75 45.08 51.19 50.73 2.43 22M Percentile PTQ 8-8 53.70 29.82 45.79 50.42 47.75 2.46 22M OMSE PTQ 8-8 52.69 30.83 45.66 50.24 49.50 2.44 22M FQ-ViT PTQ 8-8 51.72 28.86 44.52 49.19 49.37 2.62 22M OnDev-LCT-1/1 None 32-32 84.55 66.18 76.93 79.71 80.68 2.66 0.21M OnDev-LCT-2/1 None 32-32 86.27 68.25 78.20 80.55 81.43 2.78 0.31M OnDev-LCT-4/1 None 32-32 86.61 67.24 77.68 80.87 81.84 3.13 0.51M OnDev-LCT-8/1 None 32-32 86.64 68.44 78.28 82.42 82.02 3.77 0.91M Table 7: Comparative analysis with low-bit vision transformers on CIFAR-10 and FEMNIST datasets."
  },
  {
    "prompt": "Understanding the computational aspects of accuracy, bold, RTX.",
    "response": "Three best accuracy (%) values are marked in bold. Inference time represents the wall-clock time, measured on a single NVIDIA RTX 3080 GPU for the CIFAR-10 test set with a batch size of 1000."
  },
  {
    "prompt": "This is a technical discussion about Importance, Params, Baseline in theoretical computer science.",
    "response": "Model Pruning Rate CIFAR-10 #Params Params Ratio Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 ViT-Tiny (Baseline) 0.0 55.86 36.25 47.85 50.76 5.49M 1.000× L1-norm Importance 0.3 52.91 33.23 44.94 46.88 3.01M 0.548× L2-norm Importance 0.3 53.21 33.59 44.86 46.71 3.01M 0.548× Taylor Importance 0.3 53.90 32.51 44.97 47.58 3.01M 0.548× Random Importance 0.3 39.88 27.66 37.59 39.08 3.01M 0.548× OnDev-LCT-1/1 0.0 84.55 66.18 76.93 79.71 0.21M 0.038× OnDev-LCT-2/1 0.0 86.27 68.25 78.20 80.55 0.31M 0.056× OnDev-LCT-4/1 0.0 86.61 67.24 77.68 80.87 0.51M 0.093× OnDev-LCT-8/1 0.0 86.64 68.44 78.28 82.42 0.91M 0.166× Table 8: Comparative analysis with pruned vision transformers on CIFAR-10 dataset."
  },
  {
    "prompt": "Read and understand this section about accuracy, bold, marked from a research paper.",
    "response": "Three best accuracy (%) values are marked in bold. the relative order of the self-attention results after quantiza- tion. Yuan et al. (2022) develop the efficient PTQ4ViT frame- work, employing twin uniform quantization to handle the spe- cial distributions of post-softmax and post-GELU activations."
  },
  {
    "prompt": "Read and understand this section about PTF, LIS, FQ-ViT from a research paper.",
    "response": "FQ-ViT (Lin et al., 2022) is then implemented under the PTQ, a training-free paradigm, by introducing the power-of-two fac- tor (PTF) and log-int-softmax (LIS) quantization methods for LayerNorm and Softmax modules, which are not quantized in PTQ4ViT."
  },
  {
    "prompt": "Explain a concept involving Max, EMA, Percentile in computational complexity.",
    "response": "In comparison to the adaptation of vision transform- ers to prior PTQ-based quantization schemes, including Min- Max, EMA (Jacob et al., 2018), Percentile (Li et al., 2019), OMSE (Choukroun et al., 2019), and PTQ4ViT (Yuan et al., 2022), FQ-ViT achieves a nearly lossless quantization perfor- mance with full-precision models. On the other hand, Li et al."
  },
  {
    "prompt": "Scientific context on QAT, analyze, pipeline and their role in graph theory.",
    "response": "(2022) analyze the quantization robustness of each component in the vision transformer to implement a fully quantized ViT baseline under the straightforward QAT pipeline. The Q-ViT is then proposed by introducing an information rectification mod- ule (IRM) and a distribution-guided distillation (DGD) scheme towards accurate and fully quantized vision transformers."
  },
  {
    "prompt": "Read and understand this section about Deng, perimental, ImageNet from a research paper.",
    "response": "Ex- perimental results on the ImageNet (Deng et al., 2009) dataset show that Q-ViTs outperform the baselines and achieve com- petitive performance with the full-precision counterparts while significantly reducing memory and computational resources."
  },
  {
    "prompt": "Understanding the computational aspects of Table, CIFAR, Krizhevsky.",
    "response": "In Table 7, we present a comprehensive comparison of our proposed OnDev-LCTs with various low-bit transformers (i.e., ViT-S backbone) in terms of accuracy, inference time, and num- ber of parameters for image classification tasks on CIFAR- 10 (Krizhevsky et al., 2009) and FEMNIST (Caldas et al., 2018) datasets. To ensure a fair comparison, we implement Q-ViT without the DGD scheme, which relies on the performance of a powerful teacher model."
  },
  {
    "prompt": "Understanding the computational aspects of schedulers, models, trained.",
    "response": "All models are trained from scratch without pre-training or applying data augmentation techniques and learning rate schedulers. The results clearly show that our proposed OnDev-LCTs outperform the low-bit vision trans- formers in all cases."
  },
  {
    "prompt": "Explain a concept involving transformers, Nonetheless, vision in computational complexity.",
    "response": "Nonetheless, we maintain the perspective that the performance of low-bit vision transformers can also be significantly improved by leveraging a powerful backbone model along with a proper configuration of data augmentation and learning rate scheduler while being smaller and faster than traditional vision transformers. However, it is worth noting that low-bit vision transformers can be more complex to train and may not be as effective for certain tasks."
  },
  {
    "prompt": "Read and understand this section about pruning, vision, result from a research paper.",
    "response": "As a result, the choice of whether to use our OnDev-LCT or a low-bit vision trans- former may depend on the specific task and application. Comparative analysis with vision transformer pruning. In this section, we delve into prior research on vision transformer pruning, which is an emerging study within the broader domain of model compression and optimization."
  },
  {
    "prompt": "This is a technical discussion about Han, Liu, Molchanov in theoretical computer science.",
    "response": "This process system- atically identifies and removes unnecessary parameters, such as unimportant weights or neurons, from a pre-trained vision transformer, guided by various criteria, including weight mag- nitude (LeCun et al., 1989; Han et al., 2016; Liu et al., 2017), gradient magnitude (Molchanov et al., 2019, 2016), activation values (Chen et al., 2021), or importance scores (Zhu et al., 2021)."
  },
  {
    "prompt": "This is a technical discussion about performance, making, crucial in theoretical computer science.",
    "response": "The primary goal is to create a more compact vision transformer that maintains reasonable performance, making it suitable for deployment on resource-constrained devices where computational efficiency is crucial."
  },
  {
    "prompt": "Read and understand this section about Hybrid, Vision, Model from a research paper.",
    "response": "One notable approach, 11 Model CIFAR-10 FEMNIST (C = 10) #Params Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 EdgeNeXt-XXS 58.17 36.22 45.24 53.16 63.65 1.17M MobileViT-XXS 52.91 28.30 45.91 49.37 75.37 0.94M MobileViTv2-0.5 52.84 28.28 48.95 51.80 75.64 1.13M CCT-2/2 79.71 55.67 68.87 73.45 80.71 0.28M CCT-4/2 80.92 56.05 69.82 74.52 81.28 0.48M Hybrid Vision Transformer (ViT) 74.64 45.22 65.70 70.93 80.25 0.75M Hybrid Vision Performer (ViP) 77.52 43.87 65.53 70.89 80.37 0.76M Hybrid Vision Linformer (ViL) 50.68 31.83 49.01 57.91 76.65 0.95M Hybrid Vision Nystr¨oformer (ViN) 77.35 51.25 63.81 68.56 81.31 0.76M OnDev-LCT-1/1 84.55 66.18 76.93 79.71 80.68 0.21M OnDev-LCT-2/1 86.27 68.25 78.20 80.55 81.43 0.31M OnDev-LCT-4/1 86.61 67.24 77.68 80.87 81.84 0.51M OnDev-LCT-8/1 86.64 68.44 78.28 82.42 82.02 0.91M Table 9: Comparative analysis with convolutional transformers on CIFAR-10 and FEMNIST datasets."
  },
  {
    "prompt": "This is a technical discussion about schedulers, model, variant in theoretical computer science.",
    "response": "Each model variant is trained from scratch without applying data augmentation or learning rate schedulers. Three best accuracy (%) values are marked in bold."
  },
  {
    "prompt": "This is a technical discussion about ViT-Tiny, MobileViT-XXS, FEMNIST in theoretical computer science.",
    "response": "Model Pre-trained CIFAR-10 FEMNIST (C = 10) #Params Centralized FedAvg with 50 clients β = 0.1 β = 0.5 β = 5 MobileNetv2/1.0 ✓ 82.22 49.85 80.74 84.00 79.53 2.30M MobileNetv2/1.0 ✗ 36.76 31.34 47.92 50.72 71.81 2.30M ViT-Tiny ✓ 67.51 48.82 64.09 68.20 61.77 5.49M ViT-Tiny ✗ 55.86 36.25 47.85 50.76 57.10 5.49M MobileViT-XXS ✓ 81.27 56.75 79.12 82.77 81.04 0.94M MobileViT-XXS ✗ 52.91 28.30 45.91 49.37 75.37 0.94M MobileViTv2-0.5 ✓ 82.69 44.06 80.52 83.13 80.65 1.13M MobileViTv2-0.5 ✗ 52.84 28.28 48.95 51.80 75.64 1.13M OnDev-LCT-8/1 ✗ 86.64 68.44 78.28 82.42 82.02 0.91M OnDev-LCT-8/3 ✗ 87.65 68.56 78.56 82.70 82.75 0.95M Table 10: Comparative analysis with ImageNet pre-trained models on CIFAR-10 and FEMNIST datasets."
  },
  {
    "prompt": "Understanding the computational aspects of accuracy, bold, VTP.",
    "response": "Three best accuracy (%) values are marked in bold. VTP (Zhu et al., 2021), extends the principles of network slim- ming (Liu et al., 2017) to reduce the number of embedding di- mensions through the introduction of learnable coefficients that evaluate importance scores. Neurons with small coefficients are removed based on a predefined threshold. However, it’s worth noting that VTP requires manual tuning of thresholds for all layers and subsequent model fine-tuning."
  },
  {
    "prompt": "Explain a concept involving WDPruning, method, transformers in computational complexity.",
    "response": "Another method, WDPruning (Yu et al., 2022), is a structured pruning technique for vision transformers, which employs a 0/1 mask to differ- entiate unimportant and important parameters based on their magnitudes. While it uses a differentiable threshold, the non- differentiable mask may introduce gradient bias, potentially re- sulting in suboptimal weight retention."
  },
  {
    "prompt": "Understanding the computational aspects of X-Pruner, ang, focuses.",
    "response": "X-Pruner (Yu and Xi- ang, 2023) focuses on removing less contributing units, where each prunable unit is assigned an explainability-aware mask to quantify its contribution to predicting each class in terms of explainability. Finally, DepGraph (Fang et al., 2023) serves as a grouping algorithm used to analyze dependencies in net- works for enabling any structural pruning over various network architectures."
  },
  {
    "prompt": "Read and understand this section about Nonetheless, vision, pre from a research paper.",
    "response": "Nonetheless, while vision transformer pruning can yield smaller and faster models, the challenge remains in finding the right balance between reducing model size and pre- serving task-specific features. Table 8 provides a performance comparison between our proposed OnDev-LCTs and pruned vision transformers on the CIFAR-10 (Krizhevsky et al., 2009) dataset. We focus on key metrics such as accuracy and the reduction in parameters com- pared to the ViT-Tiny (Dosovitskiy et al., 2021) backbone."
  },
  {
    "prompt": "Understanding the computational aspects of Fang, Torch-Pruning, implementation.",
    "response": "We followed the Torch-Pruning (TP) implementation for structural pruning, which employs the DepGraph algorithm to physically remove parameters (Fang et al., 2023). Several important cri- teria are considered, including L-p Norm, Taylor, and Random. The results demonstrate that our proposed OnDev-LCTs con- sistently outperform the pruned ViT models in all scenarios."
  },
  {
    "prompt": "Scientific context on pruning, view, developing and their role in graph theory.",
    "response": "From one point of view, developing effective pruning strategies can be a complex task, often involving iterative trial and error to identify the optimal model components for pruning. Aggressive pruning carries the risk of significant model generalization loss, as removing too many parameters may lead to underfitting and reduced performance. In summary, vision transformer pruning is a technique applied to existing large models to reduce their size."
  },
  {
    "prompt": "Understanding the computational aspects of hand, OnDev-LCT, architecture.",
    "response": "On the other hand, OnDev-LCT provides an efficient and lightweight neural network architecture, offering an alternative solution for resource-constrained on-device vision applications. Comparative analysis with convolutional transformers. In this analysis, we conduct a comparative study between our OnDev- LCTs and other convolutional transformers that do not rely on heavy data augmentation techniques."
  },
  {
    "prompt": "Explain a concept involving schedulers, models, trained in computational complexity.",
    "response": "All models are trained from scratch without utilizing data augmentation or learning rate schedulers."
  },
  {
    "prompt": "Explain a concept involving Centralized, Acc, clients in computational complexity.",
    "response": "It is important to note that while CCTs (Hassani et al., 2021) require high-level data augmentation techniques to 12 Model Centralized Top-1 Acc Centralized Top-5 Acc FedAvg with 10 clients FedAvg with 50 clients #Params MACs β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 26.99 50.38 13.91 22.18 26.13 9.08 16.64 22.09 0.34M 0.04G ResNet-32 27.92 51.98 14.78 22.82 27.03 10.23 17.89 21.84 0.53M 0.07G ResNet-44 28.12 52.35 15.16 23.45 27.67 11.32 17.25 23.04 0.73M 0.10G ResNet-56 28.18 52.57 15.03 23.56 27.93 12.45 18.42 23.06 0.93M 0.13G MobileNetv2/0.5 18.44 39.22 11.16 18.87 20.28 8.90 14.09 16.19 1.99M <0.01G MobileNetv2/0.75 20.66 42.72 16.27 23.60 25.58 10.63 15.85 18.04 2.66M <0.01G MobileNetv2/1.0 29.84 54.60 16.58 24.78 27.76 11.00 16.92 20.14 3.54M 0.01G ViT-Lite-1/4 11.76 27.39 4.85 8.53 10.61 3.42 7.26 10.18 1.46M 0.04G ViT-Lite-2/4 12.94 30.31 5.55 9.35 11.73 3.81 8.00 11.08 1.48M 0.07G CCT-2/2 26.97 50.04 14.07 22.71 27.43 10.26 19.16 24.14 0.40M 0.03G CCT-4/2 30.27 53.89 15.82 25.21 29.99 11.20 20.92 26.46 0.60M 0.05G OnDev-LCT-1/1 38.46 62.95 20.91 32.01 36.93 13.87 24.68 31.07 0.34M 0.03G OnDev-LCT-2/1 40.59 65.53 21.78 32.93 38.59 14.84 26.23 32.39 0.44M 0.04G OnDev-LCT-4/1 42.34 67.33 23.27 34.66 39.98 15.54 27.28 33.77 0.64M 0.05G OnDev-LCT-8/1 43.37 68.30 24.87 35.23 41.03 16.32 28.10 35.84 1.04M 0.08G Table 11: Performance analysis on ImageNet-32 dataset."
  },
  {
    "prompt": "Read and understand this section about accuracy, bold, marked from a research paper.",
    "response": "Three best accuracy (%) values are marked in bold. achieve their best performance, we include them in our com- parison to provide a complete evaluation of existing lightweight convolutional transformers. Based on the reported values in Ta- ble 9, our models outperform the other benchmarks, demon- strating a significant performance gap compared to variants with similar sizes."
  },
  {
    "prompt": "Understanding the computational aspects of cost, highlights, superior.",
    "response": "This highlights the superior performance and efficiency of our proposed architecture in terms of parameters and computation cost. In contrast to other methods that com- bine convolutions and transformers, the distinguishing factor of our OnDev-LCT lies in its unique and efficient LCT tokenizer design, effectively leveraging depthwise separable convolutions and residual linear bottleneck blocks."
  },
  {
    "prompt": "Scientific context on LCT, specific, OnDev-LCT and their role in graph theory.",
    "response": "By integrating the LCT tokenizer before the transformer encoder, we introduce image- specific inductive biases to our OnDev-LCT, enabling it to cap- ture essential local details from image representations. Further- more, the LCT encoder’s MHSA mechanism aids in learning global representations of images."
  },
  {
    "prompt": "Read and understand this section about comple, mentary, nature from a research paper.",
    "response": "This combination of comple- mentary components allows our model to achieve high accuracy while maintaining its lightweight nature, making it well-suited for on-device vision tasks with limited resources. Comparative analysis with pre-trained models. In this analy- sis, we evaluate the performance of our OnDev-LCTs in com- parison to variants of existing state-of-the-art models that are pre-trained on the ImageNet (Deng et al., 2009) dataset."
  },
  {
    "prompt": "Scientific context on Wightman, models, Paszke and their role in graph theory.",
    "response": "We implement all the pre-trained models using the PyTorch (Paszke et al., 2019) framework, following the official implementation provided by the PyTorch Image Models (timm) (Wightman, 2019) collection. According to the results shown in Tabel 10, our OnDev-LCTs achieve competitive performance even with limited training data, illustrating their robustness and effective- ness for on-device vision tasks."
  },
  {
    "prompt": "This is a technical discussion about tasks, ten, datasets in theoretical computer science.",
    "response": "While pre-trained models have shown promising performance across various vision tasks, their practicality in resource-constrained on-device scenarios is of- ten hindered by challenges related to accessing and utilizing large pre-training datasets. Such datasets may not be readily available in many domains (Varoquaux and Cheplygina, 2022; Parekh et al., 2022; Mutis and Ambekar, 2020; Ghassemi et al., 2020) due to privacy concerns or data distribution heterogene- ity across devices."
  },
  {
    "prompt": "Explain a concept involving Params, Table, Baseline in computational complexity.",
    "response": "Since pre-trained models are designed and Model Augment Augment + LR Scheduler Baseline #Params OnDev-LCT-1/1 90.04 91.05 84.55 0.21M OnDev-LCT-8/1 90.36 91.14 86.64 0.91M OnDev-LCT-1/3 90.25 91.41 85.73 0.25M OnDev-LCT-8/3 90.50 91.99 87.65 0.95M Table 12: Performance analysis of our OnDev-LCT architecture on CIFAR-10 dataset for the impact of simple data augmentation and learning rate scheduler."
  },
  {
    "prompt": "Read and understand this section about sources, optimized, objectives from a research paper.",
    "response": "optimized for specific datasets and objectives, which may not closely align with the target domain, fine-tuning a pre-trained model on downstream tasks may not always be straightfor- ward for on-device scenarios with limited training data and re- sources."
  },
  {
    "prompt": "Understanding the computational aspects of MHSA, datasets, images.",
    "response": "Despite not relying on extensive pre-training datasets, our OnDev-LCTs leverage depthwise separable convolutions and MHSA mechanism to efficiently capture both local and global features from images, making them well-suited for on- device vision tasks with limited data."
  },
  {
    "prompt": "Scientific context on schemes, providing, scenarios and their role in graph theory.",
    "response": "Thus, the unique design of our OnDev-LCT offers a practical and effective alternative to pre-train-and-fine-tune schemes, providing more robust and efficient solutions for on-device vision tasks in FL scenarios, where pre-training datasets may not be readily available. Analysis on ImageNet-32 dataset."
  },
  {
    "prompt": "Understanding the computational aspects of Deng, RGB, samples.",
    "response": "Table 11 compares the cen- tralized and FL performance of our proposed models for im- age classification on ImageNet-32, the down-sampled version of the original ImageNet-1K (Deng et al., 2009) dataset, which contains 1,281,167 training samples and 50,000 test samples of 32×32 RGB images in 1,000 classes. We apply the same hyper- parameter settings and training strategy used with the CIFAR- 100 dataset and train all models from scratch without data aug- mentation and learning rate schedulers."
  },
  {
    "prompt": "Scientific context on performance, ported, baselines and their role in graph theory.",
    "response": "According to the re- ported values, our models outperform the other baselines, and there is a significant gap between their performance and that of variants with comparable sizes. Thus, we can summarize that our proposed architecture is better in performance and also ef- ficient in terms of parameters and computation cost. Analysis on the impact of data augmentation and learning rate scheduler."
  },
  {
    "prompt": "This is a technical discussion about MNIST, Fashion, EMNIST in theoretical computer science.",
    "response": "In this experiment, we analyze the improved 13 Model FedAvg with 10 clients FedAvg with 50 clients MNIST Fashion MNIST EMNIST Balanced MNIST Fashion MNIST EMNIST Balanced β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 β = 0.1 β = 0.5 β = 5 ResNet-20 98.99 99.50 99.58 81.33 91.83 92.75 81.47 88.46 89.68 97.65 99.46 99.54 70.63 90.71 92.06 82.98 88.64 89.50 ResNet-32 99.13 99.49 99.59 82.23 92.04 92.77 83.18 89.23 89.64 97.38 99.42 99.51 72.24 90.75 92.12 83.70 88.78 89.51 ResNet-44 98.99 99.51 99.57 82.27 92.15 92.73 83.86 88.99 89.88 97.85 99.44 99.52 71.51 91.01 92.04 83.90 88.74 89.61 MobileNetv2/0.2 98.31 99.02 99.26 72.01 89.49 89.87 67.78 84.97 83.94 94.16 98.64 98.77 46.82 86.62 87.59 69.66 80.64 80.69 MobileNetv2/0.5 98.71 99.24 99.32 75.03 90.15 90.62 73.37 85.70 86.90 96.85 98.97 98.93 50.20 87.41 89.58 75.46 83.87 80.07 ViT-Lite-1/8 93.58 97.95 98.10 73.90 87.82 88.79 57.96 81.02 84.00 83.33 96.89 97.23 57.54 82.33 85.17 58.55 77.72 80.24 ViT-Lite-2/8 93.71 97.99 98.11 75.74 87.89 88.82 58.92 81.10 84.10 84.19 97.01 97.31 58.97 82.17 85.42 58.86 77.85 80.26 CCT-2/2 98.45 99.29 99.41 81.90 90.42 91.33 82.74 87.94 89.39 95.56 99.03 99.18 72.44 88.07 89.75 81.77 86.87 88.45 CCT-4/2 98.47 99.34 99.45 82.80 90.74 91.53 83.64 88.32 89.53 96.49 99.07 99.34 73.82 87.30 89.98 82.77 87.47 88.69 OnDev-LCT-1/1 98.97 99.43 99.53 82.75 92.03 92.85 82.32 88.96 89.95 97.92 99.29 99.38 75.30 90.92 91.88 82.13 87.92 89.34 OnDev-LCT-2/1 99.18 99.46 99.52 83.12 92.07 92.97 84.43 89.06 90.06 97.85 99.37 99.37 78.56 90.78 92.21 82.98 88.04 89.58 OnDev-LCT-4/1 99.07 99.50 99.54 83.71 92.26 93.13 85.96 89.25 90.32 98.21 99.38 99.36 78.24 90.92 92.03 84.15 88.32 89.64 OnDev-LCT-8/1 99.10 99.43 99.59 83.42 92.37 93.15 86.99 89.48 90.35 97.87 99.28 99.43 78.82 90.84 91.94 84.56 88.84 89.88 Table 13: Comparison on image classification performance in FL under different degrees of data heterogeneity."
  },
  {
    "prompt": "This is a technical discussion about runs, accuracy, reported in theoretical computer science.",
    "response": "All reported accuracy (%) values are the best of 3 runs. All experiments were conducted without applying any data augmentation or learning rate schedulers. Three best accuracy values are marked in bold. 16 28 32 48 56 64 Image size 76.0 77.5 79.0 80.5 82.0 83.5 85.0 86.5 88.0 Accuracy (%) OnDev-LCT-1/1 OnDev-LCT-1/3 OnDev-LCT-8/1 OnDev-LCT-8/3 Figure 8: Accuracy (%) versus image size on CIFAR-10."
  },
  {
    "prompt": "Understanding the computational aspects of CIFAR, Krizhevsky, performance.",
    "response": "performance of our proposed OnDev-LCTs on the CIFAR- 10 (Krizhevsky et al., 2009) dataset by applying simple data augmentation and learning rate schedulers. We conduct exper- iments in the centralized scenario by selecting the smallest and largest model variants of our OnDev-LCT with different num- bers of standard convolutions in the LCT tokenizer. First, we apply simple data augmentation techniques, including horizon- tal flip, random rotation, and zoom."
  },
  {
    "prompt": "Scientific context on Table, aug, mentations and their role in graph theory.",
    "response": "As shown in Table 12, our models significantly improve accuracy even with simple aug- mentations applied to the training data. Then, we set a linear learning rate warm-up for the initial 10 epochs, followed by a gradual reduction of the learning rate per epoch using cosine annealing (Loshchilov and Hutter, 2017b)."
  },
  {
    "prompt": "Read and understand this section about enhanced, demonstrating, applications from a research paper.",
    "response": "The performance is further enhanced, demonstrating that a proper configuration of data augmentation and learning rate scheduler can effectively improve the efficiency of our OnDev-LCT when implementing it in real-world applications. Analysis on the impact of input image resolution. Here, we analyze the performance of our OnDev-LCTs on various im- age sizes using the CIFAR-10 (Krizhevsky et al., 2009) dataset."
  },
  {
    "prompt": "Read and understand this section about sizes, images, smaller from a research paper.",
    "response": "We conduct this analysis by downsampling the images to two smaller sizes, i.e., 16 × 16, 28 × 28 or upsampling the images to three larger sizes, i.e., 48 × 48, 56 × 56, and 64 × 64. From Fig- ure 8, we can observe that our models still perform well with smaller images, even without data augmentation."
  },
  {
    "prompt": "This is a technical discussion about Accuracy, Figure, Attention in theoretical computer science.",
    "response": "Thus, we can infer that by incorporating inductive biases via the LCT tok- 1 2 4 8 12 16 Attention heads 84.0 84.5 85.0 85.5 86.0 86.5 87.0 87.5 88.0 Accuracy (%) OnDev-LCT-1/1 OnDev-LCT-1/3 OnDev-LCT-8/1 OnDev-LCT-8/3 Figure 9: Accuracy (%) versus number of attention heads on CIFAR-10. enizer, our OnDev-LCTs can maintain the spatial information of images, which helps in learning better image representations. Analysis on the impact of multi-head self-attention."
  },
  {
    "prompt": "Understanding the computational aspects of LCT, heads, attention.",
    "response": "In this ex- periment, we investigate the impact of varying the number of attention heads in the LCT encoder on the performance of our OnDev-LCTs. Multiple attention heads in the MHSA block of an LCT encoder help in parallel processing, giving our OnDev- LCT greater power to encode multiple relationships between image patches. Figure 9 demonstrates the performance of our OnDev-LCTs with different numbers of attention heads."
  },
  {
    "prompt": "This is a technical discussion about heads, Obvi, ously in theoretical computer science.",
    "response": "Obvi- ously, small variants of our models perform better with more attention heads, but larger variants gradually drop in accuracy when using 12 and 16 attention heads. Still, all our models pre- serve their performance even with a single attention head. Analysis on various FL settings."
  },
  {
    "prompt": "Read and understand this section about MNIST, Fashion, Table from a research paper.",
    "response": "Image classification perfor- mance of our proposed OnDev-LCTs on the MNIST, Fashion- MNIST, and EMNIST-Balanced datasets, compared to the other baselines under various data heterogeneity settings, is shown in Table 13. The reported values are the best median values from the last 5 FL rounds of 3 separate runs. Our findings indicate that OnDev-LCTs outperform the competition in most situa- tions."
  },
  {
    "prompt": "Read and understand this section about Figure, scenarios, shows from a research paper.",
    "response": "Figure 10 shows the accuracy versus the number of FL rounds on the CIFAR-10 dataset with varying degrees of data heterogeneity for both 10-client and 50-client scenarios; thus, our proposed models outperform the other baselines in all cases."
  },
  {
    "prompt": "This is a technical discussion about Accuracy, clients, rounds in theoretical computer science.",
    "response": "Especially for higher β values, OnDev-LCT variants converge 14 0 10 20 30 40 50 60 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) OnDev-LCT-1/1 OnDev-LCT-2/1 OnDev-LCT-4/1 OnDev-LCT-8/1 MobileNetv2/0.5 MobileNetv2/0.2 Resnet-20 Resnet-32 Resnet-44 ViT-Lite-1/8 ViT-Lite-2/8 CCT-2/2 CCT-4/2 (a) For 10 clients (β = 0.1) 0 10 20 30 40 50 60 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (b) For 10 clients (β = 0.5) 0 10 20 30 40 50 60 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (c) For 10 clients (β = 5) 0 20 40 60 80 100 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (d) For 50 clients (β = 0.1) 0 20 40 60 80 100 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (e) For 50 clients (β = 0.5) 0 20 40 60 80 100 FL rounds 10 20 30 40 50 60 70 80 90 Accuracy (%) (f) For 50 clients (β = 5) Figure 10: Accuracy (%) versus FL rounds on CIFAR-10 test set under different degrees of data heterogeneity."
  },
  {
    "prompt": "This is a technical discussion about color, viewed in theoretical computer science.",
    "response": "Best viewed in color."
  },
  {
    "prompt": "Read and understand this section about clients, Accuracy, Local from a research paper.",
    "response": "1 5 10 Local epochs 50 55 60 65 70 75 80 Accuracy (%) OnDev-LCT-1/1 (10 clients) OnDev-LCT-2/1 (10 clients) OnDev-LCT-4/1 (10 clients) OnDev-LCT-8/1 (10 clients) OnDev-LCT-1/1 (50 clients) OnDev-LCT-2/1 (50 clients) OnDev-LCT-4/1 (50 clients) OnDev-LCT-8/1 (50 clients) (a) β = 0.1 1 5 10 Local epochs 70 72 74 76 78 80 82 84 Accuracy (%) (b) β = 0.5 1 5 10 Local epochs 78 80 82 84 86 Accuracy (%) (c) β = 5 Figure 11: Accuracy (%) versus the number of local epochs on CIFAR-10 test set under different degrees of data heterogeneity."
  },
  {
    "prompt": "Read and understand this section about local, color, viewed from a research paper.",
    "response": "Best viewed in color. faster than the other baselines with a significant performance gap in between. Figure 11 depicts the impact of local epochs in each FL round for the CIFAR-10 dataset. When we increase the local epoch from 1 to 5, the performance of our models dra- matically improves, but just a minor gain when increasing to 10. As a result, we set the default number of local epochs to 5. 7."
  },
  {
    "prompt": "Explain a concept involving LCT, transformer, Conclusion in computational complexity.",
    "response": "Conclusion In this study, we propose a novel design, namely OnDev- LCT, which introduces inductive biases of CNNs to the vi- sion transformer by incorporating an early convolutional stem before the transformer encoder. We leverage efficient depth- wise separable convolutions to build residual linear bottleneck blocks of the LCT tokenizer for extracting local features from images. The LCT encoder enables our models to learn global representations of images via multi-head self-attention."
  },
  {
    "prompt": "Scientific context on bining, mechanisms, resources and their role in graph theory.",
    "response": "Com- bining the strengths of convolutions and attention mechanisms, we design our OnDev-LCTs for on-device vision tasks with limited training data and resources. We conduct extensive ex- periments to analyze the efficiency of our proposed OnDev- LCTs on various image classification tasks compared to the popular lightweight vision models. Centralized experiments on five benchmark image datasets show the superiority of our OnDev-LCTs over the other baselines."
  },
  {
    "prompt": "Scientific context on extensive, munication, outper and their role in graph theory.",
    "response": "Furthermore, extensive FL experiments under various settings indicate the efficacy of our OnDev-LCTs in dealing with data heterogeneity and com- munication bottlenecks since our models significantly outper- form the other baselines while having fewer parameters and lower computational demands. We advocate that the proper configuration of data augmentations and learning rate sched- ulers could further improve the performance of our OnDev- LCT when implementing it in real-world low-data scenarios."
  },
  {
    "prompt": "This is a technical discussion about Agarwal, Barham, Brevdo in theoretical computer science.",
    "response": "References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., et al., 2015. Tensorflow: Large-scale machine learning on heterogeneous systems. Agarap, A.F., 2018. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375 . 15 Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V., 2019."
  },
  {
    "prompt": "Read and understand this section about IEEE, CVF, Proceedings from a research paper.",
    "response": "Attention aug- mented convolutional networks, in: Proceedings of the IEEE/CVF interna- tional conference on computer vision, pp. 3286–3295. Blalock, D., Gonzalez Ortiz, J.J., Frankle, J., Guttag, J., 2020. What is the state of neural network pruning? Proceedings of machine learning and systems 2, 129–146. Caldas, S., Duddu, S.M.K., Wu, P., Li, T., Koneˇcn`y, J., McMahan, H.B., Smith, V., Talwalkar, A., 2018. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097 ."
  },
  {
    "prompt": "Read and understand this section about Chen, Gao, Kuang from a research paper.",
    "response": "Chen, D., Gao, D., Kuang, W., Li, Y., Ding, B., 2022a. pfl-bench: A compre- hensive benchmark for personalized federated learning. Advances in Neural Information Processing Systems 35, 9344–9360. Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., Wang, Z., 2021. Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems 34, 19974–19988. Chen, Y., Dai, X., Chen, D., Liu, M., Dong, X., Yuan, L., Liu, Z., 2022b."
  },
  {
    "prompt": "Understanding the computational aspects of Bridging, Proceedings, IEEE.",
    "response": "Mobile-former: Bridging mobilenet and transformer, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5270–5279. Choromanski, K.M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J.Q., Mohiuddin, A., Kaiser, L., et al., 2020. Re- thinking attention with performers, in: International Conference on Learn- ing Representations. Choukroun, Y., Kravchik, E., Yang, F., Kisilev, P., 2019."
  },
  {
    "prompt": "This is a technical discussion about IEEE, ICCVW, CVF in theoretical computer science.",
    "response": "Low-bit quantiza- tion of neural networks for efficient inference, in: 2019 IEEE/CVF Inter- national Conference on Computer Vision Workshop (ICCVW), IEEE. pp. 3009–3018. Cohen, G., Afshar, S., Tapson, J., Van Schaik, A., 2017. Emnist: Extending mnist to handwritten letters, in: 2017 international joint conference on neu- ral networks (IJCNN), IEEE. pp. 2921–2926. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009."
  },
  {
    "prompt": "This is a technical discussion about Ieee., Imagenet, Liu in theoretical computer science.",
    "response": "Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee. pp. 248–255. Deng, L., 2012. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine 29, 141–142. Ding, Y., Qin, H., Yan, Q., Chai, Z., Liu, J., Wei, X., Liu, X., 2022."
  },
  {
    "prompt": "This is a technical discussion about Proceedings, ACM, Multimedia in theoretical computer science.",
    "response": "Towards accurate post-training quantization for vision transformer, in: Proceedings of the 30th ACM International Conference on Multimedia, pp. 5380–5388. Dong, X., Chen, S., Pan, S., 2017. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in neural information process- ing systems 30. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un- terthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021."
  },
  {
    "prompt": "This is a technical discussion about Transformers, International, Repre in theoretical computer science.",
    "response": "An image is worth 16x16 words: Transformers for image recognition at scale, in: International Conference on Learning Repre- sentations. URL: https://openreview.net/forum?id=YicbFdNTTy. Fang, G., Ma, X., Song, M., Mi, M.B., Wang, X., 2023. Depgraph: Towards any structural pruning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16091–16101. Ferguson, T.S., 1973. A bayesian analysis of some nonparametric problems. The annals of statistics , 209–230."
  },
  {
    "prompt": "Explain a concept involving Dziugaite, Roy, Carbin in computational complexity.",
    "response": "Frankle, J., Dziugaite, G.K., Roy, D.M., Carbin, M., 2021. Pruning neural net- works at initialization: Why are we missing the mark?, in: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Aus- tria, May 3-7, 2021, OpenReview.net. URL: https://openreview.net/ forum?id=Ig-VyQc-MLK. Ghassemi, M., Naumann, T., Schulam, P., Beam, A.L., Chen, I.Y., Ranganath, R., 2020. A review of challenges and opportunities in machine learning for health."
  },
  {
    "prompt": "Scientific context on AMIA, Summits, Translational and their role in graph theory.",
    "response": "AMIA Summits on Translational Science Proceedings 2020, 191. Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J´egou, H., Douze, M., 2021. Levit: a vision transformer in convnet’s clothing for faster infer- ence, in: Proceedings of the IEEE/CVF international conference on com- puter vision, pp. 12259–12269. Han, K., Wang, Y., Tian, Q., Guo, J., Xu, C., Xu, C., 2020."
  },
  {
    "prompt": "Scientific context on Proceedings, IEEE, CVF and their role in graph theory.",
    "response": "Ghostnet: More fea- tures from cheap operations, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1580–1589. Han, S., Mao, H., Dally, W.J., 2016. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In- ternational Conference on Learning Representations (ICLR) . Hassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J., Shi, H., 2021. Es- caping the big data paradigm with compact transformers."
  },
  {
    "prompt": "Read and understand this section about Hassibi, Stork, Wolff from a research paper.",
    "response": "arXiv preprint arXiv:2104.05704 . Hassibi, B., Stork, D.G., Wolff, G.J., 1993. Optimal brain surgeon and gen- eral network pruning, in: IEEE international conference on neural networks, IEEE. pp. 293–299. He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778. Hendrycks, D., Gimpel, K., 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 ."
  },
  {
    "prompt": "Read and understand this section about Chen, Sandler, Chu from a research paper.",
    "response": "Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al., 2019. Searching for mobilenetv3, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314–1324. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H., 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 ."
  },
  {
    "prompt": "Read and understand this section about Brown, Hsu, Shen from a research paper.",
    "response": "Hsu, T.M.H., Qi, H., Brown, M., 2019. Measuring the effects of non- identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335 . Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks, in: Proceed- ings of the IEEE conference on computer vision and pattern recognition, pp. 7132–7141. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K., 2016."
  },
  {
    "prompt": "Understanding the computational aspects of Alexnet-level, Squeezenet, Jacob.",
    "response": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360 . Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D., 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference, in: Proceedings of the IEEE con- ference on computer vision and pattern recognition, pp. 2704–2713. Jeevan, P., Sethi, A., 2022."
  },
  {
    "prompt": "Explain a concept involving vision, Proceedings, IEEE in computational complexity.",
    "response": "Resource-efficient hybrid x-formers for vision, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2982–2990. Karimireddy, S.P., Kale, S., Mohri, M., Reddi, S., Stich, S., Suresh, A.T., 2020. Scaffold: Stochastic controlled averaging for federated learning, in: Interna- tional Conference on Machine Learning, PMLR. pp. 5132–5143. Khan, A., Sohail, A., Zahoora, U., Qureshi, A.S., 2020."
  },
  {
    "prompt": "Explain a concept involving Khan, networks, Naseer in computational complexity.",
    "response": "A survey of the recent architectures of deep convolutional neural networks. Artificial intelligence review 53, 5455–5516. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M., 2021. Transformers in vision: A survey. ACM Computing Surveys (CSUR) . Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 . Krizhevsky, A., Hinton, G., et al., 2009. Learning multiple lay- ers of features from tiny images."
  },
  {
    "prompt": "Read and understand this section about URL, University, Toronto from a research paper.",
    "response": "Technical report, University of Toronto, 2009 URL: https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR.pdf. Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information pro- cessing systems 25. LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. nature 521, 436–444. LeCun, Y., Denker, J., Solla, S., 1989. Optimal brain damage. Advances in neural information processing systems 2."
  },
  {
    "prompt": "Understanding the computational aspects of Vedaldi, Lenc, Proceedings.",
    "response": "Lenc, K., Vedaldi, A., 2015. Understanding image representations by measur- ing their equivariance and equivalence, in: Proceedings of the IEEE confer- ence on computer vision and pattern recognition, pp. 991–999. Li, Q., He, B., Song, D., 2021a. Model-contrastive federated learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10713–10722. Li, R., Wang, Y., Liang, F., Qin, H., Yan, J., Fan, R., 2019."
  },
  {
    "prompt": "Explain a concept involving CVF, Sahu, Talwalkar in computational complexity.",
    "response": "Fully quantized network for object detection, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2810–2819. Li, T., Sahu, A.K., Talwalkar, A., Smith, V., 2020a. Federated learning: Chal- lenges, methods, and future directions. IEEE Signal Processing Magazine 37, 50–60. Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V., 2020b. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems 2, 429–450."
  },
  {
    "prompt": "Scientific context on Chen, Liu, Dai and their role in graph theory.",
    "response": "Li, Y., Chen, Y., Dai, X., Chen, D., Liu, M., Yuan, L., Liu, Z., Zhang, L., Vasconcelos, N., 2021b. Micronet: Improving image recognition with ex- tremely low flops, in: Proceedings of the IEEE/CVF International confer- 16 ence on computer vision, pp. 468–477. Li, Y., Xu, S., Zhang, B., Cao, X., Gao, P., Guo, G., 2022. Q-vit: Accurate and fully quantized low-bit vision transformer, in: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (Eds.), Advances in Neural Information Processing Systems."
  },
  {
    "prompt": "Scientific context on URL, Liu, Yang and their role in graph theory.",
    "response": "URL: https://openreview.net/forum?id=fU-m9kQe0ke. Li, Z., Liu, F., Yang, W., Peng, S., Zhou, J., 2021c. A survey of convolutional neural networks: analysis, applications, and prospects. IEEE transactions on neural networks and learning systems . Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P., Zitnick, C.L., 2014. Microsoft coco: Common objects in context, in: European conference on computer vision, Springer. pp. 740–755."
  },
  {
    "prompt": "Read and understand this section about Sun, Zhou, Liu from a research paper.",
    "response": "Lin, Y., Zhang, T., Sun, P., Li, Z., Zhou, S., 2022. Fq-vit: Post-training quanti- zation for fully quantized vision transformer, in: Proceedings of the Thirty- First International Joint Conference on Artificial Intelligence, IJCAI-22, pp. 1173–1179. Liu, S.Y., Liu, Z., Cheng, K.T., 2023. Oscillation-free quantization for low-bit vision transformers. arXiv preprint arXiv:2302.02210 . Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C., 2017."
  },
  {
    "prompt": "Understanding the computational aspects of Proceedings, Hutter, IEEE.",
    "response": "Learning efficient convolutional networks through network slimming, in: Proceedings of the IEEE international conference on computer vision, pp. 2736–2744. Liu, Z., Wang, Y., Han, K., Zhang, W., Ma, S., Gao, W., 2021. Post-training quantization for vision transformer. Advances in Neural Information Pro- cessing Systems 34, 28092–28103. Loshchilov, I., Hutter, F., 2017a. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 . Loshchilov, I., Hutter, F., 2017b."
  },
  {
    "prompt": "This is a technical discussion about SGDR, Stochastic, International in theoretical computer science.",
    "response": "SGDR: Stochastic gradient descent with warm restarts, in: International Conference on Learning Representations. URL: https://openreview.net/forum?id=Skq89Scxx. van der Maaten, L., Hinton, G., 2008. Visualizing data using t-sne. Journal of Machine Learning Research 9, 2579–2605. URL: http://jmlr.org/ papers/v9/vandermaaten08a.html. Maaz, M., Shaker, A., Cholakkal, H., Khan, S., Zamir, S.W., Anwer, R.M., Shahbaz Khan, F., 2022."
  },
  {
    "prompt": "This is a technical discussion about European, Con, Springer. in theoretical computer science.",
    "response": "Edgenext: efficiently amalgamated cnn- transformer architecture for mobile vision applications, in: European Con- ference on Computer Vision, Springer. pp. 3–20. McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A., 2017. Communication-efficient learning of deep networks from decentralized data, in: Artificial intelligence and statistics, PMLR. pp. 1273–1282. Mehta, S., Rastegari, M., 2022."
  },
  {
    "prompt": "This is a technical discussion about Light-weight, International, Representations in theoretical computer science.",
    "response": "Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer, in: International Conference on Learning Representations. URL: https://openreview.net/forum?id= vh-0sUt8HlG. Mehta, S., Rastegari, M., 2023. Separable self-attention for mobile vision transformers. Transactions on Machine Learning Research URL: https: //openreview.net/forum?id=tBl4yBEjKi. Menghani, G., 2023. Efficient deep learning: A survey on making deep learning models smaller, faster, and better."
  },
  {
    "prompt": "This is a technical discussion about ACM, Surveys, Tyree in theoretical computer science.",
    "response": "ACM Computing Surveys 55, 1–37. Mitchell, B.R., et al., 2017. The spatial inductive bias of deep learning. Ph.D. thesis. Johns Hopkins University. Molchanov, P., Mallya, A., Tyree, S., Frosio, I., Kautz, J., 2019. Importance estimation for neural network pruning, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11264–11272. Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J., 2016."
  },
  {
    "prompt": "Understanding the computational aspects of Mutis, Ambekar, Pruning.",
    "response": "Pruning con- volutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440 . Mutis, I., Ambekar, A., 2020. Challenges and enablers of augmented reality technology for in situ walkthrough applications. J. Inf. Technol. Constr. 25, 55–71. Neuman, S.M., Plancher, B., Duisterhof, B.P., Krishnan, S., Banbury, C., Mazumder, M., Prakash, S., Jabbour, J., Faust, A., de Croon, G.C., et al., 2022."
  },
  {
    "prompt": "This is a technical discussion about IEEE, AICAS, International in theoretical computer science.",
    "response": "Tiny robot learning: challenges and directions for machine learning in resource-constrained robots, in: 2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS), IEEE. pp. 296–299. Parekh, D., Poddar, N., Rajpurkar, A., Chahal, M., Kumar, N., Joshi, G.P., Cho, W., 2022. A review on autonomous vehicles: Progress, methods and challenges. Electronics 11, 2162. Park, J., Woo, S., Lee, J.Y., Kweon, I.S., 2020."
  },
  {
    "prompt": "Understanding the computational aspects of networks, International, simple.",
    "response": "A simple and light-weight attention module for convolutional neural networks. International journal of computer vision 128, 783–798. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D., 2018. Image transformer, in: International Conference on Machine Learn- ing, PMLR. pp. 4055–4064. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al., 2019."
  },
  {
    "prompt": "This is a technical discussion about Pytorch, style, high-performance in theoretical computer science.",
    "response": "Pytorch: An imperative style, high-performance deep learning library. Advances in neural informa- tion processing systems 32. Qu, L., Zhou, Y., Liang, P.P., Xia, Y., Wang, F., Adeli, E., Fei-Fei, L., Rubin, D., 2022. Rethinking architecture design for tackling data heterogeneity in fed- erated learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10061–10071."
  },
  {
    "prompt": "Read and understand this section about Posner, Barfoot, Beaudoin from a research paper.",
    "response": "Roy, N., Posner, I., Barfoot, T., Beaudoin, P., Bengio, Y., Bohg, J., Brock, O., Depatie, I., Fox, D., Koditschek, D., et al., 2021. From machine learning to robotics: challenges and opportunities for embodied intelligence. arXiv preprint arXiv:2110.15245 . Sabour, S., Frosst, N., Hinton, G.E., 2017. Dynamic routing between capsules. Advances in neural information processing systems 30. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C., 2018."
  },
  {
    "prompt": "Understanding the computational aspects of Inverted, Proceedings, IEEE.",
    "response": "Mo- bilenetv2: Inverted residuals and linear bottlenecks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510– 4520. Sanh, V., Wolf, T., Rush, A., 2020. Movement pruning: Adaptive sparsity by fine-tuning. Advances in Neural Information Processing Systems 33, 20378–20389. Shi, M., Zhou, Y., Ye, Q., Lv, J., 2022. Personalized federated learning with hidden information on personalized prior. arXiv preprint arXiv:2211.10684 . Simonyan, K., Zisserman, A., 2014."
  },
  {
    "prompt": "Read and understand this section about recognition, URL, Liu from a research paper.",
    "response": "Very deep convolutional networks for large-scale image recognition. CoRR abs/1409.1556. URL: https://api. semanticscholar.org/CorpusID:14124313. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions, in: Proceedings of the IEEE conference on computer vision and pattern recog- nition, pp. 1–9. Tan, M., Le, Q., 2019."
  },
  {
    "prompt": "This is a technical discussion about Rethinking, International, Cord in theoretical computer science.",
    "response": "Efficientnet: Rethinking model scaling for convolutional neural networks, in: International conference on machine learning, PMLR. pp. 6105–6114. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´egou, H., 2021. Training data-efficient image transformers & distillation through attention, in: International Conference on Machine Learning, PMLR. pp. 10347–10357. Touvron, H., Cord, M., El-Nouby, A., Verbeek, J., J´egou, H., 2022."
  },
  {
    "prompt": "Understanding the computational aspects of ECCV, Israel, October.",
    "response": "Three things everyone should know about vision transformers, in: Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV, Springer. pp. 497–515. Varoquaux, G., Cheplygina, V., 2022. Machine learning for medical imaging: methodological failures and recommendations for the future. NPJ digital medicine 5, 48. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017."
  },
  {
    "prompt": "Scientific context on Wang, Jiang, Qian and their role in graph theory.",
    "response": "Attention is all you need. Advances in neural information processing systems 30. Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X., 2017. Residual attention network for image classification, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3156– 3164. Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., Khazaeni, Y., 2020a. Federated learning with matched averaging, in: International Conference on Learning Representations."
  },
  {
    "prompt": "This is a technical discussion about URL, BkluqlSFDS, Wang in theoretical computer science.",
    "response": "URL: https://openreview.net/forum?id= BkluqlSFDS. Wang, J., Liu, Q., Liang, H., Joshi, G., Poor, H.V., 2020b. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in neural information processing systems 33, 7611–7623. Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H., 2020c. Linformer: Self- attention with linear complexity. arXiv preprint arXiv:2006.04768 . Wightman, R., 2019. Pytorch image models. https://github.com/ rwightman/pytorch-image-models."
  },
  {
    "prompt": "This is a technical discussion about doi, Xiao, Codella in theoretical computer science.",
    "response": "doi:10.5281/zenodo.4414861. Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L., 2021a. Cvt: Introducing convolutions to vision transformers, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22–31. Wu, Y., Kang, Y., Luo, J., He, Y., Yang, Q., 2021b. Fedcg: Leverage conditional gan for protecting privacy and maintaining competitive performance in fed- erated learning, in: International Joint Conference on Artificial Intelligence."
  },
  {
    "prompt": "Explain a concept involving URL, Xiao, Rasul in computational complexity.",
    "response": "URL: https://api.semanticscholar.org/CorpusID:244130332. 17 Xiao, H., Rasul, K., Vollgraf, R., 2017. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 . Xinyi, Z., Chen, L., 2019. Capsule graph neural network, in: International conference on learning representations. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V., 2021."
  },
  {
    "prompt": "Scientific context on Intelli, Nystr, Proceedings and their role in graph theory.",
    "response": "Nystr¨omformer: A nystr¨om-based algorithm for approximating self- attention, in: Proceedings of the AAAI Conference on Artificial Intelli- gence, pp. 14138–14148. Yang, H., Yin, H., Shen, M., Molchanov, P., Li, H., Kautz, J., 2023. Global vision transformer pruning with hessian-aware saliency, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18547–18557. Yu, F., Huang, K., Wang, M., Cheng, Y., Chu, W., Cui, L., 2022."
  },
  {
    "prompt": "This is a technical discussion about Intelligence, Proceedings, AAAI in theoretical computer science.",
    "response": "Width & depth pruning for vision transformers, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 3143–3151. Yu, L., Xiang, W., 2023. X-pruner: explainable pruning for vision transform- ers, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24355–24363. Yuan, Z., Xue, C., Chen, Y., Wu, Q., Sun, G., 2022."
  },
  {
    "prompt": "Read and understand this section about Wang, Post-training, Springer. from a research paper.",
    "response": "Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization, in: Eu- ropean Conference on Computer Vision, Springer. pp. 191–207. Zhu, M., Tang, Y., Han, K., 2021. Vision transformer pruning. arXiv preprint arXiv:2104.08500 . Zuo, Z., Shuai, B., Wang, G., Liu, X., Wang, X., Wang, B., Chen, Y., 2015."
  },
  {
    "prompt": "Explain a concept involving Learning, Proceedings, IEEE in computational complexity.",
    "response": "Convolutional recurrent neural networks: Learning spatial dependencies for image representation, in: Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 18–26. 18"
  },
  {
    "prompt": "Scientific context on Objectives, Abstract, Non-Smooth and their role in graph theory.",
    "response": "Subgradient Method for System Identification with Non-Smooth Objectives Baturalp Yalcin and Javad Lavaei Abstract— This paper investigates a subgradient-based algo- rithm to solve the system identification problem for linear time-invariant systems with non-smooth objectives. This is essential for robust system identification in safety-critical applications."
  },
  {
    "prompt": "This is a technical discussion about guarantees, solvers, unexplored in theoretical computer science.",
    "response": "While existing work provides theoretical exact recovery guarantees using optimization solvers, the design of fast learning algorithms with convergence guarantees for practical use remains unexplored. We analyze the subgradient method in this setting where the optimization problems to be solved change over time as new measurements are taken, and we establish linear convergence results for both the best and Polyak step sizes after a burn-in period."
  },
  {
    "prompt": "This is a technical discussion about Additionally, sizes, characterize in theoretical computer science.",
    "response": "Additionally, we characterize the asymptotic convergence of the best average sub-optimality gap under diminishing and constant step sizes. Finally, we compare the time complexity of standard solvers with the subgradient algorithm and support our findings with experimental results. This is the first work to analyze subgradient algorithms for system identification with non-smooth objectives. I."
  },
  {
    "prompt": "This is a technical discussion about INTRODUCTION, systems, lems in theoretical computer science.",
    "response": "INTRODUCTION Dynamical systems form the foundation of modern prob- lems, such as reinforcement learning, auto-regressive models, and control systems. These systems evolve based on their current state, with the system dynamics equations determining the next state. Due to the complexity of real-world systems, these dynamics are often unknown or difficult to model precisely. Learning the unknown dynamics of a dynamical system is known as the system identification problem."
  },
  {
    "prompt": "This is a technical discussion about LSE, literature, problem in theoretical computer science.",
    "response": "The system identification problem has been extensively studied in the literature [1], [2]. The least-squares estimator (LSE) is the most widely studied approach for this problem as it provides a closed-form solution. Early research focused on the asymptotic (infinite-time) convergence properties of LSE [3]."
  },
  {
    "prompt": "Explain a concept involving Recently, finite-time, popularity in computational complexity.",
    "response": "Recently, finite-time (non-asymptotic) analyses gained popularity, leveraging high-dimensional statistical techniques for both linear [4], [5], [6], [7] and nonlinear dynamical systems [8], [9], [10]. See [11] for a survey. Traditionally, both asymptotic and non-asymptotic analyses of LSE focused on systems with independent and identically distributed (sub)- Gaussian disturbances."
  },
  {
    "prompt": "Explain a concept involving vehicles, unmanned, grids in computational complexity.",
    "response": "However, this assumption is often unrealistic for safety-critical systems such as autonomous vehicles, unmanned aerial vehicles, and power grids. In these settings, disturbances may be designed by an adversarial agent, leading to dependencies across time and arbitrary variations. Since LSE is susceptible to outliers and temporal dependen- cies, recent research focused on robust non-smooth estimators This material is based upon work supported in part by the U. S."
  },
  {
    "prompt": "Understanding the computational aspects of Research, Laboratory, Army.",
    "response": "Army Research Laboratory and the U. S. Army Research Office under grant number W911NF2010219. It was also supported by the Office of Naval Research under grant number N000142412673, as well as AFOSR, NSF, and the UC Noyce Initiative. The authors are with the University of California, Berkeley. E-mail: {byalcin, lavaei}@berkeley.edu for safety-critical systems, particularly lasso-type estimators."
  },
  {
    "prompt": "Understanding the computational aspects of Property, Null, Space.",
    "response": "[12] and [13] analyzed the properties of these estimators through the lens of the Null Space Property, a key concept in lasso analysis. Later works introduced a disturbance structure where disturbance vectors are zero with probability 1 −p and nonzero with probability p."
  },
  {
    "prompt": "This is a technical discussion about probability, proved, smooth in theoretical computer science.",
    "response": "For any p between 0 and 1, [14] and [15] proved that lasso-type estimators with non- smooth objectives can exactly recover system dynamics for linear and parameterized nonlinear systems, respectively, with probability approaching one. Additionally, [16] showed that asymmetric disturbances with nonzero probability p could be transformed into zero-mean disturbances with probability 2p, enabling exact recovery for any disturbance structure where p < 1/2."
  },
  {
    "prompt": "Explain a concept involving statistics, non-asymptotic, analyses in computational complexity.",
    "response": "These non-asymptotic analyses leverage techniques from modern high-dimensional probability and statistics [17], [18]. Despite their strong learning properties, these approaches rely on convex optimization solvers, which become computationally prohibitive as the system state dimension and time horizon grow. Safety-critical systems require rapid estimation to ensure real-time control without delays."
  },
  {
    "prompt": "Explain a concept involving frame, algorithms, imperative in computational complexity.",
    "response": "Thus, it is imperative to design fast algorithms that efficiently solve lasso-type non-smooth estimators within a reasonable time frame. Due to the non-smoothness of the objective, gradient- and Hessian-based first-order and second-order algorithms are not viable options in this context. Instead, we employ the subgradient method, a well-established technique in the literature, as the lasso-type estimator is a convex optimization problem."
  },
  {
    "prompt": "Read and understand this section about Unlike, iteration, methods from a research paper.",
    "response": "Unlike gradient-based methods, subgradient methods do not guarantee descent at each iteration. However, existing results show that the best sub-optimality gap asymptotically converges to zero over time with both constant and diminish- ing step sizes. For a comprehensive overview of subgradient methods for non-smooth objectives, see [19], [20], [21], [22], [23]."
  },
  {
    "prompt": "This is a technical discussion about LSE, identification, system in theoretical computer science.",
    "response": "Closed-form solutions and gradient-based methods are widely used in system identification problems, as the literature has primarily focused on LSE or other smooth estimators for linear systems [24]. Nonlinear system identification is inherently more complex than its linear counterpart [25], yet existing estimation techniques still rely on smooth penalty functions. In contrast, subgradient methods have not been explored in the system identification context."
  },
  {
    "prompt": "Scientific context on problem, subgradient, paper and their role in graph theory.",
    "response": "Thus, this paper is the first to analyze the subgradient algorithm for the system identification problem. It is important to note that we do not apply the subgradient method to a single optimization problem and instead apply it to a sequence of time-varying problems due to the fact that as new measurements are taken, the optimization problem to be solved changes as well. arXiv:2503.16673v1 [math.OC] 20 Mar 2025 Notations: For a matrix Z, ∥Z∥F denotes its Frobenius norm."
  },
  {
    "prompt": "Explain a concept involving vec, Kronecker, product in computational complexity.",
    "response": "For two matrices Z1 and Z2, we use ⟨Z1,Z2⟩and Z1 ⊗Z2 to denote the inner product and Kronecker product, respectively. For a matrix Z, vec(Z) is the usual vectorization operation by stacking the columns of the matrix Z into a vector. For a vector z, ∥z∥2 denotes its ℓ2-norm. ∠z1z2 denotes the angle between two vectors and ∠vec(Z1)vec(Z2) is the angle between two matrices Z1 and Z2."
  },
  {
    "prompt": "Understanding the computational aspects of positive, universal, constants.",
    "response": "Given two functions f and g, the notation f(x) = Θ(g(x)) means that there exist universal positive constants c1 and c2 such that c1g(x) ≤ f(x) ≤c2g(x). Similarly, f(x) ≤O(g(x)) implies that there exists a universal positive constant c3 such that f(x) ≤c3g(x). In stands for the n×n identity matrix and 0n is the vector of zeros with dimension n. II."
  },
  {
    "prompt": "Read and understand this section about Axt, system, unknown from a research paper.",
    "response": "NON-SMOOTH ESTIMATOR AND DISTURBANCE MODEL We consider a linear time-invariant dynamical system of order n with the system update equation xt+1 = ¯Axt + ¯dt, t = 0,1,...,T −1, where ¯A ∈Rn×n is the unknown system matrix and ¯dt ∈Rn are unknown system disturbances. Our goal is to estimate ¯A using the state samples {xt}T t=0 obtained from a single system initialization."
  },
  {
    "prompt": "Explain a concept involving case, Axt, LSE in computational complexity.",
    "response": "Note we only study the autonomous case due to space restrictions and the generalization to the case with inputs is straightforward. Traditionally, the following LSE has been widely studied in the literature: min A∈Rn×n T−1 ∑ i=0 ∥xt+1 −Axt∥2 2. (LSE) However, the LSE is vulnerable to outliers and adversarial disturbances. In safety-critical system identification, a new popular alternative is the following non-smooth estimator: min A∈Rn×n T−1 ∑ i=0 ∥xt+1 −Axt∥2."
  },
  {
    "prompt": "Understanding the computational aspects of NSE, vehicles, itoring.",
    "response": "(NSE) The primary objective of this non-smooth estimator is to achieve exact recovery in finite time, which is an essential feature for safety-critical applications such as power-grid mon- itoring, unmanned aerial vehicles, and autonomous vehicles. In these scenarios, the system should be learned from a single trajectory (since multiple initialization is often infeasible) and, moreover, it can be exposed to adversarial agents. Even under i.i.d."
  },
  {
    "prompt": "Scientific context on disturbance, vector, time and their role in graph theory.",
    "response": "disturbance vectors, if ¯dt is non-zero at every time step t, the exact recovery of the ¯A is not attainable in finite time. Motivated by [15], we, therefore, introduce a disturbance model in which the disturbance vector ¯dt is zero at certain time instances. Definition 1 (Probabilistic sparsity model [15]). For each time instance t, the disturbance vector ¯dt is zero with probability 1 −p, where p ∈(0,1)."
  },
  {
    "prompt": "This is a technical discussion about nonzero, time, disturbance in theoretical computer science.",
    "response": "Furthermore, the time instances at which the disturbances are nonzero are independent of each other (while the nonzero disturbance values could be correlated). [15] showed that the estimator (LSE) fails to achieve exact recovery in finite time even when p < 1 and in the simplest case when the disturbance vectors are i.i.d. Gaussian. Consequently, it becomes essential to employ non-smooth estimators to enable exact recovery within a finite time horizon."
  },
  {
    "prompt": "Scientific context on system, vectors, exact and their role in graph theory.",
    "response": "Furthermore, without specific assumptions on the system dynamics and disturbance vectors, exact recovery is unattainable in finite time. See Theorem 3 in [15]. First, we impose a sufficient condition for the system stability of the LTI system in Assumption 1 to guarantee learning under possibly large correlated disturbances. Assumption 1. The ground-truth ¯A satisfies ρ := ¯A 2 < 1."
  },
  {
    "prompt": "Read and understand this section about subspace, infeasible, vectors from a research paper.",
    "response": "If the disturbance vectors take arbitrary values and are chosen from a low-dimensional subspace, it is shown in [15] that learning the full system dynamics becomes infeasible. To address this, we define the filtration Ft := σ{x0,x1,...,xt}, and we add the semi-oblivious and sub-Gaussian disturbances assumption below based on [15]. Assumption 2 (Semi-Oblivious and sub-Gaussian distur- bances)."
  },
  {
    "prompt": "Understanding the computational aspects of ˆdt, Conditional, sub-Gaussian.",
    "response": "Conditional on the filtration Ft and the event that ¯dt ̸= 0n, the attack vector ¯dt is defined by the product ℓt ˆdt, where 1) ˆdt ∈Rn and ℓt ∈R are independent conditional on Ft and ¯dt ̸= 0n; 2) ˆdt obeys the uniform distribution on the unit sphere whenever ¯dt ̸= 0n; 3) ℓt is zero-mean and sub-Gaussian with parameter σ. Assumption 2 implies that the disturbance vectors ¯dt are sub-Gaussian and temporally correlated (since ℓt may depend on previous disturbances)."
  },
  {
    "prompt": "Understanding the computational aspects of real, world, attacks.",
    "response": "This correlation better reflects real- world semi-oblivious attacks, where an adversarial agent can inject disturbances that depend on prior injections. Moreover, Assumption 2 ensures exploration of the entire state space, as the direction vector ˆdt is uniformly distributed on the unit sphere. In recent years, non-smooth estimators for system iden- tification gained traction [26], [27], [14], [15], [16]."
  },
  {
    "prompt": "Scientific context on models, studies, analyzed and their role in graph theory.",
    "response": "These studies analyzed non-smooth estimators in a non-asymptotic framework under various disturbance models. Specifically, under Assumptions 1 and 2, [14] and [15] derived sample complexity bounds for exact recovery in finite time with high probability. However, they do not propose numerical algorithms and rely on existing convex optimization solvers, which suffer from two drawbacks. The first issue is that in practice (NSE) should be repeatedly solved for T = 1,2,..."
  },
  {
    "prompt": "Read and understand this section about obtained, time, measurements from a research paper.",
    "response": "as new measurements are taken until a satisfactory model is obtained. This means that the methods in [15] and [16] require solving a convex optimization problem at each time step. In addition, the problem (NSE) at time T is translated into a second-order conic program with T + n2 scalar variables and T conic inequalities. Hence, the solver performance degrades significantly as the order of the system and the time horizon grow."
  },
  {
    "prompt": "Explain a concept involving subgradient, identification, convex in computational complexity.",
    "response": "Since safety-critical applications demand fast and efficient identification, convex optimization solvers are inefficient for real-time computation. To address these limitations, we investigate the subgradient method for learning system parameters. III. SUBGRADIENT METHOD Let fT(A) denote the objective function of (NSE) and ∂fT(A) denote its subdifferential at a point A and time period T. We denote a subgradient in the subdifferential as GA,T ∈ ∂fT(A)."
  },
  {
    "prompt": "Scientific context on subdifferential, norm, Definition and their role in graph theory.",
    "response": "We first define the subdifferential of the ℓ2 norm. Definition 2 (Subdifferential of ℓ2 Norm). Let ∂∥x∥2 denote the subdifferential of the ℓ2 norm, i.e., ∂∥x∥2 = ( x ∥x∥2 , if x ̸= 0, {e : ∥e∥2 ≤1}, otherwise . The subdifferential is a unique point when x is nonzero, but it consists of all points within the unit ball when x = 0. Using the previous definition, we obtain the subdifferential of the objective function, ∂fT(A). Lemma 1."
  },
  {
    "prompt": "This is a technical discussion about subdifferential, ˆdt, Axt in theoretical computer science.",
    "response": "The subdifferential of fT(·) at a point A is ∂fT(A) = ( − T−1 ∑ t=0 ∂∥xt+1 −Axt∥2 ⊗xt ) , and the subdifferential of the fT(·) at the point ¯A is ∂fT( ¯A) = ( −∑ t∈K ˆdt ⊗xt −∑ t̸∈K et ⊗xt : ∥et∥2 ≤1,∀t ̸∈K ) , where K := {t | ¯dt ̸= 0} is the set of time indices of nonzero disturbances and ˆdt := ¯dt/∥¯dt∥2,∀t ∈K , are the normalized disturbance directions. The proof of Lemma 1 is straightforward and follows from a property of the Minkowski sum over convex sets."
  },
  {
    "prompt": "Scientific context on lemma, due, convexity and their role in graph theory.",
    "response": "Furthermore, due to the convexity of fT(A), the next lemma provides a useful bound on the difference between the objective function values at two distinct points. Lemma 2 ([28]). Given two arbitrary points A,B ∈Rn×n and any subgradient GA,T ∈∂fT(A), it holds that fT(B) ≥fT(A)+⟨GA,T,B−A⟩. To find the matrix ¯A, we propose Algorithm 1 that relies on the subgradient method. In Step (i), the subdifferential of the objective function at the current estimate is calculated."
  },
  {
    "prompt": "Understanding the computational aspects of Step, subdifferential, subgradient.",
    "response": "Since this subdifferential possibly contains multiple values, we choose a subgradient from the subdifferential in Step (ii). After that, a step size is selected in Step (iii). In Step (iv), a subgradient update is performed. Lastly, we update the objective function with the new data point in Step (v)."
  },
  {
    "prompt": "Scientific context on NSE, optimization, time and their role in graph theory.",
    "response": "The main feature of this algorithm is that we do not apply the subgradient algorithm to solve the current optimization problem (NSE) and instead we take one iteration to update our estimate of the matrix value at time t. Then, after a new measurement is taken, the optimization problem at time t +1 changes since an additional term is added to its objective function. Thus, as we update our estimates, the optimization problems also change over time."
  },
  {
    "prompt": "Read and understand this section about Algorithm, iteration, analyze from a research paper.",
    "response": "We analyze the convergence of Algorithm 1 under different step size selection rules and show that although the objective function changes at each iteration, our algorithm convergences to the ¯A."
  },
  {
    "prompt": "Explain a concept involving Choose, Update, NSE in computational complexity.",
    "response": "Algorithm 1 Subgradient Method for (NSE) Initialize ˆA(0) randomly for t = 0,...,T −1 do (i) Find ∂ft( ˆA(t)) = −∑t ℓ=0 ∂∥( ¯A−ˆA(t))xℓ+ ¯dℓ∥2 ⊗xℓ (ii) Choose G ˆA(t),t from ∂ft( ˆA(t)) (iii) Choose β (t) using an appropriate step size rule (iv) Update ˆA(t+1) = ˆA(t) −β (t)G ˆA(t),t (v) Update ft+1(A) = ft(A)+∥xt+1 −Axt∥2 end for Return ˆA(T) A."
  },
  {
    "prompt": "Scientific context on Step, Method, Size and their role in graph theory.",
    "response": "Subgradient Method with Best Step Size We analyze the subgradient method using the best step size, which is defined as the step size that maximizes improvement at each iteration. However, computing this step size requires knowledge of the ground-truth matrix ¯A, making it impractical in real-world applications. Nonetheless, studying this case offers valuable insights into the behavior of subgradient updates."
  },
  {
    "prompt": "Understanding the computational aspects of step, Appendix, size.",
    "response": "Let the step size in Algorithm 1 be chosen as β (t) = ⟨G ˆA(t),t, ˆA(t) −¯A⟩ ∥G ˆA(t),t∥2 F , It is shown in Appendix VII-A that this is the best step size possible. According to Theorem 1, the distance to the ground-truth matrix decreases by a factor of q 1−cos2( ˆθt) at each update with best step size. As long as this angle is not ±90◦or ±270◦, Algorithm 1 makes progress toward ¯A. Theorem 1."
  },
  {
    "prompt": "Read and understand this section about Algorithm, ˆθt, step from a research paper.",
    "response": "Under the best step size for Algorithm 1, it holds that ∥ˆA(t+1) −¯A∥F ≤ q 1−cos2( ˆθt)∥ˆA(t) −¯A∥F, where ˆθT denotes the angle between ˆA(t) −¯A and G ˆA(t),t. If ˆθt is obtuse, the step size will be negative, which must be avoided. To this end, we establish a positive lower bound on cos( ˆθt) in Theorem 2 and show that this angle remains acute. To guarantee that, we define the burn-in period as the number of samples required for a theoretical exact recovery."
  },
  {
    "prompt": "Explain a concept involving burn, log, min in computational complexity.",
    "response": "After the burn-in period, ¯A is the unique optimal solution of min fT(A) with high probability. Definition 3. Consider an arbitrary δ ∈(0,1). Let T (burn)(n, p,ρ,σ,δ) be defined as T (burn) := Θ max \u001a σ10 (1−ρ)3(1−p)2 , σ4 p(1−p) \u001b × \u0014 n2 log \u0012 1 (1−ρ)σ p(1−p) \u0013 +log \u0012 1 δ \u0013\u0015! . For every t ≥T (burn)(n, p,ρ,σ,δ), ¯A is the unique optimal solution to (NSE) with probability at least 1−δ. For convenience, we omit the argument (n, p,ρ,σ,δ) and denote the burn-in period as T (burn)."
  },
  {
    "prompt": "Read and understand this section about algorithm, NSE, analyze from a research paper.",
    "response": "We analyze the algorithm’s behavior after the burn-in period because it happens that the iterates of Algorithm 1 are chaotic when t is not large enough or equivalently when ¯A is not the unique solution of (NSE). The following theorem establishes that if the number of samples exceeds T (burn), the cosine of the angle can be lower-bounded by a positive term. Theorem 2."
  },
  {
    "prompt": "Explain a concept involving ˆθT, Define, angle in computational complexity.",
    "response": "Define ˆθT to be the angle between ˆA(T) −¯A and G ˆA(T),T, and let ^ ∥xT∥2 = ∑T−1 i=0 ∥xt∥2/T be the average norm of the states until time T. Suppose that Assumptions 1 and 2 hold. Then, as long as the sample complexity T ≥ T (burn)(n, p,ρ,σ,δ) and ˆA(T) ̸= ¯A, with probability at least 1−δ, the term cos( ˆθT) can be lower-bounded as cos( ˆθT) ≥Θ min ( 1, p(1−p) σ4^ ∥xT∥2 )! > 0."
  },
  {
    "prompt": "Understanding the computational aspects of bound, Theorem, large.",
    "response": "Theorem 2 provides a probabilistic lower bound on the cosine of the angle of interest when the sample complexity T is sufficiently large. The algorithm iterates move toward the ground truth ¯A when the ¯A is the global solution of (NSE). In this lower bound, the average norm of the system states over time, ^ ∥xT∥2, appears as a random variable. This term is known to have an upper bound on the order of p1/2/σ9(1−ρ) with high probability. Lemma 3 (Equation (63) in [15])."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Lemma, Equation.",
    "response": "Under the assumptions of Theorem 2, when T ≥T (burn)(n, p,ρ,σ,δ), the inequality ^ ∥xT∥2 ≤Θ p1/2 σ9(1−ρ) ! holds with probability at least 1−δ In order to establish the result in Lemma 3, we select θ := Θ(p(1−p)T/σ4) and ε := Θ( p p(1−p)2(1−ρ)2σ10) in Equation (63) of [15]. Combining this result with Theorem 2 leads to the main result of this subsection. Corollary 1."
  },
  {
    "prompt": "Read and understand this section about burn, ground-truth, time from a research paper.",
    "response": "Let D = ∥ˆA(T (burn)(n,p,ρ,σ,δ/2)) −¯A∥F denote the distance of the solution of the last iterate to the ground-truth ¯A at the end of the burn-in time. Define γ(p,ρ,σ) to be γ(p,ρ,σ) := q 1−min{1,Θ(p(1−p)2σ10(1−ρ)2)}."
  },
  {
    "prompt": "Understanding the computational aspects of Algorithm, conv, log.",
    "response": "Then, given an arbitrary ε > 0, Algorithm 1 under the best step size achieves ∥ˆA(T) −¯A∥F ≤ε with probability 1 −δ, when the number of iterations T is sufficiently large such that T > T (ε) := T (burn)(n, p,ρ,σ,δ/2) + T (conv)(p,ρ,σ,D,ε), where T (conv)(p,ρ,σ,D,ε) := log \u0012D ε \u0013 log \u0012 1 γ(p,ρ,σ) \u0013 . Algorithm 1 achieves linear convergence with high prob- ability after the burn-in period due to the log(D/ε) term."
  },
  {
    "prompt": "Understanding the computational aspects of phase, Corollary, burn-in.",
    "response": "Corollary 1 implies that to achieve an ε-level estimation error, the total number of required samples should be the sum of those needed for the burn-in phase and the linear convergence phase. With high probability, the burn-in period scales as max{(1−p)−2, p−1(1−p)−1} with respect to p and as n2 with respect to the system dimension."
  },
  {
    "prompt": "Read and understand this section about linear, sample, complexity from a research paper.",
    "response": "After the burn-in phase, the algorithm converges to the ground truth at a linear rate because cos( ˆθT) is lower-bounded by a positive factor given by min{1, p1/2(1 −p)σ5(1 −ρ)}. Consequently, the linear convergence is upper-bounded by γ(p,ρ,σ), and the sample complexity scales logarithmically with the problem parameters. Therefore, the sample complexity of the burn- in period dominates the sample complexity of the linear convergence phase."
  },
  {
    "prompt": "Read and understand this section about step, size, Polyak from a research paper.",
    "response": "Next, we now generalize the results to more practical step size rules. B. Subgradient Method with Polyak Step Size We analyze the Polyak step size. Unlike the best step size selection, the Polyak step size does not require knowledge of ¯A. However, it requires information about the optimal objective value at ¯A at time T, that is fT( ¯A). Under this step size rule, we can establish a result similar to Corollary 1. Theorem 3."
  },
  {
    "prompt": "Read and understand this section about Algorithm, Suppose, Assumptions from a research paper.",
    "response": "Consider Algorithm 1 under the Polyak step size defined as β (T) = (fT( ˆA(T))−fT( ¯A)) ∥G ˆA(T),T∥2 F . Suppose that Assumptions 1 and 2 hold. When the number of samples T is greater than T (ε), Algorithm 1 achieves ∥ˆA(T) −¯A∥F ≤ε, where T (ε) defined in Corollary 1."
  },
  {
    "prompt": "Scientific context on step, Polyak, optimal and their role in graph theory.",
    "response": "Although the Polyak step size relies on less information about the ground-truth system dynamics than the best step size (i.e., it needs to know an optimal scalar rather than an optimal matrix), it requires the same number of iterations as the best step size selection. These step sizes are implementable in practice (subject to some approximations) if some prior knowledge about the system dynamics is available. C."
  },
  {
    "prompt": "Explain a concept involving Step, Sizes, Diminishing in computational complexity.",
    "response": "Subgradient Method with Constant and Diminishing Step Sizes The step sizes discussed in previous sections require information about the ground-truth matrix ¯A and the optimal objective value fT( ¯A). While theoretically useful, these step sizes are impractical in real-world applications. Therefore, we explore the subgradient method with constant and diminishing step sizes. Unlike the best and Polyak step sizes, constant and diminishing step sizes do not guarantee descent at every iteration."
  },
  {
    "prompt": "Scientific context on recovery, solution, lead and their role in graph theory.",
    "response": "Instead, they lead to convergence within a neighborhood of the true solution rather than exact recovery. Consequently, results concerning the solution gap, ˆA(T) −¯A, are less relevant in this setting. Nevertheless, we establish the asymptotic convergence of the best average sub-optimality gap over time, defined as: min t∈[T (burn)(n,p,ρ,σ,δ),T] \u001a1 t (ft( ˆA(t))−ft( ¯A)) \u001b ."
  },
  {
    "prompt": "Read and understand this section about sub-optimality, gap, horizon from a research paper.",
    "response": "The best average sub-optimality gap represents the algorithm’s performance over its entire horizon, from the burn-in period to the final iteration. We scale the sub-optimality gap by 1/t to ensure comparability across time steps, as the objective function ft(A) in (NSE) evolves with t and consists of a sum of t terms. An ε-level estimation error at time t , i.e. ∥ˆA(t) −¯A∥F ≤ε, corresponds to a sub-optimality gap of tε."
  },
  {
    "prompt": "Read and understand this section about gap, step, sizes from a research paper.",
    "response": "Furthermore, we focus on the best average sub-optimality gap rather than the gap at the final iteration, as iterates fluctuate around the neighborhood of ¯A due to the nature of these step sizes. These step sizes may potentially remain large, preventing exact convergence, or diminish too quickly or too slowly to achieve a precise limit."
  },
  {
    "prompt": "Explain a concept involving sub-optimality, Additionally, average in computational complexity.",
    "response": "Additionally, we analyze the best average sub-optimality gap only after the burn-in period since ¯A may not be the global minimizer during this phase, leading to potential negative sub-optimality terms. Next, we demonstrate that under a constant step size, the best average sub-optimality gap of the subgradient algorithm asymptotically converges at a rate of T −1/2."
  },
  {
    "prompt": "Explain a concept involving burn, significantly, larger in computational complexity.",
    "response": "When T is significantly larger than T (burn), the upper bound for the best average sub-optimality gap can be simplified as Θ(Dp1/2/σ9(1−ρ)T 1/2). Hence, it asymptotically converges to zero with the rate T −1/2. Theorem 4. When the step size β (t) is set to the constant, β (t) = β = Θ Dσ9(1−ρ) p1/2(∑T t=T (burn) t2)1/2 !"
  },
  {
    "prompt": "Scientific context on burn, Algorithm, iterates and their role in graph theory.",
    "response": ", the best average sub-optimality gap for Algorithm 1 iterates can be bounded with probability at least 1−δ as min t∈[T (burn),T] \u001a1 t (ft( ˆA(t))−ft( ¯A)) \u001b ≤Θ Dp1/2 σ9(1−ρ) × (∑T t=T (burn) t2)1/2 ∑T t=T (burn) t ! . We establish a similar result for the diminishing step size. Theorem 5. Suppose that the step size β (t) takes the diminishing form β (t) = β/t, where β is defined as β := Θ \u0012 Dσ9(1−ρ) p1/2(T −T (burn))1/2 \u0013 ."
  },
  {
    "prompt": "Read and understand this section about burn, Algorithm, Theorem from a research paper.",
    "response": "Then, with probability at least 1−δ, the best average sub- optimality gap for Algorithm 1 iterates can be bounded as min t∈[T (burn),T] \u001a1 t (ft( ˆA(t))−ft( ¯A)) \u001b ≤Θ Dp1/2 σ9(1−ρ) × 1 (T −T (burn))1/2 ! . The proof of Theorem 5 is omitted as it follows the same reasoning as Theorem 4. The diminishing step size is chosen to satisfy ∑β (t) −→∞and ∑(β (t))2 < ∞, ensuring sufficient exploration and stable convergence without excessive os- cillations."
  },
  {
    "prompt": "Read and understand this section about Theorem, TIME, subgradient from a research paper.",
    "response": "Theorem 5 establishes that, under a diminishing step size, the best average sub-optimality gap asymptotically converges at a rate of T −1/2. IV. TIME COMPLEXITY ANALYSIS We compare the time complexities of the subgradient method and the exact solution of (NSE) using CVX. For the subgradient algorithm, each time period involves computing the subgradient, which has a per-iteration cost of O(Tn2). Running the algorithm for T iterations results in an overall complexity of O(T 2n2)."
  },
  {
    "prompt": "Scientific context on SOCP, NSE, second-order and their role in graph theory.",
    "response": "On the other hand, solving (NSE) using CVX reduces the problem to a second-order conic program (SOCP) with T +n2 variables with T second-order cone constraints (minimization of the sum of norms can be written as a SOCP [29]). Defining [T] = {0,1,...,T −1}, this problem is formulated as min vec(A)∈Rn2 ft∈R, t∈[T] T ∑ t=0 ft (SOCP) s.t. ft ≥∥xt+1 −(xt ⊗In)vec(A)∥2, t ∈[T]. The CVX solver employs primal-dual interior point methods to solve (SOCP)."
  },
  {
    "prompt": "Scientific context on complexity, log, number and their role in graph theory.",
    "response": "The number of iterations required to reach an ε-accurate solution is O(T 1/2 log(1/ε)) [30]. Since solving conic constraints has a worst-case complexity of O(n6), the per time period complexity is O(T 1/2n6), leading to an overall complexity of O(T 3/2n6). We know that T and T (burn) scale as Θ(n2). Consequently, the per-time period complexity of the subgradient method and the exact SOCP solution are O(n4) and O(n7), respectively."
  },
  {
    "prompt": "Understanding the computational aspects of SOCP, solving, approach.",
    "response": "This highlights that solving (SOCP) exactly requires significantly more computational power than the subgradient approach. Next, we present numerical experiments to validate the convergence results. V. NUMERICAL EXPERIMENTS We generate a system with ground-truth matrix ¯A whose singular values are uniform in (0,1). Given the probability p, disturbance vectors are set to zero with probability 1−p. With probability p, the disturbance ¯dt is defined as ¯dt := ℓt ˆdt."
  },
  {
    "prompt": "Read and understand this section about sample, min, uniform from a research paper.",
    "response": "Then, we sample ℓt ∼N (0,σ2 t ), where σ2 t := min{∥xt∥2 2,1/n}, and we sample ˆdt ∼uniform(Sn−1). This disturbance vector conforms to Assumption 2, and the values of the disturbance vectors are correlated over time. We generate 10 independent trajectories of the subgradient algorithm using 10 different systems. Then, we report the average solution gap and the average loss gap of Algorithm 1 at each period."
  },
  {
    "prompt": "Explain a concept involving Algorithm, gap, step in computational complexity.",
    "response": "The solution gap and the loss gap for Algorithm 1 at time T are ∥ˆA(T) −¯A∥F and fT( ˆA(T))−fT( ¯A), respectively. Algorithm 1 is experimented with different step size rules. The system has order n = 5, and p = 0.7. In addition to the best, Polyak, diminishing, and constant step sizes, we also introduce the backtracking step size. We initialize the step size at a constant value, and it is shrunk by constant factor γ ∈(0,1) until we achieve a sufficiently large descent."
  },
  {
    "prompt": "Explain a concept involving step, Armijo, size in computational complexity.",
    "response": "See Armijo step size in [31]. Figure 1 depicts the performance of Algorithm 1 under these different step sizes. Best and Polyak step sizes achieve fast and linear convergence as proven theoretically. Moreover, the backtracking step size performs similarly to the best and Polyak step sizes without using the knowledge of the ground truth ¯A or the optimal objective value fT( ¯A). Thus, the theoretical analysis of the backtracking step size is an interesting future direction."
  },
  {
    "prompt": "Scientific context on Algorithm, step, sizes and their role in graph theory.",
    "response": "Furthermore, despite the slower rates, the diminishing and constant step sizes converged to the neighborhood of the solution. We test the performance of Algorithm 1 with different system dimensions n and different probabilistic sparsity model parameters p. We choose backtracking and best step sizes to Fig. 1: Solution Gap and Loss Gap of Algorithm 1 with Different Step Sizes for n = 5, and p = 0.7 Fig."
  },
  {
    "prompt": "Explain a concept involving Gap, Loss, Solution in computational complexity.",
    "response": "2: Solution Gap and Loss Gap of Algorithm 1 with p = 0.7, Best and Backtracking Step Sizes, and n ∈{5,10,15} test the impact of different dimensions of the system states, n ∈{5,10,15} on the solution gap and loss gap when p is set to 0.7. Figure 2 shows that as n grows, the number of samples needed for convergence grows as n2. This aligns with the theoretical results from earlier sections. Moreover, the backtracking step size performs similarly to the best step size."
  },
  {
    "prompt": "Understanding the computational aspects of Figure, Likewise, algorithm.",
    "response": "Likewise, Figure 3 depicts the performance of the subgradient algorithm for the different probabilities of nonzero disturbance p ∈{0.5,0.7,0.8} with n = 5 using best and backtracking step sizes. As p increases, the convergence for the estimator needs more samples, as supported by the theoretical results. We perform experimentation on the subgradient selection in Step (ii) of Algorithm 1. Whenever the ℓ2 norm is less than 10−5, we consider its subdifferential as the unit ball."
  },
  {
    "prompt": "Understanding the computational aspects of subgradient, sphere, random.",
    "response": "For random subgradient selection, we select the subgradient randomly on the unit sphere. For the zero subgradients, we set the subgradient to zero. Lastly, we chose the minimum norm subgradient among all the available ones in the subdifferential. We test these selection criteria for a system with order n = 10 and p = 0.7 using the best step size. Figure 4 demonstrates that there is not a significant difference in terms of solution gap."
  },
  {
    "prompt": "Understanding the computational aspects of large, simulation, results.",
    "response": "Last but not least, we provide simulation results using the best step size for large systems of order n ∈{25,50,75}. We run these experiments for T = 10000. This is not possible with the CVX solver due to the high computational complexity of (SOCP). Nevertheless, the subgradient algorithm provides an accurate estimation for large systems within a reasonable time frame, as seen in Figure 5. VI."
  },
  {
    "prompt": "Scientific context on CONCLUSION, algorithm, model and their role in graph theory.",
    "response": "CONCLUSION We proposed and analyzed a subgradient algorithm for non-smooth system identification of linear-time invariant systems with disturbances following a probabilistic sparsity model. While the non-asymptotic theory for this setting is well-established in the literature, existing methods rely on solvers that use primal-dual interior-point methods for Fig. 3: Solution Gap and Loss Gap of Algorithm 1 with n = 5, Best and Backtracking Step Sizes, and p ∈{0.5,0.7,0.8} Fig."
  },
  {
    "prompt": "Understanding the computational aspects of Gap, Solution, Algorithm.",
    "response": "4: Solution Gap and Loss Gap of Algorithm 1 with Different Subgradient Selection Methods for n = 10 and p = 0.7 second-order conic programs, leading to high computational costs. To address this, we proposed a subgradient method that enables fast exact recovery. We demonstrated that the subgradient method with the best and Polyak step sizes converges linearly to the ground-truth system dynamics after a burn-in period."
  },
  {
    "prompt": "Read and understand this section about Additionally, solution, step from a research paper.",
    "response": "Additionally, we showed that constant and diminishing step sizes lead to convergence within a neighborhood of the true solution, with the best average sub-optimality gap asymptotically approaching zero. These theoretical results are supported by numerical simulations. Our experiments also highlight the practical effectiveness of the backtracking step size, suggesting it as a promising alternative. However, its theoretical properties remain an open question."
  },
  {
    "prompt": "Understanding the computational aspects of subgradient, current, implementation.",
    "response": "Furthermore, our current implementation computes the full subgradient at each iteration. To improve time complexity, a partial subgradient approach, similar to stochastic gradient descent, could be explored. VII. APPENDIX A. Proof of Theorem 1 Proof. Using the subgradient update iteration at time T, ˆA(T+1) = ˆA(T) −β (T)G ˆA(T),T, we have ∥ˆA(T+1) −¯A∥2 F = ∥( ˆA(T) −¯A)−β (T)G ˆA(T),T∥2 F = ∥ˆA(T) −¯A∥2 F +(β (T))2∥G ˆA(T),T∥2 F −2β (T)⟨G ˆA(T),T, ˆA(T) −¯A⟩."
  },
  {
    "prompt": "Scientific context on Substituting, ˆθT, step and their role in graph theory.",
    "response": "(1) Substituting the step size stated in the theorem yields ∥ˆA(T+1) −¯A∥2 F = ∥ˆA(T) −¯A∥2 F − (⟨G ˆA(T),T, ˆA(T) −¯A⟩)2 ∥G ˆA(T),T∥2 F ≤ q 1−cos2( ˆθT)∥ˆA(T) −¯A∥F. The last inequality uses the fact that ⟨G ˆA(T),T, ˆA(T) −¯A⟩= ∥G ˆA(T),T∥F∥ˆA(T) −¯A∥F cos( ˆθT). Fig. 5: Solution Gap and Loss Gap of Algorithm 1 with p = 0.7, Best Step Size, and n ∈{50,75,100} B. Proof of Theorem 2 Proof."
  },
  {
    "prompt": "This is a technical discussion about Lemma, ˆθT, cos in theoretical computer science.",
    "response": "Using Lemma 2, we can show that cos( ˆθT) = ⟨ˆA(T) −¯A,G ˆA(T),T⟩ ∥ˆA(T) −¯A∥F∥G ˆA(T),T∥F ≥ fT( ˆA(T))−fT( ¯A) ∥¯A−ˆA(T)∥F∥G ˆA(T),T∥F . We lower bound the difference between the objectives as fT( ˆA(T))−fT( ¯A) ≥ sup G ¯A,T ∈∂fT ( ¯A) n ⟨ˆA(T) −¯A,G ¯A,T⟩ o = sup ∥et∥≤1,t̸∈K (* ˆA(T) −¯A,−∑ t∈K ˆdt ⊗xt −∑ t̸∈K et ⊗xt +) = ∑ t∈K ( ¯A−ˆA(T))xt, ¯dt ∥¯dt∥2 + ∑ t̸∈K ∥( ¯A−ˆA(T))xt∥2."
  },
  {
    "prompt": "This is a technical discussion about Corollary, term, right-hand in theoretical computer science.",
    "response": "The term on the right-hand side is equivalent to the condition in Corollary 3 in [15] with f(xt) = xt and Z = ¯A−ˆA(T). Lemma 4 (Theorem 6 in [15]). Suppose that the Assumptions 1 and 2 are satisfied. Then, for any given B ∈Rn×n , the following holds with probability at least 1−δ: ∑ t∈K B−¯A, ¯dt ∥¯dt∥2 + ∑ t̸∈K ∥(B−¯A)xt∥2 ≥Θ \u0014cp(1−p)∥B−¯A∥FT σ4 \u0015 when the number of samples T ≥T (burn)(n, p,ρ,σ,δ)."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, norm, probability.",
    "response": "Using Lemma 4 and B = ˆA(T), if T ≥T (1)(δ), we have fT( ˆA(T))−fT( ¯A) ≥cp(1−p)∥ˆA(T) −¯A∥FT σ4 (2) with probability at least 1 −δ. Next, we upper-bound the norm of the subgradient as below: ∥G ˆA(T),T∥2 F = D − T ∑ t=0 ∂∥( ¯A−ˆA(T))xt + ¯dt∥2 ⊗xt, − T ∑ t=0 ∂∥( ¯A−ˆA(T))xt + ¯dt∥2 ⊗xt E ≤ T ∑ t=0 T ∑ m=0 ∥xt∥2∥xm∥2 = T−1 ∑ t=0 ∥xt∥2 !2 . (3) The second-to-last inequality holds due to the norm of subgra- dients being at most one and the Cauchy-Schwarz inequality."
  },
  {
    "prompt": "This is a technical discussion about ˆθT, statement, cos in theoretical computer science.",
    "response": "We simplify the expression as ∥G ˆA(T),T∥F ≤T ^ ∥xT∥2, where ^ ∥xT∥2 is defined in the theorem statement. We combine (2) and (3) with the lower bound on cos( ˆθT). With probability at least 1−δ, when the number of samples is greater than T ≥T (burn)(n, p,ρ,σ,δ), we have cos( ˆθT) ≥Θ min ( 1, p(1−p) σ4^ ∥xT∥2 )! > 0. C. Proof of Theorem 3 Proof."
  },
  {
    "prompt": "This is a technical discussion about subgradient, update, iteration in theoretical computer science.",
    "response": "Using the subgradient update iteration at time T, ˆA(T+1) = ˆA(T) −β (T)G ˆA(T),T, we have ∥ˆA(T+1) −¯A∥2 F = ∥( ˆA(T) −¯A)−β (T)G ˆA(T),T∥2 F = ∥( ˆA(T) −¯A)∥2 F +(β (T))2∥G ˆA(T),T∥2 F −2β (T)⟨G ˆA(T),T, ˆA(T) −¯A⟩ ≤∥( ˆA(T) −¯A)∥2 F +(β (T))2∥G ˆA(T),T∥2 F −2β (T)(fT( ˆA(T))−fT( ¯A)). (4) The last inequality uses the fact that (fT( ˆA(T))−fT( ¯A)) ≥ ⟨G ˆA(T),T, ˆA(T) −¯A⟩due to the properties of subgradient."
  },
  {
    "prompt": "Read and understand this section about theorem, Corollary, burn from a research paper.",
    "response": "Substituting the step size stated in the theorem yields ∥ˆA(T+1) −¯A∥2 F ≤∥( ˆA(T) −¯A)∥2 F −(fT( ˆA(T))−fT( ¯A))2 ∥G ˆA(T),T∥2 F . In Theorem 2 and Corollary 1, it is proved that whenever the number of samples is greater than T (burn)(n, p,ρ,σ,δ), we have (fT( ˆA(T))−fT( ¯A))2 ∥G ˆA(T),T∥2 F ≤(1−γ2)∥( ˆA(T) −¯A)∥2 F with probability at least 1−δ where γ is a positive constant that is less than 1."
  },
  {
    "prompt": "Understanding the computational aspects of burn, Proof, Lemma.",
    "response": "Thus, we have convergence to the ground truth with the linear rate γ because ∥ˆA(T+1) −¯A∥F ≤γ∥( ˆA(T) −¯A)∥F. The rest of the arguments are similar to those in Corollary 1. D. Proof of Theorem 4 Proof. We first define the following Lemma. Lemma 5. Let D = ∥ˆA(T (burn)) −¯A∥F be the distance of the iterate to the ground-truth ¯A at the end of the burn-in time. Then, the following holds: T ∑ t=T (burn) β (t)(ft( ˆA(t))−ft( ¯A)) ≤D2 + T ∑ t=T (burn) (β (t))2 t ∑ ℓ=0 ∥xℓ∥2 !2 ."
  },
  {
    "prompt": "This is a technical discussion about burn, Lemma, min in theoretical computer science.",
    "response": "Using Lemma 5 and taking the minimum of the left-hand side over periods between [T (burn),T] yields min t∈[T (burn),T] \u001a1 t (ft( ˆA(t))−ft( ¯A)) \u001b ≤ D2 +∑T t=T (burn)(β (t))2 (∑t ℓ=0 ∥xℓ∥2)2 2∑T t=T (burn)(β (t)t) . Using Lemma 3, we know that (∑t ℓ=0 ∥xℓ∥2) can be upper- bounded by Θ(tp1/2/σ9(1 −ρ)) with probability at least 1−δ whenever t is larger than the burn-in time. We have min t∈[T (burn),T] \u001a1 t (ft( ˆA(t))−ft( ¯A)) \u001b ≤Θ D2 +∑T t=T (burn)(β (t))2t2p/σ18(1−ρ)2 ∑T t=T (burn)(β (t)t) !"
  },
  {
    "prompt": "Read and understand this section about burn, probability, right-hand from a research paper.",
    "response": "with probability 1−δ. Setting the step size to be β (t) = β implies the term on the right-hand side to be Θ  D2 + β 2p σ18(1−ρ)2 ∑T t=T (burn) t2 β ∑T t=T (burn) t  . Minimizing the right-hand side with respect to β results in the following constant step size: β (t) = β = Θ Dσ9(1−ρ) p1/2(∑T t=T (burn) t2)1/2 ! ."
  },
  {
    "prompt": "This is a technical discussion about burn, Proof, Substituting in theoretical computer science.",
    "response": "Substituting this constant step size to the expression on the sub-optimality gap inequality provides the simplified upper bound that holds with probability at least 1−δ: min t∈[T (burn),T] \u001a1 t (ft( ˆA(t))−ft( ¯A)) \u001b ≤Θ Dp1/2 σ9(1−ρ) × (∑T t=T (burn) t2)1/2 ∑T t=T (burn) t ! . E. Proof of Lemma 5 Proof."
  },
  {
    "prompt": "Read and understand this section about burn, obtain, iterative from a research paper.",
    "response": "Using (4) and the iterative substitution between the time periods from T (burn)(n, p,ρ,σ,δ) to T, we obtain ∥ˆA(T+1) −¯A∥2 F ≤∥( ˆA(T (burn)) −¯A)∥2 F + T ∑ t=T (burn) (β (t))2∥G ˆA(t),t∥2 F −2 T ∑ t=T (burn) β (t)(ft( ˆA(t))−ft( ¯A)) ≤D2 + T ∑ t=T (burn) (β (t))2 t ∑ ℓ=0 ∥xℓ∥2 !2 −2 T ∑ t=T (burn) β (t)(ft( ˆA(t))−ft( ¯A)). The last inequality follows from the fact that D = ∥ˆA(T (burn)) − ¯A∥F and ∥G ˆA(t),t∥F can be upper-bounded by (∑t ℓ=0 ∥xℓ∥2)."
  },
  {
    "prompt": "Read and understand this section about side, right-hand, term from a research paper.",
    "response": "Since ∥ˆA(T+1) −¯A∥2 F is nonnegative, the last term on the right-hand side can be lower-bounded by 0. Switching the term with the negative coefficient from the right-hand side to the left-hand side completes the proof. REFERENCES [1] H.-F. Chen and L. Guo, Identification and stochastic adaptive control. Springer Science & Business Media, 2012. [2] L. Lennart, “System identification: theory for the user,” PTR Prentice Hall, Upper Saddle River, NJ, vol. 28, p. 540, 1999. [3] L."
  },
  {
    "prompt": "Read and understand this section about Convergence, IEEE, Ljung from a research paper.",
    "response": "Ljung, “Convergence analysis of parametric identification methods,” IEEE transactions on automatic control, vol. 23, no. 5, pp. 770–783, 1978. [4] M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht, “Learning without mixing: Towards a sharp analysis of linear system identification,” in Conference On Learning Theory. PMLR, 2018, pp. 439–473. [5] S. Dean, H. Mania, N. Matni, B. Recht, and S."
  },
  {
    "prompt": "This is a technical discussion about Foundations, Mathematics, Computational in theoretical computer science.",
    "response": "Tu, “On the sample complexity of the linear quadratic regulator,” Foundations of Computational Mathematics, vol. 20, no. 4, pp. 633–679, 2020. [6] A. Tsiamis and G. J. Pappas, “Finite sample analysis of stochastic system identification,” in 2019 IEEE 58th Conference on Decision and Control (CDC). IEEE, 2019, pp. 3648–3654. [7] M. K. S. Faradonbeh, A. Tewari, and G. Michailidis, “Finite time identification in unstable linear systems,” Automatica, vol. 96, pp. 342–353, 2018. [8] D. Foster, T."
  },
  {
    "prompt": "Scientific context on Rakhlin, Control, Learning and their role in graph theory.",
    "response": "Sarkar, and A. Rakhlin, “Learning nonlinear dynamical systems from a single trajectory,” in Learning for Dynamics and Control. PMLR, 2020, pp. 851–861. [9] Y. Sattar and S. Oymak, “Non-asymptotic and accurate learning of nonlinear dynamical systems,” Journal of Machine Learning Research, vol. 23, no. 140, pp. 1–49, 2022. [10] I. M. Ziemann, H. Sandberg, and N. Matni, “Single trajectory nonparametric learning of nonlinear dynamics,” in conference on Learning Theory. PMLR, 2022, pp. 3333–3364."
  },
  {
    "prompt": "Scientific context on IEEE, control, Ziemann and their role in graph theory.",
    "response": "[11] A. Tsiamis, I. Ziemann, N. Matni, and G. J. Pappas, “Statistical learning theory for control: A finite-sample perspective,” IEEE Control Systems Magazine, vol. 43, no. 6, pp. 67–97, 2023. [12] H. Feng and J. Lavaei, “Learning of dynamical systems under adversarial attacks,” in 2021 60th IEEE Conference on Decision and Control (CDC), 2021, pp. 3010–3017. [13] H. Feng, B. Yalcin, and J."
  },
  {
    "prompt": "Explain a concept involving ACC, Learning, American in computational complexity.",
    "response": "Lavaei, “Learning of dynamical systems under adversarial attacks - null space property perspective,” in 2023 American Control Conference (ACC), 2023, pp. 4179–4184. [14] B. Yalcin, H. Zhang, J. Lavaei, and M. Arcak, “Exact recovery for system identification with more corrupt data than clean data,” IEEE Open Journal of Control Systems, 2024. [15] H. Zhang, B. Yalcin, J. Lavaei, and E. D."
  },
  {
    "prompt": "Understanding the computational aspects of Sontag, Exact, Online.",
    "response": "Sontag, “Exact recovery guarantees for parameterized non-linear system identification problem under adversarial attacks,” 2024. [Online]. Available: https://arxiv.org/abs/2409.00276 [16] J. Kim and J. Lavaei, “Prevailing against adversarial noncentral disturbances: Exact recovery of linear systems with the l_1-norm estimator,” arXiv preprint arXiv:2410.03218, 2024. [17] M. J. Wainwright, High-Dimensional Statistics: A Non-Asymptotic Viewpoint, ser."
  },
  {
    "prompt": "Scientific context on Cambridge, Mathematics, Press and their role in graph theory.",
    "response": "Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. [18] R. Vershynin, High-Dimensional Probability: An Introduction with Applications in Data Science, ser. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. [19] N. Z. Shor, Minimization methods for non-differentiable functions. Springer Science & Business Media, 2012, vol. 3. [20] B. T. Polyak, “Introduction to optimization,” 1987. [21] D. P."
  },
  {
    "prompt": "Read and understand this section about Nonlinear, Journal, Society from a research paper.",
    "response": "Bertsekas, “Nonlinear programming,” Journal of the Operational Research Society, vol. 48, no. 3, pp. 334–334, 1997. [22] Y. Nesterov, “Primal-dual subgradient methods for convex problems,” Mathematical programming, vol. 120, no. 1, pp. 221–259, 2009. [23] ——, Introductory lectures on convex optimization: A basic course. Springer Science & Business Media, 2013, vol. 87. [24] K. J. Keesman, System identification: an introduction. Springer Science & Business Media, 2011. [25] J. Schoukens and L."
  },
  {
    "prompt": "Read and understand this section about Nonlinear, Magazine, IEEE from a research paper.",
    "response": "Ljung, “Nonlinear system identification: A user- oriented road map,” IEEE Control Systems Magazine, vol. 39, no. 6, pp. 28–99, 2019. [26] H. Feng and J. Lavaei, “Learning of dynamical systems under adversarial attacks,” in 2021 60th IEEE Conference on Decision and Control (CDC). IEEE, 2021, pp. 3010–3017. [27] H. Feng, B. Yalcin, and J. Lavaei, “Learning of dynamical systems under adversarial attacks-null space property perspective,” in 2023 American Control Conference (ACC). IEEE, 2023, pp."
  },
  {
    "prompt": "Explain a concept involving vol., programming, Vandenberghe in computational complexity.",
    "response": "4179–4184. [28] S. P. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2004. [29] F. Alizadeh and D. Goldfarb, “Second-order cone programming,” Mathematical programming, vol. 95, no. 1, pp. 3–51, 2003. [30] S. J. Wright, Primal-dual interior-point methods. SIAM, 1997. [31] L. Armijo, “Minimization of functions having lipschitz continuous first partial derivatives,” Pacific Journal of mathematics, vol. 16, no. 1, pp. 1–3, 1966."
  },
  {
    "prompt": "This is a technical discussion about Berkeley, University, California in theoretical computer science.",
    "response": "High Probability Complexity Bounds of Trust-Region Stochastic Sequential Quadratic Programming with Heavy-Tailed Noise Yuchen Fang1, Javad Lavaei2, Katya Scheinberg3, and Sen Na3 1Department of Mathematics, University of California, Berkeley 2Department of Industrial Engineering and Operations Research, University of California, Berkeley 3H."
  },
  {
    "prompt": "Scientific context on Engineering, Georgia, School and their role in graph theory.",
    "response": "Milton School of Industrial and Systems Engineering, Georgia Institute of Technology Abstract In this paper, we consider nonlinear optimization problems with a stochastic objective and determin- istic equality constraints. We propose a Trust-Region Stochastic Sequential Quadratic Programming (TR-SSQP) method and establish its high-probability iteration complexity bounds for identifying first- and second-order ϵ-stationary points."
  },
  {
    "prompt": "Scientific context on gradients, Hessians, algorithm and their role in graph theory.",
    "response": "In our algorithm, we assume that exact objective values, gradients, and Hessians are not directly accessible but can be estimated via zeroth-, first-, and second- order probabilistic oracles."
  },
  {
    "prompt": "Understanding the computational aspects of stationarity, light-tailed, zeroth-order.",
    "response": "Compared to existing complexity studies of SSQP methods that rely on a zeroth-order oracle with sub-exponential tail noise (i.e., light-tailed) and focus mostly on first-order stationarity, our analysis accommodates irreducible and heavy-tailed noise in the zeroth-order oracle and significantly extends the analysis to second-order stationarity."
  },
  {
    "prompt": "Scientific context on bounds, complexity, conditions and their role in graph theory.",
    "response": "We show that under weaker noise conditions, our method achieves the same high-probability first-order iteration complexity bounds, while also exhibiting promising second-order iteration complexity bounds. Specifically, the method identifies a first-order ϵ-stationary point in O(ϵ−2) iterations and a second-order ϵ-stationary point in O(ϵ−3) iterations with high probability, provided that ϵ is lower bounded by a constant deter- mined by the irreducible noise level in estimation."
  },
  {
    "prompt": "Read and understand this section about set, equality, validate from a research paper.",
    "response": "We validate our theoretical findings and evaluate the practical performance of our method on CUTEst benchmark test set. 1 Introduction In this paper, we consider stochastic optimization problems with deterministic equality constraints of the following form: min x∈Rd f(x), s.t. c(x) = 0, (1) where f : Rd →R is the stochastic objective and c : Rd →Rm are deterministic equality constraints."
  },
  {
    "prompt": "Explain a concept involving Problem, Hessian, gradient in computational complexity.",
    "response": "For Problem (1), we assume that the exact objective value f(x), together with its gradient ∇f(x) and Hessian ∇2f(x), cannot be evaluated accurately but can be estimated via stochastic probabilistic oracles. For the sake of generality, we do not specify the underlying true sampling distribution of f, since the oracles may have different sampling distributions, leading to intrinsically biased estimates."
  },
  {
    "prompt": "Scientific context on Betts, Ozekici, Mar and their role in graph theory.",
    "response": "Constrained stochastic optimization problems of form (1) appear ubiquitously in scientific and engi- neering areas, such as optimal control (Betts, 2010), portfolio optimization (C¸akmak and ¨Ozekici, 1 arXiv:2503.19091v1 [math.OC] 24 Mar 2025 2005), multi-stage optimization (Shapiro et al., 2021), supply chain network design (Santoso et al., 2005), and statistical machine learning (Cuomo et al., 2022). Deterministic constrained optimization has been extensively studied for decades."
  },
  {
    "prompt": "This is a technical discussion about SQP, Bertsekas, Sequential in theoretical computer science.",
    "response": "While numerous methods have been developed under favorable problem settings, Sequential Quadratic Programming (SQP) has proven to be robust and effective, especially for highly nonlinear problems (Bertsekas, 1982; Boggs and Tolle, 1995; Nocedal and Wright, 2006)."
  },
  {
    "prompt": "This is a technical discussion about Sun, Nocedal, SQP in theoretical computer science.",
    "response": "Recent studies on optimization problems where the objective function and constraints are evaluated subject to bounded noise further highlight the advantages of SQP methods (Sun and Nocedal, 2023; Lou et al., 2024; Oztoprak et al., 2023; Sun and Nocedal, 2024). Inspired by the success of SQP, a series of Stochastic SQP (SSQP) methods has been proposed to solve Problem (1), where the exact computations of objective quantities are replaced by the corresponding estimates obtained through sampling."
  },
  {
    "prompt": "Understanding the computational aspects of assumed, setup, quality.",
    "response": "The quality of the estimates is assumed to satisfy specific conditions that depend on different sampling frameworks. The first widely used framework is the fully stochastic setup, where at each iteration a single sam- ple or a fixed batch of samples is drawn to estimate the objective gradient, and a sequence βk assists in stepsize selection. Gradient estimates are typically assumed to be unbiased, with variance either bounded or subject to a growth condition. Under this setup, Berahas et al."
  },
  {
    "prompt": "Understanding the computational aspects of SSQP, Fang, introduced.",
    "response": "(2021) introduced the first line-search SSQP method, while Fang et al. (2024a) introduced the first trust-region SSQP method. Building on these designs, subsequent extensions in algorithm design and analysis have been reported (see, e.g., Berahas et al., 2024, 2023; Curtis et al., 2023a, 2024a,b; Beiser et al., 2023; Bollapragada et al., 2023; Berahas et al., 2025; Curtis et al., 2025; Kuang et al., 2025). The non-asymptotic itera- tion complexity has also been investigated under this setup."
  },
  {
    "prompt": "This is a technical discussion about Berahas, Based, Curtis in theoretical computer science.",
    "response": "Based on the algorithm in Berahas et al. (2021), Curtis et al. (2023b) established a worst-case iteration complexity of O(ϵ−4) for first-order ϵ- stationarity with high probability, assuming that the trial step is solved exactly in each iteration and βk is constant."
  },
  {
    "prompt": "Scientific context on Mahoney, iteration, SSQP and their role in graph theory.",
    "response": "Meanwhile, Na and Mahoney (2025) established the local convergence guarantee of the SSQP method with decaying βk, demonstrating the same O(ϵ−4) iteration complexity for first-order ϵ-stationarity while allowing the trial step to be solved inexactly in each iteration. In contrast to the fully stochastic setup, methods in the random model setup do not require un- biased gradient estimates, and the stepsizes are also selected adaptively."
  },
  {
    "prompt": "Explain a concept involving Hessians, gradients, zeroth in computational complexity.",
    "response": "This framework leverages zeroth-, first-, and second-order probabilistic oracles to estimate objective values, gradients, and Hessians, respectively, ensuring that the estimates satisfy proper, adaptive accuracy conditions with a high but fixed probability. Under this setup, Na et al. (2022a) introduced the first line-search SSQP method, while Fang et al. (2024b) introduced the first trust-region SSQP method with a second-order convergence guarantee."
  },
  {
    "prompt": "Understanding the computational aspects of Berahas, generalized, studies.",
    "response": "The studies have been further generalized in Na et al. (2023); Berahas et al. (2022); Qiu and Kungurtsev (2023) to accommodate inequality constraints, adaptive sampling mech- anisms, and robust designs. However, the above studies in the random model setup share two key limitations: (1) they assume the estimation noise vanishes with fixed probability, and (2) they lack a non-asymptotic, high-probability iteration complexity analysis. Recently, Berahas et al."
  },
  {
    "prompt": "Understanding the computational aspects of addressed, line-search, step-search.",
    "response": "(2025) addressed these two limitations in constrained optimization by analyzing a line-search (also referred to as step-search) method that allows for an irreducible noise in the estimation. The authors conducted a non-asymptotic complexity analysis and established an iter- ation complexity of O(ϵ−2) for first-order ϵ-stationarity with high probability, provided that ϵ exceeds a threshold determined by the irreducible noise level."
  },
  {
    "prompt": "Read and understand this section about noise, distribution, guarantees from a research paper.",
    "response": "However, their zeroth-order oracle assumes the noise follows a sub-exponential distribution, thereby excluding all heavy-tailed noise; and their method does not exhibit second-order convergence guarantees. Identifying only first-order stationary points can be sometimes unsatisfactory in nonlinear optimization, as such points may correspond to 2 saddle points or even local maxima, leading to highly sensitive solutions (Dauphin et al., 2014; Choro- manska et al., 2015; Jain et al., 2017)."
  },
  {
    "prompt": "Read and understand this section about knowledge, setup, high-probability from a research paper.",
    "response": "To the best of our knowledge, in the random model setup, high-probability iteration complexity for second-order stationarity has only been explored in the context of unconstrained optimization. In this setting, Cao et al. (2023) performed a non-asymptotic analysis for a trust-region method, demonstrating an iteration complexity of O(ϵ−2) for first-order ϵ-stationarity and O(ϵ−3) for second-order ϵ-stationarity."
  },
  {
    "prompt": "This is a technical discussion about sub-exponential, oracle, noise in theoretical computer science.",
    "response": "However, their zeroth-order probabilistic oracle still requires the noise to follow a sub-exponential distribution. Assuming the noise in the zeroth-order oracle following a sub-exponential distribution leads to a significant gap between non-asymptotic and asymptotic analyses, as such an oracle condition is not required in asymptotic analysis (Na et al., 2022a, 2023; Fang et al., 2024b)."
  },
  {
    "prompt": "Scientific context on noise, finite, heavy-tailed and their role in graph theory.",
    "response": "The sub-exponential noise condition can be restrictive, as it implies the existence of infinite-order moments, thereby excluding noise from any heavy-tailed distribution where only a finite number of moments are bounded. More importantly, there is no clear justification for the discrepancy between the noise conditions in the zeroth-order oracle and those in the first- and second-order oracles; the latter oracles do allow for heavy-tailed noise (e.g., only a finite second moment)."
  },
  {
    "prompt": "Explain a concept involving noise, heavy-tailed, discrepancy in computational complexity.",
    "response": "This discrepancy highlights the desire to relax light-tailed noise in the zeroth-order oracle to heavy-tailed noise when analyzing the complexity of the methods. In this paper, we make progress toward that goal. We introduce a trust-region SSQP method for constrained optimization in the random model setup that accommodates irreducible and heavy-tailed noise, and establish its high-probability iteration complexity bounds for achieving both first- and second-order ϵ-stationarities."
  },
  {
    "prompt": "Explain a concept involving Fang, Compared, second-order in computational complexity.",
    "response": "Our key contributions are summarized as follows. (a) Compared to Na et al. (2022a, 2023); Fang et al. (2024b), we allow irreducible noise in the zeroth-, first-, and second-order oracles, while their framework requires the noise to diminish with fixed prob- ability. In addition, we extend the second-order asymptotic convergence guarantee from Fang et al. (2024b) to establish the first second-order non-asymptotic guarantee for constrained problems."
  },
  {
    "prompt": "Explain a concept involving Compared, SSQP, Berahas in computational complexity.",
    "response": "(b) Compared to the step-search SSQP method in Berahas et al. (2025), we study a trust-region SSQP method and provide a complexity analysis for second-order stationarity. As a classic complement to the line-search scheme, the trust-region scheme computes the search direction and selects the stepsize jointly."
  },
  {
    "prompt": "Understanding the computational aspects of SQP, Hessian, scheme.",
    "response": "One significant difference from the line-search scheme is that the trust-region SQP subprob- lems remain well-defined without any Hessian regularizations, which are often necessary in the line- search scheme to ensure the search direction to be a decent direction. (c) Compared to Cao et al. (2023); Berahas et al."
  },
  {
    "prompt": "Understanding the computational aspects of KKT, stationarity, residual.",
    "response": "(2025) for unconstrained or constrained first-order complexity analysis, we refine the definition of the stopping criterion of the iteration by using the de- terministic KKT residual for first-order stationarity, and the maximum of deterministic KKT residual and deterministic negative curvature of the reduced Hessian for second-order stationarity. Cao et al. (2023) adopted a stopping criterion that depends on the Lipschitz constant and the irreducible noise level, while Berahas et al."
  },
  {
    "prompt": "Understanding the computational aspects of stopping, defined, function.",
    "response": "(2025) defined the stopping criterion based on the local model reduction of the merit function. Although different stopping criteria lead to the same complexity bounds, our def- inition seems more natural while necessitating additional efforts in the analysis. (d) Compared to the complexity analysis in Cao et al. (2023); Berahas et al. (2025), we significantly relax the parametric sub-exponential distribution assumption on the zeroth-order oracle noise."
  },
  {
    "prompt": "Scientific context on Specifi, cally, small and their role in graph theory.",
    "response": "Specifi- cally, instead of requiring infinite-order moments, we assume the noise has a bounded 1+δ moment for any (small) δ > 0. The key technical tools we leverage are the Burkholder-type inequality (Burkholder, 1973; Chen and Sung, 2020) and the martingale Fuk-Nagaev inequality (Fuk, 1973; Nagaev, 1979; Fan 3 et al., 2017), which help us quantify the tail behavior of the accumulated oracle noise."
  },
  {
    "prompt": "Explain a concept involving first-order, second-order, stationary in computational complexity.",
    "response": "When δ > 1, our complexity results suggest that the method can find a (first-order or second-order) stationary point within finite iterations almost surely. This matches the result in Berahas et al. (2025), where infinite- order moments are imposed."
  },
  {
    "prompt": "Scientific context on noise, designs, suggesting and their role in graph theory.",
    "response": "We highlight that our relaxation of the noise condition is achieved purely through sharper analysis and is independent of various line-search or trust-region designs, suggesting that the results in the aforementioned literature should also hold under heavy-tailed noise condition utilizing our analytical techniques. Under irreducible and heavy-tailed oracle noise conditions, we establish first- and second-order high-probability iteration complexity bounds for our trust-region SSQP method."
  },
  {
    "prompt": "Scientific context on point, iterations, ϵ-stationary and their role in graph theory.",
    "response": "In particular, we es- tablish that the method achieves a first-order ϵ-stationary point in O(ϵ−2) iterations and a second- order ϵ-stationary point in O(ϵ−3) iterations with high probability, provided that ϵ is chosen above a threshold determined by the irreducible noise levels. These results align with existing arguments for irreducible and light-tailed noise. Specifically, our results corroborate the first-order high-probability complexity bound established by Berahas et al."
  },
  {
    "prompt": "This is a technical discussion about SSQP, Cao, optimization in theoretical computer science.",
    "response": "(2025) for a step-search SSQP method in constrained stochastic optimization, as well as the first- and second-order high-probability complexity bounds de- rived by Cao et al. (2023) for a trust-region method in unconstrained stochastic optimization. We im- plement our method on problems from the CUTEst test set. The actual performance validates our the- oretical analysis. 1.1 Notation We use ∥·∥to denote the ℓ2 norm for vectors and the operator norm for matrices."
  },
  {
    "prompt": "This is a technical discussion about matrix, denotes, denote in theoretical computer science.",
    "response": "I denotes the identity matrix and 0 denotes the zero vector/matrix, whose dimensions are clear from the context. For the constraints c(x) : Rd →Rm, we let G(x) := ∇c(x) ∈Rm×d denote its Jacobian matrix and let ci(x) denote the i-th constraint for 1 ≤i ≤m (the subscript indexes the iteration). Define P(x) = I − G(x)T [G(x)G(x)T ]−1G(x) to be the projection matrix to the null space of G(x)."
  },
  {
    "prompt": "Understanding the computational aspects of Section, form, ker.",
    "response": "Then, we let Z(x) ∈ Rd×(d−m) form the orthonormal bases of ker(G(x)) such that Z(x)T Z(x) = I and Z(x)Z(x)T = P(x). Throughout the paper, we use an overline to denote a stochastic estimate of a quantity. For example, ¯f(x) denotes an estimate of f(x). 1.2 Structure of the paper We introduce the computation of trial steps and the algorithm design in Section 2. Probabilistic oracles that allow irreducible and heavy-tailed noise are introduced in Section 3."
  },
  {
    "prompt": "Explain a concept involving Section, Lagrangian, bounds in computational complexity.",
    "response": "In Section 4, we establish first- and second-order high-probability complexity bounds. Numerical experiments are presented in Section 5, and conclusions are presented in Section 6. 2 A Trust-Region SSQP Method Let L(x, λ) = f(x)+λT c(x) be the Lagrangian function of Problem (1), where λ ∈Rm represents the Lagrangian multipliers. For any ϵ > 0, we call (x∗, λ∗) a first-order ϵ-stationary point of (1) if ∥∇L(x∗, λ∗)∥= \u0012∇xL(x∗, λ∗) ∇λL(x∗, λ∗) \u0013 = \u0012∇f(x∗) + G(x∗)T λ∗ c(x∗) \u0013 ≤ϵ."
  },
  {
    "prompt": "Understanding the computational aspects of Lagrangian, Hessian, residual.",
    "response": "(2) We call (x∗, λ∗) a second-order ϵ-stationary point if, in addition to (2), τ(x∗, λ∗) ≥−ϵ, 4 where τ(x∗, λ∗) denotes the smallest eigenvalue of the reduced Lagrangian Hessian Z(x∗)T ∇2 xL(x∗, λ∗)Z(x∗). Here, we define ∇2 xL(x, λ) = ∇2f(x) + Pm i=1 λi∇2ci(x) as the Lagrangian Hessian with respect to the primal variable x. We denote the optimality residual by ∥∇xL(x, λ)∥, the feasibility residual by ∥∇λL(x, λ)∥(i.e., ∥c(x)∥), and the KKT residual by ∥∇L(x, λ)∥."
  },
  {
    "prompt": "Understanding the computational aspects of Section, xLk, denote.",
    "response": "Given the k-th iterate (xk, λk), we denote gk = ∇f(xk), ∇2fk = ∇2f(xk), and ¯gk, ¯∇2fk as their estimates, which are constructed via probabilistic oracles in Section 3. Similarly, we denote ck, Gk, {∇2ci k}m i=1. We define the estimated Lagrangian gradient as ¯∇Lk = ( ¯∇xLk, ck) with ¯∇xLk = ¯gk + GT k λk, and the estimated Lagrangian Hessian (with respect to x) as ¯∇2 xLk = ¯∇2fk + Pm i=1 λi k∇2ci k."
  },
  {
    "prompt": "Scientific context on algorithm, section, computation and their role in graph theory.",
    "response": "In the remainder of this section, we first introduce step computation, a key component of our algorithm, and then describe the algorithm."
  },
  {
    "prompt": "Scientific context on trust-region, Step, approximation and their role in graph theory.",
    "response": "2.1 Step computation Given the iterate xk and the trust-region radius ∆k in the k-th iteration, we formulate the trust-region SSQP subproblem to solve for the trial step ∆xk by using a quadratic approximation of the objective and a linear approximation of the constraints in (1), with an additional trust-region constraint: min ∆x∈Rd 1 2∆xT ¯Hk∆x + ¯gT k ∆x, s.t. ck + Gk∆x = 0, ∥∆x∥≤∆k, (3) where ¯Hk approximates the Lagrangian Hessian ∇2 xLk."
  },
  {
    "prompt": "This is a technical discussion about issue, straint, Due in theoretical computer science.",
    "response": "Due to the presence of the trust-region con- straint, we need to deal with a potential infeasibility issue: {∆x ∈Rd : ck + Gk∆x = 0} ∩{∆x ∈Rd : ∥∆x∥≤∆k} = ∅. To address this issue, we follow the existing literature (Vardi, 1985; Byrd et al., 1987; Omojokun, 1989; Fang et al., 2024a,b) by relaxing the linear constraint in (3) via the trial step decomposition. In par- ticular, we decompose the trial step ∆xk into two orthogonal components, ∆xk = wk +tk, which are computed separately."
  },
  {
    "prompt": "Explain a concept involving accounts, step, ker in computational complexity.",
    "response": "The normal step wk ∈im(GT k ) accounts for feasibility, while the tangential step tk ∈ker(Gk) accounts for optimality. To satisfy the trust-region constraint, we further decompose the trust-region radius to control the lengths of the normal and tangential steps, respectively. Depending on whether we aim to achieve first- or second-order convergence, the trial step ∆xk is computed using either a gradient step or an eigen step."
  },
  {
    "prompt": "Understanding the computational aspects of convergence, achieve, KKT.",
    "response": "The gradient step reduces the KKT residual to achieve first- order convergence, whereas the eigen step explores the negative curvature of the reduced Lagrangian Hessian to achieve second-order convergence. However, when searching for second-order stationary points, the Maratos effect (Conn et al., 2000) can cause iterates to stagnate at saddle points."
  },
  {
    "prompt": "This is a technical discussion about SOC, Maratos, LRS in theoretical computer science.",
    "response": "To mitigate this special issue in constrained optimization, we compute a second-order correction (SOC) step and append it to the trial step whenever the Maratos effect is detected. 2.1.1 Gradient steps When computing the gradient step, we decompose the trust-region radius as ˘∆k = ∥cRS k ∥ ∥¯∇LRS k ∥· ∆k and e∆k = ∥¯∇xLRS k ∥ ∥¯∇LRS k ∥· ∆k to control the lengths of wk and tk, respectively, and have ∆xk = wk + tk."
  },
  {
    "prompt": "This is a technical discussion about cRS, xLRS, LRS in theoretical computer science.",
    "response": "Here, cRS k := ck/∥Gk∥, ¯∇xLRS k := ¯∇xLk/∥¯Hk∥, ¯∇LRS k := ( ¯∇xLRS k , cRS k ) denote rescaled feasibility, optimality, and KKT 5 residual vectors, respectively. Decomposing the radius based on the rescaled residuals preserves the scale invariance property, so that if the objective and/or constraints are scaled by a (positive) scalar, the radius decomposition and further step computation remain unchanged."
  },
  {
    "prompt": "Explain a concept involving xLk, residuals, decomposition in computational complexity.",
    "response": "Such a desirable property cannot be achieved if using original residuals ∥ck∥and ∥¯∇xLk∥in the above radius decomposition. Suppose Gk has full row rank, we define vk := −GT k [GkGT k ]−1ck. Without the trust-region constraint, ck+Gk∆xk = 0 would imply wk = vk since Gktk = 0. However, the trust-region constraint enforces us to set the normal step wk by shrinking vk as wk = ¯γkvk with ¯γk := min{ ˘∆k/∥vk∥, 1}."
  },
  {
    "prompt": "This is a technical discussion about Hkwk, Zkuk, forms in theoretical computer science.",
    "response": "(4) The tangential step is set as tk = Zkuk, where Zk ∈Rd×(d−m) forms the orthonormal bases of ker(Gk) and uk ∈Rd−m. We compute uk by (inexactly) solving the following subproblem: min u∈Rd−m m(u) := 1 2uT ZT k ¯HkZku + (¯gk + ¯Hkwk)T Zku, s.t. ∥u∥≤e∆k. (5) We require the inexact solution uk to achieve at least a fixed fraction κfcd ∈(0, 1] of the Cauchy re- duction on m(·); that is m(uk) −m(0) ≤−κfcd 2 ∥ZT k (¯gk + ¯Hkwk)∥min \u001a e∆k, ∥ZT k (¯gk + ¯Hkwk)∥ ∥ZT k ¯HkZk∥ \u001b ."
  },
  {
    "prompt": "Read and understand this section about method, Condition, Chapter from a research paper.",
    "response": "(6) Condition (6) is standard in the trust-region literature, which is achievable by computing the Cauchy point of (5) or applying various approaches to solve (5), including the two-dimensional subspace minimization method, the dogleg method, and the Steihaug’s algorithm (see Nocedal and Wright, 2006, Chapter 4 for more details). 2.1.2 Eigen steps We define ¯τk to be the smallest eigenvalue of ZT k ¯HkZk and ¯τ + k := | min{¯τk, 0}|."
  },
  {
    "prompt": "Explain a concept involving cRS, step, Zkuk in computational complexity.",
    "response": "Parallel to the gradient step ∆xk = wk + tk, the normal step wk is still computed as in (4) but the radius decomposition is replaced by ˘∆k = ∥cRS k ∥ ∥(cRS k , ¯τ RS+ k )∥ · ∆k and e∆k = ¯τ RS+ k ∥(cRS k , ¯τ RS+ k )∥ · ∆k, where ¯τ RS+ k := ¯τ + k /∥¯Hk∥is the rescaled negative curvature. The tangential step is still tk = Zkuk with uk (inexactly) solved from (5), but the reduction condition is adjusted to (¯gk + ¯Hkwk)T Zkuk ≤0, ∥uk∥≤e∆k, uT k ZT k ¯HkZkuk ≤−κfcd · ¯τ + k e∆2 k."
  },
  {
    "prompt": "Explain a concept involving κfcd, steps, conditions in computational complexity.",
    "response": "(7) When the above conditions are satisfied, uk achieves the curvature reduction: m(uk) −m(0) ≤−κfcd 2 ¯τ + k e∆2 k < 0, where we use κfcd ∈(0, 1] to denote the fraction in both gradient steps and eigen steps for simplicity. 6 Conditions in (7) are standard in the literature (e.g., see Chapter 7.5 in Conn et al., 2000) and can be satisfied by computing uk in different approaches."
  },
  {
    "prompt": "Explain a concept involving ζRS, HkZk, eigenvalue in computational complexity.",
    "response": "For example, we can compute ¯ζk, the eigenvector of ZT k ¯HkZk corresponding to the eigenvalue ¯τk, and then rescale it as ¯ζRS k := ±¯ζk · e∆k/∥¯ζk∥. The rescaled vector ¯ζRS k satisfies (7) with κfcd = 1, provided that ± is properly chosen. In practice, it suffices to compute a good approximation of ¯ζk, which can be achieved by employing methods such as truncated conjugate gradient and truncated Lanczos methods. 2.1.3 Second-order correction steps It is observed by Byrd et al."
  },
  {
    "prompt": "Scientific context on step, simultaneously, point and their role in graph theory.",
    "response": "(1987) that when xk is close to a saddle point, the (gradient or eigen) step ∆xk may increase f(x) and ∥c(x)∥simultaneously. Since no reduction is made by ∆xk, the current iterate is not updated. Even worse, this issue cannot be resolved by reducing the stepsize, implying that the iterate is trapped at the saddle point. This phenomenon, known as the (second-order) Maratos effect, arises from the inaccurate linear approximation of constraints."
  },
  {
    "prompt": "Read and understand this section about SOC, Maratos, SSQP from a research paper.",
    "response": "To address this, the second-order correction (SOC) step dk is computed to better capture the curvature of the constraints: dk = −GT k [GkGT k ]−1 {c(xk + ∆xk) −ck −Gk∆xk} . (8) We will not compute SOC steps in every iteration, but only when the Maratos effect is likely to occur, that is, when the iterate fails to update and is close to the feasible region (i.e.{x : c(x) = 0}). 2.2 Trust-region SSQP design We now present the design of our trust-region SSQP method and summarize it in Algorithm 1."
  },
  {
    "prompt": "Scientific context on Section, updating, step and their role in graph theory.",
    "response": "Each iteration of the method comprises four components: (i) estimating the objective value, gradient, and Hessian using the probabilistic oracles introduced later in Section 3; (ii) computing a trial step and/or an SOC step as described in Section 2.1; (iii) updating the parameter of the merit function; and (iv) updating the iterate and the trust-region radius. We use α to indicate the final goal of stationarity, with α = 0 for first-order stationarity and α = 1 for second-order stationarity."
  },
  {
    "prompt": "Scientific context on Lagrangian, gradient, estimate and their role in graph theory.",
    "response": "In the k-th iteration, given the iterate xk, the trust-region radius ∆k, and the merit parameter ¯µk, we first obtain the gradient estimate ¯gk. Then, we compute the La- grangian multiplier ¯λk = −[GkGT k ]−1Gk¯gk and the Lagrangian gradient ¯∇Lk = ¯gk + GT k ¯λk. For the Hessian estimate, • if α = 0: we generate any matrix ¯Hk to approximate the Lagrangian Hessian ∇2 xLk and set ¯τ + k = 0."
  },
  {
    "prompt": "Read and understand this section about compute, min, Hessian from a research paper.",
    "response": "• if α = 1: we obtain the Hessian estimate ¯∇2fk, compute ¯Hk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k, and set ¯τk as the smallest eigenvalue of ZT k ¯HkZk and ¯τ + k = | min{¯τk, 0}|. Then, we decide whether to perform a gradient step or an eigen step by checking ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b ≥¯τ + k ∆k (∆k + ∥ck∥) . (9) If (9) holds, we compute the gradient step ∆xk as in Section 2.1.1; otherwise, we compute the eigen step ∆xk as in Section 2.1.2."
  },
  {
    "prompt": "Explain a concept involving merit, Predk, estimated in computational complexity.",
    "response": "Next, we define the predicted (local) model reduction of an (estimated) ℓ2 merit function Lµ(x) = f(x) + µ∥c(x)∥as Predk = ¯gT k ∆xk + 1 2∆xT k ¯Hk∆xk + ¯µk(∥ck + Gk∆xk∥−∥ck∥), (10) 7 and update the merit parameter ¯µk ←ρ¯µk until Predk ≤−κfcd 2 max \u001a ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b , ¯τ + k ∆k (∆k + ∥ck∥) \u001b . (11) With the updated merit parameter, we compute the (estimated) actual reduction of the merit function L¯µk(x)."
  },
  {
    "prompt": "Scientific context on fsk, xsk, Aredk and their role in graph theory.",
    "response": "Specifically, we set xsk = xk+∆xk, obtain the function estimates ¯fk and ¯fsk, and compute the (estimated) actual reduction as Aredk = ¯L¯µk(xsk) −¯L¯µk(xk) = ¯fsk −¯fk + ¯µk(∥csk∥−∥ck∥). (12) Finally, we check the following conditions to update xk and ∆k: (a): Aredk −ϑα Predk ≥η and (b): max \u001a ∥¯∇Lk∥ max{1, ∥¯Hk∥}, ¯τ + k \u001b ≥η∆k, (13) where ϑα = 2ϵf if α = 0 and ϑα = 2ϵf +ϵ3/2 g if α = 1. (ϵf, ϵg > 0 are irreducible noise levels in the ob- jective value and gradient estimates, respectively."
  },
  {
    "prompt": "Explain a concept involving Case, Section, SOC in computational complexity.",
    "response": "See Section 3 for details). • Case 1: (13a) holds. We update the iterate as xk+1 = xsk. Furthermore, if (13b) holds, we increase the trust-region radius by ∆k+1 = min{γ∆k, ∆max}. Otherwise, we decrease the trust-region radius by ∆k+1 = ∆k/γ. • Case 2: (13a) does not hold and α = 1. In this case, we decide whether to perform a SOC step to recheck (13a). If ∥ck∥≤r, we compute a SOC step dk (cf. Section 2.1.3), set xsk = xk +∆xk +dk and re-estimate ¯fsk."
  },
  {
    "prompt": "Understanding the computational aspects of Case, Aredk, recheck.",
    "response": "Then, we recompute Aredk as in (12) and recheck (13a). If (13a) holds, we go to Case 1 above; if (13a) does not hold, we go to Case 3 below. Otherwise, if ∥ck∥> r, the SOC step is not triggered and we directly go to Case 3 below. • Case 3: (13a) does not hold and α = 0. We do not update the current iterate by setting xk+1 = xk, and decrease the trust-region radius as ∆k+1 = ∆k/γ."
  },
  {
    "prompt": "This is a technical discussion about Omojokun, Yuan, Byrd in theoretical computer science.",
    "response": "When ϑα = 0, (13a) reduces to the standard condition in both deterministic and stochastic trust- region methods (see Powell and Yuan, 1990; Byrd et al., 1987; Omojokun, 1989; Heinkenschloss and Ridzal, 2014; Bandeira et al., 2014; Fang et al., 2024b). Inspired by Cao et al. (2023), we introduce a relaxation term ϑα in (13a) to accommodate the irreducible noise in the probabilistic oracles."
  },
  {
    "prompt": "This is a technical discussion about Aredk, Predk, updated in theoretical computer science.",
    "response": "Since both Aredk and Predk are negative, subtracting a ϑα > 0 in the numerator increases the value on the left-hand side of (13a), making it more likely for the iterate to be updated. When xk is updated, we determine whether to increase the trust-region radius based on (13b). In principle, (13b) quantifies how confident we are about the trust-region approximation at the given stationarity."
  },
  {
    "prompt": "This is a technical discussion about satisfied, trust-region, radius in theoretical computer science.",
    "response": "When (13b) is satisfied, the trust-region radius is increased to allow for a larger trial step in the next iteration, potentially enabling greater progress. Conversely, if (13b) is not satisfied, the trust-region radius is reduced to ensure more cautious movement in the subsequent iteration. We end this section by formalizing the randomness."
  },
  {
    "prompt": "Understanding the computational aspects of σ-algebras, iteration, define.",
    "response": "We define F−1 ⊆F0 ⊆F1 · · · as a filtration of σ-algebras, where Fk−1 = σ({xi}k i=0), ∀k ≥0, contains all the randomness before performing the k-th iteration. Defining Fk−0.5 = σ({xi}k i=0 ∪{¯gk, ¯∇2fk}), we find for all k ≥0, σ(xk, ∆k) ⊆Fk−1 and σ(∆xk, ¯λk, ¯µk, dk) ⊆Fk−0.5. 8 Algorithm 1 A Trust-Region SSQP Method 1: Input: Initial iterate x0 and radius ∆0 = ∆max, and parameters η ∈(0, 1), κfcd ∈(0, 1], ¯µ0, r, ϵf, ϵg > 0, ρ, γ > 1."
  },
  {
    "prompt": "Explain a concept involving compute, stationarity, Obtain in computational complexity.",
    "response": "2: Set α = 0 for first-order stationarity and α = 1 for second-order stationarity. 3: for k = 0, 1, · · · do 4: Obtain ¯gk and compute ¯λk and ¯∇Lk. 5: If α = 1, obtain ¯∇2fk, compute ¯Hk and the smallest eigenvalue ¯τk of ZT k ¯HkZk, and set ¯τ + k = | min{¯τk, 0}|. Otherwise, let ¯Hk be certain approximation of ∇2Lk and set ¯τ + k = 0. 6: If (9) holds, compute ∆xk as a gradient step; otherwise, compute ∆xk as an eigen step. 7: Perform ¯µk ←ρ¯µk until Predk satisfies (11)."
  },
  {
    "prompt": "Scientific context on Set, Case, xsk and their role in graph theory.",
    "response": "8: Set xsk = xk + ∆xk, obtain ¯fk, ¯fsk, and compute Aredk as in (12). 9: if (13a) holds then ▷(Case 1) 10: Set xk+1 = xsk. 11: if (13b) holds then 12: Set ∆k+1 = min{γ∆k, ∆max}. 13: else 14: Set ∆k+1 = ∆k/γ. 15: end if 16: else if α = 1 and ∥ck∥≤r then ▷(Case 2) 17: Compute SOC step dk, set xsk = xk + ∆xk + dk, re-estimate ¯fsk, and recompute Aredk. 18: If (13a) holds, perform Lines 10-15; otherwise, perform Line 20. 19: else ▷(Case 3) 20: Set xk+1 = xk, ∆k+1 = ∆k/γ."
  },
  {
    "prompt": "Explain a concept involving Set, Probabilistic, Oracles in computational complexity.",
    "response": "21: end if 22: Set ¯µk+1 = ¯µk. 23: end for 3 Probabilistic Oracles with Irreducible and Heavy-Tailed Noise In this section, we introduce probabilistic oracles that accommodate irreducible and heavy-tailed noise. In each iteration, the zeroth-, first-, and second-order probabilistic oracles generate estimates of the ob- jective value ¯f(x, ξ), the objective gradient ¯g(x, ξ), and the objective Hessian ¯∇2f(x, ξ), respectively, where ξ denotes a random variable defined on some probability space."
  },
  {
    "prompt": "Explain a concept involving estimates, probability, satisfy in computational complexity.",
    "response": "These estimates are required to satisfy certain adaptive accuracy conditions with a high but fixed probability. Our oracle design does not prespecify a specific method for generating estimates and allows for estimates to have irreducible noise (ϵf, ϵg, ϵh below), leading to biased estimates. This is a significant extension of Bandeira et al. (2014); Blanchet et al. (2019); Fang et al. (2024b); Na et al. (2022a, 2023), which required the esti- mation noise to diminish with a fixed probability."
  },
  {
    "prompt": "Read and understand this section about remove, parametric, assumptions from a research paper.",
    "response": "Furthermore, we remove all parametric assumptions about the estimation noise. In particular, Cao et al. (2023); Berahas et al. (2025) assumed that the noise in the zeroth-order oracle follows a sub- exponential distribution in each iteration (see (19) below). Such a condition essentially assumes the ex- istence of infinite-order moments, thereby excluding all distributions with heavy tails and limiting the scope of problems their analysis can cover."
  },
  {
    "prompt": "Explain a concept involving noise, Additionally, condition in computational complexity.",
    "response": "Additionally, the light-tailed noise condition in the zeroth-order oracle contrasts with the noise conditions in the first- and second-order oracles, which indeed allow for heavy-tailed noise. To resolve this discrepancy, as shown in (18), we relax the noise 9 condition to require only the existence of a (1+δ)-order moment for any small δ > 0, thereby covering noise from broad class of heavy-tailed distributions."
  },
  {
    "prompt": "Read and understand this section about corresponds, finding, stationary from a research paper.",
    "response": "Let ϵh, ϵg, ϵf > 0, eϵf ∈(0, ϵf], κh, κg, κf ≥0, and ph, pg, pf ∈(0, 1) be user-specified parameters, ∆k denote the trust-region radius, and recall that α = 0 corresponds to finding first-order stationary points and α = 1 corresponds to finding second-order stationary points. We first introduce the probabilistic second-order oracle defined at the current iterate xk."
  },
  {
    "prompt": "Read and understand this section about Corollary, Fang, Hessian from a research paper.",
    "response": "The or- acle indicates that the noise of the objective Hessian estimate satisfies an accuracy condition of ϵh + O(∆k) with probability at least 1 −ph; that is, it allows for irreducible noise no less than ϵh even if ∆k →0 (cf. Fang et al. (2024b), Corollary 4.14). Definition 3.1 (Probabilistic second-order oracle). When α = 1, given xk, the oracle computes ¯∇2fk := ¯∇2f(xk, ξh k), an estimate of the objective Hessian ∇2fk, such that Ak = \b ∥¯∇2fk −∇2fk∥≤ϵh + κh∆k satisfies P(Ak | Fk−1) ≥1 −ph."
  },
  {
    "prompt": "Understanding the computational aspects of Hessian, Recall, Section.",
    "response": "(14) Recall from Section 2.2 that for first-order convergence (α = 0), we do not have to estimate the ob- jective Hessian. Instead, we only require an arbitrary matrix ¯Hk to superficially approximate the La- grangian Hessian, as long as its norm is bounded with fixed probability. Specifically, when α = 0, we impose (14) but re-define Ak = \b ∥¯Hk∥≤κB for some constant κB ≥1. Next, we introduce the probabilistic first-order oracle defined at xk."
  },
  {
    "prompt": "Scientific context on noise, accuracy, condition and their role in graph theory.",
    "response": "It indicates that the noise of the gradient estimate satisfies an accuracy condition of ϵg +O(∆α+1 k ) with probability at least 1−pg; thus, it allows for irreducible noise with a level at least ϵg. Definition 3.2 (Probabilistic first-order oracle). Given xk, the oracle computes ¯gk := ¯g(xk, ξg k), an estimate of the objective gradient gk, such that Bk = {∥¯gk −gk∥≤ϵg + κg∆α+1 k } satisfies P(Bk | Fk−1) ≥1 −pg."
  },
  {
    "prompt": "Read and understand this section about Finally, xsk, SOC from a research paper.",
    "response": "(15) Finally, we introduce the zeroth-order probabilistic oracle, which is defined at both the current it- erate xk and the trial iterate xsk (since we need to evaluate the objective value for both points). Here, xsk = xk + ∆xk if the SOC step is not performed and xsk = xk + ∆xk + dk if the SOC step is performed. Definition 3.3 (Probabilistic zeroth-order oracle)."
  },
  {
    "prompt": "Explain a concept involving xsk, esk, computes in computational complexity.",
    "response": "Given xk and xsk, the oracle computes ¯f(xk, ξf k) and ¯f(xsk, ξf sk), which are estimates of the objective function value f(xk) and f(xsk). Let e(x, ξ) = | ¯f(x, ξ) −f(x)| be the absolute error with ek := e(xk, ξf k) and esk := e(xsk, ξf sk). The zeroth-order oracle has to satisfy the following three conditions. •(i) The absolute errors ek and esk are sufficiently small with a fixed probability: Ck = \b max (ek, esk) ≤ϵf + κf∆α+2 k satisfies P(Ck | Fk−1/2) ≥1 −pf."
  },
  {
    "prompt": "Scientific context on esk, eϵf, small and their role in graph theory.",
    "response": "(16) •(ii) The mean absolute errors are sufficiently small: max \b E[ek | Fk−1], E[esk | Fk−1/2] ≤eϵf. (17) •(iii) One of the following tail conditions is satisfied: (iii.1) Heavy-tailed condition. {ek}k have bounded 1 + δ moment for some δ > 0 (same for {esk}k). In other words, for some constant Υf > 0, we have max n E h |ek −E [ek | Fk−1]|1+δ | Fk−1 i , E h esk −E \u0002 esk | Fk−1/2 \u0003 1+δ | Fk−1/2 io ≤Υf. (18) 10 (iii.2) Sub-exponential tail condition."
  },
  {
    "prompt": "Read and understand this section about esk, exp, satisfied from a research paper.",
    "response": "For some constants v, b > 0, ek and esk satisfy max n E h eλ(ek−E[ek|Fk−1]) | Fk−1 i , E h eλ(esk−E[esk|Fk−1/2]) | Fk−1/2 io ≤exp \u0012λ2v2 2 \u0013 , ∀λ ∈[0, 1 b]. (19) When (iii.1) heavy-tailed condition is satisfied, we say ek, esk are generated via the proba- bilistic heavy-tailed zeroth-order oracle, while when (iii.2) sub-exponential condition is satisfied, we say ek, esk are generated via the probabilistic sub-exponential zeroth-order oracle. Remark 3.4."
  },
  {
    "prompt": "Scientific context on remark, discuss, probabilistic and their role in graph theory.",
    "response": "We further discuss the probabilistic zeroth-order oracle in this remark. • The conditions (i) and (ii) in (16) and (17) are standard in the literature (Berahas et al., 2025; Cao et al., 2023), which, however, do not provide any tail information regarding the noise. Thus, existing literature often additionally imposed (iii.2) for the considered zero-order oracle."
  },
  {
    "prompt": "Understanding the computational aspects of eϵf, condition, provided.",
    "response": "• For the sub-exponential oracle, if eϵf < ϵf, then condition (16) can be implied by (19), provided that v, b are sufficiently small (i.e., the probability mass of ek, esk is almost concentrated around their means). See Remark 4.22 for a rigorous discussion. However, (19) should only be used to capture the tail behavior of ek, esk, and requiring infinitesimally small scalars of v, b is overly stringent for the oracle (though the tail behavior remains sub-exponential)."
  },
  {
    "prompt": "This is a technical discussion about condition, interconnections, conditions in theoretical computer science.",
    "response": "Thus, one may not need to consider the interconnections among conditions (16)–(19). • The heavy-tailed condition in (iii.1) significantly relaxes the sub-exponential condition in (iii.2) by reducing the requirement from the existence of infinite-order moments to a finite 1+δ moment."
  },
  {
    "prompt": "This is a technical discussion about Burkholder, Fuk, Nagaev in theoretical computer science.",
    "response": "Lever- aging the Burkholder-type inequality (Burkholder, 1973; Chen and Sung, 2020) and the martingale Fuk-Nagaev inequality (Fuk, 1973; Nagaev, 1979; Fan et al., 2017), we establish high-probability com- plexity bounds for any δ > 0. However, we would like to emphasize a crucial gap in the convergence properties between the cases δ ∈(0, 1] and δ ∈(1, ∞)."
  },
  {
    "prompt": "Read and understand this section about Theorem, sequent, discussion from a research paper.",
    "response": "As demonstrated in Theorem 4.16 and the sub- sequent discussion, when δ ∈(1, ∞), the algorithm finds a (first- and second-order) ϵ-stationary point in a finite number of iterations almost surely. In contrast, this almost-sure convergence property is not guaranteed when δ ∈(0, 1], indicating that a bounded second moment in the objective value esti- mation noise is insufficient to ensure almost-sure convergence."
  },
  {
    "prompt": "Explain a concept involving Fang, analysis, discrepancy in computational complexity.",
    "response": "This discrepancy highlights a key gap between our non-asymptotic analysis and the asymptotic analysis in Fang et al. (2024b), where almost- sure convergence can be established when the noise in objective value estimation has a bounded second moment. • In the zeroth-order oracle, we follow the existing literature and impose conditions on the absolute objective value estimation errors ek and esk separately."
  },
  {
    "prompt": "Explain a concept involving fsk, conditions, esk in computational complexity.",
    "response": "However, we note that imposing accuracy conditions on the error difference esk −ek is sufficient for the analysis. That is, we can redefine ek := |( ¯fsk −¯fk) −(fsk −fk)| and impose oracle conditions on the newly define ek. To end this section, we discuss the sample size required to satisfy the above introduced probabilis- tic oracles when we draw independent samples from a distribution P and use sample average as the es- timates of the objective quantities."
  },
  {
    "prompt": "Explain a concept involving min, sample, Hessian in computational complexity.",
    "response": "Denoting the sample sets for estimating ¯∇2fk, ¯gk, ¯fk, ¯fsk by ξh k, ξg k, ξf k, ξf sk respectively, and the sample size by |·|, when each realization of the objective Hessian, gradient, and value estimates is unbiased with bounded variance, the oracle conditions are satisfied provided |ξh k| ≥ Ch ph(ϵh + κh∆k)2 , |ξg k| ≥ Cg pg(ϵg + κg∆α+1 k )2 , min{|ξf k|, |ξf sk|} ≥ Cf min{pf(ϵf + κf∆α+2 k )2,eϵ2 f} (20) 11 for some constants Ch, Cg, Cf > 0 (by Chebyshev’s inequality)."
  },
  {
    "prompt": "Understanding the computational aspects of Hessian, Complexity, section.",
    "response": "Such results indicate that samples of sizes O(ϵ−2 h ), O(ϵ−2 g ), O(eϵ−2 f ) are sufficient for generating Hessian, gradient and value estimates. 4 High-Probability Complexity Bounds In this section, we analyze the first- and second-order high-probability complexity bounds for the present trust-region SSQP method. In Section 4.1, we introduce fundamental lemmas."
  },
  {
    "prompt": "This is a technical discussion about Sections, oracles, assumption in theoretical computer science.",
    "response": "In Sections 4.2 and 4.3, we establish the complexity bounds under probabilistic heavy-tailed and sub-exponential zeroth-order oracles, respectively. We first state the assumption. Assumption 4.1. Let Ω⊆Rd be an open convex set containing the iterates and trial points {xk, xsk}. The objective f(x) is twice continuously differentiable and bounded below by finf over Ω. The gradi- ent ∇f(x) and Hessian ∇2f(x) are both Lipschitz continuous over Ω, with constants L∇f and L∇2f, respectively."
  },
  {
    "prompt": "Scientific context on Lipschitz, Ωwith, Jacobian and their role in graph theory.",
    "response": "Analogously, the constraint c(x) is twice continuously differentiable, its Jacobian G(x) is Lipschitz continuous over Ωwith the constant LG. For 1 ≤i ≤m, the Hessian of the i-th constraint, ∇2ci(x), is Lipschitz continuous over Ωwith constant the L∇2c. We also assume that there exist constants κc, κ∇f, κ1,G, κ2,G > 0 such that ∥ck∥≤κc, ∥∇fk∥≤κ∇f, κ1,G · I ⪯GkGT k ⪯κ2,G · I, ∀k ≥0."
  },
  {
    "prompt": "This is a technical discussion about Byrd, Powell, Yuan in theoretical computer science.",
    "response": "Assumption 4.1 is standard in the SQP literature (Byrd et al., 1987; Powell and Yuan, 1990; El-Alem, 1991; Conn et al., 2000; Berahas et al., 2021, 2024; Curtis et al., 2024b; Fang et al., 2024a,b). To analyze first-order complexity, it suffices to assume that f(x) and c(x) are continuously differen- tiable, without conditions on Hessians ∇2f(x) and ∇2ci(x). Assumption 4.1 implies that Gk has full row rank, √κ1,G ≤∥Gk∥≤√κ2,G and ∥GT k [GkGT k ]−1∥≤1/√κ1,G."
  },
  {
    "prompt": "This is a technical discussion about GkGT, grangian, counterpart in theoretical computer science.",
    "response": "Consequently, both the true La- grangian multiplier λk = −[GkGT k ]−1Gk∇fk and the estimated counterpart ¯λk = −[GkGT k ]−1Gk¯gk are well defined. Additionally, ∥∇2f(x)∥≤L∇f and ∥∇2ci(x)∥≤LG for 1 ≤i ≤m over Ω. 4.1 Fundamental lemmas In this section, we analyze basic properties of the trust-region SSQP method. Lemmas in this section apply to both the heavy-tailed and the sub-exponential oracles. We first show that on the event Ak ∩Bk (cf."
  },
  {
    "prompt": "Read and understand this section about bound, Hessian, constructed from a research paper.",
    "response": "(14) and (15)), the Hessian estimate ¯Hk constructed for second-order convergence has an upper bound. Lemma 4.2. Under Assumption 4.1 with α = 1, there exists a positive constant κB ≥1 such that ∥¯Hk∥≤κB on the event Ak ∩Bk. Proof. See Appendix A.1. ■ In the next lemma, we demonstrate that for the second-order stationarity, the difference between the true Lagrangian Hessian ∇2 xLk and its estimate ¯Hk is bounded by O(∆k)+O(ϵh+ϵg) on the event Ak ∩Bk."
  },
  {
    "prompt": "Explain a concept involving min, estimate, quantity in computational complexity.",
    "response": "Furthermore, the difference between τ + k := | min{τk, 0}| and its estimate ¯τ + k is bounded by the same quantity. This lemma ensures that ¯τ + k is an accurate estimate of τ + k provided that both the objective gradient and Hessian estimates are accurate. Lemma 4.3. Under Assumption 4.1 with α = 1, we have ∥∇2 xLk−¯Hk∥≤ϵH+κH∆k and |τ + k −¯τ + k | ≤ ϵH + κH∆k on the event Ak ∩Bk, where κH := κh + √mκgLG∆max √κ1,G and ϵH := ϵh + √mLG √κ1,G ϵg. 12 Proof. See Appendix A.2."
  },
  {
    "prompt": "Explain a concept involving xsk, Lsk, SOC in computational complexity.",
    "response": "■ Let us define Lsk ¯µk := L¯µk(xsk) and Lk ¯µk := L¯µk(xk), where ¯µk is the merit parameter in the k-th iteration after the update (11). Here, xsk = xk + ∆xk if the SOC step is not performed and xsk = xk+∆xk+dk if the SOC step is performed. The following two lemmas examine the difference between the reduction in the merit function Lsk ¯µk−Lk ¯µk and the predicted reduction Predk in (10) among the first T iterations, for any finite T."
  },
  {
    "prompt": "Understanding the computational aspects of SOC, event, step.",
    "response": "We first show that on the event Ak ∩Bk and without the SOC step, the difference has an upper bound ϵg∆k + O(∆2 k). Lemma 4.4. Under Assumption 4.1, for any T ≥1 and any 0 ≤k ≤T −1, on the event Ak ∩Bk and suppose the SOC step is not performed, we have Lsk ¯µk −Lk ¯µk −Predk ≤ϵg∆k + Υ1∆2 k, where Υ1 = κg max{1, ∆max} + 1 2(L∇f + κB + ¯µT−1LG). Proof. See Appendix A.3. ■ Next, we show that on the event Ak ∩Bk with the SOC step, the difference between the two reduc- tions is ϵ3/2 g +O(∆2 k +∆3 k)."
  },
  {
    "prompt": "Explain a concept involving max, Lemma, mLG in computational complexity.",
    "response": "Compared to Lemma 4.4, the error bound includes an additional constant ϵ3/2 g , while the linear term of ∆k is removed. Lemma 4.5. Under Assumption 4.1, for any T ≥1 and any 0 ≤k ≤T −1, on the event Ak ∩Bk and suppose the SOC step is performed, we have Lsk ¯µk −Lk ¯µk −Predk ≤ϵ3/2 g + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + Υ2∆3 k, where Υ2 = κg + 1 + L∇2f + κh 2 + L2 G∆max(0.5L∇f + √m¯µT−1LG) κ1,G + 0.5√mL∇2c(L∇f∆max + κ∇f) + 0.5√mLG(κg∆max + L∇f + 2¯µT−1LG) √κ1,G . Proof. See Appendix A.4."
  },
  {
    "prompt": "Understanding the computational aspects of stopping, Section, points.",
    "response": "■ For our non-asymptotic analysis, we recall the definitions of first- and second-order stationarity points in Section 2 and define the stopping time based on ϵ-stationary points. Definition 4.6 (Stopping time). For first-order stationarity (α = 0), the stopping time is defined as the index of the first iteration in which the true KKT residual enters the neighborhood of radius ϵ: Tϵ = min{k : ∥∇Lk∥≤ϵ}."
  },
  {
    "prompt": "This is a technical discussion about true, KKT, stopping in theoretical computer science.",
    "response": "For second-order stationarity (α = 1), the stopping time is defined as the index of the first iteration in which the true KKT residual and the true eigenvalue both enter the neighborhood of radius ϵ: Tϵ = min{k : max(∥∇Lk∥, τ + k ) ≤ϵ}. 13 We define the stopping time directly using the deterministic KKT residual and the negative eigen- value of the reduced Lagrangian Hessian. In comparison, Berahas et al."
  },
  {
    "prompt": "Read and understand this section about KKT, model, reduction from a research paper.",
    "response": "(2025) defined the first-order stopping time indirectly through the model reduction of the merit function. When transforming the model reduction to the deterministic KKT residual, the minimum of the true merit parameter is in- volved, making it challenging to measure the true KKT residual and desiring a more involved analysis. In addition, the stopping time in Cao et al. (2023) depends on the Lipschitz constants and the irre- ducible noise level. Similar to Berahas et al. (2025); Cao et al."
  },
  {
    "prompt": "Read and understand this section about max, noise, assumption from a research paper.",
    "response": "(2023), we require ϵ to exceed a threshold determined by irreducible noise levels ϵf, ϵg, and ϵh. Specifically, we impose the following assumption. Assumption 4.7. For the stationarity level ϵ, we assume If α = 0 : ϵ > 1 Υ3 s 8γ4¯µT−1ϵf κfcdη3¯µ0(p −1/2) + Υ4 Υ3 ϵg, If α = 1 : ϵ > 1 Υ5 3 s 2¯µT−1γ6 max{∆max, 1}(4ϵf + ϵ3/2 g ) κfcdη3¯µ0(p −1/2) + Υ6 Υ5 ϵg + Υ7 Υ5 ϵh, where Υ3, Υ4, Υ5, Υ6, Υ7 are constants independent to irreducible noise levels and are defined in Lemmas 4.8 and 4.9."
  },
  {
    "prompt": "Understanding the computational aspects of SOC, failure, oracle.",
    "response": "Furthermore, the failure oracle probabilities ph, pg, pf are small enough such that 1 −ph −pg −2pf =: p ∈(1/2, 1). Here, p represents the lower bound of probability that Ak∩Bk∩Ck∩C′ k holds, where C′ k denotes the event that accurate ¯fk, ¯fsk are regenerated after computing the SOC step. For notational consistency, we assume C′ k holds when the SOC step is not performed."
  },
  {
    "prompt": "Read and understand this section about stationarity, Assumption, demonstrated from a research paper.",
    "response": "As demonstrated in Assumption 4.7, for first-order stationarity, we need ϵ ≥O(√ϵf + ϵg); for second-order stationarity, we need ϵ ≥O( 3√ϵf + √ϵg + ϵh). These orders matches the ones in Berahas et al. (2025); Cao et al. (2023). In the next two lemmas, we show that before the algorithm terminates, if Ak ∩Bk ∩Ck ∩C′ k holds and the trust-region radius is sufficiently small, then both two conditions in (13) hold. We first consider α = 0. Lemma 4.8."
  },
  {
    "prompt": "Read and understand this section about κfcd, Assumptions, event from a research paper.",
    "response": "Under Assumptions 4.1, 4.7, and the event Ak ∩Bk ∩Ck ∩C′ k, for α = 0 and k < Tϵ, if ∆k ≤Υ3∥∇Lk∥−Υ4ϵg (21) with Υ3, Υ4 given by Υ3 = κfcd(1 −η) 4κf + κg + 2Υ1 + κB and Υ4 = κfcd(1 −η) + 2 4κf + κg + 2Υ1 + κB , then (13a) and (13b) both hold. Proof. See Appendix A.5. ■ In the next lemma, we show the same result holds for α = 1. Lemma 4.9."
  },
  {
    "prompt": "Read and understand this section about max, κfcd, min from a research paper.",
    "response": "Under Assumptions 4.1, 4.7, and the event Ak ∩Bk ∩Ck ∩C′ k, for α = 1 and k < Tϵ, if ∆k ≤Υ5 max{τ + k , ∥∇Lk∥} −Υ6ϵg −Υ7ϵh (22) 14 with Υ5, Υ6, Υ7 given by Υ5 = (1 −η)κfcd min{1, r} (4κf + κg) max{∆max, 1} + κB + 2Υ1 + 2Υ2 + (κH + η)(1 −η)κfcd min{1, r}, Υ6 = κfcd(1 −η) + 2 + 2√mLG/√κ1,G (4κf + κg) max{∆max, 1} + κB + 2Υ1 + 2Υ2 + (κH + η)(1 −η)κfcd min{1, r}, Υ7 = 2 4κf max{∆max, 1} + 2Υ1 + 2Υ2 + (κH + η)(1 −η)κfcd min{1, r}, then (13a) and (13b) both hold."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, defined, κfcd.",
    "response": "Here, κH is derived in Lemma 4.3, Υ1 is defined in Lemma 4.4, and Υ2 is defined in Lemma 4.5. Proof. See Appendix A.6. ■ In the next lemma, we consider the reduction on the stochastic merit function by iterations where (13a) and (13b) both hold. For brevity, we define the function hα(·) as hα(∆) =      κfcd 2 η3∆2 for α = 0, κfcd 2 max{∆max, 1}η3∆3 for α = 1. Lemma 4.10."
  },
  {
    "prompt": "Explain a concept involving Assumption, iteration, event in computational complexity.",
    "response": "Under Assumption 4.1 and the event Ak ∩Bk ∩Ck ∩C′ k, if (13a) and (13b) both hold in the k-th iteration, then we have ¯Lk+1 ¯µk −¯Lk ¯µk ≤−hα(∆k) + ϑα. Proof. See Appendix A.7. ■ To further proceed our analysis, we manipulate the lower bound of ϵ in Assumption 4.7 and define the following threshold b∆= ( Υ3ϵ −Υ4ϵg for α = 0, Υ5ϵ −Υ6ϵg −Υ7ϵh for α = 1. (23) Note that for k < Tϵ, we have ∥∇Lk∥> ϵ when α = 0 and max{∥∇Lk∥, τ + k } > ϵ when α = 1."
  },
  {
    "prompt": "Read and understand this section about max, Lemmas, fore from a research paper.",
    "response": "There- fore, b∆is smaller than the upper bounds of conditions in Lemmas 4.8 and 4.9. This implies that when ∆k ≤b∆and Ak∩Bk∩Ck∩C′ k holds, conclusions in both lemmas will apply. Without loss of generality, we suppose b∆≤∆max. Otherwise, we can always increase ∆max to ensure this condition. In fact, when α = 0, b∆is independent of ∆max; thus, a large ∆max trivially ensures b∆≤∆max. When α = 1, by Assumption 4.7, b∆grows at a rate of 3√∆max; thus, increasing ∆max still ensures b∆≤∆max."
  },
  {
    "prompt": "Explain a concept involving max, trust-region, radius in computational complexity.",
    "response": "The trust-region radius ∆k, k ≥0 always has the formula ∆k = γl∆0 with ∆0 = ∆max, where l is a non-positive integer. Therefore, we define the following threshold: b∆′ = max l∈Z {γl∆0 : γl∆0 ≤γ−1 b∆}. The threshold b∆′ is the largest trust-region radius that is less or equal to γ−1 b∆. Remark 4.11. We require b∆′ ≤γ−1 b∆instead of b∆′ ≤b∆. This additional γ−1 ensures that when ∆k ≤b∆′ and the trust-region radius increases in the k-th iteration, we will have ∆k+1 = γ∆k ≤∆max."
  },
  {
    "prompt": "Read and understand this section about Lemma, analysis, iteration from a research paper.",
    "response": "This property is essential to our analysis, as utilized in Lemma 4.12 (c). 15 To derive the complexity bounds, we categorize iterations k = 0, 1, · · · , T −1 into different classes by defining the following indicator variables: • Ik for accurate iteration: We set the indicator Ik = 1 if Ak ∩Bk ∩Ck ∩C′ k holds and call the k-th iteration accurate. Otherwise we set Ik = 0 and call the k-th iteration inaccurate."
  },
  {
    "prompt": "Understanding the computational aspects of call, k-th, iteration.",
    "response": "• Θk for sufficient iteration: We set the indicator Θk = 1 if (13a) and (13b) both hold and call the k-th iteration sufficient. Otherwise we set Θk = 0 and call the k-th iteration insufficient. • Λk for large iteration: We set the indicator Λk = 1 if min{∆k, ∆k+1} ≥b∆′ and call the k-th iteration large. We set Λk = 0 if max{∆k, ∆k+1} ≤b∆′ and call the k-th iteration small. Note that min{∆k, ∆k+1} ≥b∆′ and max{∆k, ∆k+1} ≤b∆′ are mutually disjoint."
  },
  {
    "prompt": "This is a technical discussion about max, min, implies in theoretical computer science.",
    "response": "To see why, consider the only possible case where min{∆k, ∆k+1} = b∆′ = max{∆k, ∆k+1}, which implies ∆k = ∆k+1 = b∆′. However, by the definition of b∆′, we have b∆′ ≤γ−1∆max. According to the algorithm design, the trust-region radius updates as either ∆k+1 = γ∆k or ∆k+1 = ∆k/γ, leading to a contradiction. In the next lemma, we characterize the relations of iterations in different classes. Lemma 4.12."
  },
  {
    "prompt": "Explain a concept involving ΛkΘk, logγ, Assumptions in computational complexity.",
    "response": "Under Assumptions 4.1, 4.7, for any T ≥1, we have (a) PT−1 k=0 ΛkΘk ≥PT−1 k=0 Λk(1 −Θk) −logγ(∆0/b∆′); (b) PT−1 k=0 ΛkΘk ≥1 2 PT−1 k=0 Λk −0.5 logγ(∆0/b∆′); (c) PT−1 k=0 (1 −Λk)(1 −Θk) ≥PT−1 k=0 (1 −Λk)Θk; (d) PT k=0(1 −Λk)Ik ≤PT k=0(1 −Λk)(1 −Ik). Proof. See Appendix A.8. ■ Remark 4.13. We discuss the insights behind the above lemma."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, number, large.",
    "response": "• In Lemma 4.12 (a), we demonstrate that among the first T iterations, the number of iterations that are large and sufficient (i.e., ΛkΘk = 1) is no less that the number of iterations that are large and insufficient (i.e., Λk(1 −Θk) = 1), subtracted by a constant logγ(∆0/b∆′). This conclusion is intuitive: when an iteration is large, its trust-region radius is lower bounded by a constant b∆′."
  },
  {
    "prompt": "Read and understand this section about number, radius, iterations from a research paper.",
    "response": "Thus, to maintain this lower bound, among all large iterations, the number of sufficient iterations (in which the radius increases) must be no less than the number of insufficient iterations (in which the radius decreases), except for a constant logγ(∆0/b∆′) representing the smallest number of iterations to reduce the radius from ∆0 to b∆′."
  },
  {
    "prompt": "This is a technical discussion about number, Lemma, small in theoretical computer science.",
    "response": "• In Lemma 4.12 (c), we show that the number of iterations that are small and insufficient (i.e., (1 −Λk)(1 −Θk) = 1) is no less than the number of iterations that are small and sufficient (i.e., (1 − Λk)Θk = 1). In fact, when an iteration is small, its trust-region radius is upper bounded by b∆′."
  },
  {
    "prompt": "Scientific context on iterations, number, radius and their role in graph theory.",
    "response": "Thus, to maintain the upper bound, among all small iterations, the number of iterations that are sufficient (in which the radius increases) must be no more than the number of iterations that are insufficient (in which the radius decreases). The next lemma follows from the conclusion that P(Ik = 1 | Fk−1/2) ≥p (recall p = 1−ph −pg − 2pf) and the Azuma-Hoeffding inequality, which establishes a probabilistic bound for the number of accurate iterations. 16 Lemma 4.14."
  },
  {
    "prompt": "Understanding the computational aspects of Proof, Appendix, exp.",
    "response": "For all T ≥1, bp ∈[0, p) with p ∈(1/2, 1), and both α = 0 and α = 1, we have P \"T−1 X k=0 Ik < bp T # ≤exp \u001a −(bp −p)2 2p2 T \u001b . Proof. See Appendix A.9. ■ In the following lemma, we consider an event that will happen with probability zero. Lemma 4.15. For all T ≥1, p ∈(1/2, 1), and both α = 0 and α = 1, we have P ( Tϵ > T −1, T−1 X k=0 Ik ≥pT, T−1 X k=0 ΘkIkΛk < \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆ \u0013 −1 ) = 0. Proof. See Appendix A.10."
  },
  {
    "prompt": "Read and understand this section about exponential, oracles, presented from a research paper.",
    "response": "■ Until now, we have presented all the preparatory lemmas that hold for both heavy-tailed and sub- exponential zeroth-order oracles. In the next two subsections, we establish the first- and second-order complexity bounds for these two oracles separately."
  },
  {
    "prompt": "Understanding the computational aspects of Complexity, zeroth-order, Definition.",
    "response": "4.2 Complexity bounds with heavy-tailed zeroth-order oracle In this section, we establish the first and second-order high-probability complexity bounds when the noise of the objective value estimate follows the probabilistic heavy-tailed zeroth-order oracle (i.e., conditions (i), (ii), and (iii.1) of Definition 3.3). We will demonstrate that P{Tϵ ≤T −1} ≥a probability that converges to 1 as T →∞."
  },
  {
    "prompt": "Explain a concept involving complexity, method, uni in computational complexity.",
    "response": "In the next theorem, we establish a generic iteration complexity bound of our method, which uni- fies the analysis of first and second-order stationarity. The corresponding complexity bounds are then derived in subsequent corollaries. Theorem 4.16. Under Assumptions 4.1, 4.7, and the probabilistic heavy-tailed zeroth-order oracle (i.e., bounded 1 + δ moment), for any s ≥0, when bp ∈ 1 2 + ¯µT−1(2ϵf + ϑα + 2s) ¯µ0hα(γ−2 b∆) , p !"
  },
  {
    "prompt": "This is a technical discussion about exp, eϵf, finf in theoretical computer science.",
    "response": "(24) and T > bp −0.5 ¯µT−1 −2ϵf + ϑα + 2s ¯µ0hα(γ−2 b∆) !−1 · f(x0) + ¯µ0∥c(x0)∥−finf ¯µ0hα(γ−2 b∆) + logγ(∆0/b∆) + 2 2¯µT−1 ! , (25) we have P{Tϵ ≤T −1} ≥1 −exp \u0012 −(p −bp)2 2p2 T \u0013 −2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f · T  − 32+δΥf (ϵf −eϵf + s)1+δ · T −δ. (26) 17 Proof. See Appendix B. ■ In the above theorem, the existence of s ≥0 is ensured by Assumption 4.7 along with the defini- tions of b∆in (23) and ϑα in (13)."
  },
  {
    "prompt": "This is a technical discussion about Theorem, corollary, ϵ-stationarity in theoretical computer science.",
    "response": "In particular, we have 0.5+ ¯µT−1(2ϵf +ϑα)/{¯µ0hα(γ−2 b∆)} < p for both α = 0 and α = 1. With Theorem 4.16, we derive in the following corollary the complexity bound for first-order ϵ-stationarity. The corollary follows directly from substituting hα(γ−2 b∆) = κfcd 2 η3γ−4(Υ3ϵ −Υ4ϵg)2 and ϑα = 2ϵf for α = 0 into the conclusion in Theorem 4.16. Corollary 4.17."
  },
  {
    "prompt": "This is a technical discussion about Theorem, sion, finf in theoretical computer science.",
    "response": "Under conditions of Theorem 4.16, for first-order stationarity (α = 0), the conclu- sion (26) holds given bp ∈ \u00121 2 + 4γ4¯µT−1(2ϵf + s) ¯µ0κfcdη3(Υ3ϵ −Υ4ϵg)2 , p \u0013 (27) and T > \u0012 bp −0.5 ¯µT−1 − 4γ4(2ϵf + s) ¯µ0κfcdη3(Υ3ϵ −Υ4ϵg)2 \u0013−1 · \u00122γ4(f(x0) + ¯µ0∥c(x0)∥−finf) ¯µ0κfcdη3(Υ3ϵ −Υ4ϵg)2 + logγ{∆0/(Υ3ϵ −Υ4ϵg)} + 2 2¯µT−1 \u0013 ."
  },
  {
    "prompt": "Read and understand this section about max, Theorem, κfcd from a research paper.",
    "response": "(28) Next, we establish the high-probability complexity bound for second-order ϵ-stationarity by sub- stituting hα(γ−2 b∆) = κfcd 2 max{∆max,1}η3γ−6(Υ5ϵ−Υ6ϵg −Υ7ϵh)3 and ϑα = 2ϵf +ϵ3/2 g for α = 1 into the conclusion in Theorem 4.16. Corollary 4.18. Under conditions of Theorem 4.16, for second-order stationarity (α = 1), the con- clusion (26) holds given bp ∈ 1 2 + 2γ6¯µT−1 max{∆max, 1}(4ϵf + ϵ3/2 g + 2s) ¯µ0κfcdη3(Υ5ϵ −Υ6ϵg −Υ7ϵh)3 , p !"
  },
  {
    "prompt": "Scientific context on max, finf, logγ and their role in graph theory.",
    "response": "(29) and T > bp −0.5 ¯µT−1 −2γ6 max{∆max, 1}(4ϵf + ϵ3/2 g + 2s) ¯µ0κfcdη3(Υ5ϵ −Υ6ϵg −Υ7ϵh)3 !−1 · 2γ6 max{∆max, 1}(f(x0) + ¯µ0∥c(x0)∥−finf) ¯µ0κfcdη3(Υ5ϵ −Υ6ϵg −Υ7ϵh)3 + logγ ∆0 Υ5ϵ−Υ6ϵg−Υ7ϵh + 2 2¯µT−1 ! . (30) The above corollaries indicate that our method achieves an iteration complexity of O(ϵ−2) for find- ing a first-order ϵ-stationary point and O(ϵ−3) for finding a second-order ϵ-stationary point with high probability. These complexity bounds match the results in Berahas et al."
  },
  {
    "prompt": "Scientific context on SSQP, Cao, optimization and their role in graph theory.",
    "response": "(2025) for a step-search SSQP method in identifying a first-order ϵ-stationary point in constrained optimization, and the results in Cao et al. (2023) for a trust-region method in identifying both first- and second-order ϵ- stationary points in unconstrained optimization. However, our analysis only requires ek and esk to have a finite 1 + δ moment, rather than the sub-exponential tail condition assumed in Cao et al. (2023); Berahas et al. (2025), thereby allowing for noise with heavy tails."
  },
  {
    "prompt": "Explain a concept involving Remark, Fuk, Nagaev in computational complexity.",
    "response": "18 Remark 4.19. In this remark, we demonstrate the techniques used in establishing the iteration com- plexity and discuss the sample complexity. • The heavy-tailed nature of the noise introduces significant challenges in complexity analysis. To this end, we employ the Burkholder-type inequality (Burkholder, 1973; Chen and Sung, 2020) and martin- gale Fuk–Nagaev inequality (Fuk, 1973; Nagaev, 1979; Fan et al., 2017) to study the tail behavior of ek and esk. In contrast, Cao et al."
  },
  {
    "prompt": "Understanding the computational aspects of Berahas, Section, techniques.",
    "response": "(2023); Berahas et al. (2025) utilized concentration techniques for sub-exponential variables, which we also apply in Section 4.3. We mention that the techniques used in analyzing heavy-tailed noise are independent to the algorithm designs, implying that same analysis can be applied to established trust-region and line-search methods (Cao et al., 2023; Berahas et al., 2025) and derive the same complexity bounds."
  },
  {
    "prompt": "This is a technical discussion about Due, Cao, noise in theoretical computer science.",
    "response": "• Due to the heavy-tailed noise, P{Tϵ ≤T −1} no longer grows to 1 at an exponential rate as in Cao et al. (2023); Berahas et al. (2025), but is instead reduced to a polynomial rate of O(T −δ) for δ > 0. When δ > 1, it follows from (26) that the failure probability is summable, i.e., P∞ T=1(1 −P{Tϵ ≤ T −1}) < ∞."
  },
  {
    "prompt": "Explain a concept involving Borel, Cantelli, lemma in computational complexity.",
    "response": "By the Borel–Cantelli lemma, this ensures that for both first-order and second-order stationarity, our method finds an ϵ-stationary point in a finite number of iterations with probability 1. However, this argument cannot hold for δ ∈(0, 1], illustrating a gap of the asymptotic behavior between δ ∈(0, 1] and δ > 1. • We analyze the total sample complexity of the method, following the discussion at the end of Section 3."
  },
  {
    "prompt": "Scientific context on Assumption, achieving, leading and their role in graph theory.",
    "response": "By Assumption 4.7, achieving first-order stationarity requires ϵ ≥O(√ϵf + ϵg), leading to the relations ϵf ≈O(ϵ2) and ϵg ≈O(ϵ). Similarly, for second-order stationarity, where ϵ ≥ O( 3√ϵf + √ϵg + ϵh), we obtain ϵf ≈O(ϵ3), ϵg ≈O(ϵ2), and ϵh ≈O(ϵ)."
  },
  {
    "prompt": "This is a technical discussion about objective, Hessian, requires in theoretical computer science.",
    "response": "Since estimating the objective value, gradient, and Hessian in each iteration requires sample sizes of O(eϵ−2 f ), O(ϵ−2 g ), and O(ϵ−2 h ) (see (20)), we conclude that when eϵf = O(ϵf), achieving first-order ϵ- stationarity requires O(ϵ−4) and O(ϵ−2) samples per iteration to estimate the objective value and gra- dient, respectively. Given O(ϵ−2) iteration complexity, the total sample complexity amounts to O(ϵ−6)."
  },
  {
    "prompt": "Scientific context on Hessian, Analogously, gradient and their role in graph theory.",
    "response": "Analogously, for second-order ϵ-stationarity, O(ϵ−6), O(ϵ−4), and O(ϵ−2) samples are required per iteration for estimating the objective value, gradient, and Hessian, respectively. Combined with O(ϵ−3) iteration complexity, the total sample complexity amounts to O(ϵ−9). We mention that the total sample complexity of O(ϵ−6) for first-order ϵ-stationarity matches the result in Jin et al."
  },
  {
    "prompt": "Read and understand this section about optimization, method, analyzed from a research paper.",
    "response": "(2024), which analyzed a trust-region method for identifying first-order ϵ-stationary points in unconstrained optimization. However, this result is worse than that in Curtis et al. (2023b), which established a total sample complexity of O(ϵ−4) for a line-search method. The discrepancy arises because both our method and Jin et al. (2024) require objective value estimation at each iteration, leading to a per-iteration sample complexity of O(ϵ−4)."
  },
  {
    "prompt": "Understanding the computational aspects of Curtis, design, adopt.",
    "response": "If we adopt an objective-value-free design as in Curtis et al. (2023b), where only gradient estimates are required, the per-iteration sample complexity will reduce to O(ϵ−2), yielding a total sample complexity of O(ϵ−4). However, as demon- strated in Berahas et al. (2025), objective-value-free methods tend to perform worse in practice than those using objective value estimates, especially in the presence of irreducible noise."
  },
  {
    "prompt": "Understanding the computational aspects of Complexity, Definition, bounds.",
    "response": "4.3 Complexity bounds with sub-exponential zeroth-order oracle In this section, we suppose that the noise in the objective value estimate follows the probabilistic sub- exponential zeroth-order oracle (i.e., conditions (i), (ii), and (iii.2) of Definition 3.3), and we demon- strate that the same high-probability complexity bounds hold as in the case of heavy-tailed oracles."
  },
  {
    "prompt": "Scientific context on theorem, oracle, exp and their role in graph theory.",
    "response": "19 In the next theorem, we establish a generic iteration complexity bound for both first- and second- order ϵ-stationarity under the sub-exponential oracle. Theorem 4.20. Under Assumptions 4.1, 4.7, and the probabilistic sub-exponential zeroth-order oracle, for any s ≥0, when bp satisfies condition (24) and T satisfies condition (25), we have P{Tϵ ≤T −1} ≥1−exp \u001a −(p −bp)2 2p2 T \u001b −exp \u001a −1 2 min \u0012(ϵf −eϵf + s)2 v2 , (ϵf −eϵf + s) b \u0013 T \u001b . (31) Proof. See Appendix C.1."
  },
  {
    "prompt": "Read and understand this section about Plugging, Corollaries, remain from a research paper.",
    "response": "■ Plugging the formulas of ϑα and hα(γ−2 b∆) for α = 0 or 1 into (24) and (25), we immediately know that Corollaries 4.17 and 4.18 remain true for the sub-exponential oracle. We combine them into the following corollary. Corollary 4.21. Under conditions of Theorem 4.20, for first-order stationarity (i.e., α = 0), the conclusion (31) holds when bp satisfies condition (27) and T satisfies condition (28)."
  },
  {
    "prompt": "Read and understand this section about condition, satisfies, stationarity from a research paper.",
    "response": "For second-order stationarity (i.e., α = 1), the conclusion (31) holds when bp satisfies condition (29) and T satisfies condition (30). The corollary suggests that the method enjoys an iteration complexity of O(ϵ−2) for finding a first- order ϵ-stationary point and O(ϵ−3) for finding a second-order ϵ-stationary point with high probability. These results align with the conclusions in Berahas et al. (2025); Cao et al. (2023) as well as those under the heavy-tailed oracle in Section 4.2."
  },
  {
    "prompt": "This is a technical discussion about Comparing, approaches, exponential in theoretical computer science.",
    "response": "Comparing (31) with (26), we observe that P{Tϵ ≤T−1} approaches 1 at an exponential rate. This faster convergence is attributed to the more restrictive sub- exponential condition (19). Technically, applying the Chernoff bound to analyze the concentration of the accumulated oracle noise ek and esk leads to an exponential decay rate in T. Remark 4.22."
  },
  {
    "prompt": "Explain a concept involving remark, echo, revisit in computational complexity.",
    "response": "In this remark, we echo Remark 3.4 and revisit the probabilistic sub-exponential zeroth-order oracle by exploring the connection between (16) and (19). We show that when eϵf < ϵf and v, b are sufficiently small in (19) — that is, the probability mass of ek, esk is largely concentrated around their means — then (16) holds with a proper choice of pf. Lemma 4.23."
  },
  {
    "prompt": "This is a technical discussion about Proof, Definition, eϵf in theoretical computer science.",
    "response": "Under the probabilistic sub-exponential zeroth-order oracle (i.e., conditions (i), (ii), and (iii.2) of Definition 3.3), the event Ck satisfies P(Ck | Fk−1/2) ≥1 −2 exp \u001a −1 2 min \u0012(ϵf −eϵf)2 v2 , ϵf −eϵf b \u0013\u001b . (32) Proof. See Appendix C.2. ■ It follows from the lemma that when eϵf < ϵf and v, b are sufficiently small, (19) implies (16) with pf ≤2 exp{−0.5 min((ϵf −eϵf)2/v2, (ϵf −eϵf)/b)}."
  },
  {
    "prompt": "Explain a concept involving Algo, Numerical, rithm in computational complexity.",
    "response": "5 Numerical Experiments In this section, we demonstrate the empirical performance of the trust-region SSQP method (Algo- rithm 1). We apply the methods to a subset of equality-constrained problems from the CUTEst test set 20 (Gould et al., 2014) to find first- and second-order stationary points, referred to as TR-SSQP and TR- SSQP2, respectively."
  },
  {
    "prompt": "Read and understand this section about Section, Berahas, SSQP from a research paper.",
    "response": "The methods are compared with the line-search SSQP method (Berahas et al., 2025, referred to as LS-SSQP), where a similar stochastic oracle enabling irreducible noise with first- order complexity guarantee is studied. We introduce the algorithm setup in Section 5.1. In Section 5.2, we examine the performance of the algorithms for various values of ϵ without irreducible noise. In Section 5.3, we examine their perfor- mance under different combinations of irreducible noise."
  },
  {
    "prompt": "Understanding the computational aspects of Algorithm, objective, methods.",
    "response": "5.1 Algorithm setup For all methods, we generate samples and construct estimates of objective quantities in each iteration by sample average. Given a sample ξ, we denote the realizations of objective value, gradient, and Hes- sian at x as F(x, ξ), ∇F(x, ξ), and ∇2F(x, ξ). We allow samples to be dependent within the iteration, but independent across iterations. Additionally, the sample size for constructing each estimate is limited to at most 104."
  },
  {
    "prompt": "Understanding the computational aspects of Berahas, κFO, method.",
    "response": "For the LS-SSQP method, following the notation in Berahas et al. (2025), we generate sample sets ξg k, ξf k, and ξf sk to estimate gradient and objective values, such that their sample sizes satisfy |ξg k| ≥ Cg pg max \b ϵg, κFO ¯αk∆l(xk, ¯τk, ¯gk, ¯dk) 2 and min{|ξf k|, |ξf sk|} ≥Cf pfϵ2 f , where Cg, Cf are positive constants. We set ϵτ = 10−2, ¯τ−1 = σ = 0.1, γ = 0.5, θ = 10−4, α0 = αmax = 1, κFO = 0.05, pg = 0.1, pf = 0.1, Cg = Cf = 5."
  },
  {
    "prompt": "Scientific context on SSQP, matrix, Hessian and their role in graph theory.",
    "response": "We set Hessian matrix ¯Hk = I and solve all SSQP subproblems exactly. For TR-SSQP and TR-SSQP2, we construct estimates of objective Hessians, gradients, and values by generating samples ξh k, ξg k, and ξf k, ξf sk following (20). We set ¯µ0 = 1, ∆0 = ∆max = 5, ρ = 1.2, γ = 1.5, κf = κh = κg = 0.05, pf = ph = pg = 0.1, η = 0.4, Ch = Cg = Cf = 5, r = 0.01. To solve (5), we use IPOPT solver (W¨achter and Biegler, 2005)."
  },
  {
    "prompt": "Scientific context on Hessian, Fang, matrices and their role in graph theory.",
    "response": "Since trust-region methods allow indefinite Hessian matrices, same as Fang et al. (2024a,b), we try four different ¯Hk for TR-SSQP in all problems. 1. Identity matrix (Id). It has been used in various existing SSQP literature, especially for line- search methods (see, e.g., Berahas et al., 2021, 2024, 2025; Na et al., 2022a, 2023). 2. Symmetric rank-one (SR1) update."
  },
  {
    "prompt": "This is a technical discussion about xLk, initialize, updated in theoretical computer science.",
    "response": "We initialize ¯H0 = I and for k ≥1, ¯Hk is updated as ¯Hk = ¯Hk−1 + (yk−1 −¯Hk−1∆xk−1)(yk−1 −¯Hk−1∆xk−1)T (yk−1 −¯Hk−1∆xk−1)T ∆xk−1 where yk−1 = ¯∇xLk −¯∇xLk−1, and ∆xk−1 = xk −xk−1. We employ SR1 instead of BFGS since SR1 can generate indefinite matrices, which can be preferable for constrained problems (Khalfan et al., 1993). 3. Estimated Hessian (EstH)."
  },
  {
    "prompt": "Scientific context on Hessian, Section, Averaged and their role in graph theory.",
    "response": "In the k-th iteration, we estimate the objective Hessian matrix ¯∇2fk using one single sample and set ¯Hk = ¯∇2 xLk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k, where ¯λk is the least- squares Lagrangian multiplier introduced in Section 2.2. 4. Averaged Hessian (AveH). In the k-th iteration, we estimate the Hessian matrix ¯∇2 xLk as in EstH but set ¯Hk = 1 50 Pk i=k−49 ¯∇2 xLi. Na et al."
  },
  {
    "prompt": "Explain a concept involving local, conver, gence in computational complexity.",
    "response": "(2022b); Na and Mahoney (2025) demonstrated that Hessian averaging facilitates stochastic Newton methods achieving faster (local) conver- gence."
  },
  {
    "prompt": "Understanding the computational aspects of distribution, Weibull, iii.",
    "response": "21 In addition, the objective model estimates are generated based on deterministic evaluations pro- vided by the CUTEst package, with noise (denoted by rand) following four different distributions: (i) standard normal distribution, (ii) t-distribution with degree of freedom 4, (iii) log-normal distribution with location-scale parameters (µ = 0, σ = 1), and (iv) Weibull distribution with scale-shape parame- ters (λ = 1, k = 1)."
  },
  {
    "prompt": "This is a technical discussion about Noise, rand, tail in theoretical computer science.",
    "response": "Noise from the normal distribution exhibits sub-exponential tail, while noise from the other three distributions are popular heavy-tailed noise. Specifically, for distributions (i) and (ii), we let F(xk, ξ) = fk +σ ·rand, ∇F(xk, ξ) = ∇fk +1·σ ·rand with 1 being the d-dimensional all-one vector, and [∇2F(xk, ξ)]i,j = (∇2fk)i,j +σ ·rand."
  },
  {
    "prompt": "Scientific context on rand, Rademacher, iii and their role in graph theory.",
    "response": "For distributions (iii) and (iv), rand is replaced by δ · rand, with δ being a Rademacher variable (i.e., P(δ = 1) = P(δ = −1) = 0.5), so that the noise becomes symmetric. Throughout the experiments, we set σ = 10−2. We set the maximum iteration budget to be 105 and define the stopping time as in Definition 4.6. The values of ϵ in stopping time are specified in each experiment below."
  },
  {
    "prompt": "Scientific context on problem, runs, irreducible and their role in graph theory.",
    "response": "For each algorithm and each problem, under every combination of irreducible noise level and ϵ, we report the average of Tϵ over five independent runs. 5.2 Algorithms performance without irreducible noise In this section, we aim to validate our complexity bounds in Theorems 4.16 and 4.20, which state that achieving first-order ϵ-stationarity requires at most O(ϵ−2) iterations, while achieving second-order ϵ-stationarity requires at most O(ϵ−3) iterations with high probability."
  },
  {
    "prompt": "This is a technical discussion about Figure, arbitrarily, small in theoretical computer science.",
    "response": "To allow for arbitrarily small ϵ, we set ϵf = ϵg = ϵh = 0. We vary ϵ ∈{10−1, 10−2, 10−3, 10−4} and report the average stopping time in Figure 1. First, we observe that as ϵ decreases, all methods require more iterations to converge. Specifically, when ϵ is reduced by a factor of 10, Figure 1 shows that the stopping time of TR-SSQP increases by a factor between 10 and 100, while that of TR-SSQP2 increases by approximately 100."
  },
  {
    "prompt": "This is a technical discussion about results, guarantee, worst-case in theoretical computer science.",
    "response": "This behavior aligns with our theoretical results, though it is important to emphasize that our analysis provides only a worst-case guarantee. Due to this worst-case nature, the actual iteration complexity may grow more slowly than predicted when gradually decreasing ϵ. Second, we observe that for TR-SSQP methods, performance varies across the four Hessian approx- imations."
  },
  {
    "prompt": "Read and understand this section about small, Hessian, performance from a research paper.",
    "response": "TR-SSQP-Id and LS-SSQP exhibit comparable performance, while TR-SSQP-SR1 exhibits the most unstable performance when ϵ is small. This instability is largely attributed to the Hessian updating scheme, which can accumulate estimation errors over iterations. In contrast, TR-SSQP with the averaged Hessian (TR-SSQP-AveH) achieves the best performance, followed by TR-SSQP- EstH, with their advantage becoming particularly pronounced as ϵ decreases."
  },
  {
    "prompt": "Read and understand this section about Hessian, noise, design from a research paper.",
    "response": "This highlights the benefits of fully exploring Hessian information in the algorithm design. Moreover, we observe that the noise distribution has a limited impact on the stopping time for most methods. As shown in the figure, changes in the noise distribution result in no significant variation in the average stopping time for the majority of methods. However, TR-SSQP-AveH is an exception. Even when ϵ = 10−4, this method can terminate within 10 iterations on certain datasets under normal noise."
  },
  {
    "prompt": "Understanding the computational aspects of verge, noise, heavy-tailed.",
    "response": "In contrast, under heavy-tailed noise, TR-SSQP-AveH requires at least 100 iterations to con- verge. This difference arises because heavy-tailed noise is more widely distributed than normal noise, introducing greater variability and randomness into the estimates of the objective models. Conse- quently, this increased uncertainty slows down the convergence of TR-SSQP-AveH under such noise distributions."
  },
  {
    "prompt": "Read and understand this section about normal, Log-normal, Weibull from a research paper.",
    "response": "This phenomenon is less apparent for other methods, as they require around 100 iter- ations to converge even under normal noise when ϵ = 10−4. 22 (a) Normal distribution (b) t-distribution (c) Log-normal distribution (d) Weibull distribution Figure 1: Averaged stopping time Tϵ with noise from four different distributions. In every plot, the first four boxes correspond to TR-SSQP with different choices of ¯Hk. The fifth box corresponds to TR-SSQP2, and the last box corresponds to LS-SSQP."
  },
  {
    "prompt": "Understanding the computational aspects of irreducible, noise, performance.",
    "response": "5.3 Algorithms performance with irreducible noise In this section, we evaluate the performance of all methods under different combinations of irreducible noise levels. To limit the number of total combinations, we set the default values as ϵ = 10−2, ϵf = 10−4, ϵg = 10−2, ϵh = 10−2, and vary one parameter at a time while keeping the others fixed. In partic- ular, we vary ϵ ∈{10−1, 10−3}, ϵf ∈{10−3, 10−5}, ϵg ∈{10−1, 10−3}, and ϵh ∈{10−1, 10−3}."
  },
  {
    "prompt": "Scientific context on Assumption, Rademacher, setup and their role in graph theory.",
    "response": "The de- fault setup ensures that the condition ϵ ≥O(√ϵf + ϵg) for first-order stationarity is satisfied (cf. Assumption 4.7). For consistency, we use the same default setup for second-order stationarity. When introducing irreducible noise to each method, we sample a Rademacher variable δ and directly add the noise δϵh, δϵg, δϵf to the corresponding objective quantity estimates. The Rademacher variable ensures that the irreducible noise is two-side distributed."
  },
  {
    "prompt": "Understanding the computational aspects of Results, iterations, performance.",
    "response": "Results are presented as performance profiles with respect to iterations. As in Berahas et al. (2025), we employ the convergence metric ∥∇L(x0)∥−∥∇L(x)∥≥(1 −εpp) (∥∇L(x0)∥−∥∇Lb∥). Here, x0 is the initial iterate, and ∥∇Lb∥represents the best KKT residual value identified by any algorithm 23 Figure 2: Performance profiles with noise following a normal distribution. Each line represents a differ- ent method. The first column corresponds to the default irreducible noise levels with varying ϵ."
  },
  {
    "prompt": "Understanding the computational aspects of row, columns, corresponds.",
    "response": "Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively. for the given problem instance within the iteration budget in the experiments of Section 5.2 (no irre- ducible noise). The prescribed tolerance is set to εpp = 10−3. This convergence metric extends the ap- proach in Mor´e and Wild (2009) to accommodate the constrained optimization setting and the noncon- vexity of the objective function."
  },
  {
    "prompt": "Read and understand this section about distribution, met, reached from a research paper.",
    "response": "The algorithm terminates when any of the following criteria are met: the stopping time is reached, the convergence metric is satisfied, or the iteration budget is exhausted. We present the experimental results in Figures 2, 3, 4, and 5, corresponding to noise from the nor- mal distribution, t-distribution, log-normal distribution, and Weibull distribution, respectively."
  },
  {
    "prompt": "Read and understand this section about performance, noise, levels from a research paper.",
    "response": "In each figure, the first column represents the performance of all methods under the default irreducible noise levels with varying ϵ. The last two columns represent the performance of TR-SSQP and TR- SSQP2 under varying irreducible noise levels with the default ϵ."
  },
  {
    "prompt": "This is a technical discussion about trim, performance, converges in theoretical computer science.",
    "response": "For clarity, we trim the performance ratio (x-axis) based on two conditions: (1) if all methods converge, we trim the performance ratio after the last method converges; (2) if some methods diverge on some problems, we trim the performance ra- tio when no method converges on new problems within the iteration budget. 24 Figure 3: Performance profiles with noise following a t-distribution. Each line represents a different method."
  },
  {
    "prompt": "Read and understand this section about varying, corresponds, noise from a research paper.",
    "response": "The first column corresponds to the default irreducible noise levels with varying ϵ. Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively. Comparing corresponding plots across the four figures, we first observe that all methods are robust to the noise distribution. For any fixed combination of ϵ, ϵf, ϵg, ϵh, changing the noise distribution re- sults in limited differences in the performance ratios. Next, we investigate the impact of varying ϵ, ϵf, ϵg, ϵh."
  },
  {
    "prompt": "This is a technical discussion about figures, increasing, methods in theoretical computer science.",
    "response": "• ϵ : From the first column of the four figures, we observe that as ϵ decreases from 10−1 to 10−2, all methods require more iterations to converge, increasing by a factor of 10 to 100. This observation aligns well with our theoretical complexity bounds. However, when ϵ further decreases to 10−3, all methods fail to converge on some datasets within the iteration budget."
  },
  {
    "prompt": "Scientific context on Assumption, levels, primarily and their role in graph theory.",
    "response": "This is primarily because, given the default irreducible noise levels, ϵ = 10−3 no longer satisfies the lower bound in Assumption 4.7. That being said, the impact of decreasing ϵ is less notable for TR-SSQP-AveH and TR-SSQP- EstH, highlighting the advantages of exploring Hessian information in the algorithm design."
  },
  {
    "prompt": "Understanding the computational aspects of Figure, Comparing, Performance.",
    "response": "• ϵf : Comparing the first row of the second and third columns with the default setting (middle plot in 25 Figure 4: Performance profiles with noise following a log-normal-distribution. Each line represents a different method. The first column corresponds to the default irreducible noise levels with varying ϵ. Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively."
  },
  {
    "prompt": "Scientific context on ratio, performance, increase and their role in graph theory.",
    "response": "the first column) in each figure, we observe that increasing ϵf from 10−5 to 10−4 approximately triples the performance ratio. This is expected since, by ϵ ≈O(√ϵf +ϵg), a 10-fold increase in ϵf leads to at most a √ 10-fold increase in the performance ratio. This pattern no longer holds when ϵf is further increased, as all methods fail to converge on some datasets (due to violation of Assumption 4.7)."
  },
  {
    "prompt": "This is a technical discussion about Comparing, performance, increases in theoretical computer science.",
    "response": "• ϵg : Comparing the second row of the last two columns with the default setting, we observe that when ϵg increases from 10−3 to 10−2, the performance ratio also increases by a factor of 3. This observation aligns with our analysis, as under Assumption 4.7, a 10-fold increase in ϵg should not result in more than a 10-fold increase in the performance ratio. However, when ϵg further increases from 10−2 to 10−1, the performance of all methods deteriorates rapidly."
  },
  {
    "prompt": "This is a technical discussion about plot, instances, shown in theoretical computer science.",
    "response": "As shown in the middle plot, all methods fail to converge on more than 20% of problem instances. • ϵh : The third row of the last two columns illustrates the impact of varying ϵh. We observe that TR-SSQP-AveH and TR-SSQP-EstH behave robustly to changes in ϵh, primarily because eigenvalues 26 Figure 5: Performance profiles with noise following a Weibull distribution. Each line represents a different method. The first column corresponds to the default irreducible noise levels with varying ϵ."
  },
  {
    "prompt": "This is a technical discussion about respectively., stationarity, row in theoretical computer science.",
    "response": "Each row of the last two columns corresponds to varying ϵf, ϵg and ϵh, respectively. are not included in the definition of the stopping time for first-order stationarity. In contrast, the per- formance of TR-SSQP2 is significantly affected by the increase in ϵh. When ϵh increases from 10−3 to 10−2, TR-SSQP2 requires approximately 5 times more iterations to converge."
  },
  {
    "prompt": "Understanding the computational aspects of increase, analysis, stationarity.",
    "response": "This observation also aligns with our theoretical analysis, as for second-order stationarity, ϵ ≥O( 3√ϵf + √ϵg + ϵh) implies that a 10-fold increase in ϵh should lead to no more than a 10-fold increase in the performance ratio. Furthermore, we observe that TR-SSQP2 fails to converge on approximately 25% of problem in- stances when ϵh is further increased from 10−2 to 10−1, while TR-SSQP-AveH and TR-SSQP-EstH still perform well in this case for finding first-order stationary points."
  },
  {
    "prompt": "This is a technical discussion about SSQP, Conclusion, Programming in theoretical computer science.",
    "response": "27 6 Conclusion In this paper, we derived high-probability complexity bounds for a trust-region Stochastic Sequential Quadratic Programming (SSQP) method in identifying first- and second-order ϵ-stationary points in equality-constrained optimization problems. Our method extends the existing literature by allowing irreducible and heavy-tailed noise in objective function estimation."
  },
  {
    "prompt": "This is a technical discussion about identifying, ϵ-stationary, point in theoretical computer science.",
    "response": "Under weaker oracle conditions, we established that our method achieves an iteration complexity of O(ϵ−2) for identifying a first-order ϵ-stationary point and O(ϵ−3) for identifying a second-order ϵ-stationary point with high probability, provided that ϵ is above a threshold determined by the irreducible noise level. We validated our the- oretical analysis through numerical experiments on problems from the CUTEst dataset. References A. S. Bandeira, K. Scheinberg, and L. N. Vicente."
  },
  {
    "prompt": "Explain a concept involving Optimization, Convergence, models in computational complexity.",
    "response": "Convergence of trust-region methods based on probabilistic models. SIAM Journal on Optimization, 24(3):1238–1264, 2014. F. Beiser, B. Keith, S. Urbainczyk, and B. Wohlmuth. Adaptive sampling strategies for risk-averse stochastic optimization with constraints. IMA Journal of Numerical Analysis, 43(6):3729–3765, 2023. A. S. Berahas, F. E. Curtis, D. Robinson, and B. Zhou. Sequential quadratic optimization for nonlinear equality constrained stochastic optimization."
  },
  {
    "prompt": "Understanding the computational aspects of SIAM, Journal, Berahas.",
    "response": "SIAM Journal on Optimization, 31(2): 1352–1379, 2021. A. S. Berahas, R. Bollapragada, and B. Zhou. An adaptive sampling sequential quadratic program- ming method for equality constrained stochastic optimization. arXiv preprint arXiv:2206.00712, 2022. A. S. Berahas, J. Shi, Z. Yi, and B. Zhou. Accelerating stochastic sequential quadratic program- ming for equality constrained optimization using predictive variance reduction. Computational Optimization and Applications, 86(1):79–116, 2023. A. S."
  },
  {
    "prompt": "Scientific context on Curtis, Robinson, Berahas and their role in graph theory.",
    "response": "Berahas, F. E. Curtis, M. J. O’Neill, and D. P. Robinson. A stochastic sequential quadratic op- timization algorithm for nonlinear-equality-constrained optimization with rank-deficient jacobians. Mathematics of Operations Research, 49(4):2212–2248, 2024. A. S. Berahas, M. Xie, and B. Zhou. A sequential quadratic programming method with high- probability complexity bounds for nonlinear equality-constrained stochastic optimization. SIAM Journal on Optimization, 35(1):240–269, 2025. D. P."
  },
  {
    "prompt": "This is a technical discussion about Bertsekas, Optimization, Programming in theoretical computer science.",
    "response": "Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Elsevier, 1982. J. T. Betts. Practical Methods for Optimal Control and Estimation Using Nonlinear Programming. Society for Industrial and Applied Mathematics, 2010. J. Blanchet, C. Cartis, M. Menickelly, and K. Scheinberg. Convergence rate analysis of a stochastic trust-region method via supermartingales. INFORMS Journal on Optimization, 1(2):92–119, 2019. 28 P. T. Boggs and J. W. Tolle. Sequential quadratic programming."
  },
  {
    "prompt": "Scientific context on Numerica, Acta, Bollapragada and their role in graph theory.",
    "response": "Acta Numerica, 4:1–51, 1995. R. Bollapragada, C. Karamanli, B. Keith, B. Lazarov, S. Petrides, and J. Wang. An adaptive sampling augmented lagrangian method for stochastic optimization with deterministic constraints. Computers & Mathematics with Applications, 149:239–258, 2023. D. L. Burkholder. Distribution function inequalities for martingales. the Annals of Probability, 1(1): 19–42, 1973. R. H. Byrd, R. B. Schnabel, and G. A. Shultz."
  },
  {
    "prompt": "Explain a concept involving optimization, SIAM, Analysis in computational complexity.",
    "response": "A trust region algorithm for nonlinearly constrained optimization. SIAM Journal on Numerical Analysis, 24(5):1152–1170, 1987. L. Cao, A. S. Berahas, and K. Scheinberg. First- and second-order high probability complexity bounds for trust-region methods with noisy oracles. Mathematical Programming, 207(1–2):55–106, 2023. P. Chen and S. H. Sung. Rosenthal type inequalities for random variables. Journal of Mathematical Inequalities, 14(2):305–18, 2020. A. Choromanska, M. Henaff, M. Mathieu, G. B."
  },
  {
    "prompt": "Explain a concept involving Arous, LeCun, networks in computational complexity.",
    "response": "Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pages 192–204. PMLR, 2015. A. R. Conn, N. I. M. Gould, and P. L. Toint. Trust Region Methods. Society for Industrial and Applied Mathematics, 2000. S. Cuomo, V. S. Di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F. Piccialli. Scientific machine learning through physics–informed neural networks: Where we are and what’s next. Journal of Scientific Computing, 92(3), 2022. F. E. Curtis, V."
  },
  {
    "prompt": "This is a technical discussion about Robinson, Wang, Kungurtsev in theoretical computer science.",
    "response": "Kungurtsev, D. P. Robinson, and Q. Wang. A stochastic-gradient-based interior- point algorithm for solving smooth bound-constrained optimization problems. arXiv preprint arXiv:2304.14907, 2023a. F. E. Curtis, M. J. O’Neill, and D. P. Robinson. Worst-case complexity of an sqp method for nonlinear equality constrained stochastic optimization. Mathematical Programming, 205(1–2): 431–483, 2023b. F. E. Curtis, D. P. Robinson, and B. Zhou."
  },
  {
    "prompt": "Explain a concept involving optimization, Curtis, Journal in computational complexity.",
    "response": "Sequential quadratic optimization for stochastic optimization with deterministic nonlinear inequality and equality constraints. SIAM Journal on Optimization, 34(4):3592–3622, 2024a. F. E. Curtis, D. P. Robinson, and B. Zhou. A stochastic inexact sequential quadratic optimization algorithm for nonlinear equality-constrained optimization. INFORMS Journal on Optimization, 6 (3–4):173–195, 2024b. F. E. Curtis, X. Jiang, and Q. Wang."
  },
  {
    "prompt": "This is a technical discussion about optimization, Almost-sure, Applications in theoretical computer science.",
    "response": "Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization. Journal of Optimization Theory and Applications, 204(2), 2025. 29 Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in neural information processing systems, 27, 2014. M. El-Alem."
  },
  {
    "prompt": "This is a technical discussion about dennis, celis, tapia in theoretical computer science.",
    "response": "A global convergence theory for the celis–dennis–tapia trust-region algorithm for constrained optimization. SIAM Journal on Numerical Analysis, 28(1):266–290, 1991. X. Fan, I. Grama, and Q. Liu. Deviation inequalities for martingales with applications. Journal of Mathematical Analysis and Applications, 448(1):538–566, 2017. Y. Fang, S. Na, M. W. Mahoney, and M. Kolar. Fully stochastic trust-region sequential quadratic programming for equality-constrained optimization problems."
  },
  {
    "prompt": "Read and understand this section about SIAM, Journal, Optimization from a research paper.",
    "response": "SIAM Journal on Optimization, 34 (2):2007–2037, 2024a. Y. Fang, S. Na, M. W. Mahoney, and M. Kolar. Trust-region sequential quadratic programming for stochastic optimization with random models. arXiv preprint arXiv:2409.15734, 2024b. D. H. Fuk. Certain probabilistic inequalities for martingales. Sibirskii Matematicheskii Zhurnal, 14 (1):185–193, 1973. N. I. M. Gould, D. Orban, and P. L. Toint."
  },
  {
    "prompt": "Scientific context on optimization, Cutest, Applications and their role in graph theory.",
    "response": "Cutest: a constrained and unconstrained testing environment with safe threads for mathematical optimization. Computational Optimization and Applications, 60(3):545–557, 2014. M. Heinkenschloss and D. Ridzal. A matrix-free trust-region sqp method for equality constrained optimization. SIAM Journal on Optimization, 24(3):1507–1541, 2014. P. Jain, C. Jin, S. Kakade, and P. Netrapalli. Global convergence of non-convex gradient descent for computing matrix squareroot."
  },
  {
    "prompt": "Explain a concept involving Statistics, Artificial, Intelligence in computational complexity.",
    "response": "In Artificial Intelligence and Statistics, pages 479–488. PMLR, 2017. B. Jin, K. Scheinberg, and M. Xie. Sample complexity analysis for adaptive optimization algorithms with stochastic oracles. Mathematical Programming, 209(1–2):651–679, 2024. H. F. Khalfan, R. H. Byrd, and R. B. Schnabel. A theoretical and experimental study of the symmetric rank-one update. SIAM Journal on Optimization, 3(1):1–24, 1993. W. Kuang, M. Anitescu, and S. Na."
  },
  {
    "prompt": "Scientific context on arXiv, Online, methods and their role in graph theory.",
    "response": "Online covariance matrix estimation in sketched newton methods. arXiv preprint arXiv:2502.07114, 2025. Y. Lou, S. Sun, and J. Nocedal. Design guidelines for noise-tolerant optimization with applications in robust design. arXiv preprint arXiv:2401.15007, 2024. J. J. Mor´e and S. M. Wild. Benchmarking derivative-free optimization algorithms. SIAM Journal on Optimization, 20(1):172–191, 2009. S. Na and M. W. Mahoney."
  },
  {
    "prompt": "Scientific context on programming, Statistical, quadratic and their role in graph theory.",
    "response": "Statistical inference of constrained stochastic optimization via sketched sequential quadratic programming. Journal of Machine Learning Research, 26(33), 2025. S. Na, M. Anitescu, and M. Kolar. An adaptive stochastic sequential quadratic programming with differentiable exact augmented lagrangians. Mathematical Programming, 199(1–2):721–791, 2022a. 30 S. Na, M. Derezi´nski, and M. W. Mahoney. Hessian averaging in stochastic newton methods achieves superlinear convergence."
  },
  {
    "prompt": "Scientific context on Programming, Mathematical, Anitescu and their role in graph theory.",
    "response": "Mathematical Programming, 201(1–2):473–520, 2022b. S. Na, M. Anitescu, and M. Kolar. Inequality constrained stochastic nonlinear optimization via active-set sequential quadratic programming. Mathematical Programming, 202(1–2):279–353, 2023. S. V. Nagaev. Large deviations of sums of independent random variables. The Annals of Probability, pages 745–789, 1979. J. Nocedal and S. Wright. Numerical Optimization. Springer New York, 2006. E. O. Omojokun."
  },
  {
    "prompt": "Scientific context on optimization, constraints, University and their role in graph theory.",
    "response": "Trust region algorithms for optimization with nonlinear equality and inequality constraints. PhD thesis, University of Colorado at Boulder, CO, 1989. F. Oztoprak, R. Byrd, and J. Nocedal. Constrained optimization in the presence of noise. SIAM Journal on Optimization, 33(3):2118–2136, 2023. M. J. D. Powell and Y. Yuan. A trust region algorithm for equality constrained optimization. Mathematical Programming, 49(1–3):189–211, 1990. S. Qiu and V. Kungurtsev."
  },
  {
    "prompt": "Read and understand this section about Shapiro, functions, deterministic from a research paper.",
    "response": "A sequential quadratic programming method for optimization with stochastic objective functions, deterministic inequality constraints and robust subproblems. arXiv preprint arXiv:2302.07947, 2023. T. Santoso, S. Ahmed, M. Goetschalckx, and A. Shapiro. A stochastic programming approach for supply chain network design under uncertainty. European Journal of Operational Research, 167 (1):96–115, 2005. A. Shapiro, D. Dentcheva, and A. Ruszczynski."
  },
  {
    "prompt": "Explain a concept involving Modeling, Theory, Edition in computational complexity.",
    "response": "Lectures on Stochastic Programming: Modeling and Theory, Third Edition. Society for Industrial and Applied Mathematics, 2021. S. Sun and J. Nocedal. A trust region method for noisy unconstrained optimization. Mathematical Programming, 202(1–2):445–472, 2023. S. Sun and J. Nocedal. A trust-region algorithm for noisy equality constrained optimization. arXiv preprint arXiv:2411.02665, 2024. A. Vardi."
  },
  {
    "prompt": "Understanding the computational aspects of Convergence, minimization, programming.",
    "response": "A trust region algorithm for equality constrained minimization: Convergence properties and implementation. SIAM Journal on Numerical Analysis, 22(3):575–591, 1985. A. W¨achter and L. T. Biegler. On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Mathematical Programming, 106(1):25–57, 2005. U. C¸akmak and S. ¨Ozekici. Portfolio optimization in stochastic markets. Mathematical Methods of Operations Research, 63(1):151–168, 2005."
  },
  {
    "prompt": "Scientific context on Proof, Section, Lemma and their role in graph theory.",
    "response": "31 A Proofs of Section 4.1 A.1 Proof of Lemma 4.2 Proof. Recall that ¯Hk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k, we have ∥¯Hk∥≤∥¯∇2fk −∇2fk∥+ ∥∇2fk∥+ ∥ m X i=1 (¯λi k −λi k)∇2ci k∥+ ∥ m X i=1 λi k∇2ci k∥ ≤∥¯∇2fk −∇2fk∥+ ∥∇2fk∥+ ∥¯λk −λk∥ \b m X i=1 ∥∇2ci k∥2 1/2 + ∥λk∥ \b m X i=1 ∥∇2ci k∥ 1/2 ≤∥¯∇2fk −∇2fk∥+ L∇f + √mLG √κ1,G ∥¯gk −gk∥+ √mLGκ∇f √κ1,G , where the last inequality follows from Assumption 4.1 and the definitions of λk and ¯λk."
  },
  {
    "prompt": "This is a technical discussion about max, proof, mLG in theoretical computer science.",
    "response": "On the event Ak ∩Bk, ∥¯∇2fk −∇2fk∥≤ϵh + κh∆k and ∥¯gk −gk∥≤ϵg + κg∆2 k. Since ∆k ≤∆max, it follows that ∥¯Hk∥≤ϵh + κh∆max + L∇f + √mLG √κ1,G (ϵg + κg∆2 max + κ∇f). By setting κB = max{1, ϵh + κh∆max + L∇f + √mLG/√κ1,G · (ϵg + κg∆2 max + κ∇f)}, we complete the proof. ■ A.2 Proof of Lemma 4.3 Proof."
  },
  {
    "prompt": "Scientific context on mLG, max, Assumption and their role in graph theory.",
    "response": "We have ∥∇2 xLk −¯Hk∥= ∥∇2fk −¯∇2fk + m X i=1 (λi k −¯λi k)∇2ci k∥ ≤∥∇2fk −¯∇2fk∥+ ∥¯λk −λk∥ \b m X i=1 ∥∇2ci k∥2}1/2 ≤∥∇2fk −¯∇2fk∥+ √mLG √κ1,G ∥gk −¯gk∥ (Assumption 4.1) ≤ \u0012 κh + √mκgLG∆max √κ1,G \u0013 | {z } κH ∆k + \u0012 ϵh + √mLG √κ1,G ϵg \u0013 | {z } ϵH , where the last inequality is due to the event Ak ∩Bk and ∆k ≤∆max. Next, we show |τk −¯τk| ≤ κH∆k + ϵH. Let ¯ζk be a normalized eigenvector corresponding to ¯τk, then τk −¯τk ≤¯ζT k \u0002 ZT k (∇2 xLk −¯Hk)Zk \u0003 ¯ζk ≤∥∇2 xLk −¯Hk∥≤κH∆k + ϵH."
  },
  {
    "prompt": "Explain a concept involving xLk, proof, normalized in computational complexity.",
    "response": "Let ζk be a normalized eigenvector corresponding to τk, then ¯τk −τk ≤ζT k \u0002 ZT k ( ¯Hk −∇2 xLk)Zk \u0003 ζk ≤∥∇2 xLk −¯Hk∥≤κH∆k + ϵH. Combining the above two displays, we have |τk −¯τk| ≤κH∆k + ϵH, which implies |τ + k −¯τ + k | ≤ κH∆k + ϵH. We complete the proof. ■ 32 A.3 Proof of Lemma 4.4 Proof. Since the SOC step is not performed, xsk = xk + ∆xk."
  },
  {
    "prompt": "Scientific context on csk, fsk, Predk and their role in graph theory.",
    "response": "By the definition of ℓ2 merit function Lµ(x) and (10), we have Lsk ¯µk −Lk ¯µk −Predk = fsk + ¯µk∥csk∥−fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk −¯µk∥ck + Gk∆xk∥ . By the Taylor expansion of f(x) and the Lipschitz continuity of ∇f(x), we have fsk −fk −¯gT k ∆xk ≤(gk −¯gk)T ∆xk + 1 2L∇f∥∆xk∥2. Similarly, we have ∥csk∥−∥ck + Gk∆xk∥ ≤∥csk −ck −Gk∆xk∥≤1 2LG∥∆xk∥2."
  },
  {
    "prompt": "Scientific context on Assumption, Lemma, imply and their role in graph theory.",
    "response": "Recall that the monotonic updating scheme of the merit parameter implies ¯µk ≤¯µT−1 for all k ≤T−1, and Assumption 4.1 and Lemma 4.2 imply ∥¯Hk∥≤κB under Ak. Combining the above two displays and using Cauchy-Schwartz inequality lead to Lsk ¯µk −Lk ¯µk −Predk ≤∥gk −¯gk∥∥∆xk∥+ 1 2(L∇f + κB + ¯µT−1LG)∥∆xk∥2. On the event Bk, we have ∥gk −¯gk∥≤ϵg + κg max{1, ∆max}∆k. Combining ∥∆xk∥≤∆k with the above display, we complete the proof. ■ A.4 Proof of Lemma 4.5 Proof."
  },
  {
    "prompt": "Read and understand this section about fsk, Predk, Lsk from a research paper.",
    "response": "We have Lsk ¯µk −Lk ¯µk −Predk = fsk + ¯µk∥csk∥−fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk −¯µk∥ck + Gk∆xk∥ ≤ fsk −fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk + ¯µT−1∥csk −ck −Gk∆xk∥, (A.1) where we have used Assumption 4.1. First, we analyze the second term in (A.1). Since the SOC step is performed, we have xsk = xk + ∆xk + dk. For 1 ≤i ≤m, by the Taylor expansion, we obtain |ci sk −ci k −(∇ci k)T ∆xk| (8) = |ci sk −ci(xk + ∆xk) −(∇ci k)T dk| ≤LG(∥∆xk∥∥dk∥+ ∥dk∥2). Therefore, ∥csk −ck −Gk∆xk∥≤√mLG(∥∆xk∥∥dk∥+ ∥dk∥2)."
  },
  {
    "prompt": "Scientific context on max, mLG, GkGT and their role in graph theory.",
    "response": "For ∥dk∥, we have ∥dk∥≤∥GT k [GkGT k ]−1∥∥c(xk + ∆xk) −ck −Gk∆xk∥≤ LG √κ1,G ∆2 k. (A.2) Combining the last two results and using the fact that ∆k ≤∆max, we have ∥csk −ck −Gk∆xk∥≤mLG∥∆xk∥∥dk∥+ mLG∥dk∥2 ≤ \u0012√mL2 G √κ1,G + √mL3 G∆max κ1,G \u0013 ∆3 k. (A.3) 33 Next, we analyze the first term in (A.1)."
  },
  {
    "prompt": "This is a technical discussion about points, fsk, Define in theoretical computer science.",
    "response": "For some points ϕ1 between [xk+∆xk, xk+∆xk+dk] and ϕ2 between [xk, xk + ∆xk], we have fsk = f(xk + ∆xk + dk) = f(xk + ∆xk) + ∇f(xk + ∆xk)T dk + 1 2dT k ∇2f(ϕ1)dk = fk + gT k ∆xk + ∇f(xk + ∆xk)T dk + 1 2∆xT k ∇2f(ϕ2)∆xk + 1 2dT k ∇2f(ϕ1)dk. Define eλk = −[GkGT k ]−1Gk∇f(xk + ∆xk)."
  },
  {
    "prompt": "Scientific context on eλi, Taylor, expansion and their role in graph theory.",
    "response": "By the Taylor expansion, we have ∇f(xk + ∆xk)T dk (8) = eλT k [c(xk + ∆xk) −ck −Gk∆xk] = m X i=1 eλi k[ci(xk + ∆xk) −ci k −(∇ci k)T ∆xk] = 1 2 m X i=1 eλi k∆xT k ∇2ci(ϕi 3)∆xk, where the points {ϕi 3}m i=1 are between [xk, xk + ∆xk]. Recall that ¯Hk = ¯∇2fk + Pm i=1 ¯λi k∇2ci k."
  },
  {
    "prompt": "Read and understand this section about eλk, mLG, eλi from a research paper.",
    "response": "We combine the above two displays and have fsk −fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk = (gk −¯gk)T ∆xk + 1 2∆xT k \u0000∇2f(ϕ2) −∇2fk \u0001 ∆xk + 1 2∆xT k \u0000∇2fk −¯∇2fk \u0001 ∆xk + 1 2 m X i=1 \u0000λi k −¯λi k \u0001 ∆xT k ∇2ci k∆xk + 1 2 m X i=1 \u0010 eλi k −λi k \u0011 ∆xT k ∇2ci k∆xk + 1 2dT k ∇2f (ϕ1) dk + 1 2 m X i=1 eλi k∆xT k \u0000∇2ci(ϕi 3) −∇2ci k \u0001 ∆xk ≤∥gk −¯gk∥∥∆xk∥+ L∇2f 2 ∥∆xk∥3 + 1 2∥∇2fk −¯∇2fk∥∥∆xk∥2 + √mLG 2 (∥λk −¯λk∥+ ∥eλk −λk∥)∥∆xk∥2 + L∇f 2 ∥dk∥2 + √mL∇2c 2 ∥eλk∥∥∆xk∥3 ≤∥gk −¯gk∥∆k + 1 2ϵh∆2 k + \u0012L∇2f + κh 2 \u0013 ∆3 k + √mLG 2 (∥λk −¯λk∥+ ∥eλk −λk∥)∆2 k + L∇f 2 ∥dk∥2 + √mL∇2c∥eλk∥ 2 ∆3 k (A.2) ≤∥gk −¯gk∥∆k + 1 2ϵh∆2 k + L∇2f + κh 2 + L∇fL2 G∆max 2κ1,G + √mL∇2c∥eλk∥ 2 !"
  },
  {
    "prompt": "This is a technical discussion about Assumption, inequality, mLG in theoretical computer science.",
    "response": "∆3 k + √mLG 2 (∥λk −¯λk∥+ ∥eλk −λk∥)∆2 k, (A.4) where the second inequality is by Assumption 4.1 and the third inequality is by the event Ak and the fact that ∆xk ≤∆k (note that the SOC step is performed only when α = 1)."
  },
  {
    "prompt": "Read and understand this section about GkGT, max, eλk from a research paper.",
    "response": "Furthermore, on the event Bk, it follows from Assumption 4.1 that ∥λk −¯λk∥≤∥[GkGT k ]−1Gk∥∥gk −¯gk∥≤ κg √κ1,G ∆2 k + ϵg √κ1,G ≤κg∆max √κ1,G ∆k + ϵg √κ1,G , 34 ∥λk −eλk∥≤∥[GkGT k ]−1Gk∥∥gk −∇f(xk + ∆xk)∥≤ L∇f √κ1,G ∆k, ∥eλk∥≤∥[GkGT k ]−1Gk∥∥∇f(xk + ∆xk)∥≤ 1 √κ1,G (L∇f∥∆xk∥+ ∥gk∥) ≤L∇f∆max + κ∇f √κ1,G . To simplify the first term in (A.4), we claim that under Bk, ∥gk−¯gk∥≤(κg+1)∆2 k+min{ϵ3/2 g /∆k, ϵg}."
  },
  {
    "prompt": "Scientific context on prove, notice, trivial and their role in graph theory.",
    "response": "To prove this, we notice that when ϵ3/2 g /∆k ≥ϵg, this claim is trivial due to the definition of Bk. When ϵ3/2 g /∆k < ϵg, we have ϵ1/2 g < ∆k. On the event Bk, we have ∥gk −¯gk∥≤ϵg + κg∆2 k < (κg + 1)∆2 k < (κg + 1)∆2 k + min{ϵ3/2 g /∆k, ϵg}. Thus the claim is proved."
  },
  {
    "prompt": "Understanding the computational aspects of max, mLG, proof.",
    "response": "Combining this claim with the above displays, we have fsk −fk −¯gT k ∆xk −1 2∆xT k ¯Hk∆xk ≤ϵ3/2 g + \u00121 2ϵh + √mLG √κ1,G ϵg \u0013 ∆2 k + \u0012 κg + 1 + L∇2f + κh 2 \u0013 ∆3 k + \u0012L∇fL2 G∆max 2κ1,G + √mL∇2c(L∇f∆max + κ∇f) 2√κ1,G + √mLG(κg∆max + L∇f) 2√κ1,G \u0013 ∆3 k. (A.5) We complete the proof by combining (A.1), (A.3), and (A.5). ■ A.5 Proof of Lemma 4.8 Proof. We first notice that Assumption 4.7 implies the right-hand side of (21) is positive, thus the inequality is well defined. We first prove (13a) holds."
  },
  {
    "prompt": "Scientific context on Lsk, fsk, Aredk and their role in graph theory.",
    "response": "By the algorithm design, we have ϑα = 2ϵf for α = 0. Thus, we only need to show that Aredk −2ϵf ≤ηPredk can be satisfied by (21) (recalling that Predk < 0). Since no SOC step will be performed when α = 0, xsk = xk + ∆xk and we have Aredk −2ϵf = ¯Lsk ¯µk −¯Lk ¯µk −2ϵf = ¯Lsk ¯µk −Lsk ¯µk + Lsk ¯µk −Lk ¯µk + Lk ¯µk −¯Lk ¯µk −2ϵf = ¯fsk −fsk + Lsk ¯µk −Lk ¯µk + fk −¯fk −2ϵf ≤| ¯fsk −fsk| + |fk −¯fk| + Predk + ϵg∆k + Υ1∆2 k −2ϵf."
  },
  {
    "prompt": "Explain a concept involving κfcd, fsk, Lemma in computational complexity.",
    "response": "(Lemma 4.4) Under Ck, when α = 0 we have |fk −¯fk| + | ¯fsk −fsk| ≤2ϵf + 2κf∆2 k. Since Predk ≤−κfcd 2 ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b (21) = −κfcd 2 ∥¯∇Lk∥∆k, we only need to show 2κf∆2 k + ϵg∆k + Υ1∆2 k ≤κfcd 2 (1 −η)∥¯∇Lk∥∆k. (A.6) To this end, we note that (21) ⇒{(4κf + κg) + 2Υ1 + κB}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg ⇒{(4κf + κfcd(1 −η)κg) + 2Υ1}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg, since κfcd(1 −η) < 1. Rearranging the terms, we have (2κf + Υ1)∆k ≤κfcd 2 (1 −η)(∥∇Lk∥−κg∆k −ϵg) −ϵg."
  },
  {
    "prompt": "Scientific context on κfcd, ηκB, implies and their role in graph theory.",
    "response": "(A.7) 35 Under Bk, when α = 0 we have ∥¯∇Lk∥≥∥∇Lk∥−κg∆k −ϵg. This, together with (A.7), implies (2κf + Υ1)∆k ≤κfcd 2 (1 −η)∥¯∇Lk∥−ϵg. Multiplying ∆k and rearranging the terms, we have shown (A.6), thus proved (13a). Next, we prove (13b). When α = 0, we have max{1, ∥¯Hk∥} ≤κB under Ak. Therefore it suffices to show ∥¯∇Lk∥≥ηκB∆k. Recalling κfcd ≤1, η < 1, we have (ηκB + κg)∆k ≤(4κf + κg + 2Υ1 + κB)∆k (21) ≤κfcd(1 −η)∥∇Lk∥−(κfcd(1 −η) + 2)ϵg ≤∥∇Lk∥−ϵg."
  },
  {
    "prompt": "This is a technical discussion about proof, ηκB, holds in theoretical computer science.",
    "response": "Combining the above inequality with the relation ∥¯∇Lk∥≥∥∇Lk∥−κg∆k −ϵg, we obtain ∥¯∇Lk∥≥ ηκB∆k, thus (13b) holds. We complete the proof. ■ A.6 Proof of Lemma 4.9 Proof. To show the result, we consider two cases separately. Case A: max{τ + k , ∥∇Lk∥} = ∥∇Lk∥ and Case B: max{τ + k , ∥∇Lk∥} = τ + k . • Case A. When α = 1, ϑα = 2ϵf + ϵ3/2 g . Thus, to show (13a), we only need to show Aredk −2ϵf − ϵ3/2 g ≤ηPredk can be satisfied by (22)."
  },
  {
    "prompt": "Explain a concept involving Lsk, fsk, SOC in computational complexity.",
    "response": "By the algorithm design, we first consider the scenario in which the SOC step is not computed (i.e., xsk = xk + ∆xk). We have Aredk −2ϵf −ϵ3/2 g = ¯Lsk ¯µk −¯Lk ¯µk −2ϵf −ϵ3/2 g = ¯Lsk ¯µk −Lsk ¯µk + Lsk ¯µk −Lk ¯µk + Lk ¯µk −¯Lk ¯µk −2ϵf −ϵ3/2 g = ¯fsk −fsk + Lsk ¯µk −Lk ¯µk + fk −¯fk −2ϵf −ϵ3/2 g Lemma 4.4 ≤| ¯fsk −fsk| + |fk −¯fk| + Predk + ϵg∆k + Υ1∆2 k −2ϵf −ϵ3/2 g . (A.8) We observe that under Ak ∩Bk, (22) implies ∆k ≤∥¯∇Lk∥/∥¯Hk∥."
  },
  {
    "prompt": "Scientific context on κfcd, max, Predk and their role in graph theory.",
    "response": "Thus, Predk ≤−κfcd 2 ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b = −κfcd 2 ∥¯∇Lk∥∆k. (A.9) Combining (A.8) and (A.9), and noticing that when α = 1 and Ck happens, |fk −¯fk| + | ¯fsk −fsk| ≤ 2ϵf + 2κf max{∆max, 1}∆2 k, we only need to show 2κf max{∆max, 1}∆2 k + ϵg∆k + Υ1∆2 k ≤κfcd 2 (1 −η)∥¯∇Lk∥∆k."
  },
  {
    "prompt": "Understanding the computational aspects of max, κfcd, end.",
    "response": "(A.10) To this end, we note that (22) ⇒{(4κf + κg) max{∆max, 1} + 2Υ1}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg ⇒{4κf max{∆max, 1} + κfcd(1 −η)κg max{∆max, 1} + 2Υ1}∆k ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2} ϵg, since κfcd(1 −η) < 1. Rearranging the terms, we have (2κf max{∆max, 1} + Υ1)∆k ≤κfcd 2 (1 −η)(∥∇Lk∥−κg max{∆max, 1}∆k −ϵg) −ϵg. (A.11) 36 Under Bk, we have ∥¯∇Lk∥≥∥∇Lk∥−κg max{∆max, 1}∆k −ϵg. This, together with (A.11), implies (2κf max{∆max, 1} + Υ1)∆k ≤κfcd 2 (1 −η)∥¯∇Lk∥−ϵg."
  },
  {
    "prompt": "Understanding the computational aspects of max, holds, Multiplying.",
    "response": "Multiplying ∆k and rearranging the terms, we have shown (A.10), thus proved (13a). Since (13a) holds, by the algorithm design, the SOC step will not be triggered in this iteration. Next, we show that (13b) holds. When α = 1, under Ak ∩Bk, we have max{1, ∥¯Hk∥} ≤κB. Therefore, it suffices to show ∥¯∇Lk∥≥ηκB∆k. Recalling κfcd ≤1, η < 1, we have (ηκB + κg max{∆max, 1})∆k ≤{(4κf + κg) max{∆max, 1} + 2Υ1 + κB}∆k (22) ≤κfcd(1 −η)∥∇Lk∥−{κfcd(1 −η) + 2}ϵg ≤∥∇Lk∥−ϵg."
  },
  {
    "prompt": "Explain a concept involving max, Case, ηκB in computational complexity.",
    "response": "(A.12) Since ∥¯∇Lk∥≥∥∇Lk∥−κg max{∆max, 1}∆k −ϵg under Bk, which combined with (A.12) leads to ∥¯∇Lk∥≥ηκB∆k, thus (13b) holds. • Case B. In this scenario, we show that (13a) holds by considering two subcases. Case B.1: ∥ck∥≥ r and Case B.2: ∥ck∥< r. • Case B.1. In this case, the SOC step will not be performed, so we need to show that Aredk − 2ϵf −ϵ3/2 g ≤ηPredk holds for xsk = xk +∆xk."
  },
  {
    "prompt": "This is a technical discussion about max, fsk, κfcd in theoretical computer science.",
    "response": "Using the same analysis as in Case A, we have (A.8) and |fk −¯fk| + | ¯fsk −fsk| ≤2ϵf + 2κf max{∆max, 1}∆2 k under Ck with α = 1. Since Predk ≤−κfcd 2 ¯τ + k ∥ck∥∆k ≤−κfcd 2 ¯τ + k r∆k, it suffices to show 2κf max{∆max, 1}∆2 k + ϵg∆k + Υ1∆2 k ≤(1 −η)κfcd 2 ¯τ + k r∆k. (A.13) Now, we show that this can be achieved by ∆k satisfying (22)."
  },
  {
    "prompt": "Explain a concept involving max, rκfcd, min in computational complexity.",
    "response": "We notice that τ + k (22) ≥ \u00124κf max{∆max, 1} + 2Υ1 + 2Υ2 (1 −η)κfcd min{1, r} + κH + η \u0013 ∆k + (2 + 2√mLG/√κ1,G)ϵg + 2ϵh (1 −η)κfcd min{1, r} ≥ \u00124κf max{∆max, 1} + 2Υ1 (1 −η)rκfcd + κH \u0013 ∆k + 2ϵg (1 −η)rκfcd + ϵH (by the def. of ϵH). Rearranging the terms, we have (2κf max{∆max, 1} + Υ1)∆k + ϵg ≤(1 −η)κfcd 2 r \u0000τ + k −κH∆k −ϵH \u0001 . Multiplying ∆k on both sides, using the relation ¯τ + k ≥τ + k −κH∆k−ϵH under Ak∩Bk (cf. Lemma 4.3), we can show that (A.13) holds. • Case B.2."
  },
  {
    "prompt": "Read and understand this section about Aredk, ηPredk, xsk from a research paper.",
    "response": "In this case, if Aredk −2ϵf −ϵ3/2 g ≤ηPredk holds for xsk = xk + ∆xk, then there is nothing to prove. If not, the SOC step dk (cf. Section 2.1.3) will be performed. Thus, we only need to show that Aredk −2ϵf −ϵ3/2 g ≤ηPredk holds for xsk = xk + ∆xk +dk."
  },
  {
    "prompt": "This is a technical discussion about Predk, mLG, fsk in theoretical computer science.",
    "response": "Similar to the proof in Case B.1, but using the conclusion of Lemma 4.5, we have Aredk −2ϵf −ϵ3/2 g ≤| ¯fsk −fsk| + |fk −¯fk| + Predk + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + Υ2∆3 k −2ϵf 37 ≤Predk + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + (Υ2 + 2κf)∆3 k, where the last inequality is due to the definition of Ck with α = 1. By the algorithm design, we have Predk ≤−κfcd 2 ¯τ + k ∆2 k, thus we only need to show \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k + (Υ2 + 2κf)∆3 k ≤(1 −η)κfcd 2 ¯τ + k ∆2 k."
  },
  {
    "prompt": "Scientific context on κfcd, mLG, max and their role in graph theory.",
    "response": "(A.14) To this end, we note that with the definition of ϵH, τ + k (22) ≥ \u00124κf max{∆max, 1} + 2Υ1 + 2Υ2 (1 −η)κfcd min{1, r} + κH + η \u0013 ∆k + (2 + 2√mLG/√κ1,G)ϵg + 2ϵh (1 −η)κfcd min{1, r} ≥ \u0012 4κf + 2Υ2 (1 −η)κfcd + κH \u0013 ∆k + ϵh + ϵg √mLG/√κ1,G (1 −η)κfcd + ϵH. Rearranging the terms and multiplying ∆2 k on both sides, we have (2κf + Υ2)∆3 k + \u00121 2ϵh + √mLG 2√κ1,G ϵg \u0013 ∆2 k ≤κfcd 2 (1 −η)(τ + k −κH∆k −ϵH)∆2 k. Since under Ak ∩Bk, we have ¯τ + k ≥τ + k −κH∆k −ϵH (cf. Lemma 4.3)."
  },
  {
    "prompt": "Explain a concept involving Case, proof, holds in computational complexity.",
    "response": "This relation, together with the above display, has proved (A.14). Combining Case B.1 and Case B.2, we have shown that (13a) holds in Case B. Next, we show that (13b) holds in Case B. By the algorithm design, it is sufficient to show ¯τ + k ≥η∆k. We note that (22) implies τ + k ≥(κH + η)∆k + ϵH. This, combined with the relation ¯τ + k ≥τ + k −κH∆k −ϵH under Ak ∩Bk, leads to ¯τ + k ≥η∆k. We thus complete the proof. ■ A.7 Proof of Lemma 4.10 Proof."
  },
  {
    "prompt": "This is a technical discussion about max, κfcd, Predk in theoretical computer science.",
    "response": "Since (13a) holds, we have ¯Lk+1 ¯µk −¯Lk ¯µk ≤ηPredk + ϑα. When α = 0, we have Predk ≤−κfcd 2 ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b (13b) ≤−κfcd 2 η2∆2 k. When α = 1, we similarly have Predk ≤−κfcd 2 max \u001a ∥¯∇Lk∥min \u001a ∆k, ∥¯∇Lk∥ ∥¯Hk∥ \u001b , ¯τ + k ∆k (∆k + ∥ck∥) \u001b (13b) ≤− κfcdη3 2 max{∆max, 1}∆3 k. We complete the proof by combining the above two displays. ■ 38 A.8 Proof of Lemma 4.12 Proof. (a) We define the sequence {ϕk}k as ϕk = max \u001a logγ \u0012∆k b∆′ \u0013 , 0 \u001b and discuss the following cases."
  },
  {
    "prompt": "Understanding the computational aspects of Case, logγ, ΛkΘk.",
    "response": "Case A: ΛkΘk = 1, Case B: Λk(1 −Θk) = 1, and Case C: Λk = 0. • Case A. When ΛkΘk = 1, we have min{∆k, ∆k+1} ≥b∆′ and ∆k+1 = min{γ∆k, ∆max}. Since ∆k+1 ≥∆k ≥b∆′, we have logγ \u0012∆k+1 b∆′ \u0013 ≥logγ \u0012∆k b∆′ \u0013 ≥0. Therefore, ϕk = logγ(∆k/b∆′) and ϕk+1 = logγ(∆k+1/b∆′). Since ∆k+1 ≤γ∆k, we further have logγ \u0012∆k b∆′ \u0013 + 1 ≥logγ \u0012∆k+1 b∆′ \u0013 , thus ϕk+1 −ϕk ≤1. • Case B. When Λk(1 −Θk) = 1, we have min{∆k, ∆k+1} ≥b∆′ and ∆k+1 = ∆k/γ."
  },
  {
    "prompt": "Explain a concept involving logγ, leads, Case in computational complexity.",
    "response": "Therefore, we must have ∆k ≥γ b∆′, which leads to logγ \u0012∆k b∆′ \u0013 ≥1 and logγ \u0012∆k+1 b∆′ \u0013 = logγ \u0012∆k b∆′ \u0013 −1 ≥0. Thus, ϕk = logγ(∆k/b∆′), ϕk+1 = logγ(∆k+1/b∆′) and ϕk+1 −ϕk = −1. • Case C. When Λk = 0, we have max{∆k, ∆k+1} ≤b∆′. Thus, ϕk = ϕk+1 = 0. Combining the above three cases, we sum ϕk+1 −ϕk over k = 0, · · · , T −1 and get ϕT −ϕ0 = T−1 X k=0 ϕk+1 −ϕk ≤ T−1 X k=0 ΛkΘk − T−1 X k=0 Λk(1 −Θk)."
  },
  {
    "prompt": "Understanding the computational aspects of Noticing, max, Case.",
    "response": "(A.15) Noticing that ϕ0 = logγ(∆0/b∆′) (since ∆0 > b∆′) and ϕT ≥0, we rearrange the terms in (A.15) and complete the proof. (b) This is a direct corollary of (a), which can be proved by rearranging terms of (a). (c) We define the sequence {eϕk}k as eϕk = max ( logγ b∆′ ∆k ! , 0 ) and discuss three cases. Case A: (1 −Λk)Θk = 1, Case B: (1 −Λk)(1 −Θk) = 1, Case C: Λk = 1. • Case A. When (1−Λk)Θk = 1, we have max{∆k, ∆k+1} ≤b∆′ and ∆k+1 = min{γ∆k, ∆max}. Since ∆k ≤b∆′ ≤γ−1∆max, we have γ∆k ≤∆max."
  },
  {
    "prompt": "Explain a concept involving logγ, eϕk, case in computational complexity.",
    "response": "Thus ∆k+1 = γ∆k. In this case, to ensure ∆k+1 ≤b∆′, we must have ∆k ≤γ−1 b∆′. The above relations imply logγ b∆′ ∆k ! ≥1 and logγ b∆′ ∆k+1 ! = logγ b∆′ ∆k ! −1 ≥0. 39 Therefore, eϕk = logγ(b∆′/∆k), eϕk+1 = logγ(b∆′/∆k+1), and eϕk+1 −eϕk = −1. • Case B. When (1 −Λk)(1 −Θk) = 1, we have max{∆k, ∆k+1} ≤b∆′ and ∆k+1 = ∆k/γ. Thus, logγ b∆′ ∆k ! ≥0 and logγ b∆′ ∆k+1 ! = logγ b∆′ ∆k ! + 1. Therefore, eϕk = logγ(b∆′/∆k), eϕk+1 = logγ(b∆′/∆k+1), and eϕk+1 −eϕk = 1. • Case C."
  },
  {
    "prompt": "This is a technical discussion about eϕk, min, eϕT in theoretical computer science.",
    "response": "When Λk = 1, we have min{∆k+1, ∆k} ≥b∆′, thus eϕk+1 = eϕk = 0. Combining the above cases, we sum eϕk+1 −eϕk over k = 0, · · · , T −1 and get eϕT −eϕ0 = T−1 X k=0 eϕk+1 −eϕk ≤ T−1 X k=0 (1 −Λk)(1 −Θk) − T−1 X k=0 (1 −Λk)Θk. Noticing that eϕ0 = max{logγ(b∆′/∆0), 0} = 0 (since ∆0 = ∆max ≥b∆′) and eϕT ≥0, we rearrange the terms and complete the proof."
  },
  {
    "prompt": "Understanding the computational aspects of Lemmas, Lemma, Proof.",
    "response": "(d) We note that T X k=0 (1 −Λk)Ik = T X k=0 (1 −Λk)IkΘk ≤ T X k=0 (1 −Λk)Θk ≤ T X k=0 (1 −Λk)(1 −Θk) = T X k=0 (1 −Λk)(1 −Θk)(1 −Ik) ≤ T X k=0 (1 −Λk)(1 −Ik). The first equality follows from Lemmas 4.8 and 4.9 that small and accurate iterations must be sufficient, the second inequality is by Lemma 4.12 (c), and the last equality is by the converse of Lemmas 4.8 and 4.9 that small and insufficient iterations must be inaccurate. ■ A.9 Proof of Lemma 4.14 Proof."
  },
  {
    "prompt": "Read and understand this section about proof, submartingale, Letting from a research paper.",
    "response": "It follows from P(Ik = 1 | Fk−1/2) ≥p that {PT−1 k=0 Ik −pT} is a submartingale. Moreover, T X k=0 Ik −p(T + 1) ! − T−1 X k=0 Ik −pT ! = |IT −p| ≤max{|1 −p|, |0 −p|} = p. The last equality follows from p > 1/2. By the Azuma-Hoeffding inequality, for any T ≥1 and c ≥0, we have P \"T−1 X k=0 Ik −pT ≤−c # ≤exp \u001a −c2 2Tp2 \u001b . Letting c = (p −bp)T, we complete the proof. ■ 40 A.10 Proof of Lemma 4.15 Proof."
  },
  {
    "prompt": "Read and understand this section about ΘkIkΛk, logγ, suffices from a research paper.",
    "response": "It suffices to show that Tϵ > T −1 and PT−1 k=0 Ik ≥pT imply PT−1 k=0 ΘkIkΛk ≥ \u0000p −1 2 \u0001 T − 1 2 logγ \u0010 ∆0 b∆ \u0011 −1."
  },
  {
    "prompt": "Read and understand this section about iterations, set, accurate from a research paper.",
    "response": "For k = 0, 1, · · · , T −1, we define the following sets and use | · | to denote their car- dinality: • AL = {k : IkΛk = 1} as the set of iterations that are large and accurate; • IL = {k : (1 −Ik)Λk = 1} as the set of iterations that are large and inaccurate; • AS = {k : Ik(1 −Λk) = 1} as the set of iterations that are small and accurate; • IS = {k : (1 −Ik)(1 −Λk) = 1} as the set of iterations that are small and inaccurate."
  },
  {
    "prompt": "Scientific context on Lemma, valid, noticing and their role in graph theory.",
    "response": "This classification is valid, noticing that |IS| + |AS| + |IL| + |AL| = T. It follows from Lemma 4.12 (b) and the observation PT−1 k=0 Λk = |IL| + |AL| that T−1 X k=0 ΛkΘk ≥1 2(|IL| + |AL|) −1 2 logγ \u0012∆0 b∆′ \u0013 . (A.16) Since PT−1 k=0 Ik ≥pT, by the fact that an iteration is either large or small, we have |IL| + |IS| = T−1 X k=0 (1 −Ik) = T − T−1 X k=0 Ik ≤T −pT. (A.17) Moreover, it follows from Lemma 4.12 (d) that |AS| = T−1 X k=0 (1 −Λk)Ik ≤ T−1 X k=0 (1 −Λk)(1 −Ik) = |IS|."
  },
  {
    "prompt": "Scientific context on logγ, ΛkΘk, Combining and their role in graph theory.",
    "response": "(A.18) Combining the above displays, we have T−1 X k=0 ΛkΘkIk = T−1 X k=0 ΛkΘk − T−1 X k=0 ΛkΘk(1 −Ik) (A.16) ≥ 1 2(|IL| + |AL|) −1 2 logγ \u0012∆0 b∆′ \u0013 − T−1 X k=0 Λk(1 −Ik) = 1 2(|IL| + |AL|) −1 2 logγ \u0012∆0 b∆′ \u0013 −|IL| = 1 2(T −|IS| −|AS|) −1 2 logγ \u0012∆0 b∆′ \u0013 −|IL| (A.17) ≥ 1 2(T −|IS| −|AS|) −1 2 logγ \u0012∆0 b∆′ \u0013 + |IS| −T + pT = 1 2(|IS| −|AS|) + \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆′ \u0013 (A.18) ≥ \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆′ \u0013 ≥ \u0012 p −1 2 \u0013 T −1 2 logγ \u0012∆0 b∆ \u0013 −1."
  },
  {
    "prompt": "Scientific context on logγ, proof, inequality and their role in graph theory.",
    "response": "In the last inequality, we use the relation γ−2 b∆≤b∆′, which implies ∆0/b∆′ ≤∆0/(γ−2 b∆) and thus logγ \u0012∆0 b∆′ \u0013 ≤logγ \u0012 ∆0 γ−2 b∆ \u0013 = logγ \u0012∆0 b∆ \u0013 + 2. We complete the proof. ■ 41 B Proofs of Section 4.2 B.1 Proof of Theorem 4.16 Proof. Since P{Tϵ ≤T −1} = 1 −P{Tϵ > T −1}, we will bound P{Tϵ > T −1} in the proof. By the law of total probability, we have P{Tϵ > T −1} = P ( Tϵ > T −1, T−1 X k=0 (ek + esk) > (2ϵf + 2s)T ) | {z } A + P ( Tϵ > T −1, T−1 X k=0 (ek + esk) ≤(2ϵf + 2s)T ) | {z } B ."
  },
  {
    "prompt": "Explain a concept involving separately, lemmas, Lemma in computational complexity.",
    "response": "(B.1) In what follows, we present lemmas that bound P(A) and P(B), separately. The proofs of these lemmas are presented in Appendices B.2, B.3 and B.4. Lemma B.1 (Burkholder-type inequality). Let Xk, k = 0, 1, · · · , T −1, be random variables with E[Xk | X0:k−1] = 0 and E[|Xk|p] < ∞for any k ≥0 and some p ∈(1, 2], then E \" T−1 X k=0 Xk p# ≤22−p T−1 X k=0 E[|Xk|p]. (B.2) Lemma B.2."
  },
  {
    "prompt": "This is a technical discussion about eϵf, exp, Lemma in theoretical computer science.",
    "response": "Under conditions of the probabilistic heavy-tailed zeroth-order oracle, for any T ≥1, s ≥0, and δ specified in (18), we have P(A) ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f · T  + 32+δΥf (ϵf −eϵf + s)1+δ · T −δ. Lemma B.3. Under Assumptions 4.1, 4.7, when bp satisfies (24) and T satisfies (25), we have P(B) ≤exp \u001a −(p −bp)2 2p2 T \u001b . (B.3) Combining (B.1) with the conclusions of Lemmas B.2 and B.3, we complete the proof. ■ B.2 Proof of Lemma B.1 Proof."
  },
  {
    "prompt": "Explain a concept involving sgn, inequality, result in computational complexity.",
    "response": "To prove the result, we fist show the following inequality: |a + b|p ≤|a|p + p · sgn(a) · |a|p−1b + 22−p|b|p. (B.4) When a = 0, the inequality holds trivially as 22−p ≥1 for p ∈(1, 2]. Moreover, when p = 2, the in- equality also holds trivially by the observation that sgn(a)|a| = a. In what follows, we consider a ̸= 0 and p ∈(1, 2). We divide |a|p on both sides and get 1 + b a p ≤1 + p b a + 22−p b a p , 42 where we use the observation that sgn(a)/|a| = 1/a."
  },
  {
    "prompt": "Read and understand this section about Case, Denoting, suffices from a research paper.",
    "response": "Denoting x = b/a, it suffices to show that f(x) = 1 + px + 22−p |x|p −|1 + x|p ≥0. To this end, we consider three cases. Case A: b/a > 0, Case B: −1 < b/a ≤0, and Case C: b/a ≤−1. • Case A: b/a > 0: In this case f(x) = 1 + px + 22−pxp −(1 + x)p with f(0) = 0, f′(0) = 0 and f′′(x) = p(p −1) \"\u00122 x \u00132−p − \u0012 1 1 + x \u00132−p# . Since 1 + x > x/2 for all x > 0, when p < 2, we have \u00122 x \u00132−p > \u0012 1 1 + x \u00132−p =⇒f′′(x) > 0."
  },
  {
    "prompt": "Read and understand this section about holds, implies, Case from a research paper.",
    "response": "Therefore, for all x > 0, we have f′(x) > 0, which then implies f(x) > 0, and thus (B.4) holds. • Case B: −1 < b/a ≤0: We substitute y = 1 + x so that 0 ≤y ≤1. Then, f(x) = f(y −1) = 22−p(1 −y)p + p(y −1) + 1 −yp ≥(1 −y)p + p(y −1) + 1 −yp := g(y). Then, we have g(1) = 0, g′(0) = g′(1) = 0, and g′′(y) = p(p −1) \"\u0012 1 1 −y \u00132−p − \u00121 y \u00132−p# . Note that g′′(y) has exactly one root at y = 1/2. Moreover, g′′(y) < 0 for y < 1/2 and g′′(y) > 0 for y > 1/2, so g′(y) has a minimum at y = 1 2."
  },
  {
    "prompt": "This is a technical discussion about conclude, Case, monotonically in theoretical computer science.",
    "response": "Since g′(0) = g′(1) = 0, we conclude that g′(y) < 0 for all 0 < y < 1. As g(y) is monotonically decreasing on (0, 1) and g(1) = 0, we have g(y) ≥0 for all 0 ≤y ≤1. Therefore, f(x) ≥0. • Case C: b/a ≤−1: We substitute z = −1 −x so that z ≥0. Then f(x) = f(−z −1) =: h(z) and h(z) = 22−p(1 + z)p −p(1 + z) + 1 −zp. Then, we have h(0) ≥0, h′(1) = 0, and h′(z) = p(p −1) \"\u0012 2 1 + z \u00132−p − \u00121 z \u00132−p# . Note that h′′(z) = 0 only at z = 1."
  },
  {
    "prompt": "Scientific context on prove, minimum, inequality and their role in graph theory.",
    "response": "Moreover, h′′(z) < 0 for z < 1 and h′′(z) > 0 for z > 1, so h′(z) has a minimum at z = 1. Since h′(1) = 0, we know h′(z) ≥0 for all z ≥0, and hence h(z) ≥0 for all z ≥0. Combining the above three cases, we prove (B.4). Next, we prove (B.2) by induction. The inequality holds trivially for T = 1, as 22−p ≥1 for p ∈ (1, 2]. Suppose the inequality holds for T = n, we now consider T = n + 1."
  },
  {
    "prompt": "Understanding the computational aspects of sgn, proof, Applying.",
    "response": "Applying (B.4) with a = Pn−1 k=0 Xk and b = Xn, we have n X k=0 Xk p ≤ n−1 X k=0 Xk p + p · sgn( n−1 X k=0 Xk) · n−1 X k=0 Xk p−1 · Xn + 22−p|Xn|p. 43 Taking expectation on both sides, we have E \" n X k=0 Xk p# ≤E \" n−1 X k=0 Xk p# + pE  sgn( n−1 X k=0 Xk) n−1 X k=0 Xk p−1 · Xn  + 22−pE [|Xn|p] . Using E[Xn | X0:n−1] = 0, we get E \" n X k=0 Xk p# ≤E \" n−1 X k=0 Xk p# + 22−pE [|Xn|p] . We complete the proof by induction. ■ B.3 Proof of Lemma B.2 Proof."
  },
  {
    "prompt": "This is a technical discussion about esk, eϵf, bound in theoretical computer science.",
    "response": "We observe that P(A) ≤P (T−1 X k=0 (ek + esk) > (2ϵf + 2s)T ) = P (T−1 X k=0 (ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) > (2ϵf + 2s)T − T−1 X k=0 (E[ek | Fk−1] + E[esk | Fk−1/2]) ) ≤P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf + s)T − T−1 X k=0 E[ek | Fk−1] ) + P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf + s)T − T−1 X k=0 E[esk | Fk−1/2] ) (by union bound) (17) ≤P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) + P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf −eϵf + s)T ) ."
  },
  {
    "prompt": "Read and understand this section about Case, bound, discussing from a research paper.",
    "response": "(B.5) Next we bound (B.5) by discussing two cases. Case A: δ ∈(0, 1] and Case B: δ ∈(1, ∞). • Case A: When δ ∈(0, 1]."
  },
  {
    "prompt": "Scientific context on eϵf, Lemma, Markov and their role in graph theory.",
    "response": "Since ek −E[ek | Fk−1] is a martingale difference that satisfies (18), it follows from the Markov’s inequality and the conclusion of Lemma B.1 that P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) ≤ E \u0014 PT−1 k=0 (ek −E[ek | Fk−1]) 1+δ\u0015 (ϵf −eϵf + s)1+δT 1+δ ≤ 21−δ PT−1 k=0 E h |ek −E[ek | Fk−1]|1+δi (ϵf −eϵf + s)1+δT 1+δ ≤ 21−δTΥf (ϵf −eϵf + s)1+δT 1+δ = 21−δΥf (ϵf −eϵf + s)1+δ · T −δ."
  },
  {
    "prompt": "Understanding the computational aspects of eϵf, esk, δΥf.",
    "response": "44 We can similarly show P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf −eϵf + s)T ) ≤ 21−δΥf (ϵf −eϵf + s)1+δ · T −δ. Therefore, for δ ∈(0, 1] we have P(A) ≤ 22−δΥf (ϵf −eϵf + s)1+δ · T −δ. • Case B: When δ ∈(1, ∞)."
  },
  {
    "prompt": "Explain a concept involving eϵf, inequality, Corollary in computational complexity.",
    "response": "We apply the martingale Fuk–Nagaev inequality (Fan et al., 2017, Corollary 2.5) and get P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) ≤exp \u0012 −(ϵf −eϵf + s)2T 2 2V \u0013 + C (ϵf −eϵf + s)1+δT 1+δ , (B.6) where V = 1 4(3 + δ)2e1+δ T−1 X k=0 E h (ek −E [ek | Fk−1])2 | Fk−1 i , C = \u0012 1 + 2 1 + δ \u00131+δ T−1 X k=0 E h |ek −E [ek | Fk−1]|1+δ | Fk−1 i . By the H¨older’s inequality and (18), we have E h (ek −E [ek | Fk−1])2 | Fk−1 i ≤Υ 2 1+δ f ."
  },
  {
    "prompt": "This is a technical discussion about eϵf, exp, ΥfT in theoretical computer science.",
    "response": "Thus V ≤1 4(3 + δ)2e1+δΥ 2 1+δ f T and C ≤ \u0012 1 + 2 1 + δ \u00131+δ ΥfT. (B.7) Combining (B.6) and (B.7), we have P (T−1 X k=0 (ek −E[ek | Fk−1]) > (ϵf −eϵf + s)T ) ≤exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T   + Υf \u0012 3 + δ (1 + δ)(ϵf −eϵf + s) \u00131+δ · T −δ. (B.8) By similar argument, we have P (T−1 X k=0 (esk −E[esk | Fk−1/2]) > (ϵf −eϵf + s)T ) ≤exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T   + Υf \u0012 3 + δ (1 + δ)(ϵf −eϵf + s) \u00131+δ · T −δ."
  },
  {
    "prompt": "Explain a concept involving eϵf, exp, Combining in computational complexity.",
    "response": "(B.9) 45 Combining (B.8) and (B.9), we have P(A) ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T  + 2Υf \u0012 3 + δ (1 + δ)(ϵf −eϵf + s) \u00131+δ · T −δ. Combining the conclusions in Case A. and Case B., we have P(A) ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T  + 2Υf · max \u001a\u0010 3+δ 1+δ \u00111+δ , 21−δ \u001b (ϵf −eϵf + s)1+δ · T −δ ≤2 exp  −2(ϵf −eϵf + s)2 (3 + δ)2e1+δΥ 2 1+δ f T  + 32+δΥf (ϵf −eϵf + s)1+δ · T −δ, as (3 + δ)/(1 + δ) < 3 for δ > 0. We thus complete the proof."
  },
  {
    "prompt": "This is a technical discussion about Proof, Lemma, esk in theoretical computer science.",
    "response": "■ B.4 Proof of Lemma B.3 Proof. Denoting d = logγ(∆0/b∆) + 2, by the law of total probability, we have P(B) = P ( Tϵ > T −1, T−1 X k=0 (ϑα + ek + esk) ≤(2ϵf + ϑα + 2s)T, T−1 X k=0 ΛkIkΘk ≥ \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B1 + P ( Tϵ > T −1, T−1 X k=0 (ϑα + ek + esk) ≤(2ϵf + ϑα + 2s)T, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B2 . To prove P(B1) = 0, we will show that Tϵ > T −1, PT−1 k=0 (ϑα +ek +esk) ≤(2ϵf +ϑα +2s)T together imply PT−1 k=0 ΛkIkΘk < (bp−1/2)T −d/2."
  },
  {
    "prompt": "Explain a concept involving ΛkIkΘk, Case, Suppose in computational complexity.",
    "response": "Suppose, on the contrary, PT−1 k=0 ΛkIkΘk ≥(bp−1/2)T −d/2 holds. Then unifying α = 0 and α = 1, we discuss the following two cases. Case A: ΛkIkΘk = 1 and Case B: ΛkIkΘk = 0. • Case A: ΛkIkΘk = 1. We have Lk+1 ¯µk −Lk ¯µk = Lk+1 ¯µk −¯Lk+1 ¯µk + ¯Lk+1 ¯µk −¯Lk ¯µk + ¯Lk ¯µk −Lk ¯µk ≤|fk+1 −¯fk+1| + | ¯fk −fk| + ¯Lk+1 ¯µk −¯Lk ¯µk ≤−hα(∆k) + ek + esk + ϑα (by Lemma 4.10) ≤−hα(b∆′) + ek + esk + ϑα."
  },
  {
    "prompt": "Understanding the computational aspects of finf, esk, Dividing.",
    "response": "(since ∆k ≥b∆′ when Λk = 1) Dividing ¯µk on both sides and noticing that ¯µ0 ≤¯µk ≤¯µT−1 and γ−2 b∆< b∆′, we have 1 ¯µk (Lk+1 ¯µk −finf) −1 ¯µk (Lk ¯µk −finf) ≤−1 ¯µk hα(b∆′) + 1 ¯µk (ϑα + ek + esk) < − 1 ¯µT−1 hα(γ−2 b∆) + 1 ¯µ0 (ϑα + ek + esk). 46 • Case B: ΛkIkΘk = 0. We consider two subcases. Case B.1: (13a) holds in the k-th iteration; and Case B.2: (13a) does not hold the k-th iteration."
  },
  {
    "prompt": "Explain a concept involving Case, ηPredk, esk in computational complexity.",
    "response": "• Case B.1: In this case, we have Lk+1 ¯µk −Lk ¯µk = Lk+1 ¯µk −¯Lk+1 ¯µk + ¯Lk+1 ¯µk −¯Lk ¯µk + ¯Lk ¯µk −Lk ¯µk ≤|fk+1 −¯fk+1| + | ¯fk −fk| + ηPredk + ϑα (by(13a)) ≤ek + esk + ϑα. The last inequality is because Predk < 0 but we can no longer guarantee (13b). • Case B.2: In this case, since xk+1 = xk, we have Lk+1 ¯µk −Lk ¯µk = 0. We combine Case B.1 and Case B.2 and divide ¯µk on both sides. Since ¯µ0 ≤¯µk, we have 1 ¯µk (Lk+1 ¯µk −finf) −1 ¯µk (Lk ¯µk −finf) ≤1 ¯µ0 (ϑα + ek + esk)."
  },
  {
    "prompt": "Read and understand this section about finf, ΛkIkΘk, esk from a research paper.",
    "response": "(B.10) Using the results in (B.10) , we sum over k = 0, 1, · · · , T −1. Since ¯µk ≤¯µk+1, we have 1 ¯µT−1 (LT ¯µT −1 −finf) −1 ¯µ0 (L0 ¯µ0 −finf) ≤−hα(γ−2 b∆) ¯µT−1 T−1 X k=0 ΛkIkΘk + T−1 X k=0 ϑα + ek + esk ¯µ0 . Rearranging the terms, using the relations PT−1 k=0 (ϑα+ek+esk) ≤(2ϵf+ϑα+2s)T and PT−1 k=0 ΛkIkΘk ≥ (bp −1/2)T −d/2, we have 1 ¯µT−1 (LT ¯µT −1 −finf) ≤1 ¯µ0 (L0 ¯µ0 −finf) −hα(γ−2 b∆) ¯µT−1 \u0012 bp −1 2 \u0013 T + 2ϵf + ϑα + 2s ¯µ0 T + hα(γ−2 b∆)d 2¯µT−1 ."
  },
  {
    "prompt": "This is a technical discussion about ΛkIkΘk, side, negative in theoretical computer science.",
    "response": "(B.11) However, the conditions of bp and T imply that the right-hand side of (B.11) is negative. This yields a contradiction, since the left-hand side is non-negative. We thus proved P(B1) = 0. Next, we bound P(B2). We observe that P(B2) ≤P ( Tϵ > T −1, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) = P ( Tϵ > T −1, T−1 X k=0 Ik < bpT, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B21 + P ( Tϵ > T −1, T−1 X k=0 Ik ≥bpT, T−1 X k=0 ΛkIkΘk < \u0012 bp −1 2 \u0013 T −d 2 ) | {z } B22 ."
  },
  {
    "prompt": "Read and understand this section about Lemma, proof, exp from a research paper.",
    "response": "For P(B21), we have P(B21) ≤P (T−1 X k=0 Ik < bpT ) Lemma 4.14 ≤ exp \u001a −(p −bp)2 2p2 T \u001b . 47 For P(B22), if follows from Lemma 4.15 that P(B22) = 0. Combining the above results, we have P(B) ≤exp \u001a −(p −bp)2 2p2 T \u001b . We thus complete the proof. ■ C Proofs of Section 4.3 C.1 Proof of Theorem 4.20 Proof. Same as the proof of Theorem 4.16 in Appendix B, we also bound P{Tϵ > T −1} via (B.1) and analyze P(A) and P(B) separately."
  },
  {
    "prompt": "Scientific context on esk, Markov, inequality and their role in graph theory.",
    "response": "To bound P(A), we observe that P(A) ≤P (T−1 X k=0 (ek + esk) ≥(2ϵf + 2s)T ) = P (T−1 X k=0 (ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) ≥(2ϵf + 2s)T − T−1 X k=0 (E[ek | Fk−1] + E[esk | Fk−1/2]) ) = P (T−1 X k=0 λ(ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) ≥λ(2ϵf + 2s)T − T−1 X k=0 λ(E[ek | Fk−1] + E[esk | Fk−1/2]) ) for any λ ∈ \u0014 0, 1 b \u0015 (17) ≤P (T−1 X k=0 λ(ek + esk −E[ek | Fk−1] −E[esk | Fk−1/2]) ≥λ(2ϵf −2eϵf + 2s)T ) ≤E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i · e−λ(2ϵf−2eϵf+2s)T (by Markov inequality)."
  },
  {
    "prompt": "Explain a concept involving esk, Tower, older in computational complexity.",
    "response": "Next, we prove E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i ≤e (2v)2λ2 2 T . To show this, we first note that E[eλ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2]) | Fk−1] ≤ \u0010 E[e2λ(ek−E[ek|Fk−1]) | Fk−1] · E[e2λ(esk−E[esk|Fk−1/2]) | Fk−1] \u00111/2 (by H¨older’s inequality) ≤ \u0010 E[e2λ(ek−E[ek|Fk−1]) | Fk−1] · E[E[e2λ(esk−E[esk|Fk−1/2]) | Fk−1/2] | Fk−1] \u00111/2 (by Tower’s property) (19) ≤ \u0010 e2λ2v2 · e2λ2v2\u00111/2 = e λ2(2v)2 2 for all λ ∈ \u0014 0, 1 2b \u0015 ."
  },
  {
    "prompt": "This is a technical discussion about esk, parameters, sub-exponential in theoretical computer science.",
    "response": "(C.1) 48 Thus, given Fk−1, ek + esk follows a sub-exponential distribution with parameters (2v, 2b)."
  },
  {
    "prompt": "This is a technical discussion about esk, esT, exp in theoretical computer science.",
    "response": "Since E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i = E h E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2]) | FT−2 ii = E h e PT −2 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2]) · E h eλ(eT −1+esT −1−E[eT −1|FT −2]−E[esT −1|FT −3/2]) | FT−2 ii (C.1) ≤E h e PT −2 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i · e λ2(2v)2 2 , we can inductively show E h e PT −1 k=0 λ(ek+esk−E[ek|Fk−1]−E[esk|Fk−1/2])i ≤e (2v)2λ2 2 T for any λ ∈ \u0014 0, 1 2b \u0015 . Therefore, we have P(A) ≤exp \b (2v2λ2)T −2λ(ϵf −eϵf + s)T ."
  },
  {
    "prompt": "Scientific context on eϵf, minimize, side and their role in graph theory.",
    "response": "To find the optimal λ to minimize the right-hand side, we define l(λ) = (2v2λ2)T −2λ(ϵf −eϵf + s)T. Since el(λ) is monotonically increasing with respect to l(λ), we only need to minimize l(λ), which is achieved by λ∗= (ϵf −eϵf +s)/(2v2). Recalling that λ ∈[0, 1/(2b)], when (ϵf −eϵf +s)/(2v2) ≤1/(2b), we have min λ el(λ) = el(λ∗) = e− (ϵf −eϵf +s)2 2v2 T , otherwise min λ el(λ) = el(1/(2b)) ≤e− ϵf −eϵf +s 2b T ."
  },
  {
    "prompt": "Scientific context on eϵf, Lemma, proof and their role in graph theory.",
    "response": "Combining the above two displays, we have P(A) ≤exp \u001a −1 2 min \u0012(ϵf −eϵf + s)2 v2 , ϵf −eϵf + s b \u0013 T \u001b . (C.2) Note that the proof of Lemma B.3 is independent to the probabilistic heavy-tailed zeroth-order oracle design, thus we can bound P(B) in the same approach as in Lemma B.3 and obtain (B.3). Combining (B.3) and (C.2), we complete the proof. ■ C.2 Proof of Lemma 4.23 Proof."
  },
  {
    "prompt": "Explain a concept involving esk, definition, union in computational complexity.",
    "response": "It follows from the definition of Ck and the union bound that P(Ck | Fk−1/2) ≥1 −P(ek ≥ϵf + κf∆α+2 k | Fk−1) −P(esk ≥ϵf + κf∆α+2 k | Fk−1/2), where we also use the fact that P(ek ≥ϵf + κf∆α+2 k | Fk−1) = P(ek ≥ϵf + κf∆α+2 k | Fk−1/2). We consider the probability P(ek ≥ϵf + κf∆α+2 k | Fk−1) first."
  },
  {
    "prompt": "Scientific context on eϵf, Markov, inequality and their role in graph theory.",
    "response": "We have P(ek ≥ϵf + κf∆α+2 k | Fk−1) = P(ek −E[ek | Fk−1] ≥ϵf + κf∆α+2 k −E[ek | Fk−1] | Fk−1) = P(eλ(ek−E[ek|Fk−1]) ≥eλ(ϵf+κf∆α+2 k −E[ek|Fk−1]) | Fk−1) 49 (17) ≤P(eλ(ek−E[ek|Fk−1]) ≥eλ(ϵf−eϵf) | Fk−1) ≤E[eλ(ek−E[ek|Fk−1]) | Fk−1] · e−λ(ϵf−eϵf) (by Markov’s inequality) (19) ≤e λ2v2 2 −λ(ϵf−eϵf), where the second equality is due to the monotonicity of ex and holds for any λ ∈[0, 1/b]."
  },
  {
    "prompt": "This is a technical discussion about eϵf, Appendix, Applying in theoretical computer science.",
    "response": "We apply the same approach as in Appendix C.1 to find the optimal λ that minimizes the right-hand side and get P(ek ≥ϵf + κf∆α+2 k | Fk−1) ≤exp \u001a −1 2 min \u0012(ϵf −eϵf)2 v2 , ϵf −eϵf b \u0013\u001b . (C.3) Applying the same analysis to P(esk ≥ϵf + κf∆α+2 k | Fk−1/2), we can still get (C.3). Combining them together, we show (32) and complete the proof. ■ 50"
  },
  {
    "prompt": "This is a technical discussion about Japan, Mar, Kojirakawa in theoretical computer science.",
    "response": "arXiv:2503.20550v1 [math.CO] 26 Mar 2025 On the order of the shortest solution sequences for the pebble motion problems Tomoki Nakamigawa 1 Department of Information Science Shonan Institute of Technology 1-1-25 Tsujido-Nishikaigan, Fujisawa 251-8511, Japan Tadashi Sakuma 2 Faculty of Science Yamagata University 1-4-12 Kojirakawa, Yamagata 990-8560, Japan Abstract Let G be a connected graph with N vertices."
  },
  {
    "prompt": "Scientific context on path, vertex, number and their role in graph theory.",
    "response": "Let k be the number of vertices in a longest path of G such that every vertex on the path is a cut vertex of G, and every intermediate vertex of the path is a degree-two vertex of G. Let P = {1, . . . , n} be a set of pebbles with n + k < N. A conﬁguration of P on G is deﬁned as a function f from V (G) to {0, 1, . . . , n} with |f −1(i)| = 1 for 1 ≤i ≤n, where f −1(i) is a vertex occupied with the ith pebble for 1 ≤i ≤n and f −1(0) is a set of unoccupied vertices."
  },
  {
    "prompt": "Explain a concept involving neighbor, pebble, deﬁned in computational complexity.",
    "response": "A move is deﬁned as shifting a pebble from a vertex to some unoccupied neighbor. The pebble motion problem on the pair (G, P) is to decide whether a given conﬁguration of pebbles is reachable from another by executing a sequence of moves."
  },
  {
    "prompt": "Scientific context on log, min, N-vertex and their role in graph theory.",
    "response": "In this paper, we show that the length of the shortest solution sequence of the pebble motion problem on the pair (G, P) is in O(Nn + n2 log(min{n, k})) if G is a N-vertex tree, and it is in O(N 2 + n3 N−n + n2 log(min{n, N −n})) if G is a connected general N-vertex graph. We provide an algorithm that can obtain a solution sequence of lengths that satisfy these orders, with the same computational complexity as the order of the length."
  },
  {
    "prompt": "Explain a concept involving Number, JSPS, KAKENHI in computational complexity.",
    "response": "Keywords: pebble motion, motion planning, multi-agent path ﬁnding, 15-puzzle, tree 1This work was supported by JSPS KAKENHI Grant Number JP16K05260. Email: nakamigwt@gmail.com 2This work was supported by JSPS KAKENHI Grant Number JP16K05260, JP26400185, JP18K03388. Email: sakuma@sci.kj.yamagata-u.ac.jp 1 Introduction Let G be a ﬁnite undirected graph with no multiple edges or loops. The vertex set of G and the edge set of G are denoted by V (G) and E(G), respectively. Let P = {1, . . ."
  },
  {
    "prompt": "This is a technical discussion about set, deﬁned, vertex in theoretical computer science.",
    "response": ", n} be a set of pebbles with n < |V (G)|. A conﬁguration of P on G is deﬁned as a function f from V (G) to {0, 1, . . . , n} with |f −1(i)| = 1 for 1 ≤i ≤n, where f −1(i) is a vertex occupied with the ith pebble for 1 ≤i ≤n and f −1(0) is a set of unoccupied vertices. A move is deﬁned as shifting a pebble from a vertex to some unoccupied neighbor."
  },
  {
    "prompt": "Read and understand this section about pair, pebble, pebbles from a research paper.",
    "response": "The pebble motion problem on the pair (G, P) is to decide whether a given conﬁguration of pebbles reachable from another by executing a sequence of moves. Regarding the notation ‘on the pair (G, P)’ used above, in this paper, we also allow the expression ‘on (G, n)’ when P = {1, . . . , n}. Furthermore, as long as there is no risk of misunderstanding, we will also permit the shorthand ‘on G’ that omits P and n. Let F(G, P) denote the set of all conﬁgurations on the pair (G, P)."
  },
  {
    "prompt": "Understanding the computational aspects of Loyd, problem, puzzle.",
    "response": "The well-known puzzle named “15-puzzle” due to Loyd [38] is a typical example of this problem where the graph G is a 4 × 4-grid (cf. [3,30]). In 1974, Wilson [66] solved completely the feasibility problem (i.e. the problem of determining whether all the conﬁgurations of the puzzle are rearrangeable from one another or not) for the case of |f −1(0)| = 1 on general graphs, and followed by the result of Kornhauser, Miller and Spirakis (FOCS ’84) [34] for the case of |f −1(0)| ≥2."
  },
  {
    "prompt": "Read and understand this section about Raghavan, Sudan, Tamaki from a research paper.",
    "response": "Papadimitriou, Raghavan, Sudan, and Tamaki (FOCS ’94) [45] consider the case that there exists a single special pebble (“robot”) and that the other pebbles (“obstacles”) are indistinguishable. They focus on the time complexity problems for optimal number of moves from an arbitrary given conﬁguration of the pebbles to a proper goal conﬁguration in which the robot is on the desired vertex."
  },
  {
    "prompt": "Explain a concept involving problems, constraints, science in computational complexity.",
    "response": "The pebble motion problems, and the multi-agent path ﬁnding problems, which are variants with some relaxed constraints, have not only theoretical interest but also broad applications across a wide range of ﬁelds in computer science, particularly in artiﬁcial intelligence and robotics. As a result, a vast amount of research [1–30, 32–74] has already been accumulated."
  },
  {
    "prompt": "Explain a concept involving pebble, motion, problem in computational complexity.",
    "response": "Furthermore, in light of recent advancements in the ﬁeld of artiﬁcial intelligence, the number of research papers in this area has been following a signiﬁcant upward trend. That said, there is no question that the essential underlying model in the pebble motion problem is still the pebble motion problem on trees, i.e., the problem of restricting G to being a tree in the input pair (G, P). The pebble motion problem must be discussed on a connected graph because of its nature."
  },
  {
    "prompt": "Explain a concept involving motion, pebble, hand in computational complexity.",
    "response": "On the other hand, since a connected graph always has a spanning tree as a subgraph, the pebble motion problem on trees is a versatile model that can be applied to all pebble motion problems."
  },
  {
    "prompt": "Understanding the computational aspects of reconﬁguration, board, graph.",
    "response": "As already pointed out in the monumental paper in the relevant ﬁeld, Kornhauser, Miller, and Spirakis [34], one of the main reasons why the computational complexity of the reconﬁguration algorithms for the pebble motion problem on an N-vertex board graph jumps to Θ(N 3) is due to the situation where an induced cycle of order Θ(N) is included in the board graph, in a way that it inevitably has to be used during the reconﬁguration of that part."
  },
  {
    "prompt": "This is a technical discussion about structures, referred, separable in theoretical computer science.",
    "response": "Therefore, the question of how the computational complexity is evaluated in other structures (referred to as “separable graphs” in the aforementioned paper [34]) is essentially a fundamental issue related to the core of the pebble motion problem."
  },
  {
    "prompt": "This is a technical discussion about board, number, graph in theoretical computer science.",
    "response": "Actually, a fact revealed in this paper is that, except for highly speciﬁc cases where the size of the blank space, that is, the diﬀerence N −n between the number of vertices N of the board graph and the total number of pebbles n is at most O(1) with respect to N (in real-world applications, it is natural to consider that, this size, which represents the range of motion for the agents, increases with the number of vertices N of the board graph), 2 the time complexity of the reconﬁguration algorithm on the general N-vertex board graphs can indeed be bounded by o(N 3) by applying the reconﬁguration mechanism when the board graph is a tree locally or globally."
  },
  {
    "prompt": "This is a technical discussion about today, solvers, reconﬁguration in theoretical computer science.",
    "response": "The reconﬁguration algorithms for the pebble motion problem are used as fast solvers for the multi-agent path ﬁnding problems (c.f. [68]), which are actively researched today. Therefore, reducing their computational complexity would make a signiﬁcant contribution to directly enhancing the functionality of such solvers."
  },
  {
    "prompt": "Explain a concept involving problem, algorithms, pebble in computational complexity.",
    "response": "Furthermore, many of the algorithms developed to solve the multi-agent path ﬁnding problems rely in some form on algorithms for the pebble motion problem that are restricted to a board graph constrained to a tree [4, 6, 33, 35]. Hence, the impact of improvements in the computational complexity evaluation of algorithms for solving the pebble motion problem on trees is signiﬁcant. Therefore, this paper addresses the following unresolved issue regarding this important problem. Open Problem."
  },
  {
    "prompt": "This is a technical discussion about N-vertex, upper, bound in theoretical computer science.",
    "response": "Give an appropriate upper bound on the length of the shortest solution sequence to the pebble motion problem on N-vertex trees, with respect to N. Kornhauser, Miller, and Spirakis [34] mentioned, without any proof, that the upper bound for the above problem is O(N 2). Since then, including the authors of the original paper [34], no one has clariﬁed the truth of this statement."
  },
  {
    "prompt": "Scientific context on N-vertex, case, worst and their role in graph theory.",
    "response": "Here we note that, in the worst case, it may take Ω(Nn) steps to move n pebbles from one end to another of an N-vertex board tree. Additionally, in the worst case, the parameter n may be in Ω(N). Therefore, their estimate O(N 2) in [34] can be replaced with Θ(N 2). For over 40 years, this problem has remained open. In fact, attempts to genuinely lower the upper bound of O(N 3) proven in [34] have been made up to the present day [6, 9]."
  },
  {
    "prompt": "Understanding the computational aspects of paper, threshold, knowledge.",
    "response": "However, to the author’s knowledge, the current best result, as shown in the 2024 paper [6], still does not fall below the O(N 3) threshold. In this paper, we show that the problem mentioned above has a solution sequence of moves of length ˜O(N 2). A more precise description of the results obtained in this paper follows."
  },
  {
    "prompt": "Read and understand this section about degG, called, isthmus from a research paper.",
    "response": "A path I = v1 · · · vd of a connected graph G is called an isthmus if (1) every edge of I is a bridge of G, (2) every vertex of I is a cutvertex of G, and (3) degG(vi) = 2 for 1 < i < d. An isthmus with d vertices is called a d-isthmus. First, let us clarify one point. In the pebble motion problem on a general tree G, any two conﬁgurations of pebbles on G are not guaranteed to be rearrangeable through a sequence of moves."
  },
  {
    "prompt": "Read and understand this section about feasible, puzzles, conﬁgurations from a research paper.",
    "response": "Therefore, we will only consider feasible puzzles, that is, the puzzles in which, given any two conﬁgurations f and g, a sequence of moves exists to rearrange from f to g. According to Theorem A discussed later, a given pair (G, P) is a feasible pebble motion puzzle if and only if the pair satisﬁes the inequality n + k < N, where n is the number |P| of pebbles on G, k is the maximum size of isthmuses in G, and N is the order of G."
  },
  {
    "prompt": "This is a technical discussion about problem, TREES, MOTION in theoretical computer science.",
    "response": "Thus, we will restrict the instances of the following combinatorial problem, the PEBBLE MOTION PROBLEM ON TREES, to puzzles that satisfy such conditions. One more thing to clarify is that no pebble can move across an isthmus with a size greater than or equal to the number of empty spaces N −n. Therefore, in cases with such isthmuses, it is suﬃcient to handle the reconﬁguration only on the individual ’feasible’ connected components that are divided by these isthmuses."
  },
  {
    "prompt": "Explain a concept involving Problem, MOTION, conﬁguration in computational complexity.",
    "response": "Problem (PEBBLE MOTION PROBLEM ON TREES). Instance: A tree G with N vertices, the size k of a longest isthmus of G, the set of pebbles P := {1, . . . , n} such that n + k < N holds, the initial conﬁguration f and the goal conﬁguration g on the pair (G, P). Task: Output an explicit sequence of moves to rearrange from the initial conﬁguration f to the goal conﬁguration g. 3 The following is our main theorem. Theorem 1."
  },
  {
    "prompt": "Explain a concept involving PEBBLE, MOTION, PROBLEM in computational complexity.",
    "response": "The PEBBLE MOTION PROBLEM ON TREES can be solved with a time com- plexity of O(Nn + n2 log(min{n, k})). Let Lmin(G, P, f, g) denote the minimum number of moves to rearrange from a conﬁguration f to another conﬁguration g on the pair (G, P). The time complexity of the PEBBLE MOTION PROBLEM ON TREES is clearly in Ω(Lmin(G, P, f, g)). Hence we have: Corollary 2. The length of the shortest solution sequence of the PEBBLE MOTION PROBLEM ON TREES is in O(Nn + n2 log(min{n, k}))."
  },
  {
    "prompt": "Understanding the computational aspects of surprising, result, length.",
    "response": "The above result is somewhat surprising. In fact, when the size k of a longest isthmus is in O(2N/n), the length of the shortest solution sequence belongs to O(Nn), and the estimate O(N 2) in [34] is established in a stronger form. We believe that the conjecture below will also hold. Conjecture 1. The length of the shortest solution sequence of the PEBBLE MOTION PROBLEM ON TREES is in Θ(Nn + n2 log(min{n, k}))."
  },
  {
    "prompt": "Understanding the computational aspects of Section, problem, PEBBLE.",
    "response": "As a matter of fact, considering the combinatorial structure of the base cases described in Section 3, it would be extremely diﬃcult to further improve on the order of length of a minimum solution sequence we have given in this paper. We will refer to the problem of changing the input graph in the PEBBLE MOTION PROBLEM ON TREES to a general connected N-vertex graph as the PEBBLE MOTION PROBLEM ON GENERAL GRAPHS."
  },
  {
    "prompt": "Read and understand this section about pair, Note, problem from a research paper.",
    "response": "Note that the input pair (G, P) of the problem is again restricted to be a feasible pair, that is, we assume that G is not a cycle and N −n ≥2. Then, the following statement is an immediate consequence of Theorem 1. Corollary 3. The PEBBLE MOTION PROBLEM ON GENERAL GRAPHS can be solved with time complexity of O(N 2 + n2 log n) if 2n < N holds."
  },
  {
    "prompt": "Scientific context on Theorem, board, N-vertex and their role in graph theory.",
    "response": "But astonishingly, by applying the results from this paper on the board trees, even the known time complexity of the reconﬁguration algorithm on general N-vertex board graphs, which is O(N 3), can be truly improved – except for the special case where N −n is in O(1) with respect to N, as follows: Theorem 4. The PEBBLE MOTION PROBLEM ON GENERAL GRAPHS can be solved with time complexity of O(N 2 + n3 N−n + n2 log(min{n, N −n}))."
  },
  {
    "prompt": "Understanding the computational aspects of Lastly, paper, obtained.",
    "response": "Lastly, we would like to note that by making a few modiﬁcations to certain parts of the algorithm proposed in this paper, appropriate solutions to the multi-agent path ﬁnding problems on trees can also be obtained. 2 Prior Research In this section, we will discuss a previous research result that is closely related to our problem."
  },
  {
    "prompt": "Understanding the computational aspects of vertices, number, puz.",
    "response": "For a graph X with N vertices, let n, q be a pair of positive integers with n+q = N, where p is the number of distinct pebbles and q is the number of unoccupied vertices. Let P = {1, 2, . . . , n} be the set of all the pebbles. 4 Let us deﬁne the puzzle graph puz∗(X, n) of X with n pebbles such that V (puz∗(X, n)) := F(X, n), E(puz∗(X, n)) := {(f, g) : f, g ∈F(X, n), f is rearrangeable into g by some moves}."
  },
  {
    "prompt": "Read and understand this section about vertex, moves, transitive from a research paper.",
    "response": "We say that (X, n) is transitive if for any conﬁguration f ∈F(X, n) and for any vertex u of X, a pebble f(u) can be shifted to any other vertex by a sequence of ﬁnite moves. For a graph Z, let c(Z) be the number of connected components of Z. We say that (X, n) is feasible if c(puz∗(X, n)) = 1. Theorem A(Kornhauser, Miller, Spirakis [34]). Let 2 ≤q ≤N −1 and n+q = N. Let X be a connected graph with N vertices. Suppose that X is not a cycle. Then the following conditions are equivalent."
  },
  {
    "prompt": "Understanding the computational aspects of q-isthmus, cases, transitive.",
    "response": "(1) X has no q-isthmus. (2) (X, n) is transitive. (3) (X, n) is feasible. 3 On the cases of suﬃcient blank space This section describes the base cases, where the size of the board N is signiﬁcantly larger than the number of pebbles n, which are the cornerstone of the proof."
  },
  {
    "prompt": "This is a technical discussion about log, moves, base in theoretical computer science.",
    "response": "In the base case where n ≪N, we will demonstrate that for given conﬁgurations f, g ∈F(G, P), f can be transformed into g with O(Nn + n2 log n) moves, and that such a sequence of moves can also be computed with the same time complexity O(Nn + n2 log n). Firstly, we show the outline of the proof. • Under the condition n ≪N, there exists a subtree H of G such that the puzzle (H, P) is feasible and (H, P) can be divided into a family of sub-puzzles on H."
  },
  {
    "prompt": "Read and understand this section about Choose, moves, eﬀort from a research paper.",
    "response": "• Choose a pair of conﬁgurations fH, gH ∈F(H, P) such that f and g can be transformed to fH and gH respectively with O(Nn) moves and eﬀort. • In order to transfer fH to gH with O(Nn + n2 log n) moves and eﬀort, we apply binary comparisons, where the puzzle (H, P) is divided into sub-puzzles, in which each sub-puzzle has a half number of original pebbles, based on a divide-conquer strategy."
  },
  {
    "prompt": "Understanding the computational aspects of pebbles, algorithm, conﬁguration.",
    "response": "• The algorithm works similarly to the merge sorting algorithm: it splits the puzzle into two halves until each sub-puzzle has one pebble, and merges the pebbles to build a target conﬁguration. We prepare an algorithm to move unlabeled pebbles from a set of vertices to another. Lemma 5. Let S1, S2 ⊂V (G) with |S1| = |S2| = n. Then there exists a sequence of moves of length O(Nn) which transfers all the pebbles from S1 to S2, and it can be calculated in O(Nn) time."
  },
  {
    "prompt": "Read and understand this section about support, conﬁguration, deﬁned from a research paper.",
    "response": "For a conﬁguration π ∈F(G, P), the support of π is deﬁned as π−1(P), which is the subset of vertices of G occupied by P. We denote the support of π by sup(π). For two conﬁgurations π, π∗∈F(G, P), we say that π and π∗are strongly related, if there exists an algorithm which 5 transfers π to π∗in O(Nn) time. For any conﬁguration π and for any S ⊂V (G) with |S| = n, by applying Lemma 5, we have a conﬁguration π∗such that sup(π∗) is S and π∗is strongly related to π."
  },
  {
    "prompt": "Scientific context on Lemma, rivet, size and their role in graph theory.",
    "response": "Let us consider the puzzle (G, P) where G has a rivet, a vertex dividing a tree into two parts with a suitable size. Lemma 6. Let G be a tree having a vertex o with degGo ≥3 and its subtrees G1 and G2 such that V (G1) ∩V (G2) = {o}, and |V (Gi)| ≥n + O(1) for i = 1, 2. Then the PEBBLE MOTION PROBLEM ON TREES is solved in O(Nn + n2 log n) time. Let us consider the puzzle (G, P) where G has a long isthmus with respect to the number of pebbles. Lemma 7."
  },
  {
    "prompt": "Read and understand this section about Lemma, PEBBLE, MOTION from a research paper.",
    "response": "Let G be a tree with an isthmus I of size at least n. Let G1 and G2 be the two subtrees induced by V (G) \\V0, where V0 is a set of inner vertices of I. If |V (Gi)| ≥n/2+ O(1) for i = 1, 2, then the PEBBLE MOTION PROBLEM ON TREES is solved in O(Nn + n2 log n) time. Combining Lemma 5, Lemma 6 and Lemma 7, we have the following theorem. Theorem 8. If the puzzle (G, P) satisﬁes N ≥3n, then the PEBBLE MOTION PROBLEM ON TREES is solved in O(Nn + n2 log n) time."
  },
  {
    "prompt": "Read and understand this section about Splitting, section, theorem from a research paper.",
    "response": "4 Splitting a tree into small subtrees To prove the main theorem, we need to decompose an instance puzzle into smaller sub-puzzles that satisfy the conditions of the base cases in the previous section. In this section, we will provide tools for this purpose."
  },
  {
    "prompt": "Read and understand this section about centroid, called, order from a research paper.",
    "response": "It is well-known that any tree T has a vertex v (which is called a centroid vertex of T) such that the order of each connected component of T −v does not exceed (|V (T)| −1)/2, or else it has an edge e (which is called a centroid edge of T) such that the order of each of the two connected components of T −e is exactly |V (T)|/2. Lemma 9 ([31]). Finding a centroid vertex or a centroid edge on a tree is possible in linear time. Using the above, we can derive the following lemma. Lemma 10."
  },
  {
    "prompt": "Understanding the computational aspects of vertices, arbitrary, hold.",
    "response": "Let T be an arbitrary tree with N vertices. Then one can ﬁnd, in O(N), a pair of subtrees (T1, T2) of T such that all of the following three conditions hold. 1. |V (T1) ∩V (T2)| ≤1; 2. V (T1) ∪V (T2) = V (T); 3. (N + 2)/3 ≤|V (T1)| ≤|V (T2)| ≤(N + 1)/2. Lemma 11. Let T be a tree such tha the size of a maximum isthmus of T is at most k. For any subtree T ′ of T such that the size of a maximum isthmus of T ′ is also at most k, the following statements hold."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, satisfying, ﬁnd.",
    "response": "• For any natural number d satisfying |V (T ′)| ≤d ≤|V (T)|, we can ﬁnd, in O(d), a d-vertex subtree T ′′ of T such that the size of a maximum isthmus of T ′′ is at most k and that T ′′ contains T ′ as a subgraph. 6 Lemma 12. There exists an algorithm for vertex covering a tree T of order n with 4n/k or fewer subtrees of order k(≤n) that share at most one vertex with each other. In the algorithm of Lemma 12, the subtrees covering T are stored and maintained in the following binary tree BT."
  },
  {
    "prompt": "Scientific context on denote, node, CentroidVertex and their role in graph theory.",
    "response": "For each node x of BT, let p(x) denote its parent node and ci(x)(i = 1, 2) denote its child nodes. Each node x of BT should store the three objects • X(x), • Centroid(X(x)) := (CentroidVertex(X(x)), CentroidEdge(X(x))), • Joint(X(x)) := (JointVertex(X(x)), JointEdge(X(x))) deﬁned below as its attributes: First, X(x) is a subtree of T determined by the inductive procedure we will describe. If X(x) has a centroid vertex, it is stored in CentroidVertex(X(x)), otherwise CentroidVertex(X(x)) := ∅."
  },
  {
    "prompt": "Read and understand this section about CentroidEdge, centroid, edge from a research paper.",
    "response": "In the same way, if X(x) has a centroid edge, it is stored in CentroidEdge(X(x)), otherwise CentroidEdge(X(x)) := ∅. If x is a leaf node of BT, let us deﬁne Centroid(X(x)) := (∅, ∅) to avoid wasting computation time. 1⃝For the root node r of BT, let X(r) := T, Joint(X(r)) := Centroid(X(r))."
  },
  {
    "prompt": "Understanding the computational aspects of stored, Joint, Lemma.",
    "response": "2⃝When X(x) is stored in a non-leaf node x of BT, the objects X(ci(x)), Joint(X(ci(x))))(i = 1, 2) to be stored in its child nodes ci(x)(i = 1, 2) respectively are deﬁned inductively by the following procedure: ( 2⃝-a) Using Lemma 10, ﬁnd two subtrees X1 and X2 of X(x) satisfying V (X1) ∩V (X2) = {CentroidVertex(X(x))} and V (X1) ∪V (X2) = V (X(x)). If X(x) has a centroid edge, X1, X2 are the two subtrees obtained by removing the edge from X(x)."
  },
  {
    "prompt": "This is a technical discussion about Joint, stored, child in theoretical computer science.",
    "response": "( 2⃝-b) Of the two subtrees X1 and X2 of X(x), the one that contains JointVertex(X(x)) or one of the endpoints of JointEdge(X(x)) is stored at the second child node c2(x) of x, The other subtree is stored at the ﬁrst child node c1(x) of x. ( 2⃝-c) Joint(X(c2(x))) := Joint(X(x)) ( 2⃝-d) Joint(X(c1(x))) := Centroid(X(x)) It is necessary to use symbols such as pBT(x), cBT i (x), XBT(x), etc. to specify that they are functions on the node x of the binary tree BT."
  },
  {
    "prompt": "Scientific context on XBT, cations, paper and their role in graph theory.",
    "response": "However, in this paper, to avoid notational compli- cations, even when multiple binary trees BT 1, BT 2, etc. are treated simultaneously, superscripts such as pBT 1, XBT 2 are omitted when there is no misunderstanding. Then, the following lemmata hold. Lemma 13."
  },
  {
    "prompt": "Explain a concept involving node, JointVertex, graph in computational complexity.",
    "response": "For any child node ci(r)(i = 1, 2) of the root node r of this binary tree BT, the following statement holds: • Regarding the leaf node ℓ(i) that can be reached by successively selecting only the ﬁrst child of each descendant of the node ci(r), even if we delete the part X(ℓ(i)) −JointVertex(X(ℓ(i))) from the graph X(r), the resulting graph X(r)−(X(ℓ(i))−JointVertex(X(ℓ(i)))) is connected. Lemma 14."
  },
  {
    "prompt": "Scientific context on algorithm, Lemma, tree and their role in graph theory.",
    "response": "The eﬀort required to create the binary tree BT from a given tree T with n-vertices using the algorithm from Lemma 12 is O(n log n). 7 5 Main Algorithm In this section, we present the main algorithm for solving the PEBBLE MOTION PROBLEM ON TREES in O(Nn + n2 log(min{n, k})) without any additional constraints on the instances. First, let us consider the case where the size k of a longest isthmus is bounded above by a positive constant M (for example, M = 8), i.e., k ≤M ∈O(1)."
  },
  {
    "prompt": "Read and understand this section about pebbles, Lemma, board from a research paper.",
    "response": "In this case, by applying Lemma 11 and consecutively solving at most O(N) feasible sub-puzzles – each on a board tree with at most 2M + 2 vertices – along the path connecting any two pebbles, it is possible to swap their positions without altering the positions of the other pebbles. Since the size of each board tree is bounded by the constant 2M + 2 in these sub-puzzles, the eﬀort to rearrange any conﬁguration on such a board tree is also in constant order O(1)."
  },
  {
    "prompt": "Read and understand this section about steps, general, performing from a research paper.",
    "response": "In general, after performing a suitable preprocessing of O(Nn) steps, the entire conﬁguration can be completed by repeating the operation of ’swapping the positions of two pebbles’ at most n −1 times. From these facts, it immediately follows that, in this case, the PEBBLE MOTION PROBLEM ON TREES can be solved with a time complexity of O(Nn). Therefore, in this paper, we will assume k ≫1 for all subsequent discussions."
  },
  {
    "prompt": "Understanding the computational aspects of outline, remaining, part.",
    "response": "Then, the outline of the remaining part of the main algorithm is as follows: 1. For the input (G, P), use Lemma 11 to ﬁnd a subtree T of G such that it has order n + k + 1 and the size of its maximum isthmus is k or less. Then, move all pebbles in the initial conﬁguration f on G onto T with O(Nn) eﬀort, and call this conﬁguration, which is strongly related to f, f ′. 2."
  },
  {
    "prompt": "Scientific context on Step, Lemma, conﬁguration and their role in graph theory.",
    "response": "Similarly, for the goal conﬁguration g on G, convert it in advance to a conﬁguration g′ on T that is strongly related to g and matches the operations from Step 4. 3. By using the algorithm from Lemma 12, decompose the tree T into subtrees of suﬃciently small order with respect to k. 4. Among the subtrees obtained from the procedure in Step 3, there is a tree T ′ such that removing all of T ′ except at most one vertex from T keeps the remaining graph connected."
  },
  {
    "prompt": "This is a technical discussion about conﬁguration, pebbles, place in theoretical computer science.",
    "response": "Therefore, starting from the conﬁguration f ′, repeat the following sequence of operations: ﬁnd one such tree T ′, place pebbles on it without empty space according to the conﬁguration g′, and then cut oﬀthat portion from the main body. In Step 4 of the outline above, in order to place pebbles according to the conﬁguration g′ on T ′, it is ﬁrst necessary to gather the set of pebbles g′(V (T ′)) \\ {0} that are scattered throughout the entire graph T near the vicinity of T ′."
  },
  {
    "prompt": "Read and understand this section about method, subtree, achieve from a research paper.",
    "response": "To achieve this, the following method will be used. • Using the method of Lemma 12 to decompose T, there will always be a subtree T ′′, aside from T ′ such that removing all but at most one vertex from T keeps the remaining graph connected. On such a subtree T ′′, tightly pack any pebbles that do not belong to g′(V (T ′)) and then ”temporarily” cut them oﬀfrom the whole."
  },
  {
    "prompt": "Explain a concept involving temporarily, vertex, removed in computational complexity.",
    "response": "Once the pebbles are arranged on T ′ according to the conﬁguration g′, all parts that were ”temporarily” removed in this operation will be restored. In the following sections of this paper, we will assume that each vertex v of the input tree G (and consequently all of its subgraphs) is associated with the following two attributes. Peb(v): A function that returns the pebble x(∈{1, . . . , n}) or a blank 0 placed on vertex v. 8 col(v): A function that returns the color of vertex v."
  },
  {
    "prompt": "Explain a concept involving vertex, gray, white in computational complexity.",
    "response": "The possible colors are white, gray, or black. Regarding the meaning of the color, white indicates a vertex that has been completely removed from the graph, gray signiﬁes a vertex that is temporarily removed during the execution of the algorithm but will be restored later, and black represents a vertex that remains in the graph. In other words, the graphs are treated not just as simple graphs, but also as objects that also contain information about the conﬁgurations of pebbles on them."
  },
  {
    "prompt": "This is a technical discussion about graphs, change, Step in theoretical computer science.",
    "response": "In this algorithm, the shapes such as the graphs G and T, as well as the graphs X(x) stored in each node x of BT created by the operations in Step 3 of the outline above, do not change; however, the values of the functions Peb(v) and col(v) at each vertex v of the graphs continuously change."
  },
  {
    "prompt": "Scientific context on size, pebble, Theorem and their role in graph theory.",
    "response": "According to Theorem A, if the size of the pebble set P is n and the size of the maximum isthmus of the graph T is at most k, then if the order of T is at most n + k + 1, it is guaranteed that the pebble motion problem on (T, P) is feasible. Therefore, the algorithm presented in this paper ﬁrst uses Lemma 11 to ﬁnd a subtree T of the tree G from the input (G, P) such that its order is n + k + 1 and its size of a maximum isthmus is at most k."
  },
  {
    "prompt": "Explain a concept involving conﬁguration, Alg, PackedConﬁg in computational complexity.",
    "response": "Furthermore, we rearrange the goal conﬁguration g on G into a ”suitable” conﬁguration g′ on T that is strongly related to g with a time complexity and number of moves of at most O(Nn). The algorithm used for this purpose is Alg . PackedConﬁg(BT, g) shown below. Algorithm 1 (Alg . PackedConﬁg(G, T, BT, g))."
  },
  {
    "prompt": "Explain a concept involving tree, Lemma, set in computational complexity.",
    "response": "Input: A tree G and its subtree T, the binary tree BT that stores the set of subtrees covering the tree T obtained from the algorithm stated in Lemma 12, and a conﬁguration g on the pair (G, P). Output: A conﬁguration g′ on (T, P) and a sequence of moves φ[r] · ψ to rearrange g into g′. 1⃝Set Peb := g. Set col(u) := black for each u ∈V (T). Set col(v) := white for each v ∈ V (G) \\ V (T)."
  },
  {
    "prompt": "This is a technical discussion about Rearrange, Packing, Alg in theoretical computer science.",
    "response": "2⃝Rearrange the conﬁguration g on (G, P) to some conﬁguration on (T, P) which is strongly related to g, and remember the sequence of moves performed here as ψ. 3⃝Call Alg . Packing(T, BT, r) for the root node r of BT. 4⃝Return the sequence of moves φ[r] · ψ and the conﬁguration g′ := φ[r] · ψ(g) on (T, P). Algorithm 2 (Alg . Packing(T, BT, x))."
  },
  {
    "prompt": "This is a technical discussion about tree, Lemma, node in theoretical computer science.",
    "response": "Input: A tree T, the binary tree BT that stores the set of subtrees covering the tree T obtained from the algorithm stated in Lemma 12, and a node x of BT. Output: A sequence of moves φ[x]. 1⃝If the node x is a leaf node of BT, else if there is no pebble on X(x), else if there is no unoccupied vertex in X(x) −JointVertex(X(x)), then return a zero-length sequence of moves (i.e., an identity transformation) as φ[x]."
  },
  {
    "prompt": "This is a technical discussion about Alg, Fill, Packing in theoretical computer science.",
    "response": "2⃝Fill the pebbles in the current conﬁguration into the portion of the tree X(c1(x)) except for JointVertex(X(c1(x))) as much as possible, and record the sequence of moves performed here as φ[x]. 9 3⃝Call Alg . Packing(T, BT, c1(x)) and Alg . Packing(T, BT, c2(x)). 4⃝Update φ[x] := φ[c2(x)] · φ[c1(x)] · φ[x] and return φ[x]. In the main algorithm, the initial conﬁguration f is ﬁrst rearranged to the conﬁguration g′ = φ[r]·ψ(g), which is the output of Alg . PackedConﬁg(BT, g)."
  },
  {
    "prompt": "Scientific context on applying, inverse, sequence and their role in graph theory.",
    "response": "Then, by applying the inverse sequence of moves ψ−1 · φ[r]−1 from g′ to g, we obtain g as ψ−1 · φ[r]−1(g′). After reducing the instance on (G, P) to the one on (T, P), we will use the following algorithm Alg ."
  },
  {
    "prompt": "Read and understand this section about subtree, target, CutOﬀ from a research paper.",
    "response": "CutOﬀ(G, T, BT, f, g′) to perform the following steps repeatedly: 1) ﬁnd a subtree at the tip of tree T that, when removed, does not cause the remaining parts to become disconnected (we will refer to this subtree as the target subtree), 2) ﬁll the target subtree with pebbles according to the conﬁguration g′, and 3) cut that portion oﬀfrom the whole. Algorithm 3 (Alg . CutOﬀ(G, T, BT, f, g′))."
  },
  {
    "prompt": "Read and understand this section about tree, Lemma, Alg from a research paper.",
    "response": "Input: An instance tree G of the PEBBLE MOTION PROBLEM ON TREES, and its subtree T of order n + k + 1 with the maximum isthmus size less than or equal to k, the binary tree BT that stores the set of subtrees covering the tree T obtained from the algorithm stated in Lemma 12, a conﬁguration f on (G, P), and an output conﬁguration g′ on (T, P) of Alg . PackedConﬁg(G, T, BT, g). Output: A sequence of moves σ to rearrange f into g′. 1⃝Set Peb := f. Set col(u) := black for each u ∈V (T)."
  },
  {
    "prompt": "Understanding the computational aspects of sequence, moves, conﬁguration.",
    "response": "Set col(v) := white for each v ∈ V (G) \\ V (T). 2⃝Rearrange the conﬁguration f on G to some conﬁguration on T which is strongly related to f, and remember the sequence of moves performed here as σ. 3⃝If |V (X(r))| ≤3(k+1)/2 holds for the root node r of BT, then by using Theorem 8, calculate the sequence of moves σ0 to rearrange the last remaining parts according to the conﬁguration g′, update σ := σ0 · σ, and return the sequence of moves σ that have been performed so far."
  },
  {
    "prompt": "Understanding the computational aspects of Note, Set, node.",
    "response": "(## Note that σ(f) = g′.) 4⃝Find the leaf node t that is reached by traversing the descendants of the ﬁrst child c1(r) of the root node r of BT by selecting only its ﬁrst child node from generation to generation. 5⃝Call Alg . Extraction(BT, g′, t). 6⃝Update σ := σt · σ. 7⃝Set col(v) = white for each v ∈V (X(t) −JointVertex(X(t))). 8⃝Set x := p(t), y := c2(x)."
  },
  {
    "prompt": "This is a technical discussion about Note, JointVertex, update in theoretical computer science.",
    "response": "(## Note that t = c1(x).) 9⃝If X(r) −(X(t) −JointVertex(X(t))) will have an isthmus of size k + 1 or more, to pre- vent this situation, choose an appropriate edge uv in the edge set {uv | u ∈V (X(t) − JointVertex(X(t))), v ∈X(y)}, change the color col(v) of v that was colored white in Step 7⃝back to black, ﬁnd a leaf node w of BT such that X(w) contains v, and update X(w) := X(w) + uv. 10 ⃝Delete the node t from BT and update x := y."
  },
  {
    "prompt": "Read and understand this section about Centroid, Joint, Alg from a research paper.",
    "response": "10 11 ⃝If x is the root node of BT, then update Joint(X(x)) := Centroid(X(x)). 12 ⃝Go to Step 3⃝. In the algorithm Alg . CutOﬀ(G, T, BT, f, g′), it is necessary to gather the set of pebbles g′(V (X(t)))\\ {0}, which are scattered throughout the entire T, onto the target subtree X(t). For this purpose, the following Alg . Extraction(BT, h, t) is provided. Algorithm 4 (Alg . Extraction(T, BT, g′, t))."
  },
  {
    "prompt": "Read and understand this section about output, tree, Alg from a research paper.",
    "response": "Input: A tree T of order n + k + 1 with a maximum isthmus size less than or equal to k, a binary tree BT that stores subtrees of T, an output conﬁguration g′ on (T, P) of Alg . PackedConﬁg(G, T, BT, g), and a leaf node t of BT Output: A sequence of moves σt. 1⃝Copy the binary tree BT as a binary tree named BT 2."
  },
  {
    "prompt": "This is a technical discussion about tree, Theorem, vertices in theoretical computer science.",
    "response": "2⃝If the number of vertices in the tree X(r) stored in the root node r of the binary tree BT is at most 3(k + 1)/2, use Theorem 8 to place the pebbles g′(V (X(t))) \\ {0} on the tree X(t) in accordance with the conﬁguration g′. Then, change the color of all the gray vertices in T back to black, and return the sequence of moves σt that have been applied so far."
  },
  {
    "prompt": "Read and understand this section about node, Find, child from a research paper.",
    "response": "3⃝Find the leaf node ℓby traversing the descendants of the second child c2(r) of the root node r of BT 2, selecting only the ﬁrst child node at each level of descent. 4⃝By using Lemma 11, ﬁnd a subtree H(ℓ) of T that includes the tree X(ℓ) as its subgraph, has an order of |V (X(t))| + |V (X(ℓ))| + k + 1, and its maximum isthmus size of k or less, consisting solely of black vertices. Then, move the k + 1 empty spaces over V (H(ℓ)) while recording the sequence of moves required for this task."
  },
  {
    "prompt": "Explain a concept involving Theorem, Step, JointVertex in computational complexity.",
    "response": "5⃝By applying Theorem 8 to the region H(ℓ) identiﬁed in Step 4⃝, ﬁll X(ℓ)−JointVertex(X(ℓ)) with pebbles (anything that is not included in g′(V (X(t)))) without any unoccupied space, and record the sequence of moves required for this task. 6⃝Set x := p(ℓ) and set y to be the child node of x that is not ℓ. 7⃝Change the colors of the vertices V (X(ℓ) −JointVertex(X(ℓ))) to all gray."
  },
  {
    "prompt": "Scientific context on JointVertex, node, Step and their role in graph theory.",
    "response": "8⃝If X(r) −(X(ℓ) −JointVertex(X(ℓ))) has an isthmus of size k + 1 or more, to prevent this situation, select an edge uv from the set {uv | u ∈V (X(ℓ) −JointVertex(X(ℓ))), v ∈X(u)}, change the color col(u) of u from gray back to black, ﬁnd a leaf node w of BT such that X(w) contains v, and update X(w) := X(w) + uv. 9⃝Delete the node ℓfrom BT 2, and update x := y. 10 ⃝If x is the root node of BT 2, then set Joint(X(x)) := Centroid(X(x)). 11 ⃝Go to Step 2⃝. Lemma 15. Step 9⃝and Step 10 ⃝of Alg ."
  },
  {
    "prompt": "Scientific context on Step, Alg, Extraction and their role in graph theory.",
    "response": "CutOﬀ(G, T, BT, f, g′), as well as Step 8⃝and Step 9⃝ of Alg . Extraction(T, BT, g′, t), can each be achieved in O(k). 11 Lemma 16. The total cost of Step 4⃝of Alg . Extraction(T, BT, g′, t) is in O(k(k + n)). The following main algorithm is constructed by consolidating the functions of the series above of algorithms. Algorithm 5 (Main Algorithm: Alg . Main((G, P), f, g)). Input: A tree G with N vertices, the size k of a longest isthmus of G, the set of pebbles P := {1, . . ."
  },
  {
    "prompt": "This is a technical discussion about conﬁguration, initial, goal in theoretical computer science.",
    "response": ", n} such that n + k < N holds, the initial conﬁguration f and the goal conﬁguration g on the pair (G, P). Output: An explicit sequence of moves to rearrange from the initial conﬁguration f to the goal conﬁguration g. 1⃝By using Lemma 11, ﬁnd a subtree T of G with n + k + 1 vertices such that the maximum size of isthmuses in T is at most k. 2⃝By using the algorithm from Lemma 12, decompose the tree T into subtrees of order k/8 or less, and store their data in the binary tree BT. 3⃝By using Alg ."
  },
  {
    "prompt": "Understanding the computational aspects of Proof, PackedConﬁg, Alg.",
    "response": "PackedConﬁg(G, T, BT, g), obtain the conﬁguration g′ := φ[r] · ψ(g) on T. 4⃝Call Alg . CutOﬀ(G, T, BT, f, g′). 5⃝Return the sequence of moves ψ−1 · φ[r]−1 · σ. (## Note that g = ψ−1 · φ[r]−1 · σ(f).) 6 Proof of Theorem 1 Proof of Theorem 1. Since the correctness proof of Alg . Main((G, P), f, g) is clear, here we will show that the time complexity of the algorithm is in O(Nn + n2 log(min{n, k})). From a given input tree G, the eﬀort of ﬁnding its subtree T using Lemma 11 is O(n + k)."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, log, Alg.",
    "response": "The task of creating a binary tree database BT from the obtained T using Lemma 14 has a cost of O((n+k) log(n+k)). Using Lemma 5 to convert the arrangements f and g on (G, P) into strongly related arrangements on (T < P) requires a time complexity and number of moves of at most O(Nn). In Alg . Main((G, P), f, g), the other parts are spent on repeatedly solving sub-puzzles of size O(k) (Steps 4⃝and 5⃝of Alg . Extraction(T, BT, h, t)) on (T, P)."
  },
  {
    "prompt": "Understanding the computational aspects of Extraction, Alg, Step.",
    "response": "According to Lemma 16, the repeated process of designating a suitable area of O(k) and gathering (k + 1) unoccupied spaces in that location (Step 4⃝of Alg . Extraction(T, BT, h, t)) for preparing those sub-puzzles has a time complexity and number of moves of at most O(k(k+n)) for the entire Alg . Extraction(T, BT, h, t). Therefore, in Alg . Main((G, P), f, g), the most eﬀort and number of moves required is from the rep- etition of sub-puzzles of size O(k) (Step 5⃝of Alg ."
  },
  {
    "prompt": "This is a technical discussion about Extraction, Alg, called in theoretical computer science.",
    "response": "Extraction(T, BT, g′, t)). This task is repeated O((n + k)/k) times each time Alg . Extraction(T, BT, g′, t) is called. If each of these sub-puzzles of size O(k) meets the prerequisites of Theorem 8 (the proof for this part will be postponed until the end), then the total eﬀort and number of moves for one call to Alg . Extraction(T, BT, h, t) is at most O(((n + k)/k) · (k2 log k)) = O((n + k)k log k). Alg . Extraction(T, BT, g′, t) is called O((n + k)/k) times within Alg ."
  },
  {
    "prompt": "This is a technical discussion about log, moves, total in theoretical computer science.",
    "response": "CutOﬀ(G, T, BT, f, g′), so the total cost and estimate of moves here is O(((n + k)/k)(k(k + n) + (n + k)k log k)) = O((n + k)2 log k). 12 Since the operation of scaling down the input puzzles f, g on (G, P) to their strongly related puzzles on (T, P) requires O(Nn) eﬀort and moves, assuming n > k, the estimate for the total eﬀort and number of moves in the main algorithm is O(Nn+(n+k)2 log k) = O(Nn+n2 log k). On the other hand, if n ≤k, then Step 5⃝of Alg ."
  },
  {
    "prompt": "Scientific context on Extraction, Theorem, Alg and their role in graph theory.",
    "response": "Extraction(T, BT, g′, t) is repeated only a constant number of times for the entire main algorithm, so the total eﬀort estimate is O(Nn + n2 log n), just as stated in Theorem 8. Lastly, we will conﬁrm that repeatedly performing Step 9⃝of Alg . CutOﬀ(G, T, BT, f, g′) or Step 8⃝of Alg . Extraction(T, BT, g′, t) does not increase the order of X(w) beyond twice its orig- inal value."
  },
  {
    "prompt": "Scientific context on edge, added, process and their role in graph theory.",
    "response": "In this process, the edge uv added to X(w) is a pendant edge, and since u is a leaf, by deﬁnition it is not a vertex on any isthmus. Therefore, even if this operation is repeated, no pen- dant edge will be added to u. By adding the edge uv, the size of a maximum isthmus has strictly decreased, which means that the degree of v in the current graph is 3 or more. Consequently, the degree of v in X(w) is either 3 or a unique vertex p outside X(w) is adjacent to v."
  },
  {
    "prompt": "This is a technical discussion about degree, vertex, isthmus in theoretical computer science.",
    "response": "By deﬁnition, a vertex with a degree of 3 or more is not an interior vertex of any isthmus, so as long as the degree of v in the current graph is 3, no further pendant edges will be added to v through the repetition of this operation. For the degree of v to again become 2, the only option is to delete the vertex p outside X(w) and the edge pv, but since u is not a vertex on any isthmus, the size of a maximum isthmus in the remaining graph will not increase by this deletion of pv."
  },
  {
    "prompt": "This is a technical discussion about added, pendant, edges in theoretical computer science.",
    "response": "Therefore, the number of pendant edges that can potentially be added to each vertex of X(w) is at most 1, and since no pendant edges will be added to the newly added leaves, the order of X(w) can increase by at most a factor of 2. This implies that k+1 ≥k = 2(2(k/8)+2(k/8)) ≥2(|V (X(t))|+|V (X(ℓ))|) holds, and therefore the condition of Theorem 8 (i.e. |V (X(t))|+|V (X(ℓ))|+k+1 ≥3(|V (X(t))|+|V (X(ℓ))|)) is always satisﬁed in Step 5⃝of Alg . Extraction(T, BT, g′, t)."
  },
  {
    "prompt": "Scientific context on Theorem, graphs, PEBBLE and their role in graph theory.",
    "response": "7 In the case of general board graphs In this section, we provide a proof of Theorem 4. Now that the eﬃcient algorithm for the PEBBLE MOTION PROBLEM ON TREES and its computational complexity evaluation has been clariﬁed in the previous sections, the actual proof of this theorem can be obtained in a simple manner. Lemma 17. If the board graphs are 2-connected, then the PEBBLE MOTION PROBLEM ON GENERAL GRAPHS can be solved with time complexity of O(N 2 + n3 N−n +n2 log(min{n, N −n}))."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, board, tree.",
    "response": "Proof of Lemma 17. By Corollary 3, we only need to consider the case where N ≤2n. The algorithm on the 2-connected board graphs G that we discuss here diﬀers from the algorithm on board trees only in that it uses an appropriate spanning tree T of G instead of the input board tree, and redeﬁnes the parameter k as k := N −n. It is the same in that T is decomposed into O(k) size subtrees and represented as binary tree data. The diﬀerence appears in the computational complexity evaluation."
  },
  {
    "prompt": "Scientific context on graph, board, spaces and their role in graph theory.",
    "response": "If the input board graph is 2-connected then the puzzle is feasible if and only if the board graph is not a cycle and has at least two unoccupied spaces. Therefore, no matter how the spanning subtree T of G is constructed, T may inevitably have to possess an isthmus with at least N −n + 1 vertices, and as a result, some of the subtrees in the decomposition of T may become subpaths of size O(N −n) on the isthmus. In such cases, to create a feasible sub-puzzle of size O(N −n) (c.f. Step 4 of Alg ."
  },
  {
    "prompt": "Read and understand this section about Extraction, degree, isthmus from a research paper.",
    "response": "Extraction(T, BT, g′, t)), the puzzle must be constructed around a vertex v with degree 3 or higher located somewhere on a cycle of G that includes the isthmus. To 13 achieve this, the O(N −n) pebbles that make up the sub-puzzle must be brought around v. This operation requires O((N −n)n) moves per each sub-puzzle."
  },
  {
    "prompt": "Scientific context on Theorem, algorithm, times and their role in graph theory.",
    "response": "In our algorithm, since we have to solve these sub-puzzles O(( n N−n)2) times, the cost of the above operations for the entire algorithm can be estimated as O(( n N−n)2×(N −n)n) = O( n3 N−n). The statement of our lemma can be directly derived from the above fact and Theorem 1. It should be clear that by combining Lemma 17 and Theorem 1, Theorem 4 follows immediately. Remark 1."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, Corollary, Theorem.",
    "response": "In Corollary 3, Lemma 17, and Theorem 4, it is necessary to ﬁnd the appropriate spanning tree or perform a 2-connected component decomposition on the given input board graph G, which requires O(N 2) computational complexity for each of these steps. However, when considering the shortest solution sequences for their pebble motion problems, it is suﬃcient to evaluate the number of moves for pebbles on the appropriate board graph after completing the above processes."
  },
  {
    "prompt": "Explain a concept involving order, sequences, part in computational complexity.",
    "response": "Therefore, in the case of the order of the shortest solution sequences, it is possible to replace the O(N 2) part in the order notation of these propositions with O(Nn). Appendix Proof of Corollary 3. It suﬃces to show that, for any input board graph G, we can ﬁnd a spanning tree such that its longest isthmus consists of at most N −n −1 vertices, and this can be done in O(N 2) time. Let us consider the following algorithm."
  },
  {
    "prompt": "Scientific context on vertex, Find, interior and their role in graph theory.",
    "response": "1⃝Find a spanning tree T of G that contains at least one vertex of degree 3 or higher. 2⃝If T has no isthmus with at least N −n vertices, return T. 3⃝Let I denote a unique isthmus of T with at least N −n vertices. If every interior vertex of I is a degree-2 vertex of G, then goto Step 4⃝. Find an edge of G −T that is incident to an interior vertex of I and denote it as e := uv, where v is an interior vertex of I and u is a vertex outside of I. Let C denote the unique cycle of T + e."
  },
  {
    "prompt": "Scientific context on path, degree, edge and their role in graph theory.",
    "response": "Take the longest path P in the part of C such that all the interior vertices of P have degree 2 on T + e. The path P may consist of a single edge, but in that case, it should not include v as an end-vertex. Let e′ be one of the edges in the middle of P that is not incident to v. Set T := T + e −e′. goto Step 2⃝. 4⃝Find an edge e of G −T such that the unique cycle C in T + e contains all of I. Take the longest path P in the part of C such that all the interior vertices of P have degree 2 on T +e."
  },
  {
    "prompt": "Read and understand this section about middle, incident, edges from a research paper.",
    "response": "Let e′ be one of the edges in the middle of P that is not incident to v. Set T := T + e −e′ and return T At Step 3⃝, the maximum isthmus length of T decreases strictly. The proof of this is as follows: Let the two connected components obtained by removing all edges of I from G be X and Y , where X is the component that contains vertex u, and Y is the other component. Let p be the end-vertex of I contained in X, and q be the other end-vertex of I contained in Y ."
  },
  {
    "prompt": "Explain a concept involving degree, holds, removing in computational complexity.",
    "response": "If |E(P)| ≥3 holds, removing e′ will not aﬀect the degree of any vertex with degree 3 or more, so it is clear that the maximum isthmus length of the new T strictly decreases. Therefore, we now consider the case where the number of edges in P is 2 or fewer. Let P1 denote the subpath of P connecting p and v. Let X1 denote the connected component of T −e′ containing u, and let X2 := (X −e′)−X1. We have that 14 |E(P1)| ≤|E(P)| ≤2, |V (I ∪Y )| ≥N −n + 1, and |V (X)| ≤n."
  },
  {
    "prompt": "Explain a concept involving Step, vertices, isthmus in computational complexity.",
    "response": "Then since |V (X1 + e′)| ≤n and |V (X2 + P1)| ≤n + 1 hold, the number of vertices in any isthmus contained in these subgraphs is also bounded above by N −n −1. Therefore, if T + e −e′ has an isthmus with at least N −n vertices, it must be a proper subpath of I. Step 2⃝and Step 3⃝can be accomplished in O(N) time each, while Step 1⃝and Step 4⃝can be achieved in O(N 2) time each. Since Step 2⃝and Step 3⃝are repeated at most N/2 times, the total time complexity of this algorithm is O(N 2)."
  },
  {
    "prompt": "Explain a concept involving Lemma, Proof, algorithm in computational complexity.",
    "response": "Proof of Lemma 5. We apply the following algorithm. Algorithm 6 (Alg .sbs0(G, S1, S2)). Input: A tree G with N vertices. Subsets of vertices S1, S2 ⊂V (G) with |S1| = |S2| = n. Output: A sequence of moves which transfers all the pebbles from S1 to S2. 1⃝Make a list L of leaves of G. 2⃝If L = ∅, then stop. Take a leaf v ∈L. Let u be a neighbor of v. 3⃝If v ∈S1 ∩S2, then: Set S1 = S1 \\ {v}, S2 = S2 \\ {v}. else if v ∈S1 \\ S2, then: Find a shortest path eP from v to V (G) \\ S1."
  },
  {
    "prompt": "This is a technical discussion about Set, step, endvertex in theoretical computer science.",
    "response": "Let w be another endvertex of eP other than v. Move pebbles from V ( eP) \\ {w} to V ( eP) \\ {v}. Set S1 = (S1 \\ {v}) ∪{w}. else if v ∈S2 \\ S1, then: Find a shortest path eP from v to S1. Let w be another endvertex of eP other than v. Move a pebble from w to v. Set S1 = S1 \\ {w}, S2 = S2 \\ {v}. 4⃝Set T = T −v. Remove v from L. If u is a leaf of T, then add u to L. Goto step 2⃝. In step 4⃝, |V (G)| always decreases. Hence, we have ﬁnally L = ∅. The running time is dominated by step 3⃝."
  },
  {
    "prompt": "Scientific context on case, time, steps and their role in graph theory.",
    "response": "In case v ∈S1 ∩S2 or v ̸∈S1 ∪S2, it takes a constant time. In case v ∈S1 \\ S2, it takes O(n) steps to move pebbles along eP. In this case, since N decreases by 1, by the inductive hypothesis, we get a total running time of O(n + n(N −1)) ≤O(Nn). In case v ∈S2 \\ S1, it takes O(N) steps to move pebbles along eP. In this case, since n decreases by 1, again by the inductive hypothesis, we get a total running time of O(N + (n −1)N) ≤O(Nn). Proof of Lemma 6. We prepare two algorithms for the proof."
  },
  {
    "prompt": "This is a technical discussion about algorithm, Alg, ﬁrst in theoretical computer science.",
    "response": "The ﬁrst algorithm is as follows. Algorithm 7 (Alg .sbs1A(G, Q1, Q2)). (Partition with a rivet) 15 Input: A tree G satisfying the assumption of Lemma 6. A conﬁguration π ∈F(G, P). A partition of pebbles P = Q1 ∪Q2 such that Q1 ∩Q2 = ∅and |Qi| = n/2 + O(1) for i = 1, 2. Output: A sequence of moves which transfers Qi to V (Gi) for i = 1, 2."
  },
  {
    "prompt": "Scientific context on Find, Applying, Alg and their role in graph theory.",
    "response": "1⃝Find subsets Si of V (G) for 1 ≤i ≤4, such that |Si| = n/2 + O(1) for 1 ≤i ≤4, Si ∩Sj ̸= ∅ for 1 ≤i < j ≤4, S1, S2 ⊂V (G1), S3, S4 ⊂V (G2), S2 ∩S3 = {o}, and Si ∪Si+1 induces a subtree G(i, i + 1) of G for 1 ≤i ≤3. 2⃝Applying Alg .sbs0, move n/4 + O(1) pebbles to Si for 1 ≤i ≤4. Put Pi as a set of pebbles on Si for 1 ≤i ≤4. 3⃝Consider the puzzle (G(2, 3), P2 ∪P3) and transfer half of P2 ∪P3 to S2 and the other half to S3 such that pebbles of Q1 are moved to S2 as many as possible."
  },
  {
    "prompt": "Understanding the computational aspects of Remove, pebbles, Reset.",
    "response": "Reset P2 and P3 as a set of pebbles on S2 and S3, respectively. 4⃝Remove P3 ∪P4 to S4. Load P1 ∪P2 to S2 ∪S3. Consider the puzzle (G(2, 3), P1 ∪P2) and transfer half of P1 ∪P2 to S2 and the other half to S3 such that pebbles of Q1 are moved to S2 as many as possible. Reset P1 and P2 as a set of pebbles on S2 and S3, respectively. 5⃝Remove P1 ∪P2 to S1. Load P3 ∪P4 to S2 ∪S3."
  },
  {
    "prompt": "This is a technical discussion about pebbles, half, Reset in theoretical computer science.",
    "response": "Consider the puzzle (G(2, 3), P3 ∪P4) and transfer half of P3 ∪P4 to S2 and the other half to S3 such that pebbles of Q1 are moved to S2 as many as possible. Reset P3 and P4 as a set of pebbles on S2 and S3, respectively. 6⃝Remove P4 to S4. Load P2 ∪P3 to S2 ∪S3. Consider the puzzle (G(2, 3), P2 ∪P3) and transfer half of P2 ∪P3 to S2 and the other half to S3 such that pebbles of Q1 are moved to S2 as many as possible. Reset P2 and P3 as a set of pebbles on S2 and S3, respectively."
  },
  {
    "prompt": "Understanding the computational aspects of step, Alg, works.",
    "response": "We claim that Alg .sbs1A works correctly, and it can be implemented to run in O(Nn+n2 log n) time. From steps 3⃝to 6⃝, Alg .sbs1A is recursively called four times with a half number of original pebbles. The worst case is Q1 = P3 ∪P4 initially. In this case, after step 3⃝, we have Q1 = P2 ∪P4, and after step 4⃝, we have Q1 = P1 ∪P4, and after step 5⃝, we have Q1 = P1 ∪P3, and ﬁnally after step 6⃝, we have Q1 = P1 ∪P2. Step 2⃝can be implemented in O(Nn) time by Lemma 5."
  },
  {
    "prompt": "Scientific context on denote, running, time and their role in graph theory.",
    "response": "Let us denote the running time of the algorithm from step 3⃝to 6⃝by g1(n). We want to prove g1(n) ≤c1n2 log2 n with some constant c1. we proceed by induction on n. The number of steps of the removal, loading, and restoration phase is at most cn2 with some constant c, since |Si| = n/2+O(1) for 1 ≤i ≤4. Then, by the inductive hypothesis, we have g1(n) ≤ 4g1(n 2) + cn2 ≤ 4c1(n 2)2 log2(n 2 ) + cn2 = c1n2 log2 n −c1n2 + cn2. By choosing c1 = c, we have g1(n) ≤cn2 log2 n, as claimed."
  },
  {
    "prompt": "Scientific context on algorithm, prepare, Alg and their role in graph theory.",
    "response": "Next, we prepare the second algorithm. Algorithm 8 (Alg .sbs1B(G, π1, π2)). (Sort with a rivet) 16 Input: A tree G satisfying the assumption of Lemma 6. Conﬁgurations π1, π2 ∈F(G, P). Output: A sequence of moves which transforms π1 to π2. 1⃝Find subsets Si of V (G) for 1 ≤i ≤4, such that |Si| = n/2 + O(1) for 1 ≤i ≤4, Si ∩Sj ̸= ∅ for 1 ≤i < j ≤4, S1, S2 ⊂V (G1), S3, S4 ⊂V (G2), S2 ∩S3 = {o}, and Si ∪Si+1 induces a subtree G(i, i + 1) of G for 1 ≤i ≤3."
  },
  {
    "prompt": "This is a technical discussion about Choose, Transfer, conﬁguration in theoretical computer science.",
    "response": "2⃝Choose a conﬁguration π∗ i for i = 1, 2 such that |sup(π∗ i ) ∩Sj| = n/4 + O(1) for 1 ≤j ≤4 and π∗ i is strongly related to πi for i = 1, 2. Transform π1 to π∗ 1. 3⃝Transfer π∗ 2(S1 ∪S2) to S1 ∪S2, and transfer π∗ 2(S3 ∪S4) to S3 ∪S4 by applying Alg .sbs1A. Reset Pi as a set of pebbles on Si for 1 ≤i ≤4. 4⃝Remove P3∪P4 from S3∪S4 to S4. Load P1∪P2 to S2∪S3. Consider the puzzle (G(2, 3), P1 ∪ P2) and build a conﬁguration π∗ 2,1 such that π∗ 2,1 and π∗ 2|S1∪S2 are strongly related."
  },
  {
    "prompt": "Explain a concept involving Remove, Transform, Alg in computational complexity.",
    "response": "5⃝Remove P1∪P2 from S2∪S3 to S1. Load P3∪P4 to S2∪S3. Consider the puzzle (G(2, 3), P3 ∪ P4) and build a conﬁguration π∗ 2,2 such that π∗ 2,2 and π∗ 2|S3∪S4 are strongly related. 6⃝Make π∗ 2. 7⃝Transform π∗ 2 to π2. We claim that Alg .sbs1B works correctly, and it can be implemented to run in O(n2 log n) time. In step 2⃝and step 7⃝, since πi and π∗ i are strongly related for i = 1, 2, the number of steps to transform π1 to π∗ 1 and to transform π∗ 2 to π2 are at most O(Nn) by applying Alg .sbs0."
  },
  {
    "prompt": "Scientific context on log, step, loading and their role in graph theory.",
    "response": "The number of steps of the removal, loading, and restoration phase is at most cn2 with some constant c. In step 3⃝, by applying Alg .sbs1A, we have at most c1n2 log n time. Let f1(n) be the algorithm’s running time from step 3⃝to step 6⃝. We want to prove f1(n) ≤d1n2 log n with some constant d1. In step 4⃝and step 5⃝, by the inductive hypothesis, we have f1(n) ≤ 2f1(n 2 ) + c1n2 log n + cn2 ≤ 2d1(n 2 )2 log2(n 2) + c1n2 log n + cn2 = (c1 + d1 2 )n2 log2 n + (c −d1 2 )n2."
  },
  {
    "prompt": "Read and understand this section about Lemma, max, choosing from a research paper.",
    "response": "By choosing d1 such that d1 ≥2 max{c, c1}, we have f1(n) ≤d1n2 log2 n. Proof of Lemma 7. We prepare two algorithms for the proof. The ﬁrst algorithm is as follows. Algorithm 9 (Alg .sbs2A(G, Q1, Q2)). (Partition with a long isthmus) Input: A tree G satisfying the assumption of Lemma 7. A conﬁguration π ∈F(G, P). A partition of pebbles P = Q1 ∪Q2 such that Q1 ∩Q2 = ∅and |Qi| = n/2 + O(1) for i = 1, 2. 17 Output: A sequence of moves which transfers Qi to V (Gi) for i = 1, 2."
  },
  {
    "prompt": "Scientific context on Find, Applying, Alg and their role in graph theory.",
    "response": "1⃝Find subsets Si of V (G) for 1 ≤i ≤4, such that |Si| = n/4 + O(1) for 1 ≤i ≤4, Si ∩Sj ̸= ∅ for 1 ≤i < j ≤4, S1, S2 ⊂V (G1), S3, S4 ⊂V (G2), and Si ∪Si+1 ∪V (I) induces a subtree G(i, i + 1) for 1 ≤i ≤3. 2⃝Applying Alg .sbs0, move n/4 pebbles to Si for 1 ≤i ≤4. Put Pi as a set of pebbles on Si for 1 ≤i ≤4. 3⃝Consider the puzzle (G(2, 3), P2 ∪P3) and transfer half of P2 ∪P3 to S2 and the other half to S3 such that pebbles of Q1 are moved to S2 as many as possible."
  },
  {
    "prompt": "Scientific context on pebbles, half, Reset and their role in graph theory.",
    "response": "Reset P2 and P3 as a set of pebbles on S2 and S3, respectively. 4⃝Consider the puzzle (G(1, 2), P1 ∪P2) and transfer half of P1 ∪P2 to S1 and the other half to S2 such that pebbles of Q1 are moved to S1 as many as possible. Reset P1 and P2 as a set of pebbles on S1 and S2, respectively. 5⃝Consider the puzzle (G(3, 4), P3 ∪P4) and transfer half of P3 ∪P4 to S3 and the other half to S4 such that pebbles of Q1 are moved to S3 as many as possible."
  },
  {
    "prompt": "Explain a concept involving pebbles, Reset, set in computational complexity.",
    "response": "Reset P3 and P4 as a set of pebbles on S3 and S4, respectively. 6⃝Consider the puzzle (G(2, 3), P2 ∪P3) and transfer half of P2 ∪P3 to S2 and the other half to S3 such that pebbles of Q1 are moved to S2 as many as possible. Reset P2 and P3 as a set of pebbles on S2 and S3, respectively. We claim that Alg .sbs2A works correctly, and it can be implemented to run in O(Nn+n2 log n) time."
  },
  {
    "prompt": "This is a technical discussion about Alg, step, pebbles in theoretical computer science.",
    "response": "In step 4⃝and step 5⃝, Alg .sbs1A is called two times with a half number of original pebbles, and in step 3⃝and step 6⃝, Alg .sbs2A is recursively called two times with a half number of original pebbles. The worst case is Q1 = P3 ∪P4 initially. In this case, after step 3⃝, we have Q1 = P2 ∪P4, and after step 4⃝, we have Q1 = P1 ∪P4, and after step 5⃝, we have Q1 = P1 ∪P3, and ﬁnally after step 6⃝, we have Q1 = P1 ∪P2. The running time g2(n) of the algorithm is dominated by step 3⃝- step 6⃝."
  },
  {
    "prompt": "Understanding the computational aspects of prove, constant, step.",
    "response": "We want to prove g2(n) ≤Nn + c2n2 log2 n with some constant c2. we proceed by induction on n. The number of steps of removal, loading, and restoration phase is at most cn2 with some constant c, since |S1 ∪S2| = n/2 + O(1) and |S3 ∪S4| = n/2 + O(1). The cost of step 4⃝and step 5⃝is at most c1(n/2)2 log2(n/2) by Lemma 6."
  },
  {
    "prompt": "Explain a concept involving hypothesis, inductive, algorithm in computational complexity.",
    "response": "Then, by the inductive hypothesis, we have g2(n) ≤ 2g2(n 2 ) + 2c1(n 2 )2 log2(n 2 ) + cn2 ≤ 2(N n 2 + c2(n 2 )2 log2(n 2 )) + 2c1(n 2)2 log2(n 2 ) + cn2 ≤ Nn + (c2 2 + c1 2 )n2 log2 n + (c −c2 2 −c1 2 )n2. By choosing c2 such that c2 ≥max{c1, 2c −c1}, we have g2(n) ≤Nn + c2n2 log2 n, as claimed. Next, we prepare the second algorithm. Algorithm 10 (Alg .sbs2B(G, π1, π2)). (Sort with a long isthmus) 18 Input: A tree G satisfying the assumption of Lemma 7. Conﬁgurations π1, π2 ∈F(G, P)."
  },
  {
    "prompt": "Scientific context on Output, sequence, moves and their role in graph theory.",
    "response": "Output: A sequence of moves which transforms π1 to π2. 1⃝Find subsets Si of V (G) for 1 ≤i ≤4, such that |Si| = n/4 + O(1) for 1 ≤i ≤4, Si ∩Sj ̸= ∅ for 1 ≤i < j ≤4, S1, S2 ⊂V (G1), S3, S4 ⊂V (G2), and Si ∪Si+1 ∪V (I) induces a subtree G(i, i + 1) for 1 ≤i ≤3. 2⃝Choose π∗ i for i = 1, 2 such that |sup(π∗ i ) ∩Sj| = n/4 + O(1) for 1 ≤j ≤4 and π∗ i is strongly related to πi for i = 1, 2. Transform π1 to π∗ 1."
  },
  {
    "prompt": "Read and understand this section about transfer, puzzle, Alg from a research paper.",
    "response": "3⃝Consider the puzzle (G, P) and transfer π∗ 2(S1 ∪S2) to S1 ∪S2 and transfer π∗ 2(S3 ∪S4) to S3 ∪S4 by applying Alg .sbs2A. Reset Pi as a set of pebbles on Si for 1 ≤i ≤4. 4⃝Consider the puzzle (G(1, 2), P1 ∪P2) and build a conﬁguration π∗ 2,1 such that π∗ 2,1 and π∗ 2|S1∪S2 are strongly related. 5⃝Consider the puzzle (G(3, 4), P3 ∪P4) and build a conﬁguration π∗ 2,2 such that π∗ 2,2 and π∗ 2|S3∪S4 are strongly related. 6⃝Make π∗ 2. 7⃝Transform π∗ 2 to π2."
  },
  {
    "prompt": "This is a technical discussion about Alg, step, works in theoretical computer science.",
    "response": "We claim that Alg .sbs2B works correctly, and it can be implemented to run in O(Nn+n2 log n) time. In step 2⃝and step 7⃝, since πi and π∗ i are strongly related for i = 1, 2, the number of steps to transfer π1 to π∗ 1 and to transfer π∗ 2 to π2 are at most O(Nn) by applying Alg .sbs0. In step 3⃝, Alg .sbs2A is called, and in step 4⃝and step 5⃝, Alg .sbs1B is called two times with a half number of original pebbles. The running time f2(n) of the algorithm is dominated by step 3⃝- step 6⃝."
  },
  {
    "prompt": "Scientific context on Alg, constant, steps and their role in graph theory.",
    "response": "We want to prove f2(n) ≤Nn + d2n2 log2 n with some constant d2. The number of steps of removal, loading, and restoration phase is at most cn2 with some constant c. The cost of step 3⃝is at most Nn + c2n2 log2 n by Alg .sbs2A. The cost of steps 4⃝and 5⃝is at most d1(n/2)2 log2(n/2) by Alg .sbs1B. Then, we have f2(n) ≤ Nn + c2n2 log2 n + 2d1(n 2 )2 log2(n 2 ) + cn2 ≤ Nn + (c2 + d1 2 )n2 log2 n + (c −d1 2 )n2. By choosing d2 such that d2 ≥c2 + c, we have f2(n) ≤Nn + d2n2 log2 n. Proof of Theorem 8."
  },
  {
    "prompt": "Explain a concept involving Case, centroid, tree in computational complexity.",
    "response": "Since G is a tree, G has a centroid vertex or a centroid edge. We consider two cases. Case 1. There exists a centroid o of degree at least 3. In this case, since N ≥3n, G has subtrees G1 and G2 such that V (G1) ∩V (G2) = {o} and |V (Gi)| ≥n + O(1) for i = 1, 2. Hence, by Lemma 6, the proof is ﬁnished. 19 Case 2. There exists no centroid of degree at least 3. In this case, the centroid is contained in an isthmus I. Let v1 and v2 be endvertices of I, and let V0 be the set of inner vertices of I."
  },
  {
    "prompt": "Understanding the computational aspects of subtrees, induced, Lemma.",
    "response": "We have two subtrees G1 and G2 induced by V (G) \\ V0 such that vi ∈V (Gi) for i = 1, 2. Put ni = |V (Gi)| for i = 1, 2. We may assume n1 ≤n2. Let k be the size of I. Subcase 2. 1. k < n. In this case, we have n2 ≥|V (G)\\V0|/2 > (N −n)/2 ≥n. On the other hand, since I contains the centroid, we have |V (G1) ∪V (I)| ≥N/2 ≥n. Hence, by taking v2 as the rivet in the assumption of Lemma 6, we can apply Lemma 6 to ﬁnish the proof. Subcase 2. 2. k ≥n."
  },
  {
    "prompt": "This is a technical discussion about Alg, subtree, puzzle in theoretical computer science.",
    "response": "Let H be a subtree of G induced by V (I) ∪V (G2), and let K be a subtree of G induced by V (G1) ∪V (I) ∪V (G′ 2), where G′ 2 is a subtree of G2 containing v2 with n1 vertices. We will use Alg .sbs1B and Alg .sbs2B to solve the puzzle (G, P) as follows;. 1⃝Apply Alg .sbs1B for the puzzle (H, n2) with a rivet v2. 2⃝Apply Alg .sbs2B for the puzzle (K, 2n1) with an isthmus I. 3⃝Again, apply Alg .sbs1B for the puzzle (H, n2) with a rivet v2."
  },
  {
    "prompt": "Read and understand this section about Lemma, steps, log from a research paper.",
    "response": "The running time of steps 1⃝- 3⃝is O(Nn+n2 log n) by Lemma 6 and Lemma 7. This completes the proof. Proof of Lemma 10. If T has a centroid edge, then by deﬁnition the statement of this lemma clearly holds. Hence let us assume that T has a centroid vertex o. Let us consider the set of components S of T −o. Without loss of generality, let X be a graph with the fewest order, and Y be a graph with the second-fewest order in the set S. Of course it is possible that |V (X)| = |V (Y )|."
  },
  {
    "prompt": "Explain a concept involving holds, denote, update in computational complexity.",
    "response": "If m ≥3, since |V (X)| + |V (Y )| ≤2(N−1) 3 holds, let us denote Z := X ∪Y and update S := (S \\ {X, Y }) ∪{Z}. Repeating this operation, the size of the set S becomes 2. At that stage, let us denote S = {G, H}, and let us assume |V (G)| ≤|V (H)|. In this case, we can take T1 to be the subgraph of T induced by V (G) ∪{o}, and similarly T2 to be the subgraph of T induced by V (H) ∪{o}."
  },
  {
    "prompt": "Explain a concept involving element, order, size in computational complexity.",
    "response": "Since the size of each element of the set S is between 1 and n, we can sort and manage each element of S in order of its size using a 1 × n-array A(S) whose each cell is a list. Since the minimum of the orders of the graphs in S is monotonically non-decreasing, the task of identifying a graph with the smallest order and a graph with the second smallest order in A(S) takes only O(N) eﬀort in total. Proof of Lemma 11."
  },
  {
    "prompt": "This is a technical discussion about algorithm, consists, single in theoretical computer science.",
    "response": "Let us consider the following algorithm: 1⃝If T ′ consists of a single edge e, let v be an end-vertex of e and set H := T. Otherwise, let H be the graph obtained by contracting all parts of T ′ except the pendant edges to a single vertex v. Let X be the subgraph of H corresponding to T ′."
  },
  {
    "prompt": "Read and understand this section about vertex, Continue, rank from a research paper.",
    "response": "2⃝Continue to add vertex u and edge vu to X (i.e., update to X := X + vu) as long as the rank of the subgraph T(X) of T corresponding to X is strictly less than d, and a neighboring vertex u of v that does not belong to X is found in H. 20 3⃝If the rank of the subgraph T(X) of T corresponding to X is strictly less than d, do: ( 3⃝-a) Find a vertex w outside X adjacent to some leaf ℓin X and add that vertex w and edge ℓw to X. The resulting X has the length 2 arm v −ℓ−w. ( 3⃝-b) Update X (resp."
  },
  {
    "prompt": "Scientific context on Step, resp., graph and their role in graph theory.",
    "response": "H) as the graph obtained by contracting the edge vℓof X (resp. H) and identifying ℓwith v. ( 3⃝-c) Go to Step 2⃝ 4⃝Return the subgraph T(X) of T corresponding to X. Let us show the correctness of the above algorithm. To do so, we need only check the fact that the maximal isthmus size of the subgraph T(X) of T corresponding to X does not exceed the maximal isthmus size of T by the operation of Step 2⃝or Step 3⃝."
  },
  {
    "prompt": "Scientific context on increased, Step, isthmus and their role in graph theory.",
    "response": "First, for the operation of Step 2⃝, the maximum isthmus size of T(X) is not increased, since the degree of the non-leaf vertices in T(X) is increased. Next, the vertex w that is added to T in Step 3⃝operation is not a cut vertex because it is a leaf of T(X), so it cannot be part of any isthmus in T(X). Therefore, the new isthmus arising in T(X) at this step has one end vertex at ℓ."
  },
  {
    "prompt": "Explain a concept involving degree, Step, isthmus in computational complexity.",
    "response": "On the other hand, just before entering the operation of Step 3⃝, the degree on T(X) and the degree on T are the same for any vertex other than leaves in T(X), so this newly generated isthmus in T(X) is just an isthmus of T itself. It is clear that the time complexity of this algorithm is linear. Proof of Lemma 12. Let us consider the operation X mentioned in Lemma 10 of dividing a given tree into two subtrees."
  },
  {
    "prompt": "Read and understand this section about order, subtrees, number from a research paper.",
    "response": "Let us recursively apply the operation X to each subtree of T to the point such that the order of each subtree is greater than or equal to k and that another X on those subtrees will reduce the order of the resulting subtrees to strictly less than k. Consequently, if r is the number of subtrees obtained so far, then 2n/k > (n −1)/(k −1) ≥r. In fact, one X operation increases the number of subtrees by one. Therefore, the total overlap of the vertices of the subtrees is at most r −1."
  },
  {
    "prompt": "Read and understand this section about solving, inequality, yields from a research paper.",
    "response": "From this it follows that ((r −1) + n)/k ≥r, and solving this inequality yields n −1 ≥(k −1)r. Now, obviously, if k = 1, the degree of overlap is zero, hence we can assume k ≥2, so we have 2n/k > (n −1)/(k −1) ≥r. Therefore, even if we apply one more X operation to each subtree here, the number of subtrees can be kept to 4n/k. Thus, by this algorithm, T is covered by at most 4n/k subtrees satisfying the conditions of the theorem. Proof of Lemma 13."
  },
  {
    "prompt": "Scientific context on graph, Step, JointVertex and their role in graph theory.",
    "response": "According to the deﬁnition of the subtree stored in the second child of each node in this binary tree BT (cf. Step 2b), for the sibling node c2(p(ℓ(i))) of ℓ(i), the graph X(r) −(X(ℓ(i)) −JointVertex(X(ℓ(i)))) includes the graph X(c2(p(ℓ(i)))). The statement of this lemma can be directly derived from this fact. Proof of Lemma 14. Let NodeSet(ℓ) be the set of nodes that are at distance ℓfrom the root node of the binary tree BT."
  },
  {
    "prompt": "This is a technical discussion about Step, Lemma, holds in theoretical computer science.",
    "response": "Since ∀i, P x∈NS(i) |V (X(x))| ≤4n holds, according to Lemma 9, for any distance ℓ, the eﬀort required to correctly store all the objects that should be placed in all nodes of NodeSet(ℓ) is at most O(n). The height of BT is O(log n), so the total eﬀort required to complete BT is O(n log n). 21 Proof of Lemma 15. Since Step 9⃝(Step 10 ⃝, resp.) of Alg . CutOﬀ(G, T, BT, f, g′) and Step 8⃝ (Step 9⃝, resp.) of Alg ."
  },
  {
    "prompt": "Understanding the computational aspects of Extraction, perform, operation.",
    "response": "Extraction(T, BT, g′, t) perform essentially the same operation, we will prove it only for the former. The change in the maximum isthmus size of X(r) −(X(t) − JointVertex(X(t))) can be achieved by checking at most an O(k) region around Joint(X(t)), which can be done in O(k) time."
  },
  {
    "prompt": "This is a technical discussion about time, leaf, node in theoretical computer science.",
    "response": "During the initial construction of the binary tree BT, if we provide an array containing information for each vertex of tree T about which leaf node of BT stores the subtree that includes itself (a process that can be accomplished in total at most in O(n+k) time), then a leaf node w of BT such that X(w) contains the vertex v can be found in O(1) time."
  },
  {
    "prompt": "Scientific context on Step, Alg, list and their role in graph theory.",
    "response": "Note that, conversely, for each leaf node ℓof BT, if we provide data indicating the position of ℓin the list associated with each vertex of X(ℓ) (this task also can be handled in O(n + k) overall), then maintaining the lists associated with the vertices of T with the removal of ℓfrom BT at Step 10 ⃝ of Alg . CutOﬀ(G, T, BT, f, g′) can be done in O(k) time. Proof of Lemma 16. Let the set of all nodes in the binary tree BT 2 be denoted as NodeSet(BT 2)."
  },
  {
    "prompt": "Understanding the computational aspects of NodeSet, JointVertex, JointEdge.",
    "response": "Let H be a subtree of T containing all the vertices {JointVertex(X(x)) | x ∈NodeSet(BT 2)} and all the edges {JointEdge(X(x)) | x ∈NodeSet(BT 2)}. The movement of the k + 1 unoccupied spaces through the repetition of Step 4 in the algorithm is accomplished, as a whole, by having a connected unoccupied space of size k + 1 traverse H in a depth-ﬁrst search order."
  },
  {
    "prompt": "Read and understand this section about unoccupied, space, swapping from a research paper.",
    "response": "The estimated number of moves for one unoccupied space (that is achieved by the sequence of swapping the positions of this unoccupied space and pebbles) is O(k + n), so the total eﬀort for swapping the unoccupied space of size k + 1 is bounded by O(k(k + n)). References [1] Hugo A. Akitaya, Matthew D. Jones, Matias Korman, Oliver Korten, Christopher Meier- frankenfeld, Michael J. Munje, Diane L. Souvaine, Michael Thramann, and Csaba D. Toth. Reconﬁguration of connected graph partitions."
  },
  {
    "prompt": "Read and understand this section about JAN, JOURNAL, THEORY from a research paper.",
    "response": "JOURNAL OF GRAPH THEORY, 102(1):35– 66, JAN 2023. doi:10.1002/jgt.22856. [2] Anton Andreychuk, Konstantin Yakovlev, Pavel Surynek, Dor Atzmon, and Roni Stern. Multi- agent pathﬁnding with continuous time. ARTIFICIAL INTELLIGENCE, 305, APR 2022. doi:10.1016/j.artint.2022.103662. [3] AF Archer. A modern treatment of the 15 puzzle. AMERICAN MATHEMATICAL MONTHLY, 106(9):793–799, NOV 1999. doi:10.2307/2589612. [4] S. Ardizzoni, I. Saccani, L. Consolini, and M. Locatelli."
  },
  {
    "prompt": "Scientific context on IEEE, CONTROL, DECISION and their role in graph theory.",
    "response": "Multi-agent path ﬁnding on strongly connected digraphs. In 2022 IEEE 61ST CONFERENCE ON DECISION AND CONTROL (CDC), IEEE Conference on Decision and Control, pages 7194–7199, 345 E 47TH ST, NEW YORK, NY 10017 USA, 2022. IEEE, IEEE. IEEE 61st Conference on Decision and Control (CDC), Cancun, MEXICO, DEC 06-09, 2022. doi:10.1109/CDC51059.2022.9992727. [5] S. Ardizzoni, I. Saccani, L. Consolini, and M. Locatelli. Local optimization of mapf so- lutions on directed graphs."
  },
  {
    "prompt": "Understanding the computational aspects of IEEE, CONTROL, CDC.",
    "response": "In 2023 62ND IEEE CONFERENCE ON DECISION AND CONTROL, CDC, IEEE Conference on Decision and Control, pages 8081–8086, 345 E 47TH 22 ST, NEW YORK, NY 10017 USA, 2023."
  },
  {
    "prompt": "This is a technical discussion about Technol, Univ, Sci in theoretical computer science.",
    "response": "IEEE; Soc Ind & Appl Math; Japanese Soc In- strument & Control Engineers; European Control Assoc; Shanghai Jiaotong Univ; Shandong Univ Sci & Technol; MathWorks; Harbin Engn Univ; E China Univ Sci & Technol; Nanjing Univ Informat Sci & Technol; Tongji Univ; IEEE CAA Journal Automatica Sinica; AiTEN; Franklin Open; Huazhong Univ Sci & Technol, IEEE. 62nd IEEE Conference on Decision and Control (CDC), IEEE Control Syst Soc, Singapore, SINGAPORE, DEC 13-15, 2023. doi:10.1109/CDC49753.2023.10383280."
  },
  {
    "prompt": "Scientific context on Stefano, Ardizzoni, Irene and their role in graph theory.",
    "response": "[6] Stefano Ardizzoni, Irene Saccani, Luca Consolini, Marco Locatelli, and Bernhard Nebel. An algorithm with improved complexity for pebble motion/multi-agent path ﬁnding on trees. JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH, 79:483–514, 2024. [7] Aaron Atilano, Sebastian Bejos, and Christian Rubio-Montiel. Motions of a con- nected subgraph representing a swarm of robots inside a graph of work stations. ELECTRONIC JOURNAL OF GRAPH THEORY AND APPLICATIONS, 10(2):501–521, 2022."
  },
  {
    "prompt": "Explain a concept involving doi, Persiano, Auletta in computational complexity.",
    "response": "doi:10.5614/ejgta.2022.10.2.12. [8] Dor Atzmon, Roni Tzvi Stern, Ariel Felner, Glenn Wagner, Roman Bartak, and Neng- Fa Zhou. Robust multi-agent path ﬁnding and executing. JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH, 67:549–579, 2020. [9] V Auletta, A Monti, M Parente, and P Persiano. A linear-time algorithm for the feasibility of pebble motion on trees. ALGORITHMICA, 23(3):223–245, MAR 1999. doi:10.1007/PL00009259. [10] V Auletta and P Persiano. Optimal pebble motion on a tree."
  },
  {
    "prompt": "Understanding the computational aspects of FEB, INFORMATION, COMPUTATION.",
    "response": "INFORMATION AND COMPUTATION, 165(1):42–68, FEB 25 2001. doi:10.1006/inco.2000.3005. [11] Pascal Bachor, Rolf-David Bergdoll, and Bernhard Nebel. The multi-agent transporta- tion problem. In B Williams, Y Chen, and J Neville, editors, THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 10, AAAI Conference on Artiﬁcial Intelligence, pages 11525–11532, 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA, 2023."
  },
  {
    "prompt": "Understanding the computational aspects of Intelligence, Assoc, Artiﬁcial.",
    "response": "Assoc Advancement Artiﬁcial Intelligence, ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE. 37th AAAI Conference on Artiﬁcial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artiﬁcial Intelligence / 13th Symposium on Educational Advances in Artiﬁcial Intelligence, Washington, DC, FEB 07-14, 2023. [12] Roman Bartak, Jiri Svancara, Vera Skopkova, David Nohejl, and Ivan Krasicenko. Multi- agent path ﬁnding on real robots. AI COMMUNICATIONS, 32(3):175–189, 2019. doi:10.3233/AIC-190621."
  },
  {
    "prompt": "Scientific context on Ahmad, Biniaz, Kshitij and their role in graph theory.",
    "response": "[13] Ahmad Biniaz, Kshitij Jain, Anna Lubiw, Zuzana Masarova, Tillmann Miltzow, Debajyoti Mondal, Anurag Murty Naredla, Josef Tkadlec, and Alexi Turcotte. Token swapping on trees. DISCRETE MATHEMATICS AND THEORETICAL COMPUTER SCIENCE, 24(2), 2022. [14] Adi Botea, Davide Bonusi, and Pavel Surynek. Solving multi-agent path ﬁnding on strongly biconnected digraphs. JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH, 62:273– 314, 2018. doi:10.1613/jair.1.11212."
  },
  {
    "prompt": "This is a technical discussion about Gruia, Calinescu, Adrian in theoretical computer science.",
    "response": "23 [15] Gruia Calinescu, Adrian Dumitrescu, and Janos Pach. Reconﬁgurations in graphs and grids. SIAM JOURNAL ON DISCRETE MATHEMATICS, 22(1):124–138, 2008. 7th Latin Amer- ican Symposium on Theoretical Informatics (LATIN 2006), Valdivia, CHILE, MAR 20-24, 2006. doi:10.1137/060652063. [16] Jan Chudy, Nestor Popov, and Pavel Surynek. Multi-agent path ﬁnding and acting with small reﬂex-based mobile robots."
  },
  {
    "prompt": "This is a technical discussion about ROBOVIS, GEWERBESTRASSE, CHAM in theoretical computer science.",
    "response": "In P Galambos, E Kayacan, and K Madani, editors, ROBOTICS, COMPUTER VISION AND INTELLIGENT SYSTEMS, ROBOVIS 2020, ROBOVIS 2021, volume 1667 of Communications in Computer and Information Science, pages 51–75, GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND, 2022. IN- STICC, SPRINGER INTERNATIONAL PUBLISHING AG. 2nd International Conference on Robotics, Computer Vision and Intelligent Systems (ROBOVIS), ELECTR NETWORK, OCT 27-28, 2021. doi:10.1007/978-3-031-19650-8\\_3."
  },
  {
    "prompt": "Read and understand this section about Greyson, Daugherty, Spyros from a research paper.",
    "response": "[17] Greyson Daugherty, Spyros Reveliotis, and Greg Mohler. Optimized multiagent routing for a class of guidepath-based transport systems. IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, 16(1):363–381, JAN 2019. doi:10.1109/TASE.2018.2798630. [18] Boris de Wilde, Adriaan W. ter Mors, and Cees Witteveen. Push and rotate: a complete multi- agent pathﬁnding algorithm. JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH, 51:443–492, 2014. doi:10.1613/jair.4447. [19] Erik D. Demaine, Sandor P."
  },
  {
    "prompt": "Read and understand this section about Phillip, Keldenich, Henk from a research paper.",
    "response": "Fekete, Phillip Keldenich, Henk Meijer, and Christian Scheﬀer. Coordinated motion planning: Reconﬁguring a swarm of labeled robots with bounded stretch. SIAM JOURNAL ON COMPUTING, 48(6):1727–1762, 2019. doi:10.1137/18M1194341. [20] Sandor P. Fekete, Peter Kramer, Christian Rieck, Christian Scheﬀer, and Arne Schmidt. Eﬃciently reconﬁguring a connected swarm of labeled robots. AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS, 38(2), DEC 2024. doi:10.1007/s10458-024-09668-3."
  },
  {
    "prompt": "Explain a concept involving Alex, Guy, Fink in computational complexity.",
    "response": "[21] Alex Fink and Richard Guy. Rick’s tricky six puzzle: S5 sits specially in S6. Math. Mag., 82(2):83–102, 2009. [22] Shinya Fujita, Tomoki Nakamigawa, and Tadashi Sakuma. Colored pebble motion on graphs. EUROPEAN JOURNAL OF COMBINATORICS, 33(5, SI):884–892, JUL 2012. doi:10.1016/j.ejc.2011.09.019. [23] Shinya Fujita, Tomoki Nakamigawa, and Tadashi Sakuma. Pebble exchange on graphs. Discrete Appl. Math., 184:139–145, 2015. doi:10.1016/j.dam.2013.03.009. [24] Gilad Goraly and Refael Hassin."
  },
  {
    "prompt": "Scientific context on Multi-color, graphs, ALGORITHMICA and their role in graph theory.",
    "response": "Multi-color pebble motion on graphs. ALGORITHMICA, 58(3):610–636, NOV 2010. doi:10.1007/s00453-009-9290-7. [25] Marcus Gozon and Jingjin Yu. On computing makespan-optimal solutions for generalized sliding-tile puzzles. In M Wooldridge, J Dy, and S Natarajan, editors, THIRTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 9, AAAI Confer- ence on Artiﬁcial Intelligence, pages 10288–10296, 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA, 2024."
  },
  {
    "prompt": "This is a technical discussion about Intelligence, Artiﬁcial, Assoc in theoretical computer science.",
    "response": "Assoc Advancement Artiﬁcial Intelligence, ASSOC ADVANCE- MENT ARTIFICIAL INTELLIGENCE. 38th AAAI Conference on Artiﬁcial Intelligence 24 (AAAI) / 36th Conference on Innovative Applications of Artiﬁcial Intelligence / 14th Sympo- sium on Educational Advances in Artiﬁcial Intelligence, Vancouver, CANADA, FEB 20-27, 2024. [26] Teng Guo and Jingjin Yu. Sub-1.5 time-optimal multi-robot path planning on grids in poly- nomial time."
  },
  {
    "prompt": "Scientific context on ROBOTICS, ITHACA, Hauser and their role in graph theory.",
    "response": "In K Hauser, D Shell, and S Huang, editors, ROBOTICS: SCIENCE AND SYSTEM XVIII, Robotics - Science and Systems, C/O HADAS KRESS-GAZIT CORNELL UNIV SIBLEY SCH MECHANICAL & AEROSPACE ENGINEERING UPSON HALL, RM 551, ITHACA, NY, UNITED STATES, 2022. Columbia Univ; Amazon Robot; Toyota Res Inst; Dexterity; Raytheon Technologies; Raytheon Technologies Res Ctr; Mitsubishi Elect; ZOOX; Lockheed Martin; Intrinsic, RSS FOUNDATION-ROBOTICS SCIENCE & SYS- TEMS FOUNDATION."
  },
  {
    "prompt": "Read and understand this section about RSS, JUN, Systems from a research paper.",
    "response": "Conference on Robotics - Science and Systems (RSS), New York City, NY, JUN 27-JUL 01, 2022. [27] Teng Guo and Jingjin Yu. Eﬃcient heuristics for multi-robot path planning in crowded environments. In 2023 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS), IEEE International Conference on Intelligent Robots and Systems, pages 6749–6756, 345 E 47TH ST, NEW YORK, NY 10017 USA, 2023. IEEE; RSJ, IEEE."
  },
  {
    "prompt": "Read and understand this section about IEEE, IROS, Detroit from a research paper.",
    "response": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, OCT 01-05, 2023. doi:10.1109/IROS55552.2023.10341800. [28] Teng Guo and Jingjin Yu. Expected 1 .x makespan-optimal multi-agent path ﬁnding on grid graphs in low polynomial time. JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH, 81:443–479, 2024. [29] Wolfgang Honig, T. K. Satish Kumar, Liron Cohen, Hang Ma, Hong Xu, Nora Aya- nian, and Sven Koenig. Summary: Multi-agent path ﬁnding with kinematic constraints."
  },
  {
    "prompt": "This is a technical discussion about ALBERT, GEB, GERMANY in theoretical computer science.",
    "response": "In C Sierra, editor, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, pages 4869–4873, ALBERT- LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY, 2017."
  },
  {
    "prompt": "Scientific context on Univ, Technol, Sci and their role in graph theory.",
    "response": "Int Joint Conf Artiﬁcal Intelligence; Victo- ria Govt; Melbourne Convent Bur; Artiﬁcial Intelligence Journal; Alibaba Grp; Xiaoi; Ten- cent; JD.com; Meitu Inc; Didi ChuXing; Baidu; Ant Financial Serv Grp; Australian Comp Soc; Natl Sci Fdn; Univ Technol Sydney; Griﬃth Univ; Univ Sydney; Royal Melbourne Inst Technol Univ; Melbourne Univ; Australian Natl Univ; King Abdullah Univ Sci & Technol; Data61; Adobe; IBM; NNAISENCE; AUBOT; So Univ Sci & Technol; Monash Univ; Auck- land Univ Technol; Univ New S Wales; Assumption University of Thailand; Future Univ Hakodate; Deakin Univ; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly, Nanyang Technological University; Federation Univ; Univ Queensland; Facebook; Microsoft; BigML; Essence; Nuance; NVIDIA; XENON, IJCAI-INT JOINT CONF ARTIF INTELL."
  },
  {
    "prompt": "Explain a concept involving IJCAI, Melbourne, AUSTRALIA in computational complexity.",
    "response": "26th International Joint Conference on Artiﬁcial Intelligence (IJCAI), Melbourne, AUSTRALIA, AUG 19-25, 2017. [30] Wm. Woolsey Johnson and William E. Story. Notes on the ”15” puzzle. American Journal of Mathematics, 2(4):397–404, 1879. URL: http://www.jstor.org/stable/2369492. [31] Andy N. C. Kang and David A. Ault. Some properties of a centroid of a free tree. Information Processing Lett., 4(1):18–20, 1975/76. doi:10.1016/0020-0190(75)90055-1."
  },
  {
    "prompt": "Understanding the computational aspects of Tatsuoki, Kato, Tomoki.",
    "response": "25 [32] Tatsuoki Kato, Tomoki Nakamigawa, and Tadashi Sakuma. Pebble exchange group of graphs. Eur. J. Comb., 95:11, 2021. Id/No 103325. doi:10.1016/j.ejc.2021.103325. [33] Mokhtar M. Khorshid, Robert C. Holte, and Nathan R Sturtevant. A polynomial-time al- gorithm for non-optimal multi-agent pathﬁnding. In Symposium on Combinatorial Search, 2011. URL: https://api.semanticscholar.org/CorpusID:15391281. [34] D. Kornhauser, G. Miller, and P. Spirakis."
  },
  {
    "prompt": "Explain a concept involving Coordinating, graphs, diam in computational complexity.",
    "response": "Coordinating pebble motion on graphs, the diam- eter of permutation groups, and applications. In 25th Annual Symposium on Foundations of Computer Science, 1984., pages 241–250, 1984. doi:10.1109/SFCS.1984.715921. [35] Athanasios Krontiris, Ryan Luna, and Kostas E. Bekris. From feasibility tests to path planners for multi-agent pathﬁnding. In Symposium on Combinatorial Search, 2013. URL: https://api.semanticscholar.org/CorpusID:6606518. [36] Miroslav Kulich, Tomas Novak, and Libor Preucil."
  },
  {
    "prompt": "Scientific context on IEEE, INTELLIGENT, TRANSPORTATION and their role in graph theory.",
    "response": "Push, stop, and replan: An application of pebble motion on graphs to planning in automated warehouses. In 2019 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE (ITSC), IEEE International Conference on Intelligent Transportation Systems-ITSC, pages 4456–4463. IEEE; Intel; Griﬃth Univ; Auck- land Tourism Events & Econ Dev, 2019. IEEE Intelligent Transportation Systems Conference (IEEE-ITSC), Auckland, NEW ZEALAND, OCT 27-30, 2019. [37] Geunho Lee and Cornelis Francois van Eeden."
  },
  {
    "prompt": "Read and understand this section about coordi, MAR, Visiting from a research paper.",
    "response": "Visiting pebbles on rectangular grids: coordi- nating multiple robots in mobile fulﬁlment systems. INTELLIGENT SERVICE ROBOTICS, 14(1):79–97, MAR 2021. doi:10.1007/s11370-021-00350-1. [38] S. Loyd and M. Gardner. Mathematical Puzzles. Number Volume 1 in Dover Math Games and Puzzles Series. Dover Publications, 1959. URL: https://books.google.co.jp/books?id=QCy6DzgqcI4C. [39] Hang Ma, T. K. Satish Kumar, and Sven Koenig. Multi-agent path ﬁnding with delay prob- abilities."
  },
  {
    "prompt": "This is a technical discussion about INTELLIGENCE, STE, USA in theoretical computer science.",
    "response": "In THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, AAAI Conference on Artiﬁcial Intelligence, pages 3605–3612, 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA, 2017. Assoc Advancement Artiﬁcial Intelligence, AS- SOC ADVANCEMENT ARTIFICIAL INTELLIGENCE. 31st AAAI Conference on Artiﬁcial Intelligence, San Francisco, CA, FEB 04-09, 2017. [40] Hang Ma, Craig Tovey, Guni Sharon, T. K. Satish Kumar, and Sven Koenig."
  },
  {
    "prompt": "Read and understand this section about INTELLIGENCE, Artiﬁcial, AAAI from a research paper.",
    "response": "Multi-agent path ﬁnding with payload transfers and the package-exchange robot-routing problem. In THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, AAAI Confer- ence on Artiﬁcial Intelligence, pages 3166–3173, 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA, 2016. Assoc Advancement Artiﬁcial Intelligence, ASSOC ADVANCE- MENT ARTIFICIAL INTELLIGENCE. 30th Association-for-the-Advancement-of-Artiﬁcial- Intelligence (AAAI) Conference on Artiﬁcial Intelligence, Phoenix, AZ, FEB 12-17, 2016."
  },
  {
    "prompt": "Understanding the computational aspects of Bernhard, Nebel, doi.",
    "response": "[41] Bernhard Nebel. The computational complexity of multi-agent pathﬁnding on directed graphs. ARTIFICIAL INTELLIGENCE, 328, MAR 2024. doi:10.1016/j.artint.2023.104063. [42] Naomi Nishimura. Introduction to reconﬁguration. ALGORITHMS, 11(4), APR 2018. doi:10.3390/a11040052. 26 [43] Keisuke Okumura and Xavier Defago. Solving simultaneous target assignment and path planning eﬃciently with time-independent execution. ARTIFICIAL INTELLIGENCE, 321, AUG 2023. doi:10.1016/j.artint.2023.103946."
  },
  {
    "prompt": "Read and understand this section about Keisuke, Okumura, Manao from a research paper.",
    "response": "[44] Keisuke Okumura, Manao Machida, Xavier Defago, and Yasumasa Tamura. Prior- ity inheritance with backtracking for iterative multi-agent path ﬁnding. ARTIFICIAL INTELLIGENCE, 310, SEP 2022. doi:10.1016/j.artint.2022.103752. [45] C.H. Papadimitriou, P. Raghavan, M. Sudan, and H. Tamaki. Motion planning on a graph. In Proceedings 35th Annual Symposium on Foundations of Computer Science, pages 511–520, 1994. doi:10.1109/SFCS.1994.365740. [46] Daniel Ratner and Manfred Warmuth."
  },
  {
    "prompt": "Read and understand this section about ICAART, puzzle, problems from a research paper.",
    "response": "The (n2 −1)-puzzle and related relocation problems. J. Symb. Comput., 10(2):111–137, 1990. doi:10.1016/S0747-7171(08)80001-6. [47] Vojtech Rybar and Pavel Surynek. Highways in warehouse multi-agent path ﬁnding: A case study. In AP Rocha, L Steels, and J VandenHerik, editors, ICAART: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE - VOL 1, ICAART, pages 274–281, AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL, 2022. SCITEPRESS."
  },
  {
    "prompt": "Scientific context on ICAART, International, ELECTR and their role in graph theory.",
    "response": "14th International Conference on Agents and Artiﬁcial Intelligence (ICAART), ELECTR NETWORK, FEB 03-05, 2022. doi:10.5220/0010845200003116. [48] Enrico Saccon, Luigi Palopoli, and Marco Roveri. Comparing multi-agent path ﬁnding al- gorithms in a real industrial scenario. In A Dovier, A Montanari, and A Orlandini, editors, AIXIA 2022 - ADVANCES IN ARTIFICIAL INTELLIGENCE, volume 13796 of Lecture Notes in Artiﬁcial Intelligence, pages 184–197, GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND, 2023."
  },
  {
    "prompt": "This is a technical discussion about EUSTEMA, Generali, Previnet in theoretical computer science.",
    "response": "Italian Assoc Artiﬁcial Intelligence; EUSTEMA; Danieli Automat; Generali; Intesa Sanpaolo; TechEdge; OverIT; Previnet; U Blox; BeanTech; SMC; Conﬁnd- ustria Udine, SPRINGER INTERNATIONAL PUBLISHING AG. 21st International Con- ference of the Italian-Association-for-Artiﬁcial-Intelligence (AIxIA), Udine, ITALY, NOV 28- DEC 02, 2022. doi:10.1007/978-3-031-27181-6\\_13. [49] Basudev Saha, Bidyut Das, Vineeta Shukla, and Mukta Majumder."
  },
  {
    "prompt": "Read and understand this section about Pebble, biochips, AUG from a research paper.",
    "response": "Pebble traversal-based fault detection and advanced reconﬁguration technique for digital microﬂuidic biochips. JOURNAL OF ELECTRONIC TESTING-THEORY AND APPLICATIONS, 40(4):573–587, AUG 2024. doi:10.1007/s10836-024-06137-3. [50] Joao Salvado, Masoumeh Mansouri, and Federico Pecora. A network-ﬂow reduction for the multi-robot goal allocation and motion planning problem."
  },
  {
    "prompt": "This is a technical discussion about IEEE, ENGINEERING, INTERNATIONAL in theoretical computer science.",
    "response": "In 2021 IEEE 17TH INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND ENGINEERING (CASE), IEEE International Conference on Automation Science and Engineering, pages 2194– 2201, 345 E 47TH ST, NEW YORK, NY 10017 USA, 2021. IEEE, IEEE. 17th IEEE Inter- national Conference on Automation Science and Engineering (CASE), Lyon, FRANCE, AUG 23-27, 2021. doi:10.1109/CASE49439.2021.9551640. [51] Dhruv Mauria Saxena and Maxim Likhachev."
  },
  {
    "prompt": "Scientific context on IEEE, ICRA, INTERNATIONAL and their role in graph theory.",
    "response": "Planning for complex non-prehensile ma- nipulation among movable objects by interleaving multi-agent pathﬁnding and physics- based simulation. In 2023 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS 27 AND AUTOMATION (ICRA 2023), IEEE International Conference on Robotics and Automation ICRA, pages 8141–8147, 345 E 47TH ST, NEW YORK, NY 10017 USA, 2023. IEEE; IEEE Robot & Automat Soc, IEEE. IEEE International Conference on Robotics and Automation (ICRA), London, ENGLAND, MAY 29-JUN 02, 2023."
  },
  {
    "prompt": "Read and understand this section about doi, Guni, Sharon from a research paper.",
    "response": "doi:10.1109/ICRA48891.2023.10161006. [52] Guni Sharon, Roni Stern, Ariel Felner, and Nathan R. Sturtevant. Conﬂict-based search for optimal multi-agent pathﬁnding. ARTIFICIAL INTELLIGENCE, 219:40–66, FEB 2015. doi:10.1016/j.artint.2014.11.006. [53] Rahul Shome, Kiril Solovey, Andrew Dobson, Dan Halperin, and Kostas E. Bekris. drrt*: Scalable and informed asymptotically-optimal multi-robot motion planning. AUTONOMOUS ROBOTS, 44(3-4, SI):443–467, MAR 2020. doi:10.1007/s10514-019-09832-9."
  },
  {
    "prompt": "This is a technical discussion about Kiril, Halperin, Solovey in theoretical computer science.",
    "response": "[54] Kiril Solovey and Dan Halperin. k-color multi-robot motion planning. INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH, 33(1, SI):82–97, JAN 2014. doi:10.1177/0278364913506268. [55] Roni Stern. Multi-agent path ﬁnding - an overview. In GS Osipov, AI Panov, and KS Yakovlev, editors, ARTIFICIAL INTELLIGENCE, volume 11866 of Lecture Notes in Artiﬁcial Intelligence, pages 96–115, GEWERBESTRASSE 11, CHAM, CH-6330, SWITZER- LAND, 2019."
  },
  {
    "prompt": "Explain a concept involving Yandex, Huawei, NLMK in computational complexity.",
    "response": "Russian Assoc Artiﬁcial Intelligence; Yandex; Huawei; AimTech; NLMK; Tin- koﬀ, SPRINGER INTERNATIONAL PUBLISHING AG. 5th RAAI Summer School on Ar- tiﬁcial Intelligence, Moscow Inst Phys & Technol, Dolgoprudy, RUSSIA, JUL 04-07, 2019. doi:10.1007/978-3-030-33274-7\\_6. [56] Roni Stern, Nathan R. Sturtevant, Ariel Felner, Sven Koenig, Hang Ma, Thayne T. Walker, Jiaoyang Li, Dor Atzmon, Liron Cohen, T. K. Satish Kumar, Roman Bart´ak, and Eli Bo- yarski."
  },
  {
    "prompt": "Scientific context on Deﬁnitions, variants, SOCS and their role in graph theory.",
    "response": "Multi-agent pathﬁnding: Deﬁnitions, variants, and benchmarks. In Pavel Surynek and William Yeoh, editors, Proceedings of the Twelfth International Symposium on Combinatorial Search, SOCS 2019, Napa, California, 16-17 July 2019, pages 151–158. AAAI Press, 2019. URL: https://doi.org/10.1609/socs.v10i1.18510, doi:10.1609/SOCS.V10I1.18510. [57] Pavel Surynek. An application of pebble motion on graphs to abstract multi-robot path planning."
  },
  {
    "prompt": "Read and understand this section about USA, TOOLS, ICTAI from a research paper.",
    "response": "In ICTAI: 2009 21ST INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, Proceedings-International Conference on Tools With Ar- tiﬁcial Intelligence, pages 151–158, 345 E 47TH ST, NEW YORK, NY 10017 USA, 2009. IEEE Comp Soc; Biol & Artiﬁcial Intelligent Soc, IEEE. 21st IEEE Interna- tional Conference on Tools with Artiﬁcial Intelligence, Newark, NJ, NOV 02-04, 2009. doi:10.1109/ICTAI.2009.62. [58] Pavel Surynek."
  },
  {
    "prompt": "Read and understand this section about DEC, Time-expanded, problems from a research paper.",
    "response": "Time-expanded graph-based propositional encodings for makespan-optimal solving of cooperative path ﬁnding problems. ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE, 81(3-4):329–375, DEC 2017. doi:10.1007/s10472-017-9560-z. [59] Pavel Surynek. Unifying search-based and compilation-based approaches to multi-agent path ﬁnding through satisﬁability modulo theories."
  },
  {
    "prompt": "Scientific context on FREIBURG, GEB, GERMANY and their role in graph theory.",
    "response": "In S Kraus, editor, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL 28 INTELLIGENCE, pages 1177–1183, ALBERT-LUDWIGS UNIV FREIBURG GEORGES- KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY, 2019. Int Joint Conf Artiﬁcal Intelligence, IJCAI-INT JOINT CONF ARTIF INTELL. 28th International Joint Conference on Artiﬁcial Intelligence, Macao, PEOPLES R CHINA, AUG 10-16, 2019. [60] Pavel Surynek."
  },
  {
    "prompt": "Understanding the computational aspects of IEEE, RSJ, SYSTEMS.",
    "response": "Bounded sub-optimal multi-robot path planning using satisﬁability mod- ulo theory (smt) approach. In 2020 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS), IEEE International Conference on In- telligent Robots and Systems, pages 11631–11637, 345 E 47TH ST, NEW YORK, NY 10017 USA, 2020. IEEE; RSJ, IEEE. IEEE/RSJ International Conference on Intelli- gent Robots and Systems (IROS), ELECTR NETWORK, OCT 24-JAN 24, 2020-2021. doi:10.1109/IROS45743.2020.9341047."
  },
  {
    "prompt": "Read and understand this section about Michalik, Petr, Pavel from a research paper.",
    "response": "[61] Pavel Surynek and Petr Michalik. The joint movement of pebbles in solving the ()-puzzle suboptimally and its applications in rule-based cooperative path-ﬁnding. AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS, 31(3):715–763, MAY 2017. doi:10.1007/s10458-016-9343-7. [62] Pavel Surynek, Roni Stern, Eli Boyarski, and Ariel Felner. Migrating techniques from search- based multi-agent path ﬁnding solvers to sat-based approach. JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH, 73:553–618, 2022."
  },
  {
    "prompt": "Read and understand this section about Mario, Szegedy, Jingjin from a research paper.",
    "response": "[63] Mario Szegedy and Jingjin Yu. Rubik tables and object rearrangement. INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH, 42(6, SI):459–472, MAY 2023. doi:10.1177/02783649211059844. [64] Glenn Wagner and Howie Choset. Subdimensional expansion for multi- robot path planning. ARTIFICIAL INTELLIGENCE, 219:1–24, FEB 2015. doi:10.1016/j.artint.2014.11.001. [65] Ke Wang, Wei Liang, Huaguang Shi, Jialin Zhang, and Qi Wang. Driving line-based two-stage path planning in the agv sorting system."
  },
  {
    "prompt": "Scientific context on NOV, ROBOTICS, SYSTEMS and their role in graph theory.",
    "response": "ROBOTICS AND AUTONOMOUS SYSTEMS, 169, NOV 2023. doi:10.1016/j.robot.2023.104505. [66] Richard M. Wilson. Graph puzzles, homotopy, and the alternating group. J. Comb. Theory, Ser. B, 16:86–96, 1974. doi:10.1016/0095-8956(74)90098-7. [67] Shao-Ci Wu, Wei-Yu Chiu, and Chien-Feng Wu. Deep reinforcement learning for task as- signment and shelf reallocation in smart warehouses. IEEE ACCESS, 12:58915–58926, 2024. doi:10.1109/ACCESS.2024.3392752. [68] Shao-Ci Wu, Wei-Yu Chiu, and Chien-Feng Wu."
  },
  {
    "prompt": "Explain a concept involving doi, IEEE, Deep in computational complexity.",
    "response": "Deep reinforcement learning for task as- signment and shelf reallocation in smart warehouses. IEEE Access, 12:58915–58926, 2024. doi:10.1109/ACCESS.2024.3392752. [69] Jianbin Xin, Xuwen Wu, Andrea D’Ariano, Rudy Negenborn, and Fangfang Zhang. Model predictive path planning of agvs: Mixed logical dynamical formulation and distributed co- ordination. IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, 24(7):6943–6954, JUL 2023. doi:10.1109/TITS.2023.3254147. 29 [70] K. S. Yakovlev, A. A."
  },
  {
    "prompt": "Scientific context on Skrynnik, Panov, Andreychuk and their role in graph theory.",
    "response": "Andreychuk, A. A. Skrynnik, and A. I. Panov. Planning and learning in multi-agent path ﬁnding. DOKLADY MATHEMATICS, 106(SUPPL 1, 1):S79–S84, DEC 2022. doi:10.1134/S1064562422060229. [71] Jingjin Yu. Intractability of optimal multirobot path planning on planar graphs. IEEE ROBOTICS AND AUTOMATION LETTERS, 1(1):33–40, JAN 2016. doi:10.1109/LRA.2015.2503143. [72] Jingjin Yu. Constant-factor time-optimal multi-robot routing on high-dimensional grids."
  },
  {
    "prompt": "Scientific context on CAMBRIDGE, USA, ROBOTICS and their role in graph theory.",
    "response": "In H KressGazit, S Srinivasa, T Howard, and N Atanasov, editors, ROBOTICS: SCIENCE AND SYSTEMS XIV, ONE ROGERS ST, CAMBRIDGE, MA 02142 USA, 2018. MIT PRESS. 14th Conference on Robotics - Science and Systems, Carnegie Mellon Univ, Pittsburgh, PA, JUN 26-30, 2018. [73] Jingjin Yu and Steven M. LaValle. Optimal multirobot path planning on graphs: Complete algorithms and eﬀective heuristics. IEEE TRANSACTIONS ON ROBOTICS, 32(5):1163– 1177, OCT 2016. doi:10.1109/TRO.2016.2593448."
  },
  {
    "prompt": "This is a technical discussion about Jingjin, Rus, Daniela in theoretical computer science.",
    "response": "[74] Jingjin Yu and Daniela Rus. Pebble motion on graphs with rotations: Eﬃcient feasibility tests and planning algorithms. In HL Akin, NM Amato, V Isler, and AF VanDerStappen, ed- itors, ALGORITHMIC FOUNDATIONS OF ROBOTICS XI, volume 107 of Springer Tracts in Advanced Robotics, pages 729–746, HEIDELBERGER PLATZ 3, D-14197 BERLIN, GER- MANY, 2015. Springer Tracts Adv Robot, SPRINGER-VERLAG BERLIN."
  },
  {
    "prompt": "Scientific context on WAFR, Istanbul, TURKEY and their role in graph theory.",
    "response": "11th Workshop on Algorithmic Foundations of Robotics (WAFR), Bogazici Univ, Istanbul, TURKEY, AUG 03-05, 2014. doi:10.1007/978-3-319-16595-0\\_42. 30"
  },
  {
    "prompt": "This is a technical discussion about Mar, USA, PPH in theoretical computer science.",
    "response": "arXiv:2503.17844v1 [cs.CC] 22 Mar 2025 1 Privacy-Preserving Hamming Distance Computation with Property-Preserving Hashing Dongfang Zhao University of Washington, USA dzhao@cs.washington.edu Abstract We study the problem of approximating Hamming distance in sublinear time under property-preserving hashing (PPH), where only hashed representations of inputs are available."
  },
  {
    "prompt": "Scientific context on Larsen, EUROCRYPT, Fleischhacker and their role in graph theory.",
    "response": "Building on the threshold evaluation framework of Fleischhacker, Larsen, and Simkin (EUROCRYPT 2022), we present a sequence of constructions with progressively improved complexity: a baseline binary search algorithm, a reﬁned variant with constant repetition per query, and a novel hash design that enables constant-time approximation without oracle access."
  },
  {
    "prompt": "Scientific context on guarantees, bridging, estimation and their role in graph theory.",
    "response": "Our results demonstrate that approximate distance recovery is possible under strong cryptographic guarantees, bridging efﬁciency and security in similarity estimation. I. INTRODUCTION Estimating similarity between data points lies at the heart of numerous algorithmic tasks, from nearest-neighbor search to clustering, learning, and data deduplication. In many such applications, computing distances directly may be infeasible due to performance or privacy constraints."
  },
  {
    "prompt": "Explain a concept involving Hashing, LSH, approximations in computational complexity.",
    "response": "This challenge has sparked a rich line of research on hashing-based approximations, most notably in the form of Locality-Sensitive Hashing (LSH) [IM98], [KOR00], [OWZ11], which enables sublinear-time similarity search by correlating hash collisions with proximity. However, LSH techniques are typically heuristic and fragile under adversarial manipulation, limiting their applicability in cryptographic or privacy-sensitive contexts."
  },
  {
    "prompt": "Explain a concept involving PPH, LaVigne, Hashing in computational complexity.",
    "response": "To address these limitations, the framework of Property-Preserving Hashing (PPH) was introduced by Boyle, LaVigne, and Vaikuntanathan [BLV19], initiating a program to design hash functions that retain speciﬁc structural properties of the input—such as Hamming distance—while offering provable security guarantees."
  },
  {
    "prompt": "This is a technical discussion about Simkin, Larsen, Fleischhacker in theoretical computer science.",
    "response": "This line of work was extended by Fleischhacker and Simkin [FS21] to exact distance predicates, culminating in the recent construction by Fleischhacker, Larsen, and Simkin [FLS22], who gave the ﬁrst PPH for threshold-Hamming comparison from standard cryptographic assumptions. Their approach combines robust set encodings with a non-interactive evaluation protocol, enabling binary distance comparisons between hashes while maintaining indistinguishability."
  },
  {
    "prompt": "Scientific context on Hamming, advances, prior and their role in graph theory.",
    "response": "Despite these advances, prior work has focused primarily on decisional predicates, such as testing whether the Hamming distance exceeds a ﬁxed threshold. The question of whether one can efﬁciently estimate the distance itself—particularly in sublinear time and under strong cryptographic guarantees—remains largely unexplored. In this paper, we initiate a systematic study of approximate Hamming distance computation under PPH."
  },
  {
    "prompt": "Scientific context on outputs, Binary, Search and their role in graph theory.",
    "response": "Our goal is to determine how much information about the distance can be efﬁciently and securely extracted from hash outputs, and what algorithmic mechanisms allow such recovery. We present three contributions, each building on and extending the threshold-Hamming framework of [FLS22]: • Binary Search over Thresholds. We ﬁrst show that repeated calls to the evaluation predicate enable approx- imate recovery of the Hamming distance via binary search."
  },
  {
    "prompt": "Explain a concept involving log, threshold, queries in computational complexity.",
    "response": "This naive baseline requires O(log n) threshold queries, each revealing a single bit of information. While simple, this approach incurs cumulative error and quadratic overhead under standard ampliﬁcation. • Optimized Evaluation with Constant Repetition. We then reﬁne the search algorithm by analyzing the error structure of the evaluation predicate."
  },
  {
    "prompt": "Explain a concept involving threshold, small, constant in computational complexity.",
    "response": "By exploiting its statistical reliability away from the transition threshold, we demonstrate that a small, constant number of repetitions per query sufﬁces to suppress cumulative error. This reduces the total query complexity to O(log n), without compromising correctness or security. • Constant-Time Distance Estimation. Finally, we propose a new PPH construction that encodes distance directly into the hash output."
  },
  {
    "prompt": "Understanding the computational aspects of Bloom, Hamming, Inspired.",
    "response": "Inspired by Bloom ﬁlter techniques [GM11], our scheme avoids interaction and supports constant-time approximation of Hamming distance with high statistical accuracy. The construction is provably indistinguishable and signiﬁcantly improves computational efﬁciency. Our results demonstrate that approximate Hamming distance can be computed securely in sublinear—or even constant—time from property-preserving hashes."
  },
  {
    "prompt": "Explain a concept involving approx, computation, imate in computational complexity.",
    "response": "This opens new avenues for efﬁcient secure computation, approx- imate data retrieval, and privacy-preserving analytics, and bridges the gap between algorithmic similarity search and cryptographic functionality-preserving compression. II. PRELIMINARIES AND RELATED WORK Let a, b ∈{0, 1}n be two binary strings of length n. The Hamming distance between a and b is deﬁned as dH(a, b) = n X i=1 1ai̸=bi, where 1ai̸=bi is the indicator function."
  },
  {
    "prompt": "Scientific context on PPH, function, hash and their role in graph theory.",
    "response": "Our objective is to approximate dH(a, b) in sublinear time using only their hashed representations under a public property-preserving hash (PPH) function. We now review foundational and related work in three areas. A. Property-Preserving Hashing and Hamming Distance The notion of Property-Preserving Hashing was introduced by Boyle, LaVigne, and Vaikuntanathan [BLV19], who showed how to preserve speciﬁc predicates such as gap-Hamming distance through compact encodings."
  },
  {
    "prompt": "Scientific context on Simkin, Hamming, Fleischhacker and their role in graph theory.",
    "response": "Fleischhacker and Simkin [FS21] extended this line of work to exact Hamming distance. The construction by Fleischhacker, Larsen, and Simkin [FLS22] (FLS22) gave the ﬁrst PPH for the threshold-Hamming predicate from standard assumptions, using robust set encodings and probabilistic evaluation functions. Beyond PPH, the locality-sensitive hashing (LSH) paradigm [IM98], [KOR00], [OWZ11] offers approximate similarity search via hash collisions."
  },
  {
    "prompt": "Read and understand this section about LSH, standard, predicates from a research paper.",
    "response": "However, standard LSH schemes lack adversarial robustness and do not support threshold predicates. Variants such as asymmetric similarity search [MNSW98] and streaming-based similarity estimation [AMS96] offer additional perspectives but remain unsuitable in adversarial models. Our work builds on the security guarantees of PPH constructions while improving computational efﬁciency for Hamming estimation. B."
  },
  {
    "prompt": "Understanding the computational aspects of Recovery, Difference, construction.",
    "response": "Robust Encodings and Set Difference Recovery The FLS22 construction builds on robust encodings of sets with bounded difference. Invertible Bloom Lookup Tables (IBLTs) [GM11] are central to this approach, supporting set reconciliation under noise. Additional techniques include list-decodable codes [GR09], [GGM10], [BOZ82], robust streaming under adversarial access [MNS08], [HW13], [NY15], [BJWY20], and secure difference encoding with low error [DORS08]."
  },
  {
    "prompt": "Read and understand this section about Bloom, ﬁlter, testing from a research paper.",
    "response": "The Bloom ﬁlter [Blo70], while originally designed for approximate membership testing, underpins many of these constructions and remains fundamental to compact hashing. Variants such as compressed sensing [Don06] and robust sparse signal recovery [BY20] also inform the information-theoretic limits of reconstruction from lossy encodings. Our proposed modiﬁcations retain the decoding framework while augmenting it with decodable statistical signals that enable constant-time estimation. C."
  },
  {
    "prompt": "Understanding the computational aspects of Hashing, Property-preserving, security.",
    "response": "Cryptographic Hashing and Indistinguishability Property-preserving hashing aims to balance functionality and security. Standard cryptographic hash functions (e.g., collision-resistant constructions [Ped92]) provide strong privacy but no semantic structure. In contrast, PPH functions deliberately encode semantic information and require formal indistinguishability guarantees, typically deﬁned via simulation or total variation bounds."
  },
  {
    "prompt": "Explain a concept involving proved, instance, assumptions in computational complexity.",
    "response": "FLS22 proved security in the presence of a single hash function instance, under standard hardness assumptions. Recent works have analyzed the leakage proﬁles of probabilistic data structures under adversarial models [CPS19], [RRR21], and studied how small changes in encoding distributions affect distinguishability [CN22]. Our modiﬁca- tions maintain this security by bounding the statistical distance between original and modiﬁed encodings."
  },
  {
    "prompt": "This is a technical discussion about LWE, SIS, addition in theoretical computer science.",
    "response": "In addition, recent cryptographic reductions (e.g., LWE/SIS [MP13]) and lattice-based indistinguishability proofs [LLL82] offer broader theoretical tools that inspire our security reasoning. 2 III. POLYLOGARITHMIC HAMMING COMPUTATION FROM THRESHOLD HAMMING-PPH A. Binary Search Algorithm We describe a simple method to estimate dH(a, b) using black-box access to the threshold predicate Eval(h(a), h(b), t). The algorithm performs binary search over t ∈{0, 1, . ."
  },
  {
    "prompt": "Understanding the computational aspects of Eval, reﬁning, search.",
    "response": "., n}, reﬁning the search interval based on the response of Eval."
  },
  {
    "prompt": "This is a technical discussion about tmid, Eval, tmax in theoretical computer science.",
    "response": "Algorithm 1 Binary Search Approximate Hamming Distance 1: Input: Hash values h(a), h(b); oracle access to Eval(h(a), h(b), t) 2: Output: Estimated Hamming distance ˜d 3: tmin ←0, tmax ←n 4: while tmin < tmax do 5: tmid ←⌊(tmin + tmax)/2⌋ 6: if Eval(h(a), h(b), tmid) = 1 then 7: tmin ←tmid + 1 8: else 9: tmax ←tmid 10: end if 11: end while 12: return tmin The algorithm terminates with tmin = tmax, and returns the smallest threshold t such that Eval(h(a), h(b), t) = 0."
  },
  {
    "prompt": "Read and understand this section about conditions, ideal, corresponds from a research paper.",
    "response": "Under ideal conditions, this corresponds to the true value of dH(a, b). The number of oracle queries is bounded by ⌈log2(n+1)⌉, since the search interval is halved in each iteration. Each invocation of Eval reveals only the outcome of a single threshold comparison—namely, whether dH(a, b) > t—and does not leak any other information about the inputs. This restricted model necessitates the use of adaptive querying to recover the approximate distance."
  },
  {
    "prompt": "Explain a concept involving Eval, error, subsection in computational complexity.",
    "response": "In the next subsection, we analyze how error in Eval propagates through the binary search procedure and quantify its impact on the returned estimate. B. Error Growth in Iterative Queries The correctness of Algorithm 1 depends critically on the reliability of the threshold predicate Eval(h(a), h(b), t). In the construction of Fleischhacker, Larsen, and Simkin [FLS22], this predicate is implemented via randomized encodings and supports only approximate evaluation. For any ﬁxed threshold t ∈{0, . ."
  },
  {
    "prompt": "Scientific context on Eval, Hamming, denotes and their role in graph theory.",
    "response": "., n}, the predicate satisﬁes Pr[Eval(h(a), h(b), t) = 1dH(a,b)>t] ≥1 −δ, where δ ∈(0, 1/2) is the maximum per-call error probability and dH(a, b) denotes the Hamming distance between a and b. Algorithm 1 performs up to ⌈log2(n + 1)⌉adaptive queries to Eval, with the threshold values chosen based on earlier responses. The sequential nature of these queries raises the possibility of error propagation."
  },
  {
    "prompt": "Scientific context on log, multiple, query and their role in graph theory.",
    "response": "In particular, even if each query fails with probability at most δ, multiple queries may compound into a global error. If all Eval invocations were independent, a union bound would yield: Pr[any query fails] ≤log n · δ. To ensure a global failure probability of at most ε, this would require δ ≤ε/ log n. However, [FLS22] implements threshold evaluation using shared encodings of the input, and decoding errors may be correlated across thresholds."
  },
  {
    "prompt": "Understanding the computational aspects of failure, correctness, i-th.",
    "response": "Let Si ∈{0, 1} denote the correctness of the i-th threshold query, where Si = 1 indicates success and Si = 0 indicates failure. The sequence (S1, . . . , Sk) may exhibit statistical dependence due to common decoding artifacts. To mitigate this issue, we apply an ampliﬁcation strategy: each Eval query is repeated k times independently, and the majority vote is returned. Let δ′ denote the error probability after ampliﬁcation."
  },
  {
    "prompt": "Understanding the computational aspects of Chernoff, log, bound.",
    "response": "Assuming independence among repetitions, a standard Chernoff bound gives: δ′ ≤exp \u0000−2k(1/2 −δ)2\u0001 . 3 Let ε ∈(0, 1) denote the desired total error bound for the binary search algorithm. Since the number of threshold evaluations is at most log n, it sufﬁces to require: log n · δ′ ≤ε. Solving for k yields: k ≥ 1 2(1/2 −δ)2 · \u0012 log log n + log 1 ε \u0013 ."
  },
  {
    "prompt": "This is a technical discussion about log, poly, error in theoretical computer science.",
    "response": "Thus, the total number of oracle calls becomes: O(k log n) = O \u0012log n(log log n + log(1/ε)) (1/2 −δ)2 \u0013 , which remains sublinear for any inverse-polynomial ε ≥1/poly(n). This shows that accurate approximation is still feasible under probabilistic evaluation, provided that per-query error is sufﬁciently ampliﬁed. If both the per-query error rate δ and the overall error bound ε are negligible functions in n, as is standard in cryptographic applications, the expression for k can be simpliﬁed."
  },
  {
    "prompt": "Explain a concept involving log, Speciﬁcally, term in computational complexity.",
    "response": "Speciﬁcally, since log(1/ε) = Θ(log n) and log log n = o(log n), we have: log log n + log 1 ε = Θ(log n). Furthermore, as δ →0, the term (1/2 −δ)2 = Θ(1). Substituting into the expression for the total number of oracle calls, O \u0012log n(log log n + log(1/ε)) (1/2 −δ)2 \u0013 , we obtain: O(log2 n). Hence, under negligible-error assumptions, the binary search algorithm requires only polylogarithmic overhead while maintaining correctness. IV."
  },
  {
    "prompt": "Read and understand this section about APPROXIMATION, SCHEME, AMPLIFICATION from a research paper.",
    "response": "A LOGARITHMIC-TIME APPROXIMATION SCHEME WITHOUT AMPLIFICATION A. Problem Setup and Motivation In Section III, we presented a binary-search-based method for approximating the Hamming distance using a threshold property-preserving hash function (PPH). While this construction achieves correctness with negligible error probability, it relies on ampliﬁcation to suppress the per-query error rate."
  },
  {
    "prompt": "Explain a concept involving Eval, query, Speciﬁcally in computational complexity.",
    "response": "Speciﬁcally, each threshold query Eval(h(a), h(b), t) is repeated k = Θ(log n) times, resulting in a total query complexity of O(log2 n). This raises the natural question: can we eliminate the ampliﬁcation step while still retaining sublinear complexity and negligible error? In this section, we investigate the possibility of directly using the threshold predicate without repetition—that is, invoking Eval(h(a), h(b), t) only a constant number of times per threshold in the binary search."
  },
  {
    "prompt": "Scientific context on log, Eval, error and their role in graph theory.",
    "response": "At ﬁrst glance, this may seem to introduce unacceptable error accumulation: without ampliﬁcation, the overall failure probability becomes ε = log n·δ, where δ is the error of a single Eval call. However, this bound can still be negligible provided that δ itself is sufﬁciently small. In particular, if the underlying PPH construction (e.g., [FLS22]) admits instantiation with cryptographic parameters such that δ = O(n−c) for some constant c > 1, then ε = log n · δ = O \u0012log n nc \u0013 = negl(n)."
  },
  {
    "prompt": "Scientific context on reliability, enabling, log and their role in graph theory.",
    "response": "This observation leads to a new regime of approximation in which we trade off ampliﬁcation cost for stronger per-query reliability, enabling a total complexity of O(log n) without degrading correctness. In the following subsection, we formalize this simpliﬁed algorithm and analyze its error behavior under mild cryptographic assumptions on the base construction. 4 B."
  },
  {
    "prompt": "Explain a concept involving Constant, Repetition, Search in computational complexity.",
    "response": "Binary Search with Constant Repetition While the baseline construction in Section III requires logarithmic repetition per threshold to ensure correctness, we now show that the number of repetitions can be reduced to a small constant under mild structural constraints. This substantially improves efﬁciency, reducing the total query complexity from O(log2 n) to O(log n). Our insight builds on the internal structure of the FLS22 threshold predicate [FLS22]."
  },
  {
    "prompt": "Explain a concept involving construction, evaluation, error in computational complexity.",
    "response": "In their construction, the evaluation error is not uniformly distributed across all thresholds; instead, it is concentrated near the critical transition region where t ≈dH(a, b). When the queried threshold t is signiﬁcantly above or below the true Hamming distance, the outcome of Eval(h(a), h(b), t) is highly reliable—often correct with overwhelming probability, even without ampliﬁcation. We exploit this non-uniformity by carefully controlling the search trajectory."
  },
  {
    "prompt": "Explain a concept involving transition, region, binary in computational complexity.",
    "response": "In particular, the binary search algorithm begins with coarse estimates of t and only gradually approaches the transition region. This ensures that the majority of queries are issued at thresholds satisfying |t −dH(a, b)| > τ, where τ is the width of the transition band in which the predicate becomes unreliable. For such thresholds, the predicate behaves almost deterministically. To formalize this, we introduce a piecewise error model."
  },
  {
    "prompt": "Explain a concept involving Eval, δmax, denote in computational complexity.",
    "response": "Let δ(t) denote the error probability of Eval(h(a), h(b), t). We assume the existence of a transition width parameter τ ∈N such that: δ(t) ≤ ( δmax if |t −dH(a, b)| ≤τ, δfar if |t −dH(a, b)| > τ, where δmax < 1 2 −γ for some constant γ > 0, and δfar ≪δmax. Intuitively, δmax bounds the uncertainty near the threshold, while δfar accounts for negligible ﬂuctuations far from the decision boundary."
  },
  {
    "prompt": "Understanding the computational aspects of encoding, framework, families.",
    "response": "In the FLS22 encoding framework, the value of τ can be made constant (e.g., τ = 1 or 2) by tuning the robustness of the set-difference encoding and controlling the overlap structure of the underlying families X0, X1. This adjustment affects only the decoding ambiguity and has no impact on the hash output distribution. Let Πorig denote the original FLS22 construction, and Πτ denote our modiﬁed instantiation with reduced overlap to restrict the transition width to constant τ."
  },
  {
    "prompt": "Understanding the computational aspects of scheme, Πorig, formalize.",
    "response": "We now formalize that this modiﬁcation preserves the indistinguishability guarantees of the original scheme. Lemma 1. For any probabilistic polynomial-time adversary A, its advantage in distinguishing Πτ from Πorig is negligible: |Pr[A(hτ(x)) = 1] −Pr[A(horig(x)) = 1]| = negl(n). Proof Sketch. We consider the standard IND-style security game where the adversary receives a hash value of a challenge input x, produced either using Πτ or Πorig, and must guess which construction was used."
  },
  {
    "prompt": "Scientific context on Dorig, distributions, distance and their role in graph theory.",
    "response": "Let Dτ and Dorig denote the respective output distributions. The adversary’s advantage is bounded by the total variation distance: AdvA ≤∆(Dτ, Dorig). To bound this distance, we note that both constructions encode each bit as a random subset drawn from set families X0, X1 ⊆[N], where N = Θ(λ). The only difference is that Πτ reduces the overlap between X0 and X1, thereby increasing decoding robustness."
  },
  {
    "prompt": "Read and understand this section about proﬁle, universe, affect from a research paper.",
    "response": "This change does not affect the leakage proﬁle, as the output distribution remains randomized over the same universe. We employ a hybrid argument: let h(0)(x), h(1)(x), . . . , h(λ)(x) be a sequence where h(i)(x) uses the modiﬁed encoding for the ﬁrst i bits and the original encoding for the remaining λ −i bits. Then, ∆(Dτ, Dorig) ≤ λ X i=1 ∆(h(i)(x), h(i−1)(x)). Each hybrid transition changes one encoded bit from the original to the modiﬁed version."
  },
  {
    "prompt": "This is a technical discussion about exp, mass, overlap in theoretical computer science.",
    "response": "Because the overlap reduction shifts only a small fraction of probability mass, we have ∆(h(i)(x), h(i−1)(x)) ≤exp(−cλ), for some constant c > 0. Summing over all λ positions gives ∆(Dτ, Dorig) ≤λ · exp(−cλ) = exp(−cλ + log λ) = exp(−Ω(λ)), 5 where we use the fact that log λ = o(λ), so the exponent remains −Ω(λ). Since λ = Θ(n), the statistical distance is negligible in n, completing the proof."
  },
  {
    "prompt": "This is a technical discussion about established, repetition, indistinguishability in theoretical computer science.",
    "response": "With indistinguishability established, we now analyze the failure probability of binary search with constant repetition. Each threshold evaluation is repeated k ∈O(1) times and the majority vote is taken. By the Chernoff bound, the effective error after ampliﬁcation is: δ′(t) ≤exp \u0000−2k(1/2 −δ(t))2\u0001 . Only a constant number of thresholds fall within the unreliable region |t −dH(a, b)| ≤τ, while the remaining O(log n) thresholds lie in the stable region."
  },
  {
    "prompt": "Explain a concept involving exp, constant, log in computational complexity.",
    "response": "Thus, the total failure probability is bounded by: ε = log n X i=1 δ′(ti) ≤2τ · exp \u0000−2kγ2\u0001 + o(1), which is negligible in n for constant τ and sufﬁciently large constant k. For example, if δmax ≤1/4 and τ ≤2, setting k = 5 yields ε ≤4 · exp(−2k · (1/4)2) = 4 · exp(−k/8), which is below 1/nc for moderate n and any desired constant c."
  },
  {
    "prompt": "Read and understand this section about assumptions, repetition, predicate from a research paper.",
    "response": "Thus, by leveraging the error structure of the threshold predicate and avoiding worst-case uniformity assumptions, we derive a logarithmic-time algorithm with constant repetition. This construction shows that full ampliﬁcation is unnecessary: a reﬁned understanding of the predicate’s internal geometry yields near-optimal efﬁciency with no sacriﬁce in correctness or security. C."
  },
  {
    "prompt": "Explain a concept involving Complexity, Trade-off, Accuracy in computational complexity.",
    "response": "Accuracy–Complexity Trade-off The binary search construction with constant repetition achieves a signiﬁcant improvement in query complex- ity—from O(log2 n) to O(log n)—by exploiting the structure of the threshold predicate and its asymmetric error proﬁle. This method demonstrates that full ampliﬁcation is not strictly necessary when the evaluation errors are well-behaved and non-uniformly distributed. However, this efﬁciency gain hinges on two key assumptions."
  },
  {
    "prompt": "Understanding the computational aspects of region, Eval, unreliable.",
    "response": "First, it relies on the existence of a narrow transition region τ where the predicate is unreliable, and assumes that Eval behaves almost deterministically outside this region. Second, the algorithm assumes access to a threshold predicate with the speciﬁc structure provided by [FLS22], which supports monotonic and ordered queries over t. This monotonicity is what enables the binary search to minimize the number of interactions."
  },
  {
    "prompt": "Understanding the computational aspects of result, scheme, efﬁcient.",
    "response": "As a result, the current scheme, while efﬁcient, is not interaction-free. It still requires adaptively querying the predicate Eval(h(a), h(b), t) at multiple values of t, and its performance depends on the trajectory of binary search. Moreover, the construction is not directly applicable to more general classes of property-preserving hash functions, particularly those that do not support threshold-style decomposition."
  },
  {
    "prompt": "Understanding the computational aspects of Hamming, time, predicate.",
    "response": "These limitations raise a natural question: can one design a hash function h such that the approximate Hamming distance dH(a, b) can be estimated directly from h(a) and h(b) in constant time, without any interaction or auxiliary predicate? That is, can we construct a property-preserving hashing scheme where distance estimation becomes a pure decoding problem? The remainder of this paper is devoted to addressing this question."
  },
  {
    "prompt": "Explain a concept involving structure, allowing, time in computational complexity.",
    "response": "We propose new constructions that embed approximate Hamming distance into a compact hash structure, allowing it to be recovered in O(1) time with provable guarantees. These results provide a conceptual and technical stepping stone toward fully noninteractive and constant-time PPH schemes. V. CONSTANT-TIME ESTIMATION OF HAMMING DISTANCE A."
  },
  {
    "prompt": "This is a technical discussion about Goals, Technical, Challenges in theoretical computer science.",
    "response": "Design Goals and Technical Challenges Our objective in this section is to design a property-preserving hashing scheme that supports constant-time estimation of Hamming distance."
  },
  {
    "prompt": "Explain a concept involving Dist, function, Speciﬁcally in computational complexity.",
    "response": "Speciﬁcally, we aim to construct a hash function h: {0, 1}n →{0, 1}m and a decoding function Dist: {0, 1}m × {0, 1}m →R such that Dist(h(a), h(b)) ≈dH(a, b) for all a, b ∈{0, 1}n, 6 with approximation error bounded by a negligible or constant additive term, and computational complexity O(1), independent of the input length n."
  },
  {
    "prompt": "Scientific context on Hamming, constructions, schemes and their role in graph theory.",
    "response": "This goal departs sharply from prior constructions, such as the threshold-evaluation-based schemes of FLS22 [FLS22], which require interactive protocols involving O(log2 n) queries to estimate Hamming distance. Those methods treat the hash outputs h(a), h(b) as opaque representations, and rely on auxiliary comparison procedures to test whether dH(a, b) > t for a given threshold t."
  },
  {
    "prompt": "Explain a concept involving secure, implicitly, making in computational complexity.",
    "response": "While efﬁcient and provably secure, such constructions inherently embed distance only implicitly, making direct decoding impossible without repeated predicate invocations. The core challenge we face is structural: the FLS22 construction was fundamentally designed to support robust threshold predicates, not direct metric estimation."
  },
  {
    "prompt": "Explain a concept involving Eval, universe, drawn in computational complexity.",
    "response": "The encoding of each input bit into a random subset of a universe [N], drawn from one of two overlapping families X0, X1, is optimized for enabling differential tests such as Eval(h(a), h(b), t). However, this subset-based design obscures ﬁne-grained information about dH(a, b), since it collapses the actual distance into a one-bit signal. To overcome this, we propose to carefully expose internal structure from within the FLS22 encoding pro- cess—without compromising its security properties."
  },
  {
    "prompt": "This is a technical discussion about Eval, predicate, analytically in theoretical computer science.",
    "response": "Rather than treating Eval as a black-box threshold predicate, we aim to reinterpret the underlying encodings as structured sketches from which distance can be decoded analytically. The central design question becomes Can we design a randomized encoding such that the expected symmetric difference reveals dH(a, b)? This formulation enables the estimator Dist(h(a), h(b)) to return a numerical approximation of Hamming distance, by computing a normalized difference between the hashed subsets."
  },
  {
    "prompt": "Explain a concept involving regularity, interaction, queries in computational complexity.",
    "response": "If the encoding satisﬁes sufﬁcient concentration and statistical regularity, such an estimator may achieve accurate approximation with only constant-time access to the hash values—no interaction, no queries, and no adaptive reﬁnement. The difﬁculty, of course, lies in the trade-off: exposing too much internal structure may leak sensitive infor- mation, weakening the cryptographic guarantees of property-preserving hashing."
  },
  {
    "prompt": "Scientific context on hash, indistinguishability, ensuring and their role in graph theory.",
    "response": "Thus, the design must balance decodability with indistinguishability, ensuring that the hash outputs retain their semantic security while remaining computationally meaningful. In the following subsections, we deconstruct the FLS22 encoding framework, identify the statistical features that correlate with Hamming distance, and introduce our enhanced encoding design that embeds this information directly into the hash output in a secure and analyzable manner. B."
  },
  {
    "prompt": "Read and understand this section about Encoding, Structure, Hamming from a research paper.",
    "response": "Revisiting the FLS22 Encoding Structure The FLS22 construction [FLS22] encodes each input string x ∈{0, 1}n into a subset of a large universe [N], via a randomized mapping that preserves threshold Hamming predicates. Each bit xi is independently encoded as a random subset Si ⊆[N], drawn from one of two families: xi = 0 ⇒Si ∼D0, xi = 1 ⇒Si ∼D1, where D0, D1 are distributions over subsets with overlapping support. The ﬁnal hash value is the union h(x) = n [ i=1 Si."
  },
  {
    "prompt": "Scientific context on Hamming, difference, family and their role in graph theory.",
    "response": "The core idea behind this construction is that the Hamming distance dH(a, b) between two inputs a and b correlates with the expected size of the symmetric difference D(a, b) := h(a)△h(b), since each differing bit contributes a fresh random subset drawn from the opposite family. The more positions where ai ̸= bi, the more disjoint sets enter the union, resulting in a larger difference."
  },
  {
    "prompt": "Explain a concept involving Eval, statistical, behavior in computational complexity.",
    "response": "This statistical behavior underlies the threshold predicate Eval(h(a), h(b), t), which tests whether |D(a, b)| > θ(t) for some calibrated threshold function θ: N →N. However, in the original construction, the value |D(a, b)| is not directly revealed to the evaluator; instead, it is obfuscated and tested via a separate mechanism using set-difference encodings and error-correcting thresholds."
  },
  {
    "prompt": "Scientific context on accessible, symmetric, difference and their role in graph theory.",
    "response": "As such, while the symmetric difference carries latent information about dH(a, b), it is not explicitly accessible. 7 To enable constant-time decoding, we revisit this encoding structure with a new perspective. Suppose we could evaluate—or approximate—the size of D(a, b) directly from h(a) and h(b), without auxiliary tests or interaction."
  },
  {
    "prompt": "Explain a concept involving Lipschitz, well-behaved, afﬁne in computational complexity.",
    "response": "Then, if the mapping from dH(a, b) to E[|D(a, b)|] is well-behaved (e.g., afﬁne or Lipschitz), we could invert it to recover an estimate of dH(a, b) up to additive error. This motivates us to treat the FLS22 encoding not merely as a vehicle for threshold testing, but as a high- dimensional randomized sketch of input structure—one in which Hamming distance is softly embedded in the geometry of the hash subsets."
  },
  {
    "prompt": "Scientific context on concrete, hashes, make and their role in graph theory.",
    "response": "To make this idea concrete, the next subsection develops a modiﬁed encoding and a decoding function that together allow direct estimation of dH(a, b) from their hashes. We remark that the ﬁnal FLS22 construction maps each subset Si into a Bloom ﬁlter representation to ensure ﬁxed-size outputs and facilitate efﬁcient approximate set operations."
  },
  {
    "prompt": "Explain a concept involving Bloom, distance, section in computational complexity.",
    "response": "In this section, we focus on the subset-level structure for clarity; the interaction between Bloom ﬁlters and distance estimation will be addressed in the next subsection. C. Construction: Embedding Distance in Hash Outputs To support constant-time estimation of Hamming distance, we propose a modiﬁed encoding scheme that directly embeds distance information into the hash output."
  },
  {
    "prompt": "Explain a concept involving paradigm, predicate, representation in computational complexity.",
    "response": "Our construction is inspired by the structure of FLS22 [FLS22] but departs from its threshold-evaluation paradigm: instead of repeatedly querying a predicate, we extract approximate distance analytically from the encoded representation. 1) Encoding Scheme: Let x ∈{0, 1}n be an input vector. For each coordinate i ∈[n], we deﬁne two disjoint collections of indices H0(i), H1(i) ⊆[m], where each set is sampled uniformly at random with ﬁxed cardinality r, and H0(i) ∩H1(i) = ∅."
  },
  {
    "prompt": "Explain a concept involving Hxi, deterministic, mapping in computational complexity.",
    "response": "These mapping sets are public and deterministic. The hash output h(x) ∈{0, 1}m is then computed as follows: hj(x) = _ i:j∈Hxi (i) 1. That is, position j in the hash is set to 1 if any coordinate i maps to j via its corresponding family Hxi(i). This generalizes the Bloom ﬁlter idea but enforces disjoint support between encodings of 0 and 1, yielding a sharper statistical separation. 2) Estimator and Expected Value: Let a, b ∈{0, 1}n be two inputs, and deﬁne D(a, b) = m X j=1 (hj(a) ⊕hj(b))."
  },
  {
    "prompt": "Read and understand this section about differ, positions, raw from a research paper.",
    "response": "This raw symmetric difference reﬂects the number of positions where the hash outputs differ. Since each differing coordinate i contributes 2r positions (due to disjoint support), and overlapping encodings across coordinates may cause collisions, we deﬁne a normalizing factor: α := 2r(1 −ρ), where ρ ∈[0, 1) denotes the expected fraction of overlaps between independently chosen mapping sets."
  },
  {
    "prompt": "Scientific context on Dist, expectation, distance and their role in graph theory.",
    "response": "Our distance estimator is given by: Dist(h(a), h(b)) := 1 α · D(a, b), with expectation: E[Dist(h(a), h(b))] = dH(a, b). 3) Correctness and Concentration: The output bits hj(a), hj(b) are each determined by independent random insertions, so the sum D(a, b) is concentrated around its mean. By standard Chernoff bounds, for any ε > 0, Pr [|Dist(h(a), h(b)) −dH(a, b)| > εn] ≤exp(−Θ(ε2n)). Thus, the estimator achieves additive εn error with high probability in constant time."
  },
  {
    "prompt": "Scientific context on Hash, Compression, Length and their role in graph theory.",
    "response": "8 4) Hash Length and Compression: Since each input bit contributes to r positions, the total number of insertions is nr. If mappings are fully disjoint, then m ≥nr. However, allowing controlled overlaps improves entropy diffusion and space efﬁciency. We choose m = Θ(n log n) and r = Θ(log n), consistent with FLS22, balancing output compactness and concentration quality."
  },
  {
    "prompt": "Read and understand this section about Output, Estimation, Hash from a research paper.",
    "response": "This regime guarantees: (1) Hash length remains sublinear in the input domain, supporting compression; (2) Output bits retain high entropy, preventing input leakage; (3) Estimation remains sharp with provable guarantees. 5) Computational Efﬁciency: Hash computation takes O(nr) time, and the estimator Dist(h(a), h(b)) runs in O(m)."
  },
  {
    "prompt": "Explain a concept involving cost, complexity, applications in computational complexity.",
    "response": "To further reduce evaluation cost, one can subsample a constant number of hash positions and compute an unbiased estimator with larger variance but lower complexity—useful in time-critical applications. 6) Security Considerations: Although the estimator reveals approximate distance, the hash remains lossy and randomized. For uniformly distributed inputs, the output distribution is statistically close to uniform over bounded- weight bitstrings in {0, 1}m."
  },
  {
    "prompt": "This is a technical discussion about bit, subsets, coordinate in theoretical computer science.",
    "response": "Since each output bit is affected by multiple random subsets, no single bit leaks information about a speciﬁc coordinate. We defer formal indistinguishability proofs and adversarial advantage bounds to the next section. D. Security Analysis 1) Security Deﬁnition: We adopt the standard indistinguishability-based formulation of property-preserving hash- ing (PPH), as introduced in [FLS22]. Let h : {0, 1}n →{0, 1}m be a randomized hash function."
  },
  {
    "prompt": "Explain a concept involving Hamming, property, distance in computational complexity.",
    "response": "The security goal is to ensure that, even if h approximately preserves a property (e.g., Hamming distance), it does not leak additional information about the input. Formally, let A be a probabilistic polynomial-time adversary. Consider the following indistinguishability game between A and a challenger: • The challenger samples a bit b ←{0, 1}, and then: – If b = 0, it samples x ←{0, 1}n uniformly at random."
  },
  {
    "prompt": "Explain a concept involving subject, constraints, samples in computational complexity.",
    "response": "– If b = 1, it samples x ←D, for some distribution D chosen by A (subject to min-entropy constraints). • The challenger computes h(x) and sends it to A. • The adversary outputs a guess b′ ∈{0, 1}. The adversary’s advantage is deﬁned as: AdvA = Pr[b′ = b] −1 2 . We say that the hash function h satisﬁes distributional indistinguishability if, for all PPT adversaries A, this advantage is negligible in the security parameter λ, assuming that D has min-entropy at least λ."
  },
  {
    "prompt": "Explain a concept involving Intuitively, reveals, property in computational complexity.",
    "response": "Intuitively, this captures that the hash output h(x) reveals no more than what is implied by the preserved property (in our case, approximate distance), and does not enable recovery or signiﬁcant inference about the input x. 2) Game-Based Indistinguishability: To formalize security under our constant-time estimator, we instantiate the above deﬁnition with our distance-preserving hash h : {0, 1}n →{0, 1}m constructed in Section V-C."
  },
  {
    "prompt": "Understanding the computational aspects of Hamming, remains, input.",
    "response": "The goal is to show that, despite enabling estimation of dH(a, b), the hash output h(x) remains computationally indistinguishable from one generated using uniformly random input, except for information implied by the approximate Hamming distance itself. Let A be any PPT adversary participating in the following game: • The challenger chooses a secret bit b ∈{0, 1}. • If b = 0: sample x ←{0, 1}n uniformly at random."
  },
  {
    "prompt": "Read and understand this section about sample, starts, adversary from a research paper.",
    "response": "If b = 1: sample x ←D, a distribution selected by A (but ﬁxed before the game starts), with min-entropy at least λ. • The challenger computes y ←h(x) and sends y to A. • The adversary outputs a guess b′ ∈{0, 1}. As before, the adversary’s advantage is deﬁned as AdvA := Pr[b′ = b] −1 2 . 9 Our aim is to prove that AdvA ≤negl(λ), meaning the adversary cannot distinguish whether the input was drawn from D or uniform, even after observing h(x)."
  },
  {
    "prompt": "Explain a concept involving Hamming, entropy, hybrid in computational complexity.",
    "response": "The only information leaked is an approximation of pairwise Hamming distance between inputs, which is insufﬁcient to recover x when D has sufﬁcient entropy. We will use a hybrid argument to show this, reducing the distinguishing advantage to a sequence of negligible differences induced by local randomizations in the encoding structure. 3) Hybrid Argument and Output Distribution: Let h(x) ∈{0, 1}m be the hash output under our construction for input x ∈{0, 1}n."
  },
  {
    "prompt": "This is a technical discussion about families, setup, bit in theoretical computer science.",
    "response": "The output h(x) is generated by inserting bit indices into m positions based on the randomized families H0(i), H1(i) ⊆[m], which are public and ﬁxed at setup. Let r be the number of indices assigned per input bit, and assume r = Θ(log n), m = Θ(n log n). To prove indistinguishability, we compare the distribution of h(x) when x ←{0, 1}n (uniform) versus x ←D, where D is any distribution over {0, 1}n with min-entropy at least λ."
  },
  {
    "prompt": "Read and understand this section about show, statistical, distance from a research paper.",
    "response": "We show that the statistical distance between the output distributions is negligible in λ, using a hybrid argument over the bit-level encodings of x. Let h(0)(x), h(1)(x), . . ."
  },
  {
    "prompt": "Explain a concept involving bits, replaced, bit in computational complexity.",
    "response": ", h(n)(x) be a hybrid sequence such that: (i) In h(i)(x), the ﬁrst i bits of x are replaced with uniformly random bits, and the remaining n−i bits are drawn from the original input x ∼D, and (ii) The hash output h(i)(x) is computed using the same deterministic families Hb(j), but with randomized encodings depending on whether bit j was replaced. Note that we have: ∆(h(0)(x), h(n)(x)) ≤ n X i=1 ∆(h(i)(x), h(i−1)(x)), where ∆denotes total variation distance."
  },
  {
    "prompt": "This is a technical discussion about bit, transition, i-th in theoretical computer science.",
    "response": "We now analyze each transition ∆(h(i)(x), h(i−1)(x)). The only difference between these two hybrids is the encoding of the i-th bit: in h(i−1)(x), the bit is drawn from D; in h(i)(x), it is replaced with a uniformly random bit. Let us denote the contribution of the i-th bit to the hash output as a binary vector vi ∈{0, 1}m, where: (vi)j = ( 1 if j ∈Hxi(i), 0 otherwise."
  },
  {
    "prompt": "This is a technical discussion about distribution, Replacing, probability in theoretical computer science.",
    "response": "Replacing xi with a random bit b ∈{0, 1} changes this distribution to: P[(vi)j = 1] = r m, for each j ∈H0(i) ∪H1(i), with probability 1/2 for each of H0(i), H1(i). Since H0(i) and H1(i) are disjoint, and the supports are randomized across different coordinates, we can bound the statistical distance between the two encodings as follows. Let µi be the distribution over the positions of vi under xi ∼D, and let νi be the distribution under xi ∼Unif({0, 1})."
  },
  {
    "prompt": "Understanding the computational aspects of max, min-entropy, Summing.",
    "response": "Then: ∆(µi, νi) ≤max b∈{0,1} P[xi = b | x ∼D] −1 2 · ∥Hb(i)∥1. By assumption, the marginal bias |P[xi = b] −1/2| is bounded for each i, since D has min-entropy at least λ. More formally, for any i ∈[n], the min-entropy constraint implies: P[xi = b] ≤1 −1 2λ . Thus, the per-bit variation from uniform is at most ε := 1 2λ , and the contribution to the total distance is: ∆(h(i)(x), h(i−1)(x)) ≤r · ε = r 2λ . Summing over all n hybrids: ∆(h(0)(x), h(n)(x)) ≤ n X i=1 r 2λ = nr 2λ ."
  },
  {
    "prompt": "This is a technical discussion about Unif, log, Finally in theoretical computer science.",
    "response": "Finally, since r = Θ(log n), and λ = ω(log n), we conclude that: ∆(h(x)x∼D, h(x)x∼Unif) = negl(λ). This completes the indistinguishability proof: the adversary cannot distinguish the distribution of hash outputs under x ∼D from that under x ∼Unif, except with negligible probability. 10 VI. CONCLUSION AND OPEN PROBLEMS We proposed a new line of sublinear-time algorithms for estimating the Hamming distance between binary vectors in the property-preserving hashing (PPH) model."
  },
  {
    "prompt": "Explain a concept involving Binary, Search, Hamming in computational complexity.",
    "response": "Our contributions consist of three constructions with increasingly stronger efﬁciency guarantees: • Polylogarithmic-Time via Binary Search: We ﬁrst demonstrated that, by leveraging the threshold evaluation primitive from [FLS22], one can approximate Hamming distance using a binary search strategy. This yields an estimator with O(log n) query complexity, but to ensure negligible error, each threshold query must be ampliﬁed via O(log n) repetitions, leading to an overall complexity of O(log2 n)."
  },
  {
    "prompt": "Explain a concept involving Constant, encoding, speciﬁcally in computational complexity.",
    "response": "• Logarithmic-Time with Constant Repetition: Under a structural reﬁnement of the FLS22 encoding—speciﬁcally, assuming bounded-width transition bands and non-uniform error distribution across thresholds—we showed that only a constant number of repetitions per query sufﬁces to suppress the overall error. This reduces the total complexity to O(log n), while maintaining correctness and indistinguishability. The analysis relies on a piecewise error model and a tailored hybrid argument."
  },
  {
    "prompt": "Scientific context on Finally, Encodings, Embedded and their role in graph theory.",
    "response": "• Constant-Time Estimation via Embedded Encodings: Finally, we introduced a new PPH construction that embeds distance information directly into the hash output. This enables constant-time estimation with additive approximation guarantees and exponentially small error. Unlike prior constructions, our scheme eliminates threshold evaluation altogether, while preserving cryptographic security."
  },
  {
    "prompt": "Read and understand this section about efﬁciency, sumptions, accuracy from a research paper.",
    "response": "Each construction captures a different point in the trade-off space between accuracy, efﬁciency, and structural as- sumptions. Our techniques highlight the interplay between encoding design, statistical concentration, and adversarial indistinguishability. Our work raises several theoretical directions. Can these techniques be extended to other distance metrics (e.g., edit distance, Jaccard distance)? Is it possible to generalize our constant-time scheme to support dynamic or streaming inputs?"
  },
  {
    "prompt": "Understanding the computational aspects of Finally, PPH, optimal.",
    "response": "Finally, we ask whether our constructions are optimal: are there matching lower bounds on the query complexity or hash length for approximate distance recovery under PPH constraints? We leave these questions for future investigation. REFERENCES [AMS96] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency moments. In STOC, pages 20–29, 1996. [BJWY20] Omri Ben-Eliezer, Rajesh Jayaram, David P. Woodruff, and Eylon Yogev."
  },
  {
    "prompt": "Scientific context on algorithms, PODS, framework and their role in graph theory.",
    "response": "A framework for adversarially robust streaming algorithms. In PODS, pages 63–80, 2020. [Blo70] Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13(7):422–426, 1970. [BLV19] Elette Boyle, Rio LaVigne, and Vinod Vaikuntanathan. Adversarially robust property-preserving hash functions. In ITCS, pages 16:1–16:20. Schloss Dagstuhl–Leibniz-Zentrum f¨ur Informatik, 2019. [BOZ82] Michael Ben-Or and Richard E. Zippel."
  },
  {
    "prompt": "Scientific context on pages, Science, FOCS and their role in graph theory.",
    "response": "Cayley graphs and randomized algorithms. In Foundations of Computer Science (FOCS), pages 168–177. IEEE, 1982. [BY20] Omri Ben-Eliezer and Eylon Yogev. The adversarial robustness of sampling. In PODS, pages 49–62, 2020. [CN22] Ronald Cramer and Rachel Nauta. Compactly hiding and robustly reconstructing sets. In Theory of Cryptography - TCC 2022, Part I, volume 13787 of Lecture Notes in Computer Science, pages 57–89. Springer, November 2022."
  },
  {
    "prompt": "Scientific context on Clayton, Christopher, Patton and their role in graph theory.",
    "response": "[CPS19] David Clayton, Christopher Patton, and Thomas Shrimpton. Probabilistic data structures in adversarial environments. In CCS, pages 1317–1334, 2019. [Don06] David L. Donoho. Compressed sensing. In IEEE Transactions on Information Theory, volume 52, pages 1289–1306, 2006. [DORS08] Yevgeniy Dodis, Rafail Ostrovsky, Leonid Reyzin, and Adam Smith. Fuzzy extractors: How to generate strong keys from biometrics and other noisy data. In Nigel P."
  },
  {
    "prompt": "Understanding the computational aspects of Science, EUROCRYPT, Lecture.",
    "response": "Smart, editor, Advances in Cryptology – EUROCRYPT 2008, volume 4965 of Lecture Notes in Computer Science, pages 523–540. Springer, 2008. [FLS22] Nils Fleischhacker, Kasper Green Larsen, and Mark Simkin. Property-preserving hash functions for hamming distance from standard assumptions. In Orr Dunkelman and Stefan Dziembowski, editors, Advances in Cryptology - EUROCRYPT 2022, pages 764–781, Cham, 2022. Springer International Publishing. [FS21] Nils Fleischhacker and Mark Simkin."
  },
  {
    "prompt": "Scientific context on Robust, Springer, property-preserving and their role in graph theory.",
    "response": "Robust property-preserving hash functions for hamming distance and more. In EUROCRYPT. Springer, 2021. [GGM10] Oded Goldreich, Tom Gur, and Yishay Mansour. Distribution-free testing of monotone conjunctions. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM), volume 6302 of Lecture Notes in Computer Science, pages 307–318. Springer, 2010. [GM11] Michael T. Goodrich and Michael Mitzenmacher. Invertible bloom lookup tables."
  },
  {
    "prompt": "Explain a concept involving Allerton, Control, Annual in computational complexity.",
    "response": "In 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 792–799. IEEE, 2011. [GR09] Oded Goldreich and Dana Ron. A sublinear bipartiteness tester for dense graphs. Combinatorica, 19(3):335–373, 2009. [HW13] Moritz Hardt and David P. Woodruff. How robust are linear sketches to adaptive inputs? In STOC, pages 121–130, 2013. [IM98] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality."
  },
  {
    "prompt": "This is a technical discussion about STOC, pages, Lenstra in theoretical computer science.",
    "response": "In STOC, pages 604–613, 1998. 11 [KOR00] Eyal Kushilevitz, Rafail Ostrovsky, and Yuval Rabani. Efﬁcient search for approximate nearest neighbor in high dimensional spaces. SIAM Journal on Computing, 30(2):457–474, 2000. [LLL82] Arjen K. Lenstra, Hendrik W. Lenstra, and L´aszl´o Lov´asz. Factoring polynomials with rational coefﬁcients. Mathematische Annalen, 261:515–534, 1982. [MNS08] Ilya Mironov, Moni Naor, and Gil Segev. Sketching in adversarial environments. In STOC, pages 651–660, 2008."
  },
  {
    "prompt": "Understanding the computational aspects of Peter, Miltersen, Noam.",
    "response": "[MNSW98] Peter Bro Miltersen, Noam Nisan, Shmuel Safra, and Avi Wigderson. On data structures and asymmetric communication complexity. volume 57, pages 37–49, 1998. [MP13] Daniele Micciancio and Chris Peikert. Hardness of sis and lwe with small parameters. In CRYPTO, pages 21–39, 2013. [NY15] Moni Naor and Eylon Yogev. Bloom ﬁlters in adversarial environments. In CRYPTO, pages 565–584. Springer, 2015. [OWZ11] Ryan O’Donnell, Yi Wu, and Yuan Zhou."
  },
  {
    "prompt": "Explain a concept involving Science, Computer, pages in computational complexity.",
    "response": "Optimal lower bounds for locality sensitive hashing (except when q is tiny). In Proceedings of the 2nd Symposium on Innovations in Computer Science (ICS), pages 273–282. Tsinghua University Press, 2011. [Ped92] Torben P. Pedersen. Non-interactive and information-theoretic secure veriﬁable secret sharing. In Advances in Cryptology - CRYPTO ’91, volume 576 of Lecture Notes in Computer Science, pages 129–140. Springer, 1992. [RRR21] Guy N. Rothblum, Ron D. Rothblum, and Omer Reingold."
  },
  {
    "prompt": "Scientific context on TCC, Proofs, overhead and their role in graph theory.",
    "response": "Proofs of proximity with polylog overhead. In Theory of Cryptography - TCC 2021, Part III, volume 13043 of Lecture Notes in Computer Science, pages 145–175. Springer, November 2021. 12"
  },
  {
    "prompt": "Understanding the computational aspects of Nullstellensatz, Jonas, University.",
    "response": "Graph Colouring Is Hard on Average for Polynomial Calculus and Nullstellensatz* Jonas Conneryd Lund University University of Copenhagen Susanna F."
  },
  {
    "prompt": "Explain a concept involving University, Lund, Copenhagen in computational complexity.",
    "response": "de Rezende Lund University Jakob Nordström University of Copenhagen Lund University Shuo Pang University of Copenhagen Lund University Kilian Risse EPFL March 24, 2025 Abstract We prove that polynomial calculus (and hence also Nullstellensatz) over any field requires linear degree to refute that sparse random regular graphs, as well as sparse Erdős-Rényi random graphs, are 3-colourable."
  },
  {
    "prompt": "Explain a concept involving size, proofs, proof in computational complexity.",
    "response": "Using the known relation between size and degree for polynomial calculus proofs, this implies strongly exponential lower bounds on proof size. 1 Introduction Determining the chromatic number of a graph 𝐺, i.e., how many colours are needed for the vertices of 𝐺if no two vertices connected by an edge should have the same colour, is one of the classic 21 problems shown NP-complete in the seminal work of Karp [Kar72]."
  },
  {
    "prompt": "Understanding the computational aspects of problem, understanding, graph.",
    "response": "This graph colouring problem, as it is also referred to, has been extensively studied since then, but there are still major gaps in our understanding. The currently best known approximation algorithm computes a graph colouring within at most a factor O\u0000𝑛(log log 𝑛)2/(log 𝑛)3\u0001 of the chromatic number [Hal93], and it is known that approximating this number to within a factor 𝑛1−𝜀is NP-hard [Zuc07]."
  },
  {
    "prompt": "This is a technical discussion about colours, graph, promise in theoretical computer science.",
    "response": "Even under the promise that the graph is 3-colourable, the most parsimonious algorithm with guaranteed polynomial running time needs O\u0000𝑛0.19996\u0001 colours [KT17]. This is very far from the lower bounds that are known—it is NP-hard to (2𝑘−1)-colour a 𝑘-colourable graph [BBKO21], but the question of whether colouring a 3-colourable graph with 6 colours is NP-hard remains open [KO22]."
  },
  {
    "prompt": "This is a technical discussion about time, case, widely in theoretical computer science.",
    "response": "It is widely believed that any algorithm that colours graphs optimally has to run in exponential time in the worst case, and the currently fastest algorithm for 3-colouring has time complexity O\u00001.3289𝑛\u0001 [BE05]. A survey on various algorithms and techniques for so-called exact algorithms for graph colouring can be found in [Hus15]."
  },
  {
    "prompt": "Explain a concept involving Annual, Science, FOCS in computational complexity.",
    "response": "Graph colouring instances of practical interest might not exhibit such exponential-time behaviour, however, and in such a context it is relevant to study algorithms without worst-case guarantees *This is the full-length version of a paper with the same title that appeared in the Proceedings of the 64th Annual IEEE Symposium on Foundations of Computer Science (FOCS ’23)."
  },
  {
    "prompt": "This is a technical discussion about Mar, GRAPH, arXiv in theoretical computer science.",
    "response": "arXiv:2503.17022v1 [cs.CC] 21 Mar 2025 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS and examine how they perform in practice. To understand such algorithms from a computational complexity viewpoint, it is natural to investigate bounded models of computation that are strong enough to describe the reasoning performed by the algorithms and to prove unconditional lower bounds that hold in these models."
  },
  {
    "prompt": "Scientific context on Previous, McDiarmid, Work and their role in graph theory.",
    "response": "1.1 Previous Work Focusing on random graphs, McDiarmid [McD84] developed a method for determining 𝑘-coloura- bility that captures a range of algorithmic approaches. Beame et al. [BCMM05] showed that this method could in turn be simulated by the resolution proof system [Bla37, DP60, DLL62, Rob65], and established average-case exponential lower bounds for resolution proofs of non-𝑘-colourability for random graph instances sampled so as not to be 𝑘-colourable with exceedingly high probability."
  },
  {
    "prompt": "Read and understand this section about algebraic, approaches, 𝑘-colourability from a research paper.",
    "response": "Different algebraic approaches for 𝑘-colourability have been considered in [AT92, Lov94, Mat74, Mat04]. Bayer [Bay82] seems to have been the first to use Hilbert’s Nullstellensatz to attack graph colouring. Informally, the idea is to write the problem as a set of polynomial equations {𝑝𝑖(𝑥1, . . . , 𝑥𝑛) = 0 | 𝑖∈[𝑚]} in such a way that legal 𝑘-colourings correspond to common roots of these polynomials. Finding polynomials 𝑞1, . . ."
  },
  {
    "prompt": "Explain a concept involving 𝑞𝑖𝑝𝑖, 𝑞𝑚such, 𝑘-colourable in computational complexity.",
    "response": ", 𝑞𝑚such that Í𝑚 𝑖=1 𝑞𝑖𝑝𝑖= 1 then proves that the graph is not 𝑘-colourable. This latter equality is referred to as a Nullstellensatz certificate of non-colourability, and the degree of this certificate is the largest degree of any polynomial 𝑞𝑖𝑝𝑖in the sum. Later papers based on Nullstellensatz and Gröbner bases, such as [DL95, Mnu01, HW08], culminated in an award-winning sequence of works [DLMM08, DLMO09, DLMM11, DMP+15] presenting algorithms with surprisingly good practical performance."
  },
  {
    "prompt": "Understanding the computational aspects of BIK, Nullstellensatz, time.",
    "response": "For quite some time, no strong lower bounds were known for these algebraic methods or the corresponding proof systems Nullstellensatz [BIK+94] and polynomial calculus [CEI96, ABRW02]. On the contrary, the authors of [DLMO09] reported that essentially all benchmarks they studied turned out to have Nullstellensatz certificates of small constant degree."
  },
  {
    "prompt": "Read and understand this section about DMP, linear, lower from a research paper.",
    "response": "The degree lower bound 𝑘+ 1 for 𝑘colours in [DMP+15] remained the best known until optimal, linear, degree lower bounds for polynomial calculus were established in [LN17] using a reduction from so-called functional pigeonhole principle formulas [MN15]."
  },
  {
    "prompt": "Scientific context on squares, Frege, lower and their role in graph theory.",
    "response": "A more general reduction framework was devised in [AO19] to obtain optimal degree lower bounds also for the proof systems Sherali-Adams [SA90] and sums-of- squares [Las01, Par00], as well as weakly exponential size lower bounds for Frege proofs [CR79, Rec75] of bounded depth."
  },
  {
    "prompt": "Understanding the computational aspects of lower, bounds, satisfactory.",
    "response": "The lower bounds discussed in the previous paragraph are not quite satisfactory, in that it is not clear how much they actually tell us about the graph colouring problem, as opposed to the hardness of the problems being reduced from. In order to improve our understanding for a wider range of graph instances, it seems both natural and desirable to establish average-case lower bounds for random graphs, just as for resolution in [BCMM05]."
  },
  {
    "prompt": "Read and understand this section about decades, number, goal from a research paper.",
    "response": "However, this goal has remained elusive for almost two decades, as pointed out, e.g., in [MN15, LN17, Lau18, BN21]. For sparse random graphs, where the number of edges is linear in the number of vertices, no superconstant degree lower bounds at all have been established for algebraic or semialgebraic proof systems."
  },
  {
    "prompt": "Explain a concept involving random, graphs, contrary in computational complexity.",
    "response": "On the contrary, it was shown in [BKM19], improving on [Coj05], that degree-2 sums-of-squares refutes 𝑘-colourability on random 𝑑-regular graphs asymptotically almost surely whenever 𝑑≥4𝑘2. For dense random graphs, the strongest lower bound seems to be the recent logarithmic degree bound in the sums-of-squares proof system for Erdős-Rényi random graphs with edge probability 1/2 and 𝑘= 𝑛1/2+𝜀colours [KM21]."
  },
  {
    "prompt": "Read and understand this section about Nullstellensatz, inequalities, reals from a research paper.",
    "response": "Since this result is for a problem encoding using inequalities, however, it is not clear whether this has any implications for Nullstellensatz or polynomial calculus over the reals (which are known to be polynomially simulated by sums-of-squares). And for other fields nothing has been known for the latter two proof systems—not even logarithmic lower bounds."
  },
  {
    "prompt": "This is a technical discussion about Introduction, Contribution, graphs in theoretical computer science.",
    "response": "2 1 Introduction 1.2 Our Contribution In this work, we establish optimal, linear, degree lower bounds and exponential size lower bounds for polynomial calculus proofs of non-colourability of random graphs. Theorem 1.1 (informal). For any 𝑑≥6, polynomial calculus (and hence also Nullstellensatz) requires asymptotically almost surely linear degree to refute that random 𝑑-regular graphs, as well as Erdős-Rényi random graphs, are 3-colourable."
  },
  {
    "prompt": "Understanding the computational aspects of lower, field, size.",
    "response": "These degree lower bounds hold over any field, and also imply exponential lower bounds on proof size. We prove our lower bound for the standard encoding in proof complexity, where binary variables 𝑥𝑣,𝑖indicate whether vertex 𝑣is coloured with colour 𝑖or not."
  },
  {
    "prompt": "This is a technical discussion about DMP, root, unity in theoretical computer science.",
    "response": "It should be pointed out that, just as the results in [LN17], our degree lower bounds also apply to the 𝑘-colourability encoding introduced in [Bay82] and used in computational algebra papers such as [DLMM08, DLMO09, DLMM11, DMP+15], where a primitive 𝑘th root of unity is adjoined to the field and different colours of a vertex 𝑣are encoded by a variable 𝑥𝑣taking different powers of this root of unity."
  },
  {
    "prompt": "Explain a concept involving lower, girth, proofs in computational complexity.",
    "response": "Our lower bound proofs crucially use a new idea for proving degree lower bounds for colouring graphs with large girth [RT22]."
  },
  {
    "prompt": "Understanding the computational aspects of Boolean, encoding, colouring.",
    "response": "After adapting this approach from the root-of-unity encoding to the Boolean indicator variable encoding, and replacing the proof in terms of girth with a strengthened argument using carefully chosen properties of random graphs, we obtain a remarkably clean and simple solution to the long-standing open problem of showing average-case polynomial calculus degree lower bounds for graph colouring. We elaborate on our techniques in more detail next."
  },
  {
    "prompt": "Scientific context on Discussion, Nullstellensatz, Sherali-Adams and their role in graph theory.",
    "response": "1.3 Discussion of Proof Techniques In most works on algebraic and semialgebraic proof systems such as Nullstellensatz, polynomial calculus, Sherali-Adams, and sums-of-squares, the focus has been on proving upper and lower bounds on the degree of proofs."
  },
  {
    "prompt": "This is a technical discussion about lower, bounds, size in theoretical computer science.",
    "response": "Even when proof size is the measure of interest, almost all size lower bounds have been established via degree lower bounds combined with general results saying that for all of the above proof systems except Nullstellensatz strong enough lower bounds on degree imply lower bounds on size [IPS99, AH19]. At a high level, the techniques for proving degree lower bounds for the different proof systems have a fairly similar flavour."
  },
  {
    "prompt": "This is a technical discussion about Nullstellensatz, Sherali-Adams, low-degree in theoretical computer science.",
    "response": "For the static proof systems, i.e., Nullstellensatz, Sherali-Adams, and sums-of-squares, it is enough to show that the dual problem is feasible and thus rule out low-degree proofs. In more detail, for Nullstellensatz, one constructs a design [Bus98], which is a linear functional mapping low-degree monomials to elements in the underlying field. This functional should map low-degree monomials multiplied by any input polynomial 𝑝𝑖to 0, but should map 1 to a non-zero field element."
  },
  {
    "prompt": "Scientific context on Nullstellensatz, side, 𝑞𝑖𝑝𝑖 and their role in graph theory.",
    "response": "If such a functional can be found, it is clear that there cannot exist any low-degree Nullstellensatz certificate Í𝑚 𝑖=1 𝑞𝑖𝑝𝑖= 1 of unsatisfiability, as the design would map the left-hand side of the equation to zero but the right-hand side to non-zero. For Sherali-Adams, the analogous functional furthermore has to map any low-degree monomials to non-negative numbers, and for sums-of-squares this should also hold for squares of low-degree polynomials."
  },
  {
    "prompt": "Read and understand this section about distribution, polynomials, problem from a research paper.",
    "response": "Such a pseudo-expectation can be viewed as an expectation over a fake probability distribution supported on satisfying assignments to the problem, which is indistinguishable from a true distribution for an adversary using only low-degree polynomials. Polynomial calculus is different from these proof systems in that it does not present the certificate of unsatisfiability as a static object, but instead, given a set of polynomials 𝒫, dynamically derives new polynomials in the ideal generated by 𝒫."
  },
  {
    "prompt": "Scientific context on GRAPH, CALCULUS, COLOURING and their role in graph theory.",
    "response": "The derivation ends when it reaches the polynomial 1, 3 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS i.e., the multiplicative identity in the field, showing that there is no solution. The most common way to prove degree lower bounds is to design a pseudo-reduction operator [Raz98], which maps all low-degree polynomials derived from 𝒫to 0 but sends 1 to 1, and which is indistinguishable from a true ideal reduction operator if one is limited to reasoning with low-degree polynomials."
  },
  {
    "prompt": "Explain a concept involving polynomials, consistent, set in computational complexity.",
    "response": "This means that for bounded-degree polynomial calculus derivations it seems like the set of input polynomials are consistent. Following the method in [AR03], a pseudo-reduction operator ˜𝑅can be constructed by defining it on low-degree monomials and extending it to polynomials by linearity. For every monomial 𝑚, we identify a set of related input polynomials 𝑆(𝑚), let ⟨𝑆(𝑚)⟩be the ideal generated by these polynomials, and define ˜𝑅(𝑚) = 𝑅⟨𝑆(𝑚)⟩(𝑚) to be the reduction of 𝑚modulo the ideal ⟨𝑆(𝑚)⟩."
  },
  {
    "prompt": "Read and understand this section about polynomials, Intuitively, satisfiable from a research paper.",
    "response": "Intuitively, we think of 𝑆(𝑚) as the (satisfiable) subset of polynomials that might possibly have been used in a low-degree derivation of 𝑚, but since the constant monomial 1 is not derivable in low degree it gets an empty associated set of polynomials, meaning that ˜𝑅(1) = 𝑅⟨𝑆(1)⟩(1) = 1. In order for ˜𝑅to look like a real reduction operator, we need to show that for polynomials 𝑝and 𝑝′ of not too high degree it holds that ˜𝑅(𝑝+ 𝑝′) = ˜𝑅(𝑝) + ˜𝑅(𝑝′) and ˜𝑅(𝑝· ˜𝑅(𝑝′)) = ˜𝑅(𝑝· 𝑝′)."
  },
  {
    "prompt": "Read and understand this section about 𝑅is, equality, operator from a research paper.",
    "response": "The first equality is immediate, since ˜𝑅is defined to be a linear operator, but the second equality is more problematic. Since the polynomials 𝑝and 𝑝′ will be reduced modulo different ideals—in fact, this will be the case even for different monomials within the same polynomial—a priori there is no reason why ˜𝑅should commute with multiplication."
  },
  {
    "prompt": "Scientific context on 𝑅behaves, operator, Proving and their role in graph theory.",
    "response": "Proving that a pseudo-reduction operator ˜𝑅behaves like an actual reduction operator for low-degree polynomials is typically the most challenging technical step in the lower bound proof. Very roughly, the proof method in [AR03] goes as follows. Suppose that 𝑚and 𝑚′ are monomials with associated polynomial sets 𝑆(𝑚) and 𝑆(𝑚′), respectively."
  },
  {
    "prompt": "Read and understand this section about polynomials, ideal, reduction from a research paper.",
    "response": "Using expansion properties of the constraint-variable incidence graph for the input polynomials, we argue that the true reduction operator will not change if we reduce both monomials modulo the larger ideal ⟨𝑆(𝑚) ∪𝑆(𝑚′)⟩generated by the union of their associated sets of polynomials. This implies that we have ˜𝑅(𝑚′) = 𝑅⟨𝑆(𝑚′)⟩(𝑚′) = 𝑅⟨𝑆(𝑚)∪𝑆(𝑚′)⟩(𝑚′) and ˜𝑅(𝑚· 𝑚′) = 𝑅⟨𝑆(𝑚)∪𝑆(𝑚′)⟩(𝑚· 𝑚′), from which it follows that ˜𝑅(𝑚· ˜𝑅(𝑚′)) = ˜𝑅(𝑚· 𝑚′) holds, just like for reduction modulo an actual ideal."
  },
  {
    "prompt": "Read and understand this section about ideals, reduction, act from a research paper.",
    "response": "To prove that expanding the ideals does not change the reduction operator is a delicate balancing act, though, since the ideals will need to be large enough to guarantee non-trivial reduction, but at the same time small enough so that different ideals can be “patched together” with only local adjustments. All previous attempts to apply this lower bound strategy to the graph colouring problem have failed."
  },
  {
    "prompt": "This is a technical discussion about input, calculus, lower in theoretical computer science.",
    "response": "For other polynomial calculus lower bounds it has been possible to limit the interaction between different polynomials in the input. For graph colouring, however, applying the reduction operator intuitively corresponds to partial colourings of subsets of vertices, and it has not been known how to avoid that locally assigned colours propagate new colouring constraints through the rest of the graph."
  },
  {
    "prompt": "Scientific context on vertices, language, spread and their role in graph theory.",
    "response": "In technical language, what is needed is a way to order the vertices in the graph so that there will be no long ordered paths of vertices along which colouring constraints can spread. It has seemed far from obvious how to construct such an ordering, or even whether such an ordering should exist, and due to this technical problem it has not been possible to join local ideal reduction operators into a globally consistent pseudo-reduction operator."
  },
  {
    "prompt": "Scientific context on idea, paper, ingenious and their role in graph theory.",
    "response": "This technical problem was addressed in a recent paper [RT22] by an ingenious, and in hindsight surprisingly simple, idea. The main insight is to consider a proper colouring of the graph 𝐺 with 𝜒(𝐺) colours, and then order the vertices in each colour class consecutively. In this way, order-decreasing paths are of length at most 𝜒(𝐺), and one can guarantee some form of locality."
  },
  {
    "prompt": "This is a technical discussion about reductions, graph, place in theoretical computer science.",
    "response": "Once this order is in place, the final challenge is to ensure that small cycles do not interfere when “patching together” reductions. In [RT22], such conflicts are avoided by ensuring that the graph 4 2 Preliminaries should have high girth, which results in a degree lower bound linear in the girth of the graph."
  },
  {
    "prompt": "Explain a concept involving logarithmic, number, size in computational complexity.",
    "response": "In terms of graph size, this cannot give better than logarithmic lower bounds, however, since the girth is at most logarithmic in the number of vertices for any graph with chromatic number larger than 3 [Bol78]. In our work, we employ the same ordering as in [RT22], but instead of girth use the fact that random graphs are locally very sparse. Once the necessary technical concepts are in place, the proof becomes quite simple and elegant, which we view as an additional strength of our result."
  },
  {
    "prompt": "Explain a concept involving Section, Paper, Outline in computational complexity.",
    "response": "1.4 Outline of This Paper The rest of this paper is organized as follows. In Section 2 we present some preliminaries and then, as a warm-up, reprove the resolution width lower bound for the colourability formula in Section 3. After this, we proceed to revisit the general framework to obtain polynomial calculus lower bounds in Section 4. In Section 5 we introduce the important notion of a closure and in Section 6 prove our main theorem."
  },
  {
    "prompt": "Scientific context on Preliminaries, Section, complexity and their role in graph theory.",
    "response": "We conclude with some final remarks and open problems in Section 7. 2 Preliminaries Let us start by briefly reviewing the necessary preliminaries from proof complexity, graph theory, and algebra. We use standard asymptotic notation. In this paper log denotes the logarithm base 2, while ln denotes the natural logarithm. For a field F we let F[𝑥1, . . . , 𝑥𝑛] denote the polynomial ring over F in 𝑛variables and let a monomial denote a product of variables."
  },
  {
    "prompt": "Explain a concept involving Vars, 𝐼𝑥𝑖, 𝐼𝑥𝑖and in computational complexity.",
    "response": "We denote by Vars(𝑚) the variables of a monomial 𝑚, that is, if 𝑚= Î 𝑖∈𝐼𝑥𝑖, then Vars(𝑚) = ∪𝑖∈𝐼𝑥𝑖and extend this notation to polynomials 𝑝= Í 𝑚𝑎𝑚𝑚by Vars(𝑝) = ∪𝑚Vars(𝑚). For polynomials 𝑝1, . . . , 𝑝𝑚∈F[𝑥1, . . . , 𝑥𝑛] we let ⟨𝑝1, . . . , 𝑝𝑚⟩denote the ideal generated by these polynomials: ⟨𝑝1, . . . , 𝑝𝑚⟩contains all polynomials of the form Í𝑚 𝑖=1 𝑞𝑖𝑝𝑖, for 𝑞𝑖∈F[𝑥1, . . . , 𝑥𝑛]."
  },
  {
    "prompt": "Scientific context on polynomial, Proof, 𝜌denote and their role in graph theory.",
    "response": "For a polynomial 𝑞and a partial function 𝜌mapping variables to polynomials we let 𝑞↾𝜌denote the polynomial obtained from 𝑞by substituting every occurrence of a variable 𝑥𝑖 in the domain of 𝜌by 𝜌(𝑥𝑖). 2.1 Proof Complexity Polynomial calculus (PC) [CEI96] is a proof system that uses algebraic reasoning to deduce that a system 𝒫of polynomials over a field F involving the variables 𝑥1, . . . , 𝑥𝑛is infeasible, i.e., that the polynomials in 𝒫have no common root."
  },
  {
    "prompt": "This is a technical discussion about Multiplication, Linear, 𝒫as in theoretical computer science.",
    "response": "Polynomial calculus interprets 𝒫as a set of generators of an ideal and derives new polynomials in this ideal through two derivation rules: Linear combination: 𝑝 𝑞 𝑎𝑝+ 𝑏𝑞, 𝑎, 𝑏∈F ; (2.1a) Multiplication: 𝑝 𝑥𝑖𝑝, 𝑥𝑖any variable. (2.1b) A polynomial calculus derivation 𝜋of a polynomial 𝑝starting from the set 𝒫is a sequence of polynomials (𝑝1, . . . , 𝑝𝜏), where 𝑝𝜏= 𝑝and each polynomial 𝑝𝑖either is in 𝒫or is obtained by applying one of the derivation rules (2.1a)-(2.1b) to polynomials 𝑝𝑗with 𝑗< 𝑖."
  },
  {
    "prompt": "Read and understand this section about polynomial, calculus, 𝒫is from a research paper.",
    "response": "A polynomial calculus refutation of 𝒫is a derivation of the constant polynomial 1 from 𝒫. It is well-known that polynomial calculus is sound and complete when the system 𝒫contains all Boolean axioms {𝑥2 1−𝑥1, . . . , 𝑥2 𝑛−𝑥𝑛}. We often refer to 𝒫as the set of axioms, and we say that a subset of axioms 𝒬⊆𝒫is satisfiable if the polynomials in 𝒬have a common root."
  },
  {
    "prompt": "This is a technical discussion about CALCULUS, GRAPH, POLYNOMIAL in theoretical computer science.",
    "response": "5 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS The most common complexity measures of polynomial calculus refutations are size and degree. The size of a polynomial 𝑝is its number of monomials when expanded into a linear combination of distinct monomials, and the degree of 𝑝is the maximum degree among all of its monomials."
  },
  {
    "prompt": "Understanding the computational aspects of polynomials, 𝜋is, degree.",
    "response": "The size of a polynomial calculus refutation 𝜋is the sum of the sizes of the polynomials in 𝜋, and the degree of 𝜋is the maximum degree among all polynomials in 𝜋. We follow the convention of not counting applications of the Boolean axioms toward degree or size by tacitly working over F[𝑥1, . . . , 𝑥𝑛]/⟨𝑥2 1 −𝑥1, . . . , 𝑥2 𝑛−𝑥𝑛⟩, which only strengthens a lower bound on either measure."
  },
  {
    "prompt": "Explain a concept involving Boolean, 𝐷is, 𝒫must in computational complexity.",
    "response": "Polynomial calculus size and degree are connected through the size-degree relation [IPS99]: if 𝒫 consists of polynomials with initial degree 𝑑and contains all Boolean axioms, and if 𝐷is the minimal degree among all polynomial calculus refutations of 𝒫, then every refutation of 𝒫must have size exp \u0000Ω\u0000(𝐷−𝑑)2/𝑛\u0001\u0001."
  },
  {
    "prompt": "Read and understand this section about PCR, calculus, polynomial from a research paper.",
    "response": "The size-degree relation also applies to the stronger proof system polynomial calculus resolution (PCR) [ABRW02], which is polynomial calculus where additionally each variable 𝑥𝑖appearing in 𝒫 has a formal negation 𝑥𝑖, enforced by adding polynomials 𝑥𝑖+ 𝑥𝑖−1 to 𝒫. Polynomial calculus and PCR are equivalent with respect to degree, since the map 𝑥𝑖↦→1 −𝑥𝑖sends any PCR proof to a valid polynomial calculus proof of the same degree."
  },
  {
    "prompt": "Scientific context on PCR, lower, prove and their role in graph theory.",
    "response": "Therefore, to prove a lower bound on PCR size it suffices to prove a lower bound on polynomial calculus degree, and in particular all size lower bounds in this paper also apply to PCR. Finally, we remark that lower bounds on polynomial calculus degree or size also apply to the weaker Nullstellensatz proof system mentioned in Section 1.1 and Section 1.2."
  },
  {
    "prompt": "Read and understand this section about colour, 𝑥𝑣,𝑖, Graph from a research paper.",
    "response": "2.2 Graph Colouring and Pseudo-Reductions Given a graph 𝐺, we study the polynomial calculus degree required to refute the system Col(𝐺, 𝑘) of polynomials 𝑘 Õ 𝑖=1 𝑥𝑣,𝑖−1 𝑣∈𝑉(𝐺) [every vertex is assigned a colour] (2.2a) 𝑥𝑣,𝑖𝑥𝑣,𝑖′ 𝑣∈𝑉(𝐺), 𝑖≠𝑖′ ∈[𝑘] [no vertex gets more than one colour] (2.2b) 𝑥𝑢,𝑖𝑥𝑣,𝑖 (𝑢, 𝑣) ∈𝐸(𝐺), 𝑖∈[𝑘] [no two adjacent vertices get the same colour] (2.2c) 𝑥2 𝑣,𝑖−𝑥𝑣,𝑖 𝑣∈𝑉(𝐺), 𝑖∈[𝑘] [Boolean axioms] (2.2d) whose common roots correspond precisely to proper 𝑘-colourings of 𝐺."
  },
  {
    "prompt": "Scientific context on axioms, CNF, formula and their role in graph theory.",
    "response": "We refer to axioms in (2.2a) and (2.2b) as vertex axioms and to (2.2c) as edge axioms. Note that the system Col(𝐺, 𝑘) of polynomials is not the standard polynomial translation of the usual CNF formula (defined in Section 3.1) as (2.2a) does not correspond to a single clause. We work with the above formulation for the sake of exposition and our lower bound also applies to the standard translation of the CNF formula."
  },
  {
    "prompt": "Scientific context on unity, Proposition, root and their role in graph theory.",
    "response": "If the field F we are working over contains, or can be extended to contain, a primitive 𝑘th root of unity, then it is known [LN17, Proposition 2.2] that a polynomial calculus degree lower bound for Col(𝐺, 𝑘) also applies to Bayer’s formulation [Bay82] of 𝑘-colourability, where each colour corresponds to a 𝑘th root of unity. This encoding has received considerable attention in computational algebra [DLMM08, DLMO09, DLMM11, DMP+15, RT22]."
  },
  {
    "prompt": "Explain a concept involving Theorem, 𝑅-operator, pseudo-reduction in computational complexity.",
    "response": "Our proof of Theorem 1.1 is based on the notion of a pseudo-reduction operator or 𝑅-operator. The following lemma is due to Razborov [Raz98]. Definition 2.1 (Pseudo-reduction). Let 𝐷∈N+ and 𝒫be a set of polynomials over F[𝑥1, . . . , 𝑥𝑛]. An F-linear operator ˜𝑅: F[𝑥1, . . . , 𝑥𝑛] →F[𝑥1, . . . , 𝑥𝑛] is a degree-𝐷pseudo-reduction for 𝒫if 6 2 Preliminaries 1. ˜𝑅(1) = 1, 2. ˜𝑅(𝑝) = 0 for every polynomial 𝑝∈𝒫, and 3. ˜𝑅(𝑥𝑖𝑚) = ˜𝑅\u0000𝑥𝑖˜𝑅(𝑚)\u0001 for any monomial 𝑚of degree at most 𝐷−1 and any variable 𝑥𝑖."
  },
  {
    "prompt": "Read and understand this section about Lemma, 𝒫be, calculus from a research paper.",
    "response": "Lemma 2.2 ([Raz98]). Let 𝐷∈N+ and 𝒫be a set of polynomials over F[𝑥1, . . . , 𝑥𝑛]. If there is a degree-𝐷 pseudo-reduction for 𝒫, then any polynomial calculus refutation of 𝒫over F requires degree strictly greater than 𝐷. The proof of Lemma 2.2 is straightforward: apply ˜𝑅to all polynomials in a purported polynomial calculus refutation of 𝒫and conclude by induction that it is impossible to reach contradiction in degree at most 𝐷."
  },
  {
    "prompt": "Scientific context on Algebra, Background, definition and their role in graph theory.",
    "response": "2.3 Algebra Background The definition of our pseudo-reduction operator requires some standard notions from algebra. A total well-order ≺on the monomials in F[𝑥1, . . . , 𝑥𝑛] is admissible if the following properties hold: 1. For any monomial 𝑚it holds that 1 ≺𝑚. 2. For any monomials 𝑚1, 𝑚2 and 𝑚such that 𝑚1 ≺𝑚2, it holds that 𝑚𝑚1 ≺𝑚𝑚2."
  },
  {
    "prompt": "This is a technical discussion about order, Note, algebra in theoretical computer science.",
    "response": "Note that this definition is the more standard definition of admissible order (also known as monomial order) used in algebra, and differs from that introduced in [Raz98], and used subsequently in [AR01, MN15], since it is defined over any monomial in F[𝑥1, . . . , 𝑥𝑛] and not only multilinear monomials, and it does not necessarily have to respect the total degree of monomials. We use the above definition due to its simplicity."
  },
  {
    "prompt": "This is a technical discussion about monomial, Recall, 𝑚is in theoretical computer science.",
    "response": "Recall that we only concern ourselves with multilinear polynomials in this article by tacitly working over F[𝑥1, . . . , 𝑥𝑛]/⟨𝑥2 1 −𝑥1, . . . , 𝑥2 𝑛−𝑥𝑛⟩. We write 𝑚1 ⪯𝑚2 to denote that 𝑚1 ≺𝑚2 or 𝑚1 = 𝑚2. The leading monomial of a polynomial 𝑝 is the largest monomial according to ≺appearing in 𝑝with a non-zero coefficient. For an ideal 𝐼 over F[𝑥1, . . . , 𝑥𝑛], a monomial 𝑚is reducible modulo 𝐼if 𝑚is the leading monomial of some polynomial 𝑞∈𝐼; otherwise 𝑚is irreducible modulo 𝐼."
  },
  {
    "prompt": "Scientific context on 𝐼and, modulo, reduction and their role in graph theory.",
    "response": "Under a total monomial order it is well-known that for any ideal 𝐼and any polynomial 𝑝there exists a unique representation 𝑝= 𝑞+ 𝑟such that 𝑞∈𝐼and 𝑟is a linear combination of irreducible monomials modulo 𝐼. We call 𝑟the reduction of 𝑝 modulo 𝐼, and denote by 𝑅𝐼the reduction operator which maps polynomials 𝑝to the reduction of 𝑝 modulo 𝐼. It is straightforward to verify that the reduction operator is linear over the vector space of polynomials. We will use the following standard fact."
  },
  {
    "prompt": "Read and understand this section about Observation, 𝑚′𝑚, reduction from a research paper.",
    "response": "Observation 2.3. If 𝐼1 and 𝐼2 are ideals over F[𝑥1, . . . , 𝑥𝑛] and 𝐼1 ⊆𝐼2, then for any monomials 𝑚and 𝑚′ it holds that 𝑅𝐼2 \u0000𝑚′𝑅𝐼1(𝑚)\u0001 = 𝑅𝐼2(𝑚′𝑚). Proof. Write 𝑚= 𝑞1 + 𝑟1 where 𝑟1 is the reduction of 𝑚modulo 𝐼1 and hence 𝑞1 ∈𝐼1. Similarly, let 𝑚′ · 𝑅𝐼1(𝑚) = 𝑚′ · 𝑟1 = 𝑞2 + 𝑟2 for 𝑞2 ∈𝐼2 and 𝑟2 the reduction of 𝑚′ · 𝑟1 modulo 𝐼2. We have that 𝑅𝐼2 \u0000𝑚′𝑅𝐼1(𝑚)\u0001 = 𝑟2 and we now argue that 𝑅𝐼2(𝑚′𝑚) = 𝑟2. Note that 𝑚′(𝑚−𝑞1) = 𝑟2 + 𝑞2 and hence 𝑚′ · 𝑚= 𝑟2 + 𝑞2 + 𝑚′ · 𝑞1."
  },
  {
    "prompt": "This is a technical discussion about Finally, closed, multiplication in theoretical computer science.",
    "response": "Since 𝐼1 ⊆𝐼2 and ideals are closed under multiplication and addition it holds that 𝑞2 + 𝑚2𝑞1 ∈𝐼2. Moreover, since 𝑟2 is irreducible modulo 𝐼2, it follows that 𝑅𝐼2(𝑚′𝑚) = 𝑟2 by uniqueness of a reduction modulo an ideal. □ Finally, let us record the following form of Hilbert’s Nullstellensatz on the Boolean cube. 7 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS Lemma 2.4. Let 𝑔be a polynomial and 𝑄be a set of polynomials in F[𝑥1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of Boolean, axioms, suppose.",
    "response": ", 𝑥𝑛], and suppose that 𝑄 contains all the Boolean axioms. Then it holds that 𝑔vanishes on all common roots of 𝑄if and only if 𝑔∈⟨𝑄⟩. Note that the interesting direction of Lemma 2.4 immediately follows from the implicational completeness of the Nullstellensatz proof system. For the convenience of the reader we provide a self-contained proof in Appendix A. 2.4 Graph Theory We consider graphs 𝐺= (𝑉, 𝐸) that are finite and undirected, and contain no self-loops or multi- edges."
  },
  {
    "prompt": "Explain a concept involving 𝑈in, neighbourhood, 𝑉we in computational complexity.",
    "response": "Given a vertex set 𝑈⊆𝑉, the neighbourhood of 𝑈in 𝐺is 𝑁(𝑈) = {𝑣∈𝑉| ∃𝑢∈𝑈: (𝑢, 𝑣) ∈𝐸}, and for a second set 𝑊⊆𝑉we let the neighbourhood of 𝑈in 𝑊be denoted by 𝑁𝑊(𝑈) = 𝑁(𝑈) ∩𝑊. The set of edges between vertices in 𝑈is denoted by 𝐸(𝑈) and we let 𝐺[𝑈] denote the subgraph induced by 𝑈in 𝐺, that is, 𝐺[𝑈] = (𝑈, 𝐸(𝑈)). A graph is said to be 𝑑-regular if all vertices are of degree 𝑑. Note that a graph on 𝑛vertices can be 𝑑-regular only if 𝑑< 𝑛and 𝑑𝑛is even."
  },
  {
    "prompt": "This is a technical discussion about 𝐺with, 𝐸the, 𝐺by in theoretical computer science.",
    "response": "For an edge 𝑒= (𝑢, 𝑣) ∈𝐸the contraction of 𝐺with respect to 𝑒is the graph obtained from 𝐺by identifying the vertices 𝑢and 𝑣as a single, new, vertex 𝑣𝑒and adding an edge between a vertex 𝑤∈𝑉\\ {𝑢, 𝑣} and 𝑣𝑒if and only if there is an edge between 𝑤and at least one of 𝑢or 𝑣in 𝐺. For a set of edges 𝑆⊆𝐸we let the contraction of 𝐺with respect to 𝑆be the graph obtained from 𝐺by contracting the edges in 𝑆one at a time, in any order."
  },
  {
    "prompt": "This is a technical discussion about satisfying, edges, 𝑘-colourable in theoretical computer science.",
    "response": "A graph is said to be 𝑘-colourable if there is a mapping 𝜒: 𝑉→[𝑘] satisfying 𝜒(𝑢) ≠𝜒(𝑣) for all edges (𝑢, 𝑣) ∈𝐸, in which case we refer to 𝜒as a proper 𝑘-colouring of 𝐺. The chromatic number of 𝐺, denoted by 𝜒(𝐺), is the smallest integer 𝑘such that 𝐺is 𝑘-colourable. Definition 2.5 (Sparsity). A graph 𝐺= (𝑉, 𝐸) is (ℓ, 𝜀)-sparse if every vertex set 𝑈⊆𝑉of size at most ℓsatisfies |𝐸(𝑈)| ≤(1 + 𝜀)|𝑈|."
  },
  {
    "prompt": "This is a technical discussion about 𝑉of, size, essential in theoretical computer science.",
    "response": "The essential property of sparse graphs we use to obtain our main result is that large subsets of vertices in such graphs are 3-colourable. Lemma 2.6. If a graph 𝐺= (𝑉, 𝐸) is (ℓ, 𝜀)-sparse for some 𝜀< 1/2, then it holds for every subset 𝑈⊆𝑉of size at most ℓthat 𝐺[𝑈] is 3-colourable. Proof. By induction on |𝑈|. The base case |𝑈| = 1 is immediate. For the inductive step we may assume that the claim holds for sets of size at most 𝑠−1. Consider a set 𝑈⊆𝑉of size 𝑠≤ℓ."
  },
  {
    "prompt": "This is a technical discussion about vertex, sparsity, degree in theoretical computer science.",
    "response": "The average degree of a vertex in 𝐺[𝑈] is 2|𝐸(𝑈)|/𝑠, which is at most 2(1 + 𝜀) < 3 by the assumption on sparsity. Hence, since graph degrees are integers, there exists a vertex 𝑣∈𝑈with degree at most 2 in 𝐺[𝑈]. The graph 𝐺[𝑈\\ {𝑣}] is 3-colourable by the inductive hypothesis, and every 3-colouring witnessing this will leave at least one colour available to properly colour 𝑣. Hence every 3-colouring of 𝐺[𝑈\\ {𝑣}] can be extended to 𝐺[𝑈], which concludes the proof."
  },
  {
    "prompt": "Scientific context on graph, random, model and their role in graph theory.",
    "response": "□ We consider two models of random graphs. One is the Erdős-Rényi random graph model G(𝑛, 𝑝), which is the distribution over graphs on 𝑛vertices where each edge is included independently with probability 𝑝. The other is the random 𝑑-regular graph model G𝑛,𝑑, which is the uniform distribution over 𝑑-regular graphs on 𝑛vertices. A graph property 𝑃holds asymptotically almost surely for a random graph model G = {G𝑛}∞ 𝑛=1 if lim𝑛→∞Pr𝐺∼G𝑛[𝐺has property 𝑃] = 1."
  },
  {
    "prompt": "This is a technical discussion about lemma, sparse, Random in theoretical computer science.",
    "response": "Random graphs are sparse with excellent parameters, as stated in the following lemma, which is essentially due to [Raz17]. (See Appendix B for a proof.) 8 3 A Simple Resolution Lower Bound for 4-Colourability on Sparse Graphs Lemma 2.7 (Sparsity lemma). For 𝑛, 𝑑∈N+ and 𝜀, 𝛿∈R+ such that 𝜀𝛿= 𝜔(1/log 𝑛), the following holds asymptotically almost surely. 1. If 𝐺is a graph sampled from G(𝑛, 𝑑/𝑛), then it is ((4𝑑)−(1+𝛿)(1+𝜀)/𝜀𝑛, 𝜀)-sparse. 2."
  },
  {
    "prompt": "Scientific context on G𝑛,𝑑, sparse, 𝐺is and their role in graph theory.",
    "response": "For 𝑑2 ≤𝜀𝛿log 𝑛, if 𝐺is a graph sampled from G𝑛,𝑑, then it is ((8𝑑)−(1+𝛿)(1+𝜀)/𝜀𝑛, 𝜀)-sparse. Finally, we need some bounds on the chromatic number of graphs sampled from G(𝑛, 𝑑/𝑛) or G𝑛,𝑑, where, in particular, the lower bounds ensure that Col(𝐺, 𝑘) is unsatisfiable for large enough 𝑑. Lemma 2.8 ([KPGW10, CFRR02, AN05, Łuc91])."
  },
  {
    "prompt": "Explain a concept involving 𝐺sampled, G𝑛,𝑑it, Simple in computational complexity.",
    "response": "For 𝑛∈N, 𝑑≤𝑛0.1 and a graph 𝐺sampled from either G(𝑛, 𝑑/𝑛) or G𝑛,𝑑it holds asymptotically almost surely that the chromatic number 𝜒(𝐺) is at most 2𝑑/log 𝑑and, if 𝑑≥6, then 𝜒(𝐺) ≥4. 3 A Simple Resolution Lower Bound for 4-Colourability on Sparse Graphs In this section we reprove the main result of Beame et al. [BCMM05], namely that resolution requires large width to refute the claim that random graphs have small chromatic number."
  },
  {
    "prompt": "Scientific context on polynomial, calculus, lower and their role in graph theory.",
    "response": "We hope that the exposition below may serve as a gentle introduction to the polynomial calculus lower bounds that will follow later, which build on similar albeit slightly more complicated concepts. Readers only interested in the polynomial calculus lower bounds may safely skip this section. For expert readers let us remark that while Beame et al."
  },
  {
    "prompt": "Scientific context on Wigderson, Pudlák, build and their role in graph theory.",
    "response": "[BCMM05] build on the width lower bound methodology introduced by Ben-Sasson and Wigderson [BW01], we rely on the game characterization due to Pudlák [Pud00]. We recover the bounds of Beame et al. precisely for 𝑘≥4, but for 𝑘= 3 we incur a slight loss in parameters. In the following we recollect some standard notions to then prove the resolution width lower bound in Section 3.2."
  },
  {
    "prompt": "Explain a concept involving 𝑆ℓ𝑖, Graph, CNF in computational complexity.",
    "response": "3.1 Graph Colouring in CNF and the Resolution Proof System A clause is a disjunction over a set of literals Ô 𝑖∈𝑆ℓ𝑖, where every literal ℓ𝑖is either a Boolean variable 𝑥or the negation 𝑥thereof. The width of a clause is the number of literals in it. A CNF formula 𝐹= 𝐶1 ∧· · · ∧𝐶𝑚is a conjunction of clauses, and the width of 𝐹is the maximum width of any clause 𝐶𝑖in 𝐹. A resolution refutation of a CNF formula 𝐹is as an ordered sequence of clauses 𝜋= (𝐷1, . . ."
  },
  {
    "prompt": "This is a technical discussion about 𝐷𝑖either, 𝐹or, resolution in theoretical computer science.",
    "response": ", 𝐷𝜏) such that 𝐷𝜏= ⊥is the empty clause and each 𝐷𝑖either occurs in 𝐹or is derived from clauses 𝐷𝑗1 and 𝐷𝑗2, with 𝑗1 < 𝑖and 𝑗2 < 𝑖, by the resolution rule 𝐵∨𝑥 𝐶∨𝑥 𝐵∨𝐶 . (3.1) The width of a resolution refutation 𝜋= (𝐷1, . . . , 𝐷𝜏) is the maximum width of any clause 𝐷𝑖in 𝜋and the length of 𝜋is 𝜏."
  },
  {
    "prompt": "This is a technical discussion about CNF, refutation, 𝑊required in theoretical computer science.",
    "response": "The size-width relation [BW01] relates the minimum width 𝑊required to refute a CNF formula 𝐹to the minimum length of any resolution refutation of 𝐹: if 𝐹is of initial width 𝑤and defined over 𝑛variables, then any resolution refutation of 𝐹requires size exp\u0000Ω\u0000(𝑊−𝑤)2/𝑛\u0001\u0001. As resolution operates over clauses, in contrast to polynomial calculus that operates over polynomials, we need to define the colourability formula as a CNF formula."
  },
  {
    "prompt": "Read and understand this section about 𝑥𝑣,𝑖, GRAPH, Col from a research paper.",
    "response": "In this section, for a 9 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS graph 𝐺and integer 𝑘, we let Col(𝐺, 𝑘) denote the CNF formula consisting of the clauses 𝑘Ü 𝑖=1 𝑥𝑣,𝑖, 𝑣∈𝑉(𝐺) [every vertex is assigned a colour] (3.2a) 𝑥𝑣,𝑖∨𝑥𝑣,𝑖′, 𝑣∈𝑉(𝐺), 𝑖≠𝑖′ [no vertex gets more than one colour] (3.2b) 𝑥𝑢,𝑖∨𝑥𝑣,𝑖, (𝑢, 𝑣) ∈𝐸(𝐺), 𝑖∈[𝑘]. [no two adjacent vertices get the same colour] (3.2c) Clearly, the CNF formula Col(𝐺, 𝑘) is satisfiable if and only if 𝐺is 𝑘-colourable."
  },
  {
    "prompt": "Understanding the computational aspects of Lower, Bounds, theorem.",
    "response": "3.2 Resolution Lower Bounds For the sake of simplicity we prove the theorem below for the 4-colourability formula only. The theorem can be extended to 3-colourability using concepts from Section 5. Theorem 3.1. Let 𝐺be a graph, and suppose that ℓ∈N+ and 𝜀> 0 are such that 𝐺is (ℓ, 1/2 −𝜀)-sparse. Then every resolution refutation of Col(𝐺, 4) requires width at least ℓ/4. Combining Lemma 2.7 with the above theorem we obtain the following corollaries for random graphs. Corollary 3.2 ([BCMM05])."
  },
  {
    "prompt": "Understanding the computational aspects of Col, 𝐺is, requires.",
    "response": "For any 𝜀> 0, if 𝐺is a graph sampled from G(𝑛, 𝑑/𝑛), then asymptotically almost surely every resolution refutation of Col(𝐺, 4) requires width 𝑛/4(4𝑑)3+𝜀. While Beame et al. [BCMM05] do not consider random regular graphs, it is not hard to see that their techniques can be used to prove the following statement. Corollary 3.3. For any 𝜀> 0, if 𝐺is a graph sampled from G𝑛,𝑑and 𝑑2 = o(log 𝑛), then asymptotically almost surely every resolution refutation of Col(𝐺, 4) requires width 𝑛/4(8𝑑)3+𝜀."
  },
  {
    "prompt": "This is a technical discussion about colouring, Theorem, Pudlák in theoretical computer science.",
    "response": "We prove Theorem 3.1 using the prover-adversary game as introduced by Pudlák [Pud00], which we describe here adapted to the colouring formula. The width-𝑤prover-adversary game for colouring proceeds in rounds. In each round the prover either queries or forgets the colouring of a vertex. In response to the former, the adversary has to respond with a colouring of the queried vertex. The prover has limited memory and may only remember the partial colouring of up to 𝑤 vertices."
  },
  {
    "prompt": "Explain a concept involving prover, improper, refutation in computational complexity.",
    "response": "The prover wins whenever the remembered partial colouring is improper. This game characterises resolution refutation width while in our setting a more precise statement is that the prover has a winning strategy in the width-𝑤prover-adversary game for colouring if and only if there is a resolution refutation of the colourability formula where every clause in the refutation mentions at most 𝑤vertices."
  },
  {
    "prompt": "Scientific context on partial, colouring, adversary and their role in graph theory.",
    "response": "Ultimately we want to design a strategy for the adversary so that the prover cannot win with limited memory. In order not to reach a partial colouring that is impossible to extend to the newly queried vertex, whenever the prover remembers a partial colouring defined on a subset 𝑈of the vertices, the adversary maintains a consistent partial colouring defined on a closure of 𝑈, as defined below."
  },
  {
    "prompt": "This is a technical discussion about 𝑈should, 𝑈and, closure in theoretical computer science.",
    "response": "Intuitively, a closure of 𝑈should be a slightly larger set that contains 𝑈and other vertices that, if not taken into account when colouring 𝑈, might not be possible to properly colour later on. Definition 3.4 (Closure). Let 𝐺= (𝑉, 𝐸) be a graph and let 𝑈⊆𝑉. We say that 𝑈is closed if all vertices 𝑣∈𝑉\\ 𝑈satisfy |𝑁𝑈(𝑣)| ≤1. A closure of 𝑈is any minimal closed set that contains 𝑈. Proposition 3.5. Every set of vertices has a unique closure."
  },
  {
    "prompt": "Scientific context on Proof, Simple, Resolution and their role in graph theory.",
    "response": "10 3 A Simple Resolution Lower Bound for 4-Colourability on Sparse Graphs Proof. Suppose there are two distinct closures 𝑊1 and 𝑊2 of a set 𝑈. As both 𝑊1 and 𝑊2 contain 𝑈 it holds that 𝑈⊆𝑊∩= 𝑊1 ∩𝑊2. By minimality of a closure, the set 𝑊∩is not a closure of 𝑈and hence there is a vertex 𝑣not in 𝑊∩such that |𝑁𝑊∩(𝑣)| ≥2. But this implies that the set 𝑊𝑖satisfying 𝑣∉𝑊𝑖is not closed, which contradicts the initial assumption."
  },
  {
    "prompt": "Explain a concept involving Proposition, 𝑈we, set in computational complexity.",
    "response": "□ In light of Proposition 3.5, for a set 𝑈we write Cl(𝑈) to denote the unique closure of 𝑈. We now show that if the underlying graph 𝐺is sparse, then the closure of a set 𝑈is not much larger than 𝑈itself. Lemma 3.6. Let 𝐺be an (ℓ, 1/2 −𝜀)-sparse graph for ℓ∈N+ and 𝜀> 0. Then any set 𝑈⊆𝑉of size at most ℓ/4 satisfies |Cl(𝑈)| ≤4|𝑈| ≤ℓ. Proof. Let us consider the following process: set 𝑈0 = 𝑈and 𝑖= 0. While there is a vertex 𝑣∈𝑉\\𝑈𝑖 satisfying |𝑁𝑈𝑖(𝑣)| ≥2, set 𝑈𝑖+1 = 𝑈𝑖∪{𝑣} and increment 𝑖."
  },
  {
    "prompt": "Understanding the computational aspects of 𝑡iterations, iteration, Suppose.",
    "response": "Suppose this process terminates after 𝑡iterations. Clearly the final set 𝑈𝑡contains 𝑈and is closed. Furthermore it is a minimal set with these properties and is hence the closure of 𝑈. As we add at least 2 edges to 𝐸(𝑈𝑖) in every iteration it holds that |𝐸(𝑈𝑖)| ≥2𝑖and, as we add a single vertex in each iteration, we have |𝑈𝑖| = |𝑈| + 𝑖. Suppose 𝑡≥3|𝑈|. In iteration 𝑖= 3|𝑈| it holds that |𝐸(𝑈𝑖)| ≥6|𝑈|, while |𝑈𝑖| = 4|𝑈| ≤ℓ. Hence |𝐸(𝑈𝑖)| ≥3|𝑈𝑖|/2, which contradicts the assumption on sparsity."
  },
  {
    "prompt": "Read and understand this section about 𝑈is, set, size from a research paper.",
    "response": "We may thus conclude that the closure of a set 𝑈is of size at most 4|𝑈|, as claimed. □ In order to design a strategy for the adversary, we prove that it is always possible to extend a colouring on a small closed set of vertices to a slightly larger set. Lemma 3.7. Let 𝐺= (𝑉, 𝐸) be an (ℓ, 1/2 −𝜀)-sparse graph, let 𝑈⊆𝑉be a closed set of size at most ℓand suppose 𝜒is a proper 4-colouring of 𝐺[𝑈]. Then 𝜒can be extended to 𝐺[𝑊] for any set 𝑊⊇𝑈of size at most |𝑊| ≤ℓ. Proof. By induction on 𝑠= |𝑊\\ 𝑈|."
  },
  {
    "prompt": "This is a technical discussion about holds, Theorem, statement in theoretical computer science.",
    "response": "The statement clearly holds for 𝑠= 0. For the inductive step, let 𝑣∈𝑊\\ 𝑈be such that |𝑁𝑊\\𝑈(𝑣)| ≤2, as guaranteed to exist by the assumption on sparsity since it holds that |𝐸(𝑊\\ 𝑈)| ≤(3/2 −𝜀)|𝑊\\ 𝑈|. By the inductive hypothesis there is an extension of 𝜒to the set 𝑊\\ {𝑣}. As 𝑈is closed, the vertex 𝑣has at most one neighbour in 𝑈. Hence it holds that |𝑁𝑊(𝑣)| ≤3 and we may thus extend the colouring to 𝑣. □ With Lemmas 3.6 and 3.7 at hand we are ready to prove Theorem 3.1. Proof of Theorem 3.1."
  },
  {
    "prompt": "Understanding the computational aspects of adversary, closure, describe.",
    "response": "Let us describe a strategy for the adversary. At any point in the game the adversary maintains a partial 4-colouring 𝜒to the closure of the vertices 𝑈the prover remembers. Whenever the prover queries a vertex in the closure the adversary responds accordingly. If a vertex 𝑣outside the closure of 𝑈is queried, then the adversary extends 𝜒to Cl(𝑈∪{𝑣}) by virtue of Lemma 3.7 and responds accordingly."
  },
  {
    "prompt": "Read and understand this section about closure, Lemma, Finally from a research paper.",
    "response": "Finally, if the prover forgets the colouring of a vertex 𝑢∈𝑈, then we shrink our closure to the closure of 𝑈\\ {𝑢}. Here we use the fact that by minimality of the closure it holds that Cl(𝑈) ⊇Cl(𝑈\\ {𝑢}). As, by Lemma 3.6, the closure of a set 𝑈is at most a factor 4 larger than 𝑈and, by Lemma 3.7, we may maintain a valid 4-colouring to the closure as long as the size of the closure is bounded by ℓ, the prover cannot win the game if 𝑤≤ℓ/4."
  },
  {
    "prompt": "Understanding the computational aspects of formula, sparse, vertices.",
    "response": "Therefore, every resolution refutation of the 4-colourability formula defined over an (ℓ, 1/2 −𝜀)-sparse graph contains a clause that mentions at least ℓ/4 vertices and hence has width at least ℓ/4. □ Combining Theorems 3.2 and 3.3 with the mentioned size-width relation we obtain optimal exp(Ω(𝑛)) resolution size lower bounds for constant 𝑑."
  },
  {
    "prompt": "Scientific context on Bounds, CALCULUS, Lower and their role in graph theory.",
    "response": "11 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS 4 Polynomial Calculus Lower Bounds: The General Framework In this section we provide a proof overview of Theorem 1.1 and then revisit the general framework, as introduced by Alekhnovich and Razborov [AR03], for obtaining polynomial calculus lower bounds."
  },
  {
    "prompt": "This is a technical discussion about Proof, 𝑅for, Overview in theoretical computer science.",
    "response": "4.1 Proof Overview As outlined in the introduction, the construction of our pseudo-reduction ˜𝑅for the colouring formula follows the general paradigm introduced by [AR03] which has been used in several subsequent papers [GL10a, GL10b, MN15]. The idea is that given an initial set of polynomials 𝒫, for every monomial 𝑚we identify a subset 𝑆(𝑚) of polynomials in 𝒫that are in some sense relevant to 𝑚."
  },
  {
    "prompt": "Scientific context on polynomials, 𝑅on, 𝑅linearly and their role in graph theory.",
    "response": "Then we define ˜𝑅on the monomial 𝑚as the reduction modulo the ideal ⟨𝑆(𝑚)⟩generated by the polynomials, and extend ˜𝑅linearly to arbitrary polynomials. The goal is to show that ˜𝑅satisfies properties 1-3 in Lemma 2.2, which typically requires showing that 𝑆satisfies two main technical properties."
  },
  {
    "prompt": "Understanding the computational aspects of lemma, states, property.",
    "response": "The first property is captured by what we call a satisfiability lemma, which states that if the degree of 𝑚is at most some parameter 𝐷, then the associated set 𝑆(𝑚) is satisfiable and is in some sense well-structured. The second property is described by the reducibility lemma, which states that for every ideal 𝐼that is generated by a well-structured set of polynomials that contains 𝑆(𝑚) and is satisfiable, it holds that 𝑅𝐼(𝑚) = 𝑅⟨𝑆(𝑚)⟩(𝑚)."
  },
  {
    "prompt": "Scientific context on 𝑆satisfies, 𝐷follows, lower and their role in graph theory.",
    "response": "Once these lemmas are in place, and as long as 𝑆satisfies some other simple conditions, a degree lower bound of 𝐷follows by an argument presented in [AR03]. We formalise these arguments in Lemma 4.6 by extracting the essential parts of the Alekhnovich– Razborov [AR03] framework and making explicit the properties the map 𝑆must satisfy in order for the proof of the lower bound to go through."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, Definition, 𝑆and.",
    "response": "Apart from the properties in Definition 4.1 capturing some type of multilinearity, monotonicity and closedness of 𝑆and ensuring that axioms with leading monomial 𝑚are in 𝑆(𝑚), we introduce in Lemma 4.6 two sufficient conditions for the lower bound to follow: the first corresponds to the satisfiability lemma and the second to the reducibility lemma."
  },
  {
    "prompt": "Understanding the computational aspects of 𝑆and, hand, conditions.",
    "response": "With this in hand, the ensuing sections can focus on defining the map 𝑆and proving that it satisfies the conditions, without having to refer to reduction operators. 4.2 Revisiting the Alekhnovich–Razborov Framework We now revisit the general framework introduced by Alekhnovich and Razborov [AR03] for proving polynomial calculus lower bounds."
  },
  {
    "prompt": "This is a technical discussion about conditions, mapping, monomials in theoretical computer science.",
    "response": "We extract from their proofs a formal statement that if the function 𝑆, mapping monomials to relevant subsets of axioms, satisfies two conditions, namely the satisfiability and the reducibility conditions, along with some natural conditions as discussed in the following, then this implies polynomial calculus degree lower bounds. Before discussing the additional conditions that the mapping 𝑆needs to satisfy, we need to fix an admissible order ≺on the monomials in F[𝑥1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of Section, order, defined.",
    "response": ", 𝑥𝑛] as defined in Section 2.3. Recall that an admissible order is a total order that respects multiplication, that is, if 𝑚1 ≺𝑚2 then it holds that 𝑚𝑚1 ≺𝑚𝑚2. The leading monomial of a polynomial 𝑝is the largest monomial according to ≺that appears in 𝑝. For the remainder of this section we implicitly assume that monomials in F[𝑥1, . . . , 𝑥𝑛] are ordered according to an admissible order ≺."
  },
  {
    "prompt": "Scientific context on 𝑖𝑎𝑖𝑚𝑖, adopt, convention and their role in graph theory.",
    "response": "Let us further adopt the convention that whenever we write a polynomial 𝑝as a sum of monomials 𝑝= Í 𝑖𝑎𝑖𝑚𝑖, then 𝑎𝑖∈F are field elements and the 𝑚𝑖∈F[𝑥1, . . . , 𝑥𝑛] are monomials ordered such that 𝑚𝑖≺𝑚𝑗for all 𝑗< 𝑖. In particular, 𝑚1 is the leading monomial of 𝑝."
  },
  {
    "prompt": "Explain a concept involving Polynomial, Bounds, Calculus in computational complexity.",
    "response": "12 4 Polynomial Calculus Lower Bounds: The General Framework Our first additional requirement on 𝑆is that it maps monomials according to the variables in the monomial, so that if two monomials 𝑚and 𝑚′ both contain the same set of variables, then 𝑆(𝑚) = 𝑆(𝑚′). We further require that 𝑆is in some sense monotone, namely, for any variable 𝑥, we require that if 𝑆(𝑚′) ⊆𝑆(𝑚) for monomials 𝑚′ ≺𝑚, then it also holds that 𝑆(𝑥𝑚′) ⊆𝑆(𝑥𝑚)."
  },
  {
    "prompt": "Understanding the computational aspects of Vars, 𝑆should, closed.",
    "response": "Moreover, the image of 𝑆should consist of sets that are “closed” in the sense that for any 𝑚, the set of axioms 𝑆(𝑚) contains the union of all 𝑆(𝑚′) where 𝑚′ ≺𝑚and Vars(𝑚′) ⊆Vars(𝑆(𝑚)). Finally, we require that if 𝑚is the leading monomial of an axiom 𝑝, then 𝑝∈𝑆(𝑚). These properties are formalised in the following definition. Definition 4.1 (Support). Let 𝒫be a set of polynomials over F[𝑥1, . . . , 𝑥𝑛], let ≺be an admissible order on the monomials in F[𝑥1, . . ."
  },
  {
    "prompt": "Scientific context on Vars, holds, subsets and their role in graph theory.",
    "response": ", 𝑥𝑛], and let 𝑆: 2{𝑥1,...,𝑥𝑛} →2𝒫be a function that maps subsets 𝑌⊆{𝑥1, . . . , 𝑥𝑛} of variables to subsets 𝑆(𝑌) ⊆𝒫of polynomials. For a monomial 𝑚we write 𝑆(𝑚) for 𝑆\u0000Vars(𝑚)\u0001 and we say that 𝑆is a 𝒫-support if the following holds. 1. For every variable 𝑥and for all monomials 𝑚and 𝑚′ such that 𝑚′ ≺𝑚, it holds that if 𝑆(𝑚′) ⊆𝑆(𝑚), then 𝑆(𝑥𝑚′) ⊆𝑆(𝑥𝑚). 2. For all monomials 𝑚and 𝑚′ such that 𝑚′ ≺𝑚, it holds that if Vars(𝑚′) ⊆Vars\u0000𝑆(𝑚)\u0001, then 𝑆(𝑚′) ⊆𝑆(𝑚). 3."
  },
  {
    "prompt": "Scientific context on holds, 𝑚is, leading and their role in graph theory.",
    "response": "For all 𝑝∈𝒫, it holds that 𝑝∈𝑆(𝑚), where 𝑚is the leading monomial in 𝑝. If the set of polynomials 𝒫is not essential we call a 𝒫-support simply a support. Let us record four technical claims that follow from the above properties of a support and that we will later use to prove that an appropriate map 𝑆implies a polynomial calculus degree lower bound."
  },
  {
    "prompt": "Scientific context on Vars, Definition, claim and their role in graph theory.",
    "response": "The first claim is that if 𝑚1 is the leading monomial of an axiom 𝑝, not only does 𝑆(𝑚1) contain 𝑝 but it also contains 𝑆(𝑚𝑖) for all monomials 𝑚𝑖that appear in 𝑝. Claim 4.2. If 𝑆is a 𝒫-support, then for all 𝑝∈𝒫, if 𝑝= Í 𝑖𝑎𝑖𝑚𝑖, then {𝑝} ∪Ð 𝑖 \u0000𝑆(𝑚𝑖)\u0001 ⊆𝑆(𝑚1). Proof. By property 3 of Definition 4.1 it holds that 𝑝∈𝑆(𝑚1). This implies that Vars(𝑚𝑖) ⊆Vars(𝑝) is contained in Vars\u0000𝑆(𝑚1)\u0001 and thus for 𝑖≠1 since 𝑚𝑖≺𝑚1, property 2 of Definition 4.1 implies that 𝑆(𝑚𝑖) ⊆𝑆(𝑚1)."
  },
  {
    "prompt": "Explain a concept involving variables, Vars, Definition in computational complexity.",
    "response": "□ The next claim states that if the variables of 𝑚′ is a subset of the variables of 𝑚, then 𝑆(𝑚′) ⊆𝑆(𝑚). Claim 4.3. If 𝑆is a support, then for all monomials 𝑚and 𝑚′ such that Vars(𝑚′) ⊆Vars(𝑚) it holds that 𝑆(𝑚′) ⊆𝑆(𝑚). Proof. Note that 𝑆(1) ⊆𝑆(𝑚) by property 2 of Definition 4.1, where we use that 1 ≺𝑚. Thus, by inductively applying property 1 of Definition 4.1 and using the fact that ≺respects multiplication, we obtain that 𝑆(𝑚′) ⊆𝑆(𝑚′ · 𝑚)."
  },
  {
    "prompt": "Explain a concept involving Vars, Finally, 𝑥𝑚′ in computational complexity.",
    "response": "Finally, since Vars(𝑚′) ⊆Vars(𝑚), it holds that Vars(𝑚′) = Vars(𝑚′ · 𝑚) and hence 𝑆(𝑚) = 𝑆(𝑚′ · 𝑚) ⊇𝑆(𝑚′). □ We also prove that monomials 𝑚′ that appear when 𝑚is reduced modulo 𝑆(𝑚) must be such that 𝑆(𝑚′) ⊆𝑆(𝑚) and that 𝑆(𝑥𝑚′) ⊆𝑆(𝑥𝑚). Claim 4.4. If 𝑆is a support, then for all monomials 𝑚and 𝑚′ such that 𝑚′ appears in 𝑅⟨𝑆(𝑚)⟩(𝑚) it holds that 𝑆(𝑚′) ⊆𝑆(𝑚) and 𝑆(𝑥𝑚′) ⊆𝑆(𝑥𝑚) for any variable 𝑥. 13 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS Proof."
  },
  {
    "prompt": "Scientific context on Vars, trivially, assume and their role in graph theory.",
    "response": "We assume 𝑚′ ≠𝑚, and thus 𝑚′ ≺𝑚, otherwise the claim follows trivially. Moreover, we observe that it suffices to show that 𝑆(𝑚′) ⊆𝑆(𝑚), since we can then use property 1 of Definition 4.1 to conclude that 𝑆(𝑥𝑚′) ⊆𝑆(𝑥𝑚). We now argue that Vars(𝑚′) ⊆Vars(𝑚) ∪Vars\u0000𝑆(𝑚)\u0001. Towards contradiction, suppose this is not the case. Let 𝜌denote the assignment that assigns all variables in 𝑚′ that are not in Vars\u0000𝑆(𝑚)\u0001 ∪ Vars(𝑚) to 0."
  },
  {
    "prompt": "Scientific context on Vars, Recall, denotes and their role in graph theory.",
    "response": "Recall that 𝑅⟨𝑆(𝑚)⟩(𝑚) denotes the reduction of 𝑚modulo ⟨𝑆(𝑚)⟩and that there is a unique representation 𝑚= 𝑞+ 𝑅⟨𝑆(𝑚)⟩(𝑚) with 𝑞∈⟨𝑆(𝑚)⟩. No variable in 𝑚nor in any of the generators of ⟨𝑆(𝑚)⟩is assigned by 𝜌, so 𝑚↾𝜌= 𝑚and 𝑞↾𝜌∈⟨𝑆(𝑚)⟩. Moreover, we have 𝑚′↾𝜌= 0 so 𝑅⟨𝑆(𝑚)⟩(𝑚)↾𝜌≠𝑅⟨𝑆(𝑚)⟩(𝑚). But this contradicts that the representation 𝑚= 𝑞+ 𝑅⟨𝑆(𝑚)⟩(𝑚) is unique, and thus Vars(𝑚′) ⊆Vars(𝑚) ∪Vars\u0000𝑆(𝑚)\u0001. Write 𝑚′ = 𝑚′ 1 · 𝑚′ 2 such that Vars(𝑚′ 1) ⊆Vars(𝑚) and Vars(𝑚′ 2) ⊆Vars\u0000𝑆(𝑚)\u0001."
  },
  {
    "prompt": "This is a technical discussion about Vars, Definition, order in theoretical computer science.",
    "response": "Observe that 𝑚′ 2 ≺ 𝑚since 1 ≺𝑚′ ≺𝑚and the order ≺respects multiplication as it is admissible. Hence by property 2 of Definition 4.1 applied to 𝑚′ 2 and 𝑚it follows that 𝑆(𝑚′ 2) ⊆𝑆(𝑚). By iteratively applying property 1 of Definition 4.1 for each 𝑥∈Vars(𝑚′ 1), and again using that the order ≺respects multiplication, we can conclude that 𝑆(𝑚′) = 𝑆(𝑚′ 1 · 𝑚′ 2) ⊆𝑆(𝑚′ 1 · 𝑚). Finally, since Vars(𝑚′ 1) ⊆Vars(𝑚), we have that 𝑆(𝑚′ 1 · 𝑚) = 𝑆(𝑚), and therefore 𝑆(𝑚′) ⊆𝑆(𝑚)."
  },
  {
    "prompt": "Scientific context on ideal, modulo, claim and their role in graph theory.",
    "response": "□ From this claim we can deduce that if irreducibility is preserved modulo some larger ideal, then the reduced polynomials are the same as done in the following. Claim 4.5. Let 𝑆be a 𝒫-support and let 𝒬⊆𝒫. Suppose that every monomial 𝑚irreducible modulo ⟨𝑆(𝑚)⟩that satisfies 𝑆(𝑚) ⊆𝒬is also irreducible modulo ⟨𝒬⟩. Then for all monomials 𝑚′ such that 𝑆(𝑚′) ⊆𝒬it holds that 𝑅⟨𝒬⟩(𝑚′) = 𝑅⟨𝑆(𝑚′)⟩(𝑚′). Proof."
  },
  {
    "prompt": "Scientific context on 𝒬and, 𝒬is, monomial and their role in graph theory.",
    "response": "Let 𝑚′ be an arbitrary monomial satisfying 𝑆(𝑚′) ⊆𝒬and suppose that any irreducible monomial 𝑚modulo ⟨𝑆(𝑚)⟩such that 𝑆(𝑚) ⊆𝒬is also irreducible modulo ⟨𝒬⟩. We want to prove that 𝑅⟨𝒬⟩(𝑚′) = 𝑅⟨𝑆(𝑚′)⟩(𝑚′). Let us write 𝑅⟨𝑆(𝑚′)⟩(𝑚′) = Í 𝑖𝑎𝑖𝑚𝑖. Note that by definition each such 𝑚𝑖is irreducible modulo ⟨𝑆(𝑚′)⟩. As by Claim 4.4 it holds that 𝑆(𝑚𝑖) ⊆𝑆(𝑚′) we may conclude that each 𝑚𝑖is irreducible modulo ⟨𝑆(𝑚𝑖)⟩."
  },
  {
    "prompt": "Read and understand this section about modulo, assumption, inclusions from a research paper.",
    "response": "Thus, by assumption along with the inclusions 𝑆(𝑚𝑖) ⊆𝑆(𝑚′) ⊆𝒬, each 𝑚𝑖is irreducible modulo ⟨𝒬⟩. This implies that 𝑅⟨𝑆(𝑚′)⟩(𝑚′) = 𝑅⟨𝒬⟩ \u0000𝑅⟨𝑆(𝑚′)⟩(𝑚′)\u0001 = 𝑅⟨𝒬⟩(𝑚′), using once more that 𝑆(𝑚′) ⊆𝒬. □ We can now make the formal claim that polynomial calculus degree lower bounds follow from the existence of a support 𝑆satisfying the two conditions corresponding to the satisfiability lemma and the reducibility lemma. This fact is implicit in [AR03] and we make it explicit below."
  },
  {
    "prompt": "Understanding the computational aspects of Filmus, lemma, establishes.",
    "response": "It is worth remarking that Filmus [Fil19] establishes a lemma of similar flavour. However, in contrast to Filmus, we do not introduce another layer of abstraction but rather state the lemma directly in terms of 𝑆. Lemma 4.6. Let 𝐷∈N+, let 𝒫be a set of polynomials over F[𝑥1, . . . , 𝑥𝑛] of degree at most 𝐷, and denote by 𝑆a 𝒫-support. If all monomials 𝑚of degree at most 𝐷satisfy that 1. Satisfiability condition: the set of axioms 𝑆(𝑚) is satisfiable; and 2."
  },
  {
    "prompt": "This is a technical discussion about modulo, reducible, 𝒫over in theoretical computer science.",
    "response": "Reducibility condition: for all monomials 𝑚′, if 𝑆(𝑚′) ⊆𝑆(𝑚), then 𝑚′ is reducible modulo ⟨𝑆(𝑚′)⟩ if and only if 𝑚′ is reducible modulo ⟨𝑆(𝑚)⟩; then any polynomial calculus refutation of 𝒫over F requires degree strictly greater than 𝐷. Proof. Let ˜𝑅be the operator defined on monomials 𝑚by ˜𝑅(𝑚) = 𝑅⟨𝑆(𝑚)⟩(𝑚) and extended by linearity to polynomials. Our goal is to show that ˜𝑅is a degree-𝐷pseudo-reduction for 𝒫."
  },
  {
    "prompt": "Understanding the computational aspects of Bounds, Definition, 𝑅maps.",
    "response": "That 14 4 Polynomial Calculus Lower Bounds: The General Framework is, according to Definition 2.1, we need to show that ˜𝑅(1) = 1, that ˜𝑅maps all polynomials in 𝒫to 0, and that for every monomial 𝑚of degree at most 𝐷−1 and every variable 𝑥, it holds that ˜𝑅(𝑥𝑚) = ˜𝑅(𝑥˜𝑅(𝑚)). If we manage to show these properties, then we can appeal to Lemma 2.2 to reach the desired conclusion."
  },
  {
    "prompt": "Read and understand this section about common, root, satisfiable from a research paper.",
    "response": "By the satisfiability condition we have that 𝑆(1) is satisfiable, that is, the set of polynomials 𝑆(1) has a common root. This implies that the entire ideal ⟨𝑆(1)⟩has a common root and, therefore, 1 is not in ⟨𝑆(1)⟩and hence is not reducible to 0 modulo ⟨𝑆(1)⟩. As 1 is the smallest monomial in the order we conclude that ˜𝑅(1) = 𝑅⟨𝑆(1)⟩(1) = 1. To see that ˜𝑅maps each polynomial 𝑝∈𝒫to 0, let 𝑝= Í 𝑗𝑎𝑗𝑚𝑗. By Claim 4.2 it follows that 𝑝∈𝑆(𝑚1) and that 𝑆(𝑚𝑗) ⊆𝑆(𝑚1) for all 𝑗."
  },
  {
    "prompt": "Scientific context on 𝑎𝑗𝑅, definition, reducibility and their role in graph theory.",
    "response": "It therefore holds that ˜𝑅(𝑝) = Õ 𝑗 𝑎𝑗𝑅⟨𝑆(𝑚𝑗)⟩(𝑚𝑗) [by definition of ˜𝑅and 𝑝] (4.1a) = Õ 𝑗 𝑎𝑗𝑅⟨𝑆(𝑚1)⟩(𝑚𝑗) [by Claim 4.5 and the reducibility condition] (4.1b) = 𝑅⟨𝑆(𝑚1)⟩ \u0010 Õ 𝑗 𝑎𝑗𝑚𝑗 \u0011 [by linearity of 𝑅⟨𝑆(𝑚1)⟩] (4.1c) = 𝑅⟨𝑆(𝑚1)⟩(𝑝) [by definition of 𝑝] (4.1d) = 0 , [since 𝑝∈𝑆(𝑚1)] (4.1e) where we note that in order to apply the reducibility condition we use the assumption that 𝑝has degree at most 𝐷."
  },
  {
    "prompt": "Understanding the computational aspects of 𝑥𝑚𝑗, Finally, degree.",
    "response": "Finally, we need to show that for every monomial 𝑚of degree at most 𝐷−1 and every variable 𝑥, it holds that ˜𝑅(𝑥𝑚) = ˜𝑅(𝑥˜𝑅(𝑚)). Let ˜𝑅(𝑚) = 𝑅⟨𝑆(𝑚)⟩(𝑚) = Í 𝑗𝑎𝑗𝑚𝑗. By definition of ˜𝑅we have that ˜𝑅(𝑥˜𝑅(𝑚)) = Õ 𝑗 𝑎𝑗˜𝑅(𝑥𝑚𝑗) = Õ 𝑗 𝑎𝑗𝑅⟨𝑆(𝑥𝑚𝑗)⟩(𝑥𝑚𝑗) . (4.2) We now argue that since 𝑚is a monomial of degree at most 𝐷−1, reducing modulo ⟨𝑆(𝑥𝑚𝑗)⟩or ⟨𝑆(𝑥𝑚)⟩results in the same polynomial. More formally, we claim that 𝑅⟨𝑆(𝑥𝑚𝑗)⟩(𝑥𝑚𝑗) = 𝑅⟨𝑆(𝑥𝑚)⟩(𝑥𝑚𝑗) (4.3) follows from Claim 4.5 with 𝒬= 𝑆(𝑥𝑚) and 𝑚′ = 𝑥𝑚𝑗."
  },
  {
    "prompt": "Understanding the computational aspects of modulo, Claim, check.",
    "response": "Let us check that the conditions of Claim 4.5 are satisfied. We need to check that 1. any monomial ˜𝑚irreducible modulo ⟨𝑆( ˜𝑚)⟩satisfying that 𝑆( ˜𝑚) ⊆𝑆(𝑥𝑚) is also irreducible modulo ⟨𝑆(𝑥𝑚)⟩, and 2. 𝑆(𝑥𝑚𝑗) ⊆𝑆(𝑥𝑚). The latter follows immediately from Claim 4.4 and item 1 follows from the reducibility condition: since the monomial 𝑥𝑚has degree at most 𝐷, it holds for all monomials ˜𝑚, if 𝑆( ˜𝑚) ⊆𝑆(𝑥𝑚), then ˜𝑚 is irreducible modulo ⟨𝑆( ˜𝑚)⟩if and only if ˜𝑚is irreducible modulo ⟨𝑆(𝑥𝑚)⟩."
  },
  {
    "prompt": "Read and understand this section about Claim, 𝑥𝑚𝑗, conclude from a research paper.",
    "response": "We may thus conclude (4.3) from Claim 4.5 applied with 𝒬= 𝑆(𝑥𝑚) and 𝑚′ = 𝑥𝑚𝑗. 15 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS 𝑈 Figure 1: A set 𝑈with a 3-hop to the left, a 2-hop on top and a lasso to the right."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, Observation, Claim.",
    "response": "We finish the proof of Lemma 4.6 by noting that ˜𝑅\u0000𝑥˜𝑅(𝑚)\u0001 = Õ 𝑗 𝑎𝑗𝑅⟨𝑆(𝑥𝑚)⟩(𝑥𝑚𝑗) [by (4.2) and (4.3)] (4.4a) = 𝑅⟨𝑆(𝑥𝑚)⟩ \u0010 Õ 𝑗 𝑎𝑗𝑥𝑚𝑗 \u0011 [by linearity of 𝑅⟨𝑆(𝑥𝑚)⟩] (4.4b) = 𝑅⟨𝑆(𝑥𝑚)⟩ \u0000𝑥𝑅⟨𝑆(𝑚)⟩(𝑚)\u0001 [as 𝑅⟨𝑆(𝑚)⟩(𝑚) = Í 𝑗𝑎𝑗𝑚𝑗] (4.4c) = 𝑅⟨𝑆(𝑥𝑚)⟩(𝑥𝑚) [by Observation 2.3, using Claim 4.3] (4.4d) = ˜𝑅(𝑥𝑚) . (4.4e) This establishes that ˜𝑅is a degree-𝐷pseudo-reduction for 𝒫as defined in Definition 2.1."
  },
  {
    "prompt": "Scientific context on Lemma, appeal, obtain and their role in graph theory.",
    "response": "We may thus appeal to Lemma 2.2 to obtain the desired polynomial calculus refutation degree lower bound for 𝒫. This completes the proof. □ 5 Closure for Graph Colouring While the previous section was fairly generic and we made few assumptions on the polynomial system 𝒫to be refuted, we now turn our attention to the colouring formula and make the first steps towards defining a support 𝑆for it."
  },
  {
    "prompt": "This is a technical discussion about 𝑆for, definition, introduce in theoretical computer science.",
    "response": "The goal of this section is to introduce the closure of a vertex set, a crucial notion in the definition of a support 𝑆for the colouring formula. In order to state the definition of our closure we need to introduce some terminology. Let 𝐺= (𝑉, 𝐸) be a graph. A walk of length 𝜏in 𝐺is a tuple of vertices (𝑣1, . . . , 𝑣𝜏+1) satisfying (𝑣𝑖, 𝑣𝑖+1) ∈𝐸for all 𝑖∈[𝜏]. A simple path is a walk where all vertices are distinct and a simple cycle is a walk (𝑣1, . . ."
  },
  {
    "prompt": "Scientific context on vertices, 𝑉we, descendant and their role in graph theory.",
    "response": ", 𝑣𝜏+1) of length 𝜏≥3 where 𝑣1 = 𝑣𝜏+1 and all other vertices are distinct. Suppose the set of vertices 𝑉has a linear order ≺on 𝑉. An increasing (decreasing) path in 𝐺is a simple path (𝑣1, . . . , 𝑣𝜏) where 𝑣𝑖≺𝑣𝑖+1 (𝑣𝑖≻𝑣𝑖+1) for all 𝑖∈[𝜏−1]. For vertices 𝑢, 𝑣∈𝑉we say that 𝑣is a descendant of 𝑢if there exists a decreasing path from 𝑢to 𝑣, and for a set of vertices 𝑈⊆𝑉we say that 𝑣is a descendant of 𝑈if it is a descendant of some vertex in 𝑈."
  },
  {
    "prompt": "Read and understand this section about Desc, descendants, write from a research paper.",
    "response": "We write Desc(𝑈) to denote the set of all descendants of 𝑈. We define every vertex to be a descendant of itself so that 𝑈⊆Desc(𝑈). The definition of our closure, besides the notion of descendants, also requires the notions of a hop and a lasso: a 𝜏-hop with respect to a set 𝑈⊆𝑉is a simple path or a simple cycle of length 𝜏 with the property that the two endpoints are both contained in 𝑈(in the case of cycles, the two endpoints coincide), while all other vertices are not in 𝑈."
  },
  {
    "prompt": "Explain a concept involving 𝑈is, lasso, graph in computational complexity.",
    "response": "Similarly a lasso with respect to 𝑈is a paw graph, that is, a walk (𝑣1, 𝑣2, 𝑣3, 𝑣4, 𝑣5) with 𝑣2 = 𝑣5 and all other vertices being distinct, such that only 𝑣1 is in 𝑈. See Figure 1 for an example of a set with some hops and a lasso. 16 5 Closure for Graph Colouring Algorithm 1 A procedure to obtain the closure of a given set 𝑈."
  },
  {
    "prompt": "Explain a concept involving Desc, 𝑊end, Closure in computational complexity.",
    "response": "1: procedure Closure(𝑈) 2: 𝑊0 ←Desc(𝑈) 3: 𝑖←0 4: while exists 2-, 3-, 4-hop or lasso 𝑄𝑖+1 with respect to 𝑊𝑖do 5: 𝑊𝑖+1 ←Desc(𝑊𝑖∪𝑉(𝑄𝑖+1)) 6: 𝑖←𝑖+ 1 7: 𝑊end ←𝑊𝑖 8: return 𝑊end With these notions in place we can now define a closure of a set 𝑈⊆𝑉. After showing the closure is unique, we define a process that given 𝑈constructs its closure and then prove that in sparse graphs the closure of 𝑈satisfies two important properties: 1."
  },
  {
    "prompt": "Explain a concept involving 𝑈itself, set, large in computational complexity.",
    "response": "it is the set of descendants of a set that is not much larger than 𝑈, as long as 𝑈itself is not too large, and, 2. if we remove the closure from the graph, then any small enough vertex set has a specially structured proper 3-colouring. These two properties will then be used to prove that the satisfiability condition and the reducibility condition which will allow us to apply Lemma 4.6. Definition 5.1 (Closure). Let 𝐺= (𝑉, 𝐸) be a graph and let 𝑈⊆𝑉."
  },
  {
    "prompt": "Scientific context on Desc, 𝑈is, closed and their role in graph theory.",
    "response": "We say that 𝑈is closed if 𝑈= Desc(𝑈) and there are no 2-, 3-, 4-hops or lassos with respect to 𝑈. A closure of 𝑈is any minimal closed set that contains 𝑈. Proposition 5.2. Every set of vertices has a unique closure. Proof. Suppose there are two distinct closures 𝑊1 and 𝑊2 of 𝑈. As 𝑊1 as well as 𝑊2 contains 𝑈it holds that 𝑈⊆𝑊∩= 𝑊1 ∩𝑊2."
  },
  {
    "prompt": "Explain a concept involving Desc, lasso, 𝑈and in computational complexity.",
    "response": "However, by minimality, 𝑊∩is not a closure of 𝑈and hence either there is a 2-, 3-, 4-hop or lasso 𝑄with respect to 𝑊∩or there is a vertex 𝑣in Desc(𝑊∩) that is not in 𝑊∩. In the first case this implies that some subgraph of 𝑄is either a 2-, 3-, 4-hop or a lasso with respect to 𝑊1 or 𝑊2 since a subgraph of a hop or lasso is again hop or a lasso. In the second case this implies that 𝑊1 ⊉Desc(𝑊1) or 𝑊2 ⊉Desc(𝑊2), since 𝑣∈Desc(𝑊1) ∩Desc(𝑊2)."
  },
  {
    "prompt": "This is a technical discussion about Algorithm, contradicts, assumption in theoretical computer science.",
    "response": "Either way, this contradicts the assumption that both 𝑊1 and 𝑊2 are closed. □ In light of Proposition 5.2, for a set 𝑈we write Cl(𝑈) to denote the unique closure of 𝑈. In order to establish a bound on the size of the closure the algorithmic, but otherwise equivalent, description in Algorithm 1 will be useful. Clearly the set of vertices 𝑊end returned by Algorithm 1 is closed and minimal by construction: only adding part of a hop or a lasso results in a smaller hop."
  },
  {
    "prompt": "Read and understand this section about 𝑊end, Proposition, 𝑊is from a research paper.",
    "response": "Hence Cl(𝑈) = 𝑊end by Proposition 5.2. The main property we use of our notion of closure is that the neighbourhood of a closed set 𝑊is very structured: since there are no 2-hops with respect to 𝑊, every vertex in 𝑁(𝑊) \\ 𝑊has a single neighbour in 𝑊, and since there are no 3-hops with respect to 𝑊the neighbourhood of 𝑊is an independent set. The absence of longer 4-hops and lassos imply similar, more technical properties for sets of vertices that are connected to 𝑊via short paths."
  },
  {
    "prompt": "Scientific context on Figure, graph, Lemma and their role in graph theory.",
    "response": "We leverage this structure to argue that after removing a closed set 𝑊from a sparse graph, all small vertex sets have a proper 3-colouring 17 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS 𝑊 𝑈 𝑁𝑈\\𝑊(𝑊) 𝐵𝑢 Figure 2: A depiction of the graphs 𝐵𝑢as defined in the proof of Lemma 5.3. with the additional property that the neighbours of each vertex at distance 1 from 𝑊are coloured with a single colour. Lemma 5.3. Suppose that 𝐺= (𝑉, 𝐸) is (ℓ, 1/3Δ)-sparse."
  },
  {
    "prompt": "Explain a concept involving 𝑉be, 𝑊is, vertex in computational complexity.",
    "response": "Let 𝑈, 𝑊⊆𝑉be vertex sets such that |𝑈| ≤ℓ, 𝑊is closed, and every vertex in 𝑁𝑈\\𝑊(𝑊) has degree at most Δ in 𝐺[𝑈\\ 𝑊]. Then there exists a proper 3-colouring of the subgraph 𝐺[𝑈\\ (𝑊∪𝑁𝑈(𝑊))] such that for every 𝑢∈𝑁𝑈\\𝑊(𝑊), the set 𝑁𝑈\\𝑊(𝑢) is monochromatic. Proof. We start the proof by establishing the following two properties. 1. First, since there are no lassos in 𝑈with respect to 𝑊, for every vertex 𝑢∈𝑁𝑈\\𝑊(𝑊), it holds that the neighbours 𝑁𝑈\\𝑊(𝑢) of 𝑢are an independent set."
  },
  {
    "prompt": "This is a technical discussion about graph, center, 𝑢and in theoretical computer science.",
    "response": "Hence the graph 𝐵𝑢= 𝐺[{𝑢} ∪𝑁𝑈\\𝑊(𝑢)] is a star with center 𝑢and leaves 𝑁𝑈\\𝑊(𝑢). 2. Second, as there are no 3-, 4-hops in 𝑈with respect to 𝑊, the graphs {𝐵𝑢| 𝑢∈𝑁𝑈\\𝑊(𝑊)} as just defined are pairwise vertex disjoint. See Figure 2 for an illustration. Let 𝐺′ be the graph obtained from 𝐺[𝑈\\ 𝑊] by contracting each star 𝐵𝑢to a vertex e𝑢."
  },
  {
    "prompt": "Scientific context on Note, proper, star and their role in graph theory.",
    "response": "Note that from a 3-colouring e𝜒of 𝐺′ we can define a 3-colouring 𝜒of 𝐺[𝑈\\ (𝑊∪𝑁𝑈(𝑊))] with the desired property: we claim that the 3-colouring 𝜒: 𝑈\\ (𝑊∪𝑁𝑈(𝑊)) →{1, 2, 3} defined by 𝜒(𝑤) = ( e𝜒(e𝑢) if 𝑤is a leaf of a star 𝐵𝑢, e𝜒(𝑤) otherwise (5.1) is a proper 3-colouring of 𝐺[𝑈\\ (𝑊∪𝑁𝑈(𝑊))]. Note that since the graphs {𝐵𝑢| 𝑢∈𝑁𝑈\\𝑊(𝑊)} are pairwise disjoint, the colouring 𝜒assigns precisely one colour per vertex and, as each of the induced graphs 𝐵𝑢is a star, the colouring is proper."
  },
  {
    "prompt": "Scientific context on Finally, construction, vertices and their role in graph theory.",
    "response": "Finally, for all vertices 𝑢∈𝑁𝑈\\𝑊(𝑊) the set 𝑁𝑈\\𝑊(𝑢) is monochromatic by construction. It remains to argue that 𝐺′ is 3-colourable. Here we want to use the extreme sparsity of 𝐺: we enforce such extreme sparsity on 𝐺that even though 𝐺′ is obtained from 𝐺by contractions and hence has higher edge density than 𝐺, the graph 𝐺′ is still (|𝑉(𝐺′)|, 1/3)-sparse. Thus by Lemma 2.6 it follows that 𝐺′ is 3-colourable."
  },
  {
    "prompt": "This is a technical discussion about sparse, satisfies, subset in theoretical computer science.",
    "response": "Let us verify that 𝐺′ is indeed (|𝑉(𝐺′)|, 1/3)-sparse, that is, we need to argue that every subset 𝑇′ ⊆𝑉(𝐺′) satisfies |𝐸(𝑇′)| ≤(1 + 1/3)|𝑇′|. Fix such a subset 𝑇′ ⊆𝑉(𝐺′) and let 𝑇be the preimage of 𝑇′ in the contraction. We estimate |𝐸(𝑇′)| in terms of |𝐸(𝑇)|. Let {𝑢1, . . . , 𝑢𝑡} = 𝑇∩𝑁𝑈\\𝑊(𝑊). Observe that in 𝑇′ each star 𝐵𝑢𝑖is contracted to a vertex e𝑢𝑖. Let 𝑠= Í𝑡 𝑖=1(|𝑉(𝐵𝑢𝑖)| −1)."
  },
  {
    "prompt": "Read and understand this section about holds, Closure, Graph from a research paper.",
    "response": "It holds that 𝑠≤(Δ −1)𝑡≤(Δ −1)|𝑇′| , (5.2) 18 5 Closure for Graph Colouring where we use the assumption that the degree of every vertex 𝑢𝑖in 𝐺[𝑈\\ 𝑊] is bounded by Δ. Furthermore, it holds that |𝑇| = |𝑇′| + 𝑠and, because all the edges in the stars 𝐵𝑢𝑖are contracted, that |𝐸(𝑇)| ≥|𝐸(𝑇′)| + 𝑠. By assumption, 𝐺is (ℓ, 1/3Δ)-sparse and |𝑇| ≤|𝑈| ≤ℓ. This implies that |𝐸(𝑇)| ≤(1 + 1/3Δ)|𝑇| which, if combined with the above, gives |𝐸(𝑇′)| + 𝑠≤(1 + 1/3Δ)(|𝑇′| + 𝑠)."
  },
  {
    "prompt": "Read and understand this section about Lemma, sparse, conclude from a research paper.",
    "response": "Using (5.2) we may conclude that |𝐸(𝑇′)| ≤ \u0012 1 + 1 3Δ \u0013 |𝑇′| + 𝑠 3Δ ≤ \u0012 1 + 1 3Δ \u0013 |𝑇′| + (Δ −1)|𝑇′| 3Δ = \u0012 1 + 1 3 \u0013 |𝑇′| . (5.3) Therefore, 𝐺′ is (|𝐺′|, 1/3)-sparse and thus, by Lemma 2.6, it follows that 𝐺′ is 3-colourable. □ The second property of the closure that we need is that we have some control on how large it is, which is necessary to show that the satisfiability condition of Lemma 4.6 holds."
  },
  {
    "prompt": "Read and understand this section about set, closure, 𝑈is from a research paper.",
    "response": "We establish an upper bound on the size of the closure by proving that for sparse graphs the closure of a not too large set 𝑈is the set of descendants of a set 𝑍that is not much larger than 𝑈, as stated in the lemma below. Combining this with Lemma 2.6, it follows that in order to establish that the set Cl(𝑈) is 3-colourable—and thus Col(𝐺[Cl(𝑈)], 3) is satisfiable—it is sufficient to prove an upper bound on the size of the set of descendants of 𝑍."
  },
  {
    "prompt": "Explain a concept involving Jumping, bit, vertices in computational complexity.",
    "response": "Jumping ahead a bit, we will be able to establish such an upper bound in the next section by choosing an appropriate ordering of the vertices. Lemma 5.4. Suppose that 𝐺= (𝑉, 𝐸) has a linear order on 𝑉and is (ℓ, 1/3𝑎)-sparse for 𝑎≥2. Let 𝑈⊆𝑉 be a set of size |𝑈| ≤ℓ/25𝑎such that any decreasing path in 𝐺[𝑉\\ 𝑈] has at most 𝑎vertices. Then there exists a set 𝑍⊆𝑉such that 𝑍⊇𝑈, |𝑍| ≤25|𝑈| and Cl(𝑈) = Desc(𝑍). Proof."
  },
  {
    "prompt": "Read and understand this section about Algorithm, 𝑉can, set from a research paper.",
    "response": "Recall from Algorithm 1 that the closure of a set 𝑈⊆𝑉can be defined to be the final set in a sequence (𝑊0, 𝑊1, . . . , 𝑊end), where 𝑊0 = Desc(𝑈) and 𝑊𝑖+1 is obtained from 𝑊𝑖by appending a 2-, 3-, 4-hop or a lasso with respect to 𝑊𝑖and then taking the descendants of the resulting set of vertices. A key observation is that adding such hops or lassos to 𝑊𝑖adds more edges to the induced graph 𝐺[𝑊𝑖] than vertices, thus increasing the edge density."
  },
  {
    "prompt": "Scientific context on 𝐺is, sequence, locally and their role in graph theory.",
    "response": "As the graph 𝐺is locally sparse we can conclude that the sequence (𝑊0, 𝑊1, . . . , 𝑊end) needs to be rather short, which allows us to argue the size upper bound on 𝑍. In the following, for each 𝑊𝑖we identify a vertex set 𝑈𝑖such that the edge density of the graph 𝐺[𝑈𝑖] increases with 𝑖. The idea is as follows. Since the sets 𝑈𝑖grow very slowly and thus the local sparsity always applies, we will be able to conclude that the number of iterations in the construction of Cl(𝑈) is bounded."
  },
  {
    "prompt": "Explain a concept involving set, vertices, descendants in computational complexity.",
    "response": "As the vertices in 𝑊𝑖are the descendants of the set 𝑈𝑖, it holds that the vertices in 𝑊end are the descendants of a set which is not much larger than the initial set 𝑈, whereby the lemma follows. Let us now implement this plan. We define 𝑈𝑖inductively as follows. Let 𝑈0 = 𝑈and let 𝑄𝑖 be the hop or lasso added to 𝑊𝑖−1 at iteration 𝑖≥1."
  },
  {
    "prompt": "Explain a concept involving 𝑃𝑣be, 𝑃𝑢and, 𝑈𝑖to in computational complexity.",
    "response": "If we denote by 𝑢and 𝑣the endpoints of 𝑄𝑖 (where we could have 𝑢= 𝑣) and let 𝑃𝑢and 𝑃𝑣be two shortest decreasing paths from 𝑈𝑖−1 to 𝑢 and 𝑣, respectively, then it holds that 𝑈𝑖= 𝑈𝑖−1 ∪𝑉(𝑃𝑢∪𝑃𝑣∪𝑄𝑖). See Figure 3 for an illustration. For our definition of 𝑈𝑖to be meaningful, we need to establish that the paths 𝑃𝑢and 𝑃𝑣always exist. Claim 5.5. For every vertex 𝑣in 𝑊𝑖, there exists a decreasing path in 𝑊𝑖from some vertex in 𝑈𝑖to 𝑣. Proof. The proof is by induction on 𝑖."
  },
  {
    "prompt": "Scientific context on Desc, holds, base and their role in graph theory.",
    "response": "The base case 𝑖= 0 holds because 𝑊0 = Desc(𝑈0). For the induction step, suppose that the claim holds for 𝑖−1. By definition, the vertices in 𝑊𝑖\\ 𝑊𝑖−1 are descendants of a vertex in 𝑄𝑖, and all vertices in 𝑄𝑖are contained in 𝑈𝑖. The claim follows. □ 19 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS 𝑊𝑖−1 = Desc(𝑈𝑖−1) 𝑊𝑖= Desc(𝑈𝑖) 𝑈𝑖−1 𝑢 𝑣 𝑄𝑖 𝑃𝑣 𝑈𝑖 Figure 3: A depiction of the construction of 𝑈𝑖as defined in the proof of Lemma 5.4."
  },
  {
    "prompt": "Read and understand this section about grows, density, exceeds from a research paper.",
    "response": "Next, we show that |𝑈𝑖| grows slowly with 𝑖and that the edge density |𝐸(𝑈𝑖)|/|𝑈𝑖| exceeds the sparsity threshold (1 + 1/3𝑎) after a small number of iterations. Claim 5.6. For 𝑈𝑖as commented above it holds that |𝑈𝑖\\ 𝑈𝑖−1| ≤2𝑎+ |𝑉(𝑄𝑖)| −4 and |𝐸(𝑈𝑖)| ≥ |𝐸(𝑈𝑖−1)| + |𝑈𝑖\\ 𝑈𝑖−1| + 1. Proof. Let 𝐹be the graph defined by the union of the edges in 𝑃𝑢, 𝑃𝑣and 𝑄𝑖."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, 𝑃𝑣are, paths.",
    "response": "Since the paths 𝑃𝑢and 𝑃𝑣are decreasing paths, according to the statement of Lemma 5.4, they contain at most 𝑎vertices each, so 3 ≤|𝑉(𝐹)| ≤2𝑎+ |𝑉(𝑄𝑖)| −2. Moreover, the endpoints of 𝐹are contained in 𝑈𝑖−1 and all other vertices in 𝐹are outside of 𝑈𝑖−1. By our choice of 𝑃𝑢and 𝑃𝑣there are two cases, depending on whether 𝐹contains a cycle or not. Case 1: If there is no cycle in 𝐹, then |𝑉(𝐹) ∩𝑈𝑖−1| = 2 so |𝑈𝑖\\ 𝑈𝑖−1| = |𝑉(𝐹)| −2. Moreover |𝐸(𝐹)| ≥|𝑉(𝐹)| −1 since 𝐹is connected."
  },
  {
    "prompt": "This is a technical discussion about 𝐹contains, cycle, number in theoretical computer science.",
    "response": "Case 2: If 𝐹contains a cycle, then |𝑉(𝐹) ∩𝑈𝑖−1| = 1, hence |𝑈𝑖\\ 𝑈𝑖−1| = |𝑉(𝐹)| −1. In addition, |𝐸(𝐹)| ≥|𝑉(𝐹)| since 𝐹is connected and contains a cycle. Moreover, it holds that |𝑉(𝐹)| ≤2𝑎+ |𝑉(𝑄𝑖)| −3. Since in both cases the number of added vertices is bounded |𝑈𝑖\\ 𝑈𝑖−1| ≤2𝑎+ |𝑉(𝑄𝑖)| −4 and the number of edges in the subgraph induced by 𝑈𝑖can be lower bounded |𝐸(𝑈𝑖)| ≥ |𝐸(𝑈𝑖−1)| + |𝑈𝑖\\ 𝑈𝑖−1| + 1 the statement follows."
  },
  {
    "prompt": "Scientific context on Recall, increases, show and their role in graph theory.",
    "response": "□ Recall that we want to show that the edge density of the induced subgraph 𝐺[𝑈𝑖] increases with 𝑖. Since 𝐺is sparse it thus follows that the number of iterations in the construction of Cl(𝑈) is bounded. Let 𝑠= |𝑈|. Towards contradiction, suppose that 𝑖≥5𝑠+ 1. Note that |𝑉(𝑄𝑖)| ≤5, so by Claim 5.6 we have |𝑈𝑖\\ 𝑈𝑖−1| ≤2𝑎+ 1."
  },
  {
    "prompt": "Scientific context on 𝐺is, sparse, implies and their role in graph theory.",
    "response": "This implies that |𝑈5𝑠+1| = 𝑠+ 5𝑠+1 Õ 𝑖=1 |𝑈𝑖\\ 𝑈𝑖−1| ≤𝑠+ (5𝑠+ 1)(2𝑎+ 1) < 16𝑎|𝑈| < ℓ, (5.4) and therefore, since 𝐺is (ℓ, 1/3𝑎)-sparse, it holds that |𝐸(𝑈5𝑠+1)|/|𝑈5𝑠+1| ≤1 + 1/3𝑎."
  },
  {
    "prompt": "Read and understand this section about Bound, Claim, Lower from a research paper.",
    "response": "However, by 20 6 A Lower Bound for 3-Colourability on Sparse Random Graphs Claim 5.6, we have that |𝐸(𝑈5𝑠+1)| |𝑈5𝑠+1| = |𝐸(𝑈)| + Í5𝑠+1 𝑖=1 |𝐸(𝑈𝑖) \\ 𝐸(𝑈𝑖−1)| 𝑠+ Í5𝑠+1 𝑖=1 |𝑈𝑖\\ 𝑈𝑖−1| (5.5a) ≥|𝐸(𝑈)| + Í5𝑠+1 𝑖=1 (|𝑈𝑖\\ 𝑈𝑖−1| + 1) 𝑠+ Í5𝑠+1 𝑖=1 |𝑈𝑖\\ 𝑈𝑖−1| (5.5b) ≥0 + Í5𝑠+1 𝑖=1 (2𝑎+ 2) 𝑠+ Í5𝑠+1 𝑖=1 (2𝑎+ 1) (5.5c) > 1 + 1 3𝑎, (5.5d) where for (5.5b) we use that |𝐸(𝑈𝑖)| ≥|𝐸(𝑈𝑖−1)| + |𝑈𝑖\\ 𝑈𝑖−1| + 1, and then for (5.5c), we observe that the fraction decreases asÍ5|𝑈|+1 𝑖=1 |𝑈𝑖\\𝑈𝑖−1| increases and thus, along with the bound |𝑈𝑖\\𝑈𝑖−1| ≤2𝑎+1, we obtain the claimed inequality."
  },
  {
    "prompt": "This is a technical discussion about inequality, final, 𝐺is in theoretical computer science.",
    "response": "For the final inequality (5.5d) we use 𝑎≥2. This contradicts the assumption that 𝐺is (ℓ, 1/3𝑎)-sparse and it hence follows that 𝑖≤5|𝑈|. Let 𝑖end be the last iteration and let 𝑍= 𝑈∪Ð 𝑗≤𝑖end 𝑉(𝑄𝑗). We claim that Desc(𝑍) = Cl(𝑈) and |𝑍| ≤25|𝑈|. Indeed, at iteration 𝑖in the construction, 𝑊𝑖is the set of descendants of 𝑈∪Ð 𝑗≤𝑖𝑉(𝑄𝑗), and the hop or lasso 𝑄𝑖added to 𝑊𝑖−1 contains at most 4 vertices not already in 𝑊𝑖."
  },
  {
    "prompt": "Understanding the computational aspects of Desc, Bound, Random.",
    "response": "Therefore, Desc(𝑍) = Cl(𝑈) and since 𝑖end ≤5|𝑈| it follows that |𝑍| ≤|𝑈| + 4 · 5|𝑈| ≤25|𝑈|. □ 6 A Lower Bound for 3-Colourability on Sparse Random Graphs We are now ready to prove a linear degree lower bound for polynomial calculus refutations of the claim that sparse random graphs are 3-colourable."
  },
  {
    "prompt": "Read and understand this section about graph, fact, stronger from a research paper.",
    "response": "In fact, we prove something slightly stronger: we show that if a graph is locally very sparse and has only few vertices of high degree, then it is hard for polynomial calculus to refute the claim that the graph is 3-colourable. Theorem 6.1 (Main theorem). Let 𝑐, Δ, 𝑘and ℓbe integers such that 𝑐> 𝑘≥3 and let 𝐺= (𝑉, 𝐸) be a 𝑐-colourable, (ℓ, 1/3Δ)-sparse graph."
  },
  {
    "prompt": "Explain a concept involving Col, degree, Theorem in computational complexity.",
    "response": "For any set 𝑇Δ such that 𝐺[𝑉\\ 𝑇Δ] has maximum degree at most Δ, any polynomial calculus refutation of Col(𝐺, 𝑘) over any field requires degree ℓ/50Δ𝑐−1 −|𝑇Δ|. We defer the proof of Theorem 6.1 to Section 6.1, and first show how our results follow from this theorem. Intuitively, the above theorem holds because sparse graphs are locally 3-colourable (see Lemma 2.6) and hence the colouring formula defined over sparse graphs is locally satisfiable."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Lemma, graphs.",
    "response": "Before diving into the proof of Theorem 6.1, let us see how we can use Lemma 2.7 to obtain degree lower bounds for random graphs. Note that the condition on the graph 𝐺in Theorem 6.1 is not inherently random. However, to the best of our knowledge, there are no explicit constructions of graphs that are this sparse. In fact, it is stated as an open problem in the survey by Hoory et al. [HLW06, Open problem 10.8] to explicitly construct such graphs."
  },
  {
    "prompt": "This is a technical discussion about Lemma, G𝑛,𝑑are, distribution in theoretical computer science.",
    "response": "Recall from Lemma 2.8 that if 𝑑≥6, then graphs sampled from the Erdős-Rényi random graph distribution G(𝑛, 𝑑/𝑛) or the random 𝑑-regular graph distribution G𝑛,𝑑are asymptotically almost surely not 3-colourable. In light of this, the below statements are interesting in the parameter regime 𝑑≥6. Corollary 6.2 (Colouring lower bound for random regular graphs). There exists an absolute constant 𝐶such that for positive integers 𝑛and 𝑑≥2 satisfying 6𝑑3 ≤log 𝑛the following holds."
  },
  {
    "prompt": "Read and understand this section about Col, G𝑛,𝑑and, polynomial from a research paper.",
    "response": "If 𝐺is a graph sampled from G𝑛,𝑑and 𝑘≥3 is an integer, then asymptotically almost surely every polynomial calculus refutation of Col(𝐺, 𝑘) over any field requires degree 𝑑−𝐶𝑑· 𝑛. 21 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS Proof. Let Δ = 𝑑and 𝑇Δ = ∅. By Lemma 2.8 we have that asymptotically almost surely 𝐺 is 𝑐-colourable for 𝑐≤2𝑑/log 𝑑, and by Lemma 2.7 that asymptotically almost surely 𝐺is (ℓ, 1/3𝑑)-sparse for ℓ= (8𝑑)−6𝑑𝑛."
  },
  {
    "prompt": "Scientific context on Lemma, Note, apply and their role in graph theory.",
    "response": "Note that to apply Lemma 2.7, we use 𝜀= 1/3𝑑and 𝛿= 1/2. We can now apply Theorem 6.1 and conclude that any polynomial calculus refutation of Col(𝐺, 𝑘), over any field, requires degree ℓ/50Δ𝑐−1 −|𝑇Δ| = (8𝑑)−6𝑑𝑛/50𝑑2𝑑/log 𝑑−1 ≥𝑑−𝐶𝑑· 𝑛, using that 𝐶is a large enough constant. □ To prove the result for Erdős-Rényi random graphs we need the additional property that asymptotically almost surely there exists a small set 𝑇Δ of vertices such that 𝐺[𝑉\\ 𝑇Δ] has maximum degree at most Δ. Lemma 6.3."
  },
  {
    "prompt": "Understanding the computational aspects of log, sampled, bound.",
    "response": "Let 𝐺= (𝑉, 𝐸) be a graph sampled from G(𝑛, 𝑑/𝑛) where 𝑑= O(log 𝑛). If Δ ≥𝑑is such that (Δ/𝑒𝑑)Δ = o(𝑛), then asymptotically almost surely there exists a set 𝑇Δ of size at most (𝑒𝑑/Δ)Δ · 2𝑒𝑛such that the maximum degree in 𝐺[𝑉\\ 𝑇Δ] is at most Δ −1. The proof of Lemma 6.3 is mostly a standard calculation and we present it in Appendix C for completeness. We are now ready to prove our lower bound for Erdős-Rényi random graphs. Corollary 6.4 (Colouring lower bound for Erdős-Rényi random graphs)."
  },
  {
    "prompt": "Scientific context on 𝐶such, satisfying, log and their role in graph theory.",
    "response": "There exists an absolute constant 𝐶such that for 𝑛∈N+ and 𝑑∈R+ satisfying that 𝑑> 1 and 𝑑5 = o(log 𝑛) the following holds. If 𝐺is a graph sampled from G(𝑛, 𝑑/𝑛) and 𝑘≥3 is an integer, then asymptotically almost surely every polynomial calculus refutation of Col(𝐺, 𝑘) over any field requires degree 𝑑−𝐶𝑑5 · 𝑛. Proof. Fix Δ = (5𝑑)5."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, 𝐺is, asymptotically.",
    "response": "By Lemma 2.8 we have that asymptotically almost surely 𝐺is 𝑐-colourable for 𝑐≤2𝑑/log 𝑑, and by Lemma 2.7 it holds asymptotically almost surely that 𝐺is (ℓ, 𝜀)-sparse for ℓ= (4𝑑)−4(5𝑑)5𝑛and 𝜀= 1/3Δ. Note that to apply Lemma 2.7, we can use 𝛿= 1/4, so that (1 + 𝜀)(1 + 𝛿)/𝜀≤4Δ. Let 𝑇Δ ⊆𝑉be a minimum size set such that 𝐺[𝑉\\ 𝑇Δ] has maximum degree at most Δ."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, inequality.",
    "response": "By Lemma 6.3, we have that |𝑇Δ| ≤(𝑒𝑑/Δ)Δ · 2𝑒𝑛= 2𝑒𝑛 (55𝑑4/𝑒)(5𝑑)5 < 𝑛 2 · (5𝑑)4(5𝑑)5 , (6.1) where the last inequality follows since (5/𝑒)(5𝑑)5 ≥(5/𝑒)55 > 4𝑒."
  },
  {
    "prompt": "Explain a concept involving log, inequality, Theorem in computational complexity.",
    "response": "We can now apply Theorem 6.1 and conclude that any polynomial calculus refutation of Col(𝐺, 𝑘), over any field, requires degree ℓ 50Δ𝑐−1 −|𝑇Δ| ≥ 𝑛 50 · (5𝑑)10𝑑/log 𝑑· (4𝑑)4(5𝑑)5 − 𝑛 2 · (5𝑑)4(5𝑑)5 (6.2a) ≥ 𝑛 (5𝑑)4(5𝑑)5 − 𝑛 2 · (5𝑑)4(5𝑑)5 (6.2b) ≥𝑑−𝐶𝑑5 · 𝑛, (6.2c) where for the second inequality we use that 50 · (5𝑑)10𝑑/log 𝑑< 210 · (10)10𝑑≤2(5𝑑)5 and for the last inequality we use that 𝐶is a large enough constant."
  },
  {
    "prompt": "Scientific context on Col, Theorem, Proof and their role in graph theory.",
    "response": "□ 6.1 Proof of Main Theorem Fix 𝑇Δ such that 𝐺[𝑉\\ 𝑇Δ] has maximum degree at most Δ and let 𝑋= \b 𝑥𝑣,𝑖| 𝑣∈𝑉(𝐺), 𝑖∈[𝑘] . 22 6 A Lower Bound for 3-Colourability on Sparse Random Graphs To prove Theorem 6.1, our goal is to define a Col(𝐺, 𝑘)-support 𝑆that maps monomials in the polynomial ring F[𝑋] to subsets of Col(𝐺, 𝑘) such that Lemma 4.6 holds. For brevity, given a set 𝑈⊆𝑉we denote the ideal ⟨Col(𝐺[𝑈], 𝑘)⟩by ⟨𝑈⟩and refer to the polynomials in Col(𝐺[𝑈], 𝑘) as the generators of ⟨𝑈⟩."
  },
  {
    "prompt": "Explain a concept involving variables, vertices, lemma in computational complexity.",
    "response": "Given a monomial 𝑚, we let 𝑉(𝑚) denote the set of vertices mentioned by the variables in 𝑚. Moreover, given a linear order ≺v on the vertices, we say an admissible order ≺of the monomials over the variables 𝑋of Col(𝐺, 𝑘) respects ≺v if for any colours 𝑖, 𝑗∈[𝑘] it holds that 𝑥𝑢,𝑖≺𝑥𝑣,𝑗 whenever 𝑢≺v 𝑣. The main technical lemma we need in order to prove Theorem 6.1 is the reducibility lemma below, from which the reducibility condition of Lemma 4.6 will follow."
  },
  {
    "prompt": "Scientific context on 𝑊that, reducibility, lemma and their role in graph theory.",
    "response": "The reducibility lemma implies, in particular, that reducing a monomial 𝑚modulo ⟨𝑊⟩for any closed set 𝑊⊇𝑉(𝑚) ∪𝑇Δ is the same as reducing modulo ⟨𝑈⟩for any superset 𝑈⊇𝑊that is not too large. Lemma 6.5 (Reducibility lemma). Let 𝐺= (𝑉, 𝐸) be a (ℓ, 1/3Δ)-sparse graph with a linear order ≺v on 𝑉and consider an admissible order that respects ≺v."
  },
  {
    "prompt": "Read and understand this section about modulo, reducible, 𝑈satisfy from a research paper.",
    "response": "If the vertex sets 𝑊⊆𝑈satisfy hat 𝑊is closed, that the size of 𝑈is |𝑈| ≤ℓ, and that every vertex in 𝑁𝑈\\𝑊(𝑊) has degree at most Δ in 𝐺[𝑈\\ 𝑊], then for every monomial 𝑚such that 𝑉(𝑚) ⊆𝑊, it holds that 𝑚is reducible modulo ⟨𝑈⟩if and only if 𝑚is reducible modulo ⟨𝑊⟩. We postpone the proof of this lemma to the end of this section."
  },
  {
    "prompt": "Scientific context on monomial, 𝑊that, closure and their role in graph theory.",
    "response": "In order to ensure that we are always reducing a monomial 𝑚by ⟨𝑊⟩for some closed set 𝑊that contains 𝑉(𝑚) and is such that every vertex in 𝑁𝑈\\𝑊(𝑊) has degree at most Δ in 𝐺[𝑈\\ 𝑊] for all 𝑈⊇𝑊, we define the closure of a monomial to include the set 𝑇Δ. Definition 6.6 (Monomial closure). The monomial closure of a monomial 𝑚, denoted by ClΔ(𝑚), is the vertex set Cl(𝑉(𝑚) ∪𝑇Δ)."
  },
  {
    "prompt": "Understanding the computational aspects of Col, Lemma, ClΔ.",
    "response": "Looking ahead, we will prove Theorem 6.1 by showing that the map 𝑆defined by mapping monomials 𝑚to Col(𝐺[ClΔ(𝑚)], 𝑘) ⊆Col(𝐺, 𝑘) is a Col(𝐺, 𝑘)-support and appealing to Lemma 4.6. To establish the satisfiability condition in Lemma 4.6, we must prove that Col(𝐺[ClΔ(𝑚)], 𝑘) is satisfiable whenever 𝑚is of low degree. Since 𝐺is sparse, by Lemma 2.6, it suffices to show that the monomial closure of 𝑚is not too large. This, in turn, will follow from the next lemma."
  },
  {
    "prompt": "This is a technical discussion about Lemma, size, assumptions in theoretical computer science.",
    "response": "Lemma 6.7 is an almost direct consequence of Lemma 5.4 and states that under suitable technical assumptions, the size of the monomial closure of 𝑚is closely related to the degree of 𝑚and the size of 𝑇Δ. Lemma 6.7 (Satisfiability lemma). Let 𝑎, Δ, ℓ∈N+ such that 𝑎≥2 and let 𝐺= (𝑉, 𝐸) be a (ℓ, 1/3𝑎)-sparse graph with a linear order on 𝑉. Let 𝑇Δ ⊆𝑉be such that 𝐺[𝑉\\ 𝑇Δ] has maximum degree at most Δ, Desc(𝑇Δ) ⊆𝑇Δ, and that any decreasing path in 𝐺[𝑉\\ 𝑇Δ] has at most 𝑎vertices."
  },
  {
    "prompt": "Explain a concept involving Deg, ClΔ, holds in computational complexity.",
    "response": "Then it holds for any monomial 𝑚that if Deg(𝑚) + |𝑇Δ| ≤ℓ/25𝑎, then |ClΔ(𝑚)| ≤50Δ𝑎−1 · (Deg(𝑚) + |𝑇Δ|). Proof. Let 𝑈= 𝑉(𝑚) ∪𝑇Δ and note that ClΔ(𝑚) = Cl(𝑈). Note that |𝑈| = |𝑉(𝑚) ∪𝑇Δ| ≤Deg(𝑚) + |𝑇Δ| ≤ℓ/25𝑎and that any decreasing path in 𝐺[𝑉\\ 𝑈] has at most 𝑎vertices. We can therefore apply Lemma 5.4 to deduce that there exists a set 𝑍⊆𝑉such that 𝑈⊆𝑍, Cl(𝑈) = Desc(𝑍) and |𝑍| ≤25|𝑈|."
  },
  {
    "prompt": "Explain a concept involving Desc, descendants, Note in computational complexity.",
    "response": "Note that since 𝑍⊇𝑇Δ and all the descendants of vertices in 𝑇Δ are in 𝑇Δ, we have that Desc(𝑍) = (Desc(𝑍\\ 𝑇Δ) \\ 𝑇Δ) ∪𝑇Δ. Moreover, since any vertex 𝑣∈𝑍\\ 𝑇Δ has degree at most Δ in 𝐺[𝑉\\𝑇Δ], and since any decreasing path in 𝐺[𝑉\\𝑇Δ] has at most 𝑎vertices, it follows that 𝑣has at most 2Δ𝑎−1 descendants in 𝑉\\ 𝑇Δ."
  },
  {
    "prompt": "Read and understand this section about Desc, Deg, COLOURING from a research paper.",
    "response": "We thus have the upper bound |(Desc(𝑍\\ 𝑇Δ) \\ 𝑇Δ)| ≤2Δ𝑎−1 · |𝑍\\ 𝑇Δ| from which we conclude that |Desc(𝑍)| ≤2Δ𝑎−1 · |𝑍\\ 𝑇Δ| + |𝑇Δ| ≤2Δ𝑎−1 · |𝑍| ≤50Δ𝑎−1 · (Deg(𝑚) + |𝑇Δ|) , 23 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS as claimed in the lemma. □ We are now ready to prove our main theorem, which we restate here for convenience. Theorem 6.1 (Main theorem, restated). Let 𝑐, Δ, 𝑘and ℓbe integers such that 𝑐> 𝑘≥3 and let 𝐺= (𝑉, 𝐸) be a 𝑐-colourable, (ℓ, 1/3Δ)-sparse graph."
  },
  {
    "prompt": "Understanding the computational aspects of Col, set, degree.",
    "response": "For any set set 𝑇Δ such that 𝐺[𝑉\\ 𝑇Δ] has maximum degree at most Δ, then any polynomial calculus refutation of Col(𝐺, 𝑘) over any field requires degree ℓ/50Δ𝑐−1 −|𝑇Δ|. Proof of Theorem 6.1. We start by defining a linear order on 𝑉. Let 𝜒𝑐: 𝑉\\ 𝑇Δ →[𝑐] be a proper 𝑐-colouring of 𝐺[𝑉\\𝑇Δ]. We let the order ≺v on 𝑉be any linear order that satisfies 𝑢≺v 𝑣whenever 𝑢∈𝑇Δ and 𝑣∈𝑉\\ 𝑇Δ and whenever 𝑢, 𝑣∈𝑉\\ 𝑇Δ and 𝜒𝑐(𝑢) < 𝜒𝑐(𝑣)."
  },
  {
    "prompt": "This is a technical discussion about Desc, Observe, 𝑥𝑢,𝑖 in theoretical computer science.",
    "response": "Observe that any decreasing path in 𝐺[𝑉\\ 𝑇Δ] has at most 𝑐vertices and that Desc(𝑇Δ) ⊆𝑇Δ. We can define an admissible ordering ≺of the monomials over the variables {𝑥𝑣,𝑖}𝑣∈𝑉,𝑖∈[𝑘] of Col(𝐺, 𝑘) that respects ≺v as follows: for distinct vertices 𝑢, 𝑣let 𝑥𝑢,𝑖≺𝑥𝑣,𝑗whenever 𝑢≺v 𝑣and for variables associated with the same vertex 𝑢let 𝑥𝑢,𝑖≺𝑥𝑢,𝑗whenever 𝑖< 𝑗."
  },
  {
    "prompt": "Scientific context on ordering, monomials, Lemma and their role in graph theory.",
    "response": "With this order fixed we then obtain the admissible ordering on monomials by first ordering the monomials by degree and then lexicographically according to the ordering on the variables. To prove Theorem 6.1 we use Lemma 4.6. For this, we show that the map 𝑆: 𝑚↦→Col(𝐺[ClΔ(𝑚)], 𝑘) is a Col(𝐺, 𝑘)-support in the sense of Definition 4.1 and that it satisfies the satisfiability condition and the reducibility condition in Lemma 4.6 for 𝐷= ℓ/(50Δ𝑐−1) −|𝑇Δ|."
  },
  {
    "prompt": "Read and understand this section about Col, 𝑆is, Vars from a research paper.",
    "response": "We start by proving, via the properties of the closure, that the map 𝑆is a Col(𝐺, 𝑘)-support. We actually show something slightly stronger, namely that 𝑆satisfies the following four properties, which are the same as those in Definition 4.1 except that we do not require that 𝑚′ ≺𝑚in item 2 and item 3. 1. For all monomials 𝑚and 𝑚′ such that Vars(𝑚) = Vars(𝑚′), it holds that 𝑆(𝑚) = 𝑆(𝑚′). 2. For every variable 𝑥and for all monomials 𝑚and 𝑚′, if 𝑆(𝑚′) ⊆𝑆(𝑚), then 𝑆(𝑥𝑚′) ⊆𝑆(𝑥𝑚). 3."
  },
  {
    "prompt": "Scientific context on Vars, ClΔ, Item and their role in graph theory.",
    "response": "For all monomials 𝑚and 𝑚′, if Vars(𝑚′) ⊆Vars\u0000𝑆(𝑚)\u0001, then 𝑆(𝑚′) ⊆𝑆(𝑚). 4. For all 𝑝∈𝒫, it holds that 𝑝∈𝑆(𝑚), where 𝑚is the leading monomial in 𝑝. Item 1 follows immediately from the definition of 𝑆, since the closure of a monomial 𝑚only depends on 𝑉(𝑚). Note that since 𝑆(𝑚) = Col(𝐺[ClΔ(𝑚)], 𝑘) it holds that 𝑆(𝑚′) ⊆𝑆(𝑚) if and only if ClΔ(𝑚′) ⊆ClΔ(𝑚) and, therefore, item 2 is equivalent to showing that if ClΔ(𝑚′) ⊆ClΔ(𝑚), then ClΔ(𝑥𝑚′) ⊆ClΔ(𝑥𝑚)."
  },
  {
    "prompt": "Explain a concept involving ClΔ, minimality, Recall in computational complexity.",
    "response": "Recall that ClΔ(𝑚) = Cl(𝑉(𝑚) ∪𝑇Δ) and hence, by minimality of closure, we obtain that 𝑉(𝑥) ∪𝑉(𝑚′) ∪𝑇Δ ⊆Cl(𝑉(𝑥)) ∪Cl(𝑉(𝑚′) ∪𝑇Δ) [since 𝑈⊆Cl(𝑈)] (6.3a) ⊆Cl(𝑉(𝑥)) ∪Cl(𝑉(𝑚) ∪𝑇Δ) [as ClΔ(𝑚′) ⊆ClΔ(𝑚) by assumption] (6.3b) ⊆Cl(𝑉(𝑥𝑚) ∪𝑇Δ) , (6.3c) where the final equation relies on the fact that Cl(𝐴)∪Cl(𝐵) ⊆Cl(𝐴∪𝐵) which follows by minimality of the sets Cl(𝐴) and Cl(𝐵)."
  },
  {
    "prompt": "Read and understand this section about ClΔ, monomial, definition from a research paper.",
    "response": "This allows us to derive that ClΔ(𝑥𝑚′) = Cl(𝑉(𝑥) ∪𝑉(𝑚′) ∪𝑇Δ) [by definition of monomial closure] (6.4a) ⊆Cl(Cl(𝑉(𝑥𝑚) ∪𝑇Δ)) [by (6.3)] (6.4b) = Cl(𝑉(𝑥𝑚) ∪𝑇Δ) [since closure is idempotent by minimality] (6.4c) = ClΔ(𝑥𝑚) , [by definition of monomial closure] (6.4d) 24 6 A Lower Bound for 3-Colourability on Sparse Random Graphs and thus item 2 holds. Observe furthermore that if Vars(𝑚′) ⊆Vars(𝑆(𝑚)) then 𝑉(𝑚′) ⊆ClΔ(𝑚) by the definition of monomial closure and since 𝑆(𝑚) = Col(𝐺[ClΔ(𝑚)], 𝑘)."
  },
  {
    "prompt": "This is a technical discussion about ClΔ, item, conclude in theoretical computer science.",
    "response": "Thus, using again the observation that 𝑆(𝑚′) ⊆𝑆(𝑚) if and only if ClΔ(𝑚′) ⊆ClΔ(𝑚), in order to conclude that item 3 holds it suffices to show that if 𝑉(𝑚′) ⊆ClΔ(𝑚), then ClΔ(𝑚′) ⊆ClΔ(𝑚). By again using the minimality of closure and the fact that 𝑈⊆Cl(𝑈), we can conclude that ClΔ(𝑚′) = Cl(𝑉(𝑚′) ∪𝑇Δ) ⊆Cl(ClΔ(𝑚) ∪𝑇Δ) = ClΔ(𝑚) . (6.5) Finally, item 4 follows easily from the definition of 𝑆."
  },
  {
    "prompt": "Scientific context on Col, 𝑥𝑣,𝑖, axiom and their role in graph theory.",
    "response": "Indeed, if 𝑝∈Col(𝐺, 𝑘) is an edge axiom, say 𝑥𝑢,𝑖𝑥𝑣,𝑖, then it holds that 𝑆(𝑥𝑢,𝑖𝑥𝑣,𝑖) = Col(𝐺[Cl({𝑢, 𝑣} ∪𝑇Δ)], 𝑘) ∋𝑝; and if 𝑝∈Col(𝐺, 𝑘) is a vertex axiom (Í𝑘 𝑖=1 𝑥𝑣,𝑖−1 or 𝑥𝑣,𝑖𝑥𝑣,𝑖′) or a Boolean axiom (𝑥2 𝑣,𝑖−𝑥𝑣,𝑖) mentioning a vertex 𝑣and 𝑚 is the leading monomial in 𝑝it holds that 𝑆(𝑚) = Col(𝐺[Cl({𝑣} ∪𝑇Δ)], 𝑘) ∋𝑝. Thus, the map 𝑆is a Col(𝐺, 𝑘)-support. We now show that the satisfiability condition and the reducibility condition in Lemma 4.6 hold for the map 𝑆and for 𝐷= ℓ/(50Δ𝑐−1) −|𝑇Δ|."
  },
  {
    "prompt": "This is a technical discussion about degree, true, assume in theoretical computer science.",
    "response": "We can assume 𝐷≥2, since otherwise the theorem is trivially true. Observe that the polynomials in Col(𝐺, 𝑘) have degree at most 𝐷. To see that the satisfiability condition holds, note that by Lemma 6.7 every monomial 𝑚of degree at most 𝐷satisfies |ClΔ(𝑚)| ≤50Δ𝑐−1(𝐷+ |𝑇Δ|) = ℓ, where we use that any decreasing path in 𝐺[𝑉\\ 𝑇Δ] has at most 𝑐vertices, that Desc(𝑇Δ) ⊆𝑇Δ holds, and that 𝐺[𝑉\\ 𝑇Δ] has maximum degree at most Δ."
  },
  {
    "prompt": "Scientific context on ClΔ, 𝐺is, Lemma and their role in graph theory.",
    "response": "Since 𝐺is (ℓ, 1/3Δ)-sparse and |ClΔ(𝑚)| ≤ℓ, it follows from Lemma 2.6 that the graph 𝐺[ClΔ(𝑚)] is 3-colourable and so Col(𝐺[ClΔ(𝑚)], 𝑘) is satisfiable. To establish the reducibility condition, let 𝑚and 𝑚′ be monomials of degree at most 𝐷such that 𝑆(𝑚′) ⊆𝑆(𝑚), i.e., such that ClΔ(𝑚′) ⊆ClΔ(𝑚). Note that as argued above, it holds that |ClΔ(𝑚)| ≤ℓsince 𝑚has degree at most 𝐷."
  },
  {
    "prompt": "Scientific context on ClΔ, modulo, reducible and their role in graph theory.",
    "response": "Therefore, we can apply Lemma 6.5 with 𝑊= ClΔ(𝑚′) and 𝑈= ClΔ(𝑚) to conclude that if 𝑚′ is reducible modulo ⟨𝑊⟩= ⟨𝑆(𝑚′)⟩if and only if it is also reducible modulo ⟨𝑈⟩= ⟨𝑆(𝑚)⟩. Note that we use the fact that all vertices in 𝐺[𝑉\\ 𝑇Δ]—and hence also all vertices in 𝐺[𝑈\\ 𝑊]—have degree at most Δ. With the satisfiability and reducibility conditions of Lemma 4.6 in hand, we conclude that every polynomial calculus refutation of Col(𝐺, 𝑘) requires degree strictly greater than 𝐷, as desired."
  },
  {
    "prompt": "Understanding the computational aspects of Reducibility, Lemma, convenience.",
    "response": "□ 6.2 The Reducibility Lemma It remains to prove the reducibility lemma which we restate here for convenience. Lemma 6.5 (Reducibility Lemma, restated). Let 𝐺= (𝑉, 𝐸) be a (ℓ, 1/3Δ)-sparse graph with a linear order ≺v on 𝑉and consider an admissible order that respects ≺v."
  },
  {
    "prompt": "Scientific context on modulo, reducible, 𝑚is and their role in graph theory.",
    "response": "If the vertex sets 𝑊⊆𝑈satisfy hat 𝑊is closed, that the size of 𝑈is |𝑈| ≤ℓ, and that every vertex in 𝑁𝑈\\𝑊(𝑊) has degree at most Δ in 𝐺[𝑈\\ 𝑊], then for every monomial 𝑚such that 𝑉(𝑚) ⊆𝑊, it holds that 𝑚is reducible modulo ⟨𝑈⟩if and only if 𝑚is reducible modulo ⟨𝑊⟩. The proof idea is to construct a function 𝜌mapping variables associated with vertices in 𝑈\\ 𝑊 to either constants or polynomials of smaller order such that all axioms in ⟨𝑈⟩\\ ⟨𝑊⟩are either satisfied or mapped to a polynomial in ⟨𝑊⟩."
  },
  {
    "prompt": "Scientific context on polynomial, leading, monomial and their role in graph theory.",
    "response": "It is not hard to show that such a mapping turns any polynomial in ⟨𝑈⟩with leading monomial 𝑚into a smaller polynomial in ⟨𝑊⟩whose leading monomial is also 𝑚. It then follows that a monomial 𝑚is reducible modulo ⟨𝑈⟩if 𝑚is reducible modulo ⟨𝑊⟩. The other direction is immediate, so this suffices to prove the lemma. Let us first outline the construction of 𝜌."
  },
  {
    "prompt": "Read and understand this section about Lemma, Variables, closure from a research paper.",
    "response": "Using the definition of closure, we show in Lemma 5.3 that there exists a proper 3-colouring 𝜒of the subgraph 𝐺[𝑈\\ (𝑊∪𝑁𝑈(𝑊))] that uses a single colour for each set 𝑁𝑈\\𝑊(𝑢), where 𝑢∈𝑁𝑈\\𝑊(𝑊). Variables far from 𝑊, which here means 25 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS variables associated with a vertex in 𝑈\\ (𝑊∪𝑁𝑈(𝑊)), are mapped according to the 3-colouring 𝜒. It remains to define 𝜌on variables associated with each vertex 𝑢∈𝑁𝑈\\𝑊(𝑊)."
  },
  {
    "prompt": "Scientific context on colour, coloured, vertex and their role in graph theory.",
    "response": "Since 𝑢has precisely one adjacent vertex 𝑣in 𝑊, and since furthermore the set 𝑁𝑈\\𝑊(𝑢) is coloured with a single colour, no matter how the vertex 𝑣is coloured there is always a colour 𝑐𝑢available to properly colour 𝑢. We may think of 𝑐𝑢as a function that, given 𝜒and the colour of 𝑣, assigns a colour to 𝑢that is consistent with the colouring of 𝑁𝑈(𝑢). Variables associated with the vertex 𝑢are mapped according to 𝑐𝑢 by 𝜌. Proof of Lemma 6.5."
  },
  {
    "prompt": "Explain a concept involving modulo, reducible, 𝑊is in computational complexity.",
    "response": "Since 𝑊is a subset of 𝑈, it follows that if 𝑚is reducible modulo ⟨𝑊⟩, then 𝑚 is also reducible modulo ⟨𝑈⟩. For the reverse direction, we define a mapping 𝜌on variables as outlined above. To this end, let 𝜒be a proper 3-colouring of the subgraph 𝐺[𝑈\\ (𝑊∪𝑁𝑈(𝑊))] that uses a single colour for each set 𝑁𝑈\\𝑊(𝑢), where 𝑢∈𝑁𝑈\\𝑊(𝑊). Such a colouring exists by Lemma 5.3."
  },
  {
    "prompt": "This is a technical discussion about vertex, Variables, 𝑥𝑢,𝑖 in theoretical computer science.",
    "response": "Variables associated with a vertex 𝑢∈𝑈\\ \u0000𝑊∪𝑁𝑈(𝑊)\u0001 are mapped according to 𝜒: if 𝜒(𝑢) = 𝑖, then 𝜌(𝑥𝑢,𝑖) = 1 and 𝜌(𝑥𝑢,𝑖′) = 0 for all 𝑖′ ≠𝑖. Next, for each vertex 𝑢∈𝑁𝑈\\𝑊(𝑊), we define 𝜌on the variables associated with 𝑢. Since 𝜒 assigns, for each 𝑢∈𝑁𝑈\\𝑊(𝑊), a single colour to each set 𝑁𝑈\\𝑊(𝑢) there are at least two distinct colours 𝑐1, 𝑐2 ∈[𝑘] that are not assigned to any vertex in 𝑁𝑈\\𝑊(𝑢). Since there are no 2-hops in 𝑈 with respect to 𝑊the vertex 𝑢has a single neighbour 𝑣∈𝑊."
  },
  {
    "prompt": "Read and understand this section about 𝑈with, coloured, 𝑣is from a research paper.",
    "response": "Furthermore, as there are no 3-hops in 𝑈with respect to 𝑊, it holds that 𝑁𝑈\\𝑊(𝑢) ∩𝑁𝑈\\𝑊(𝑊) = ∅, which implies that the vertex 𝑣is the only neighbour of 𝑢that is not coloured by 𝜒. Hence no matter how 𝑣is coloured by 𝜒, either 𝑐1 or 𝑐2 can be used to properly colour 𝑢. Let us make this choice explicit by defining 𝜌on 𝑢by 𝜌(𝑥𝑢,𝑐) =   𝑥𝑣,𝑐2 if 𝑐= 𝑐1, Í 𝑖∈[𝑘],𝑖≠𝑐2 𝑥𝑣,𝑖 if 𝑐= 𝑐2, and 0 otherwise, that is, if 𝑐∉{𝑐1, 𝑐2}. (6.6) This completes the definition of 𝜌."
  },
  {
    "prompt": "This is a technical discussion about 𝑘-colouring, proper, Note in theoretical computer science.",
    "response": "Note that the mapping 𝜌extends any proper 𝑘-colouring of 𝑊 to a proper 𝑘-colouring of 𝑈. Let 𝑞be a polynomial in ⟨𝑈⟩with leading monomial 𝑚. We claim that 1. 𝑞↾𝜌∈⟨𝑊⟩, 2. that all monomials 𝑚′ satisfy 𝑚′↾𝜌⪯𝑚′, and 3. that 𝑚= 𝑚↾𝜌. If we can show this, then we are done, since 𝑚is then the leading monomial of the polyno- mial 𝑞↾𝜌∈⟨𝑊⟩and we may thus conclude that if 𝑚is reducible modulo ⟨𝑈⟩, then 𝑚is also reducible modulo ⟨𝑊⟩. We now argue that the three properties hold."
  },
  {
    "prompt": "Explain a concept involving subset, 𝜌does, map in computational complexity.",
    "response": "The latter two are almost immediate: since 𝜌does not map variables associated with 𝑊(of which 𝑉(𝑚) is a subset) we have 𝑚= 𝑚↾𝜌. Furthermore, since Desc(𝑊) = 𝑊and since ≺is admissible, it holds for every variable 𝑥that 𝑥↾𝜌⪯𝑥, and hence every monomial 𝑚′ satisfies 𝑚′↾𝜌⪯𝑚′. It remains to prove that 𝑞↾𝜌∈⟨𝑊⟩. Since 𝑞∈⟨𝑈⟩we may write 𝑞= Í 𝑖𝑞𝑖𝑝𝑖for polynomials 𝑞𝑖∈ F[𝑋] and axioms 𝑝𝑖∈Col(𝐺[𝑈], 𝑘)."
  },
  {
    "prompt": "Read and understand this section about Col, 𝑘-colouring, proper from a research paper.",
    "response": "Note that the mapping 𝜌extends any proper 𝑘-colouring of 𝑊 to a proper 𝑘-colouring of 𝑈, and thus it follows by Lemma 2.4 that every axiom 𝑝𝑖∈Col(𝐺[𝑈], 𝑘) satisfies 𝑝𝑖↾𝜌∈⟨𝑊⟩. We can therefore conclude that the polynomial 𝑞↾𝜌= Í 𝑖𝑞𝑖↾𝜌· 𝑝𝑖↾𝜌is in ⟨𝑊⟩ as claimed. □ 26 7 Concluding Remarks With the proof of Lemma 6.5 completed we have shown the last missing piece of the proof of Theorem 6.1."
  },
  {
    "prompt": "This is a technical discussion about polynomial, calculus, lower in theoretical computer science.",
    "response": "This thus establishes our polynomial calculus degree lower bounds for the colouring formula over sparse graphs. 7 Concluding Remarks In this work, we show that polynomial calculus over any field requires linear degree to refute that a sparse random regular graph or Erdős-Rényi random graph is 3-colourable. Our lower bound is optimal up to constant factors, and implies strongly exponential size lower bounds by the well-known size-degree relation for polynomial calculus [IPS99]."
  },
  {
    "prompt": "This is a technical discussion about flavour, technique, earlier in theoretical computer science.",
    "response": "Our overall proof technique is the same as that of earlier papers such as [AR03, GL10a, GL10b, MN15], but our proofs have a different flavour. A central technical concept in [AR03, MN15] is (variations of) the so-called constraint-variable incidence graph: this graph consists of a vertex per constraint and variable, and has an edge between a constraint 𝐶and a variable 𝑥if and only if 𝐶 depends on 𝑥."
  },
  {
    "prompt": "Explain a concept involving graph, sets, set in computational complexity.",
    "response": "This graph is commonly used to argue that by expansion small sets of constraints are satisfiable, even after the removal of a closed set of vertices. By contrast, we never need to make any (explicit) use of this graph. This raises the question of whether it is possible to rephrase our proofs in language closer to that of [AR03, MN15], or are the two approaches inherently different? The lower bound techniques in this paper, as well as those in [MN15], work over any field."
  },
  {
    "prompt": "This is a technical discussion about 𝑘-colourability, derivations, polynomial in theoretical computer science.",
    "response": "For NP-hard problems such as 𝑘-colourability, we expect a polynomial calculus lower bound to hold regardless of which field is used for the derivations. However, other formulas such as the Tseitin contradictions are easy for polynomial calculus over a field of characteristic 2 and hard in other characteristics. The techniques in, for instance, [BI99, BGIP01, AR03] capture this fact, while those in [MN15] and this paper cannot."
  },
  {
    "prompt": "Understanding the computational aspects of lower, bounds, degree.",
    "response": "Another interesting question is therefore whether the techniques in these papers can be unified into a general approach that works both for field-dependent and field-independent lower bounds. Our degree lower bounds for 3-colourability are of the form 𝑛/𝑓(𝑑), where 𝑑is either degree or average degree of the graph depending on the random graph model. In our work, 𝑓is at least exponential in 𝑑, but in previous results [BCMM05, LN17], 𝑓is at most polynomial in 𝑑."
  },
  {
    "prompt": "Understanding the computational aspects of graphs, improved, dependence.",
    "response": "While the precise dependence on 𝑑is immaterial for sparse random graphs, it would be interesting to see if the parameters in our result can be improved. We remark that it is far from clear what the correct dependence on 𝑑should be."
  },
  {
    "prompt": "Read and understand this section about random, graphs, 𝑘-colourability from a research paper.",
    "response": "For the sums-of-squares proof system, which simulates polynomial calculus over the reals [Ber18], there exist strong upper bounds for 𝑘-colourability on random graphs and random regular graphs in some parameter regimes: the paper [BKM19] showed that asymptotically almost surely, degree-2 sums-of-squares refutes 𝑘-colourability on 𝑑-regular random graphs if 𝑑≥4𝑘2."
  },
  {
    "prompt": "Read and understand this section about linear, degree, fixed from a research paper.",
    "response": "These results rule out a polynomial dependence on 𝑑in any linear sums-of-squares degree lower bound for 𝑘-colourability whenever 𝑘is fixed. However, similar upper bounds are not known to hold for polynomial calculus, and it should be pointed out that the latter proof system is incomparable to sum-of-squares when considered over fields of finite characteristic."
  },
  {
    "prompt": "Understanding the computational aspects of systems, work, colouring.",
    "response": "More broadly it would be interesting to investigate whether the ideas and concepts underlying this work could be extended to prove lower bounds for colouring principles in other proof systems, the most obvious candidates being Sherali-Adams and sums-of-squares. Regarding polynomial calculus, it is worth noting that the closure operation defined in [RT22] and generalized in this work is not, per se, restricted to graph colouring."
  },
  {
    "prompt": "This is a technical discussion about degree, lower, graph in theoretical computer science.",
    "response": "It is natural to ask whether similar techniques could be useful for proving degree lower bounds for other graph problems. One open problem is to improve the degree lower bound for matching on random graphs in [AR22] to linear in the graph size, and 27 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS to make it hold for graphs of small constant degree."
  },
  {
    "prompt": "This is a technical discussion about bounds, cover, analogously in theoretical computer science.",
    "response": "Another problem is to establish polynomial calculus size lower bounds for independent set and vertex cover, analogously to what was done for the resolution proof system in [BIS07]. Finally, an intriguing technical challenge is to prove degree lower bounds for variants of the dense linear ordering principle [AD08] for graphs of bounded degree."
  },
  {
    "prompt": "Read and understand this section about Albert, Atserias, Gaia from a research paper.",
    "response": "Acknowledgements The authors would like to thank Albert Atserias, Gaia Carenini and Amir Yehudayoff for helpful discussions during the course of this work, and we also thank Albert for making us aware of some relevant references. In addition, we benefitted from feedback of the participants of the Dagstuhl Seminar 23111 “Computational Complexity of Discrete Problems” and Oberwolfach workshop 2413 “Proof Complexity and Beyond”."
  },
  {
    "prompt": "Read and understand this section about Finally, Maryia, Kapytka from a research paper.",
    "response": "Finally, we are grateful to Maryia Kapytka for feedback on preliminary versions of this manuscript and also to the anonymous FOCS reviewers—all these comments helped us improve the exposition in the paper considerably."
  },
  {
    "prompt": "This is a technical discussion about Theory, Practice, Part in theoretical computer science.",
    "response": "Part of this work was carried out while the authors were taking part in the semester programme Meta-Complexity and the extended reunion of the programme Satisfiability: Theory, Practice, and Beyond at the Simons Institute for the Theory of Computing at UC Berkeley in the spring of 2023. Susanna F. de Rezende received funding from ELLIIT, from the Knut and Alice Wallenberg grant KAW 2021.0307, and from the Swedish Research Council grant 2021-05104."
  },
  {
    "prompt": "Understanding the computational aspects of Conneryd, Jakob, Nordström.",
    "response": "Jonas Conneryd and Jakob Nordström were funded by the Swedish Research Council grant 2016-00782, and in addition Jonas Conneryd was also partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, whereas Jakob Nordström together with Shuo Pang were supported by the Independent Research Fund Denmark grant 9040-00389B."
  },
  {
    "prompt": "Read and understand this section about Randomness, Algorithms, Risse from a research paper.",
    "response": "Kilian Risse was supported by the Swiss National Science Foundation project 200021-184656 “Randomness in Problem Instances and Randomized Algorithms”. A On Boolean Implication and Ideal Membership In this appendix, we provide a proof of the folklore result that being implied by a set of polynomials and being in the ideal generated by these polynomials is the same in the Boolean setting. Lemma 2.4 (restated). Let 𝑔be a polynomial and 𝑄be a set of polynomials in F[𝑥1, . . ."
  },
  {
    "prompt": "Read and understand this section about 𝑄contains, Boolean, common from a research paper.",
    "response": ", 𝑥𝑛], and suppose that 𝑄contains all the Boolean axioms. Then it holds that 𝑔vanishes on all common roots of 𝑄if and only if 𝑔∈⟨𝑄⟩. Proof. If 𝑔∈⟨𝑄⟩, then we can write 𝑔= Í 𝑖𝑓𝑖𝑞𝑖for 𝑓𝑖∈F[𝑥1, . . . , 𝑥𝑛] and 𝑞𝑖∈𝑄. Observe that Í 𝑖𝑓𝑖𝑞𝑖↾𝜉= 0 for any common root 𝜉∈{0, 1}𝑛of 𝑄. Hence 𝑔vanishes on all common roots of 𝑄. For the other direction, let 𝜉= (𝜉1, 𝜉2, . . ."
  },
  {
    "prompt": "Scientific context on multilinear, polynomial, Random and their role in graph theory.",
    "response": ", 𝜉𝑛) be an element of {0, 1}𝑛and write 1𝜉for the multilinear polynomial that evaluates to 1 on 𝜉and to 0 on all other elements of {0, 1}𝑛, that is, 1𝜉(𝑥) = Ö 𝑖:𝜉𝑖=1 𝑥𝑖 Ö 𝑗:𝜉𝑗=0 (1 −𝑥𝑗) . (A.1) Clearly, every function 𝑓: {0, 1}𝑛→F can be expressed as a multilinear polynomial through the identity 𝑓= Õ 𝜉∈{0,1}𝑛 𝑓(𝜉) · 1𝜉, (A.2) 28 B Random Graphs Are Sparse and as the polynomials {1𝜉| 𝜉∈{0, 1}𝑛} form a basis of the vector space of multilinear polynomials over F[𝑥1, . . ."
  },
  {
    "prompt": "This is a technical discussion about unique, polynomials, representation in theoretical computer science.",
    "response": ", 𝑥𝑛], this representation is unique. Let 𝑆⊆{0, 1}𝑛be the set of common roots of the polynomials in 𝑄. Since 𝑔vanishes on 𝑆, we may write 𝑔= Õ 𝜉∈{0,1}𝑛\\𝑆 𝑔(𝜉) · 1𝜉. (A.3) We now show that if 𝜉∈{0, 1}𝑛\\ 𝑆, then it holds that 1𝜉∈⟨𝑄⟩. This suffices to prove the desired result, as 𝑔is then a linear combination of polynomials in ⟨𝑄⟩and hence is also in ⟨𝑄⟩. Let 𝜉be an element in {0, 1}𝑛\\ 𝑆. Because 𝑆is the set of common roots of the polynomials in 𝑄, there exists a polynomial 𝑞∈𝑄such that 𝑞(𝜉) ≠0."
  },
  {
    "prompt": "Read and understand this section about Boolean, axioms, Lemma from a research paper.",
    "response": "The polynomial 1𝜉· (𝑞(𝜉))−1𝑞coincides with 1𝜉on all of {0, 1}𝑛, so 1𝜉· (𝑞(𝜉))−1𝑞= 1𝜉modulo the Boolean axioms. Since ⟨𝑄⟩contains both 1𝜉· (𝑞(𝜉))−1𝑞and the Boolean axioms, it follows that 1𝜉∈⟨𝑄⟩. □ B Random Graphs Are Sparse In this appendix we prove Lemma 2.7, which is a quantitative version of Lemma 4.15 in [Raz17]. We make no claim of originality, but present this result here to make the paper self-contained."
  },
  {
    "prompt": "Scientific context on lemma, G𝑛,𝑑in, random and their role in graph theory.",
    "response": "We start by proving the sparsity lemma for graphs sampled from the Erdős-Rényi random graph distribution G(𝑛, 𝑑/𝑛) in Lemma B.1 and then establish the analogous result for random regular graphs sampled according to G𝑛,𝑑in Lemma B.4. Lemma B.1 (Sparsity lemma for Erdős-Rényi random graphs). Let 𝑛, 𝑑∈N+ and 𝜀, 𝛿∈R+ be such that 𝜀𝛿= 𝜔(1/log 𝑛). If 𝐺is a graph sampled from G(𝑛, 𝑑/𝑛), then asymptotically almost surely it is ((4𝑑)−(1+𝛿)(1+𝜀)/𝜀𝑛, 𝜀)-sparse. Proof."
  },
  {
    "prompt": "Explain a concept involving 𝐺is, 𝒜the, sparse in computational complexity.",
    "response": "Let 𝛼= (4𝑑)−(1+𝛿)(1+𝜀)/𝜀and denote by 𝒜the event “𝐺is (𝛼𝑛, 𝜀)-sparse”. For a set 𝑈⊆𝑉 of size 𝑠, the random variable |𝐸(𝐺[𝑈])| is a sum of 𝑠(𝑠−1)/2 random indicator variables for the edges that are 1 with probability 𝑑/𝑛and 0 otherwise."
  },
  {
    "prompt": "Understanding the computational aspects of exp, 𝜀ln, GRAPH.",
    "response": "We apply a union bound over sets of size 𝑠≤𝛼𝑛to conclude that Pr[¬𝒜] ≤ Õ 𝑈⊆𝑉 |𝑈|≤𝛼𝑛 Pr[|𝐸(𝐺[𝑈])| ≥(1 + 𝜀)|𝑈|] (B.1a) ≤ 𝛼𝑛 Õ 𝑠=1 \u0012𝑛 𝑠 \u0013 \u0012 𝑠(𝑠−1) 2 (1 + 𝜀)𝑠 \u0013 · \u0012 𝑑 𝑛 \u0013(1+𝜀)𝑠 (B.1b) ≤ 𝛼𝑛 Õ 𝑠=1 \u0010 𝑒𝑛 𝑠 \u0011𝑠\u0012 𝑒(𝑠−1) 2(1 + 𝜀) \u0013(1+𝜀)𝑠 · \u0012 𝑑 𝑛 \u0013(1+𝜀)𝑠 (B.1c) ≤ 𝛼𝑛 Õ 𝑠=1 exp \u0012 −𝜀𝑠ln \u0010 𝑛 𝑠 \u0011 + (1 + 𝜀)𝑠 \u0012 ln \u0012 𝑒2𝑑 2(1 + 𝜀) \u0013\u0013\u0013 (B.1d) ≤ 𝛼𝑛 Õ 𝑠=1 exp (−𝛿𝜀𝑠ln(𝑛/𝑠)) (B.1e) ≤𝑜(1) , (B.1f) where for (B.1e) we use that 𝑛/𝑠≥1/𝛼= (4𝑑)(1+𝛿)(1+𝜀)/𝜀to estimate that (1 + 𝜀) \u0012 ln \u0012 𝑒2𝑑 2(1 + 𝜀) \u0013\u0013 ≤(1 + 𝜀) ln (4𝑑) = 𝜀ln(1/𝛼) 1 + 𝛿 ≤𝜀ln(𝑛/𝑠) 1 + 𝛿 , (B.2) 29 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS and for (B.1f) we use that 𝜀𝛿= 𝜔(1/log 𝑛) and that 𝑠ln(𝑛/𝑠) ≥ln 𝑛+ 𝑠−1 (for 1 ≤𝑠≤𝑛/𝑒2)."
  },
  {
    "prompt": "Scientific context on graphs, prove, sparsity and their role in graph theory.",
    "response": "□ We next prove the sparsity lemma for random regular graphs. In order to sample 𝐺from G𝑛,𝑑 we use the configuration model, which is defined as follows. Given 𝑛and 𝑑such that 𝑑𝑛is even, we have a vertex set 𝑉of size 𝑛and for each vertex 𝑣∈𝑉there is a cell 𝐶𝑣with 𝑑elements."
  },
  {
    "prompt": "Scientific context on 𝐸if, 𝑀such, 𝐶𝑢and and their role in graph theory.",
    "response": "We sample a perfect matching 𝑀uniformly from the set ℳ𝑑𝑛of all possible perfect matchings of the 𝑑𝑛elements and consider the corresponding multi-graph 𝐺𝑀= (𝑉, 𝐸), possibly with parallel edges and loops, where (𝑢, 𝑣) ∈𝐸if and only if there exists (𝑥, 𝑦) ∈𝑀such that 𝑥∈𝐶𝑢and 𝑦∈𝐶𝑣. Since all simple graphs (without parallel edges or loops) are sampled with the same probability, we can sample 𝐺from G𝑛,𝑑by sampling according to the configuration model repeatedly until we sample a simple graph."
  },
  {
    "prompt": "Read and understand this section about probability, Theorem, 𝑀is from a research paper.",
    "response": "Theorem B.2 ([MW91, Wor99]). For 𝑑= 𝑜(𝑛1/2), the probability that 𝐺𝑀is simple when 𝑀is sampled uniformly from ℳ𝑑𝑛is equal to exp \u0012 −𝑑2 −1 4 −𝑑3 12𝑛+ 𝑂(𝑑2/𝑛) \u0013 . Let 𝑆ℓ,𝑞denote the sum of ℓrandom variables that are 1 with probability 𝑞and 0 otherwise. We argue that we can bound the probability that a set of vertices 𝑈⊆𝑉witnesses that the graph is not sparse by bounding the probability that 𝑆ℓ,𝑞is too large. Claim B.3."
  },
  {
    "prompt": "Read and understand this section about ℳ𝑑𝑛, 𝑆𝑑𝑠,𝑞, Proof from a research paper.",
    "response": "For any 𝑠≤𝑛/2 and 𝐵∈R+, if 𝑈⊆𝑉is of size 𝑠and 𝑞= 𝑠/(𝑛−𝑠), it holds that Pr 𝑀∼ℳ𝑑𝑛 [|𝐸(𝐺𝑀[𝑈])| ≥𝐵] ≤Pr[𝑆𝑑𝑠,𝑞≥𝐵] . Proof. To see why this is true, consider the random process in the configuration model that matches one by one the elements in the cells 𝐶𝑣for all 𝑣∈𝑈. At each step, there are at most 𝑑𝑠elements in cells 𝐶𝑣where 𝑣∈𝑈that are not yet matched, and at least 𝑑(𝑛−𝑠) elements in cells 𝐶𝑣where 𝑣∉𝑈that are not yet matched."
  },
  {
    "prompt": "Understanding the computational aspects of 𝑈is, step, implies.",
    "response": "This implies that at each step the probability that we obtain an edge between cells in 𝑈is at most 𝑑𝑠/𝑑(𝑛−𝑠) = 𝑞. Since at least one element in a cell of 𝑈is matched at every step, there are at most 𝑑𝑠steps in total. Hence the claim follows. □ Lemma B.4 (Sparsity lemma for random regular graphs). Let 𝑛, 𝑑∈N+ and 𝜀, 𝛿∈R+ be such that 𝜀𝛿= 𝜔(1/log 𝑛) and 𝑑2 ≤𝜀𝛿log 𝑛. If 𝐺is a graph sampled from G𝑛,𝑑, then asymptotically almost surely it is ((8𝑑)−(1+𝛿)(1+𝜀)/𝜀𝑛, 𝜀)-sparse. Proof."
  },
  {
    "prompt": "Scientific context on 𝒜the, sparse, probability and their role in graph theory.",
    "response": "Fix 𝜀> 0, let 𝛼= (8𝑑)−2(1+1/𝜀), and denote by 𝒜the event “𝐺is (𝛼𝑛, 𝜀)-sparse”. We want to prove that 𝐺∼G𝑛,𝑑is (𝛼𝑛, 𝜀)-sparse with probability that goes to 1 as 𝑛goes to infinity. To this end, we prove that if we sample 𝐺from the configuration model, the probability that it is not (𝛼𝑛, 𝜀)-sparse is much smaller than the probability that 𝐺is a random regular graph."
  },
  {
    "prompt": "Explain a concept involving exp, 𝐺in, Graphs in computational complexity.",
    "response": "More formally, our goal is to prove that Pr[¬𝒜] · exp \u0012 𝑑2 −1 4 \u0013 ≤𝑜(1) , (B.3) where we recall that the probability here and in what follows is taken over sampling 𝐺in the 30 C Erdős-Rényi Graphs Almost Have Small Maximum Degree configuration model. By union bound and using Claim B.3 we have that Pr[¬𝒜] · exp \u0012 𝑑2 −1 4 \u0013 ≤ Õ 𝑈⊆𝑉 |𝑈|≤𝛼𝑛 Pr[|𝐸(𝐺[𝑈])| ≥(1 + 𝜀)|𝑈|] · exp \u0012 𝑑2 4 \u0013 (B.4a) ≤ 𝛼𝑛 Õ 𝑠=1 \u0012𝑛 𝑠 \u0013 Pr[𝑆𝑑𝑠,𝑞≥(1 + 𝜀)𝑠] · exp \u0012 𝑑2 4 \u0013 ."
  },
  {
    "prompt": "Explain a concept involving exp, 𝑆𝑑𝑠,𝑞, 𝜀𝛿𝑠ln in computational complexity.",
    "response": "(B.4b) Our goal is to show that 𝑃𝑟[𝑆𝑑𝑠,𝑞≥(1 + 𝜀)𝑠] is so small that it compensates for the other factors. More concretely, we wish to show that for 𝑠≤𝛼𝑛it holds that \u0012𝑛 𝑠 \u0013 Pr[𝑆𝑑𝑠,𝑞≥(1 + 𝜀)𝑠] ≤exp (−𝜀𝛿𝑠ln(𝑛/𝑠)) , (B.5) from which it follows, since 𝑑2 ≤𝜀𝛿log 𝑛≤2𝜀𝛿ln 𝑛, that Pr[¬𝒜] · exp \u0012 𝑑2 −1 4 \u0013 ≤ 𝛼𝑛 Õ 𝑠=1 exp \u0012 −𝜀𝛿𝑠ln(𝑛/𝑠) + 𝑑2 4 \u0013 (B.6a) ≤ 𝛼𝑛 Õ 𝑠=1 exp \u0012 −𝜀𝛿𝑠ln(𝑛/𝑠) 2 \u0013 (B.6b) = 𝑜(1) , (B.6c) where we use the fact that 𝜀𝛿= 𝜔(1/log 𝑛) and that 𝑠ln(𝑛/𝑠) ≥ln 𝑛+ 𝑠−1 (for 1 ≤𝑠≤𝑛/𝑒2)."
  },
  {
    "prompt": "This is a technical discussion about holds, remains, show in theoretical computer science.",
    "response": "It remains to show that (B.5) holds."
  },
  {
    "prompt": "Explain a concept involving exp, 𝜀𝑠ln, 𝑠ln in computational complexity.",
    "response": "This follows from the sequence of calculations \u0012𝑛 𝑠 \u0013 Pr[𝑆𝑑𝑠,𝑞≥(1 + 𝜀)𝑠] ≤ \u0012𝑛 𝑠 \u0013 \u0012 𝑑𝑠 (1 + 𝜀)𝑠 \u0013 · \u0010 𝑠 𝑛−𝑠 \u0011(1+𝜀)𝑠 (B.7a) ≤ \u0010 𝑒𝑛 𝑠 \u0011𝑠\u0012 𝑒𝑑𝑠 (1 + 𝜀)𝑠 \u0013(1+𝜀)𝑠 · \u0010 𝑠 𝑛−𝑠 \u0011(1+𝜀)𝑠 (B.7b) ≤exp \u0012 −𝜀𝑠ln \u0010 𝑛−𝑠 𝑠 \u0011 + 𝑠+ (1 + 𝜀)𝑠ln \u0012 𝑒𝑑 (1 + 𝜀) \u0013\u0013 (B.7c) ≤exp \u0012 −𝜀𝑠ln \u0010 𝑛 𝑠 \u0011 + (1 + 𝜀)𝑠ln \u0012 𝑒2𝑑 (1 + 𝜀) \u0013\u0013 (B.7d) ≤exp (−𝜀𝛿𝑠ln (𝑛/𝑠)) , (B.7e) where for (B.7d) we use that ln(𝑛/𝑠−1) ≥ln(𝑛/𝑠) −1 (for 𝑠≤𝑛/2), and for (B.7e) we use that 𝑛/𝑠≥1/𝛼= (8𝑑)−(1+𝛿)(1+𝜀)/𝜀to bound (1 + 𝜀) \u0012 ln \u0012 𝑒2𝑑 (1 + 𝜀) \u0013\u0013 ≤(1 + 𝜀) ln (8𝑑) = 𝜀ln(1/𝛼) 1 + 𝛿 ≤𝜀ln(𝑛/𝑠) 1 + 𝛿 ."
  },
  {
    "prompt": "Explain a concept involving Lemma, Small, proof in computational complexity.",
    "response": "(B.8) This concludes the proof of Lemma 2.7. □ C Erdős-Rényi Graphs Almost Have Small Maximum Degree In this section we provide a proof of Lemma 6.3 stating that Erdős-Rényi random graphs have small degree except for a small set of vertices. 31 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS Lemma 6.3 (restated). Let 𝐺= (𝑉, 𝐸) be a graph sampled from G(𝑛, 𝑑/𝑛) where 𝑑= O(log 𝑛)."
  },
  {
    "prompt": "Read and understand this section about exists, set, maximum from a research paper.",
    "response": "If Δ ≥𝑑is such that (Δ/𝑒𝑑)Δ = o(𝑛), then asymptotically almost surely there exists a set 𝑇Δ of size at most (𝑒𝑑/Δ)Δ ·2𝑒𝑛 such that the maximum degree in 𝐺[𝑉\\ 𝑇Δ] is at most Δ −1. Proof. Let 𝒜denote the event that there exists a set 𝑇Δ ⊆𝑉of size ℓ= (𝑒𝑑/Δ)Δ · 2𝑒𝑛such that the maximum degree in 𝐺[𝑉\\ 𝑇Δ] is at most Δ −1. We prove that Pr[¬𝒜] = o(1)."
  },
  {
    "prompt": "Read and understand this section about 𝒜does, 𝐺in, 𝐺any from a research paper.",
    "response": "Note that if 𝒜does not hold, then, in particular, if we go over the vertices of 𝐺in any given fixed order, and remove from 𝐺any vertex of degree at least Δ that we encounter, we will end up removing at least ℓvertices. Observe further that after the removal of 𝑖vertices, the probability that a vertex has degree at least Δ is at most \u0000𝑛−𝑖 Δ \u0001(𝑑/𝑛)Δ and is independent of the fact that the removed vertices had degree at least Δ."
  },
  {
    "prompt": "Read and understand this section about bound, 𝒜not, conclude from a research paper.",
    "response": "Therefore, by taking a union bound over all sets of size ℓ, we can bound the probability of the event 𝒜not holding by Pr[¬𝒜] ≤ \u0012𝑛 ℓ \u0013 ℓÖ 𝑖=1 \u0012𝑛−𝑖 Δ \u0013 \u0012 𝑑 𝑛 \u0013Δ (C.1a) ≤ \u0012𝑛 ℓ \u0013 \u0012𝑛 Δ \u0013 \u0012 𝑑 𝑛 \u0013Δ!ℓ (C.1b) ≤ \u0010 𝑒𝑛 ℓ \u0011ℓ\u0012 𝑒𝑑 Δ \u0013Δℓ (C.1c) = o(1) , (C.1d) where for (C.1d) we use that ℓ= 2𝑒𝑛(𝑒𝑑/Δ)Δ to derive that (𝑒𝑛/ℓ)ℓ(𝑒𝑑/Δ)Δℓ= 2−ℓ, and then use that ℓ= 𝜔(1), which holds since (Δ/𝑒𝑑)Δ = o(𝑛), to conclude 2−ℓ= o(1). The lemma follows. □ References [ABRW02] Michael Alekhnovich, Eli Ben-Sasson, Alexander A."
  },
  {
    "prompt": "Understanding the computational aspects of Wigderson, Avi, Razborov.",
    "response": "Razborov, and Avi Wigderson. Space complexity in propositional calculus. SIAM Journal on Computing, 31(4):1184–1211, April 2002. Preliminary version in STOC ’00. [AD08] Albert Atserias and Víctor Dalmau. A combinatorial characterization of resolution width. Journal of Computer and System Sciences, 74(3):323–334, May 2008. Preliminary version in CCC ’03. [AH19] Albert Atserias and Tuomas Hakoniemi. Size-degree trade-offs for Sums-of-Squares and Positivstellensatz proofs."
  },
  {
    "prompt": "Understanding the computational aspects of Proceedings, July, Annual.",
    "response": "In Proceedings of the 34th Annual Computational Complexity Conference (CCC ’19), volume 137 of Leibniz International Proceedings in Informatics (LIPIcs), pages 24:1–24:20, July 2019. [AN05] Dimitris Achlioptas and Assaf Naor. The two possible values of the chromatic number of a random graph. Annals of Mathematics, 162(3):1335–1351, November 2005. [AO19] Albert Atserias and Joanna Ochremiak. Proof complexity meets algebra. ACM Trans- actions on Computational Logic, 20:1:1–1:46, February 2019."
  },
  {
    "prompt": "This is a technical discussion about References, ICALP, Michael in theoretical computer science.",
    "response": "Preliminary version in ICALP ’17. 32 References [AR01] Michael Alekhnovich and Alexander A. Razborov. Lower bounds for polynomial calculus: Non-binomial case. In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science (FOCS ’01), pages 190–199, October 2001. [AR03] Michael Alekhnovich and Alexander A. Razborov. Lower bounds for polynomial calculus: Non-binomial case. Proceedings of the Steklov Institute of Mathematics, 242:18–35, 2003."
  },
  {
    "prompt": "Explain a concept involving June, Preliminary, version in computational complexity.",
    "response": "Available at http://people.cs.uchicago.edu/~razborov/files/misha.pdf. Preliminary version in FOCS ’01. [AR22] Per Austrin and Kilian Risse. Perfect matching in random graphs is as hard as Tseitin. TheoretiCS, 1:Art. 2, 47, 2022. Preliminary version in SODA ’22. [AT92] Noga Alon and Michael Tarsi. Colorings and orientations of graphs. Combinatorica, 12(2):125–134, June 1992. [Bay82] David Allen Bayer. The Division Algorithm and the Hilbert Scheme. PhD thesis, Harvard University, June 1982."
  },
  {
    "prompt": "Explain a concept involving Jakub, August, Libor in computational complexity.",
    "response": "Available at https://www.math.columbia.edu/~bayer/papers/ Bayer-thesis.pdf. [BBKO21] Libor Barto, Jakub Bulín, Andrei Krokhin, and Jakub Opršal. Algebraic approach to promise constraint satisfaction. Journal of the ACM, 68(4):28:1–28:66, August 2021. Preliminary version in STOC ’19. [BCMM05] Paul Beame, Joseph C. Culberson, David G. Mitchell, and Cristopher Moore. The resolution complexity of random graph 𝑘-colorability. Discrete Applied Mathematics, 153(1-3):25–47, December 2005."
  },
  {
    "prompt": "Read and understand this section about Richard, Eppstein, Beigel from a research paper.",
    "response": "[BE05] Richard Beigel and David Eppstein. 3-coloring in time 𝑂(1.3289𝑛). Journal of Algorithms, 54(2):168–204, February 2005. [Ber18] Christoph Berkholz. The relation between polynomial calculus, Sherali-Adams, and sum-of-squares proofs. In Proceedings of the 35th Symposium on Theoretical Aspects of Computer Science (STACS ’18), volume 96 of Leibniz International Proceedings in Informatics (LIPIcs), pages 11:1–11:14, February 2018. [BGIP01] Samuel R."
  },
  {
    "prompt": "Scientific context on Dima, Grigoriev, Pitassi and their role in graph theory.",
    "response": "Buss, Dima Grigoriev, Russell Impagliazzo, and Toniann Pitassi. Linear gaps between degrees for the polynomial calculus modulo distinct primes. Journal of Computer and System Sciences, 62(2):267–289, March 2001. Preliminary version in CCC ’99. [BI99] Eli Ben-Sasson and Russell Impagliazzo. Random CNF’s are hard for the polynomial calculus. In Proceedings of the 40th Annual IEEE Symposium on Foundations of Computer Science (FOCS ’99), pages 415–421, October 1999. Journal version in [BI10]."
  },
  {
    "prompt": "This is a technical discussion about Eli, Impagliazzo, Russell in theoretical computer science.",
    "response": "[BI10] Eli Ben-Sasson and Russell Impagliazzo. Random CNF’s are hard for the polynomial calculus. Computational Complexity, 19(4):501–519, 2010. Preliminary version in FOCS ’99. [BIK+94] Paul Beame, Russell Impagliazzo, Jan Krajíček, Toniann Pitassi, and Pavel Pudlák. Lower bounds on Hilbert’s Nullstellensatz and propositional proofs. In Proceedings of the 35th Annual IEEE Symposium on Foundations of Computer Science (FOCS ’94), pages 794–806, November 1994."
  },
  {
    "prompt": "Understanding the computational aspects of CALCULUS, Paul, Beame.",
    "response": "33 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS [BIS07] Paul Beame, Russell Impagliazzo, and Ashish Sabharwal. The resolution complexity of independent sets and vertex covers in random graphs. Computational Complexity, 16(3):245–297, October 2007. Preliminary version in CCC ’01. [BKM19] Jess Banks, Robert Kleinberg, and Cristopher Moore. The Lovász theta function for random regular graphs and community detection in the hard regime. SIAM Journal on Computing, 48(3):1098–1119, 2019."
  },
  {
    "prompt": "Explain a concept involving Archie, Blake, Algebra in computational complexity.",
    "response": "[Bla37] Archie Blake. Canonical Expressions in Boolean Algebra. PhD thesis, University of Chicago, 1937. [BN21] Samuel R. Buss and Jakob Nordström. Proof complexity and SAT solving. In Armin Biere, Marĳn J. H. Heule, Hans van Maaren, and Toby Walsh, editors, Handbook of Satisfiability, volume 336 of Frontiers in Artificial Intelligence and Applications, chapter 7, pages 233–350. IOS Press, 2nd edition, February 2021. [Bol78] Béla Bollobás. Chromatic number, girth and maximal degree."
  },
  {
    "prompt": "Read and understand this section about Mathematics, Discrete, Samuel from a research paper.",
    "response": "Discrete Mathematics, 24(3):311–314, 1978. [Bus98] Samuel R. Buss. Lower bounds on Nullstellensatz proofs via designs. In Proof Complexity and Feasible Arithmetics, volume 39 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 59–71. American Mathematical Society, 1998. Available at http://www.math.ucsd.edu/~sbuss/ResearchWeb/designs/. [BW01] Eli Ben-Sasson and Avi Wigderson. Short proofs are narrow—resolution made simple."
  },
  {
    "prompt": "Explain a concept involving March, Journal, ACM in computational complexity.",
    "response": "Journal of the ACM, 48(2):149–169, March 2001. Preliminary version in STOC ’99. [CEI96] Matthew Clegg, Jeffery Edmonds, and Russell Impagliazzo. Using the Groebner basis algorithm to find proofs of unsatisfiability. In Proceedings of the 28th Annual ACM Symposium on Theory of Computing (STOC ’96), pages 174–183, May 1996. [CFRR02] Colin Cooper, Alan Frieze, Bruce Reed, and Oliver Riordan. Random regular graphs of non-constant degree: independence and chromatic number."
  },
  {
    "prompt": "This is a technical discussion about Probability, Computing, Combinatorics in theoretical computer science.",
    "response": "Combinatorics, Probability and Computing, 11(4):323–341, 2002. [Coj05] Amin Coja-Oghlan. The Lovász number of random graphs. Combinatorics, Probability and Computing, 14(4):439–465, 2005. [CR79] Stephen A. Cook and Robert A. Reckhow. The relative efficiency of propositional proof systems. Journal of Symbolic Logic, 44(1):36–50, March 1979. Preliminary version in STOC ’74. [DL95] Jesús A. De Loera. Gröbner bases and graph colorings. Beiträge zur Algebra und Geometrie, 36(1):89–96, January 1995."
  },
  {
    "prompt": "Read and understand this section about Martin, Davis, George from a research paper.",
    "response": "[DLL62] Martin Davis, George Logemann, and Donald Loveland. A machine program for theorem proving. Communications of the ACM, 5(7):394–397, July 1962. [DLMM08] Jesús A. De Loera, Jon Lee, Peter N. Malkin, and Susan Margulies. Hilbert’s Nullstel- lensatz and an algorithm for proving combinatorial infeasibility. In Proceedings of the 21st International Symposium on Symbolic and Algebraic Computation (ISSAC ’08), pages 197–206, July 2008. 34 References [DLMM11] Jesús A. De Loera, Jon Lee, Peter N."
  },
  {
    "prompt": "Read and understand this section about Margulies, Nullstellensatz, Malkin from a research paper.",
    "response": "Malkin, and Susan Margulies. Computing infeasibility certificates for combinatorial problems through Hilbert’s Nullstellensatz. Journal of Symbolic Computation, 46(11):1260–1283, November 2011. [DLMO09] Jesús A. De Loera, Jon Lee, Susan Margulies, and Shmuel Onn. Expressing combi- natorial problems by systems of polynomial equations and Hilbert’s Nullstellensatz. Combinatorics, Probability and Computing, 18(4):551–582, July 2009. [DMP+15] Jesús A."
  },
  {
    "prompt": "Understanding the computational aspects of Loera, Susan, Margulies.",
    "response": "De Loera, Susan Margulies, Michael Pernpeintner, Eric Riedl, David Rolnick, Gwen Spencer, Despina Stasi, and Jon Swenson. Graph-coloring ideals: Nullstellensatz certificates, Gröbner bases for chordal graphs, and hardness of Gröbner bases. In Proceedings of the 40th International Symposium on Symbolic and Algebraic Computation (ISSAC ’15), pages 133–140, July 2015. [DP60] Martin Davis and Hilary Putnam. A computing procedure for quantification theory. Journal of the ACM, 7(3):201–215, 1960."
  },
  {
    "prompt": "Understanding the computational aspects of Yuval, Filmus, calculus.",
    "response": "[Fil19] Yuval Filmus. Another look at degree lower bounds for polynomial calculus. Theoretical Computer Science, 796:286–293, December 2019. [GL10a] Nicola Galesi and Massimo Lauria. On the automatizability of polynomial calculus. Theory of Computing Systems, 47(2):491–506, August 2010. [GL10b] Nicola Galesi and Massimo Lauria. Optimality of size-degree trade-offs for polynomial calculus. ACM Transactions on Computational Logic, 12(1):4:1–4:22, November 2010. [Hal93] Magnús M. Halldórsson."
  },
  {
    "prompt": "This is a technical discussion about January, coloring, Letters in theoretical computer science.",
    "response": "A still better performance guarantee for approximate graph coloring. Information Processing Letters, 45(1):19–23, January 1993. [HLW06] Shlomo Hoory, Nathan Linial, and Avi Wigderson. Expander graphs and their applications. Bulletin of the American Mathematical Society, 43(4):439–561, October 2006. [Hus15] Thore Husfeldt. Graph colouring algorithms. In Lowell W. Beineke and Robin J."
  },
  {
    "prompt": "Read and understand this section about Topics, Encyclopedia, Applications from a research paper.",
    "response": "Wilson, editors, Topics in Chromatic Graph Theory, Encyclopedia of Mathematics and its Applications, chapter 13, pages 277–303. Cambridge University Press, May 2015. [HW08] Christopher J. Hillar and Troels Windfeldt. Algebraic characterization of uniquely vertex colorable graphs. Journal of Combinatorial Theory, Series B, 98(2):400–414, March 2008. [IPS99] Russell Impagliazzo, Pavel Pudlák, and Jiří Sgall. Lower bounds for the polynomial calculus and the Gröbner basis algorithm."
  },
  {
    "prompt": "Understanding the computational aspects of Computational, Complexity, Richard.",
    "response": "Computational Complexity, 8(2):127–144, 1999. [Kar72] Richard M. Karp. Reducibility among combinatorial problems. In Complexity of Computer Computations, The IBM Research Symposia Series, pages 85–103. Springer, 1972. [KM21] Pravesh K. Kothari and Peter Manohar. A stress-free sum-of-squares lower bound for coloring."
  },
  {
    "prompt": "Understanding the computational aspects of July, Proceedings, Annual.",
    "response": "In Proceedings of the 36th Annual IEEE Conference on Computational Complexity (CCC ’21), volume 200 of Leibniz International Proceedings in Informatics (LIPIcs), pages 23:1–23:21, July 2021. [KO22] Andrei Krokhin and Jakub Opršal. An invitation to the promise constraint satisfaction problem. ACM SIGLOG News, 9(3):30–59, 2022. 35 GRAPH COLOURING IS HARD ON AVERAGE FOR POLYNOMIAL CALCULUS [KPGW10] Graeme Kemkes, Xavier Pérez-Giménez, and Nicholas Wormald."
  },
  {
    "prompt": "Understanding the computational aspects of January, Mathematics, March.",
    "response": "On the chromatic number of random 𝑑-regular graphs. Advances in Mathematics, 223(1):300–328, January 2010. [KT17] Ken-Ichi Kawarabayashi and Mikkel Thorup. Coloring 3-colorable graphs with less than 𝑛1/5 colors. Journal of the ACM, 64(1), March 2017. Preliminary version in STACS ’14. [Las01] Jean B. Lasserre. An explicit exact SDP relaxation for nonlinear 0-1 programs."
  },
  {
    "prompt": "Understanding the computational aspects of International, Optimization, IPCO.",
    "response": "In Proceedings of the 8th International Conference on Integer Programming and Combinatorial Optimization (IPCO ’01), volume 2081 of Lecture Notes in Computer Science, pages 293–303. Springer, June 2001. [Lau18] Massimo Lauria. Algorithm analysis through proof complexity. In Proceedings of the 14th Conference on Computability in Europe (CiE ’18), Sailing Routes in the World of Computation, volume 10936 of Lecture Notes in Computer Science, pages 254–263."
  },
  {
    "prompt": "Scientific context on July, Publishing, Springer and their role in graph theory.",
    "response": "Springer International Publishing, July 2018. [LN17] Massimo Lauria and Jakob Nordström. Graph colouring is hard for algorithms based on Hilbert’s Nullstellensatz and Gröbner bases. In Proceedings of the 32nd Annual Computational Complexity Conference (CCC ’17), volume 79 of Leibniz International Proceedings in Informatics (LIPIcs), pages 2:1–2:20, July 2017. [Lov94] László Lovász. Stable sets and polynomials. Discrete Mathematics, 124(1–3):137–153, January 1994. [Łuc91] Tomasz Łuczak."
  },
  {
    "prompt": "This is a technical discussion about Yuri, Matiyasevich, chromatic in theoretical computer science.",
    "response": "The chromatic number of random graphs. Combinatorica, 11(1):45–54, 1991. [Mat74] Yuri V. Matiyasevich. A criterion for vertex colorability of a graph stated in terms of edge orientations. Diskretnyi Analiz, 26:65–71, 1974. English translation of the Russian original. Available at http://logic.pdmi.ras.ru/~yumat/papers/22_paper/. [Mat04] Yuri V. Matiyasevich. Some algebraic methods for calculating the number of colorings of a graph. Journal of Mathematical Sciences, 121(3):2401–2408, May 2004."
  },
  {
    "prompt": "Understanding the computational aspects of Colin, McDiarmid, October.",
    "response": "[McD84] Colin McDiarmid. Colouring random graphs. Annals of Operations Research, 1(3):183–200, October 1984. [MN15] Mladen Mikša and Jakob Nordström. A generalized method for proving polynomial calculus degree lower bounds. In Proceedings of the 30th Annual Computational Complexity Conference (CCC ’15), volume 33 of Leibniz International Proceedings in Informatics (LIPIcs), pages 467–487, June 2015. [Mnu01] Michal Mnuk. Representing graph properties by polynomial ideals."
  },
  {
    "prompt": "This is a technical discussion about September, International, Computing in theoretical computer science.",
    "response": "In Proceedings of the 4th International Workshop on Computer Algebra in Scientific Computing (CASC ’01), pages 431–444, September 2001. [MW91] Brendan D. McKay and Nicholas C. Wormald. Asymptotic enumeration by degree sequence of graphs with degrees 𝑜(𝑛1/2). Combinatorica, 11(4):369–382, December 1991. [Par00] Pablo A. Parrilo. Structured Semidefinite Programs and Semialgebraic Geometry Methods in Robustness and Optimization. PhD thesis, California Institute of Technology, May 2000."
  },
  {
    "prompt": "Scientific context on Alexander, Razborov, References and their role in graph theory.",
    "response": "Available at http://resolver.caltech.edu/CaltechETD:etd-05062004-055516. 36 References [Pud00] Pavel Pudlák. Proofs as games. American Mathematical Monthly, pages 541–550, 2000. [Raz98] Alexander A. Razborov. Lower bounds for the polynomial calculus. Computational Complexity, 7(4):291–324, December 1998. [Raz17] Alexander A. Razborov. On the width of semialgebraic proofs and algorithms. Mathe- matics of Operations Research, 42(4):1106–1134, May 2017. [Rec75] Robert A. Reckhow."
  },
  {
    "prompt": "Understanding the computational aspects of Calculus, Lengths, Proofs.",
    "response": "On the Lengths of Proofs in the Propositional Calculus. PhD thesis, University of Toronto, 1975. Available at https://www.cs.toronto.edu/~sacook/ homepage/reckhow_thesis.pdf. [Rob65] John Alan Robinson. A machine-oriented logic based on the resolution principle. Journal of the ACM, 12(1):23–41, January 1965. [RT22] Julián Ariel Romero Barbosa and Levent Tunçel. Graphs with large girth and chromatic number are hard for Nullstellensatz. Technical Report 2212.05365, arXiv.org, December 2022."
  },
  {
    "prompt": "Explain a concept involving Hanif, Adams, Sherali in computational complexity.",
    "response": "[SA90] Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations between the continuous and convex hull representations for zero-one programming problems. SIAM Journal on Discrete Mathematics, 3:411–430, 1990. [Wor99] Nicholas C. Wormald. Models of random regular graphs. In J. D. Lamb and D. A. Preece, editors, Surveys in Combinatorics, 1999, London Mathematical Society Lecture Note Series, pages 239––298. Cambridge University Press, 1999. [Zuc07] David Zuckerman."
  },
  {
    "prompt": "Read and understand this section about August, Linear, number from a research paper.",
    "response": "Linear degree extractors and the inapproximability of max clique and chromatic number. Theory of Computing, 3(6):103–128, August 2007. Preliminary version in STOC ’06. 37"
  },
  {
    "prompt": "Scientific context on Interactive, Garg, Rafael and their role in graph theory.",
    "response": "Primes via Zeros: Interactive Proofs for Testing Primality of Natural Classes of Ideals Abhibhav Garg * Rafael Oliveira * Nitin Saxena † Abstract A central question in mathematics and computer science is the question of determining whether a given ideal I is prime, which geometrically corresponds to the zero set of I, denoted Z(I), being irreducible."
  },
  {
    "prompt": "Read and understand this section about Kaltofen, problem, ideals from a research paper.",
    "response": "The case of principal ideals (i.e., m = 1) corresponds to the more familiar absolute irreducibility testing of polynomials, where the seminal work of (Kaltofen 1995) yields a randomized, polynomial time algorithm for this problem. However, when m > 1, the complexity of the primality testing problem seems much harder. The current best algorithms for this problem are only known to be in EXPSPACE."
  },
  {
    "prompt": "Read and understand this section about problem, ideals, classes from a research paper.",
    "response": "Such drastic state of affairs has prompted research on the primality testing problem (and its more gen- eral variants, the primary decomposition problem, and the problem of counting the number of irreducible components) for natural classes of ideals. Notable classes of ideals are the class of radical ideals, com- plete intersections (and more generally Cohen-Macaulay ideals)."
  },
  {
    "prompt": "Read and understand this section about Scheiblechner, PSPACE, urgisser from a research paper.",
    "response": "For radical ideals, the current best upper bounds are given by (B¨urgisser & Scheiblechner, 2007), putting the problem in PSPACE. For complete in- tersections, the primary decomposition algorithm of (Eisenbud, Huneke, Vasconcelos 1992) coupled with the degree bounds of (DFGS 1991), puts the ideal primality testing problem in EXP."
  },
  {
    "prompt": "Scientific context on ideals, primality, equidimensional and their role in graph theory.",
    "response": "In these situations, the only known complexity-theoretic lower bound for the ideal primality testing problem is that it is coNP- hard for the classes of radical ideals, and equidimensional Cohen-Macaulay ideals. In this work, we significantly reduce the complexity-theoretic gap for the ideal primality testing prob- lem for the important families of ideals I (namely, radical ideals and equidimensional Cohen-Macaulay ideals)."
  },
  {
    "prompt": "Understanding the computational aspects of classes, ideals, Hypothesis.",
    "response": "For these classes of ideals, assuming the Generalized Riemann Hypothesis, we show that primality testing lies in Σp 3 ∩Πp 3 . This significantly improves the upper bound for these classes, approaching their lower bound, as the primality testing problem is coNP-hard for these classes of ideals."
  },
  {
    "prompt": "Understanding the computational aspects of PSPACE, Email, ideals.",
    "response": "Another consequence of our results is that for equidimensional Cohen-Macaulay ideals, we get the first PSPACE algorithm for primality testing, exponentially improving the space and time complexity of prior known algorithms. *University of Waterloo, Cheriton School of Computer Science. Email: {a65garg, rafael}@uwaterloo.ca †IIT Kanpur, Department of CSE. Email: nitin@cse.iitk.ac.in 1 arXiv:2503.20071v1 [cs.CC] 25 Mar 2025 Contents 1 Introduction 3 1.1 Proof overview . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about Acknowledgements, Preliminaries, Notation from a research paper.",
    "response": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.2 Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2 Preliminaries 10 2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Results from complexity theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3 Algebraic circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "This is a technical discussion about Results, Degree, algebra in theoretical computer science.",
    "response": ". . . . . . . . . . . . . 13 2.4 Results from linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.5 Results from algebraic geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.6 Results from number theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.7 Degree bounds for polynomial ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Height, Effective.",
    "response": "16 2.8 An Effective Bertini Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3 Height bounds 18 3.1 Height bounds for elementary operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.2 Height bounds for primitive elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.3 Height bounds for membership in ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Explain a concept involving Absolutely, polynomials, Interactive in computational complexity.",
    "response": "21 3.4 Absolutely irreducible factors of bivariate polynomials . . . . . . . . . . . . . . . . . . . . . . 23 4 Geometric Irreducibility and base change 24 4.1 Dimension zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.2 Dimension one . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5 Interactive proofs of primality testing 29 5.1 Interactive proof for radical ideals . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Explain a concept involving Conclusion, Equidimensional, Proof in computational complexity.",
    "response": ". . . . . . . . . . . . . . . . 31 5.2 Equidimensional Cohen-Macaulay ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.3 Proof of main theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6 Conclusion & open problems 33 2 1 Introduction Given a set of polynomials f1, . . . , fm ∈C[x1, . . . , xn], a fundamental algorithmic task in computer algebra and algebraic geometry is the “factorization” of the ideal I := (f1, . . ."
  },
  {
    "prompt": "Scientific context on components, generalizing, factors and their role in graph theory.",
    "response": ", fm) into “irreducible” components, generalizing the task of factoring a given polynomial into its irreducible factors."
  },
  {
    "prompt": "Scientific context on ideals, ideal, polynomial and their role in graph theory.",
    "response": "Given the more complex structure of ideals in polynomial rings, the right generalization of the polynomial factoring problem is a pri- mary decomposition of the ideal I, where one decomposes I into “primary ideals,” which can be thought of, in the geometric sense, as the “irreducible components” of the zero set defined by the ideal I, accounted with their “multiplicities.” This task was first undertaken in the pioneering works of Lasker and Her- mann [Las05; Her26], and due to its paramount importance in computer algebra and algebraic geometry, it has been the subject of extensive research by algebraic geometers, computer algebraists and complexity theorists ever since, as can be seen in [Sei74; Sei78; GTZ88; EHV92; BM93] and references therein."
  },
  {
    "prompt": "Scientific context on geometry, ideal, prime and their role in graph theory.",
    "response": "The above task naturally leads to the basic problem of deciding whether a given ideal is prime, which also has been extensively studied in theoretical computer science and algebraic geometry, as can be seen in [HS81; Kal95; BC04; BS07; Eis13] and references therein. Recall that an ideal I ⊂R is said to be prime if the quotient ring R/I is an integral domain, equivalently if whenever fg ∈I we have f ∈I or g ∈I."
  },
  {
    "prompt": "This is a technical discussion about ideal, Turing, model in theoretical computer science.",
    "response": "In the Turing model, which is the model that we will focus on in this work, the ideal primality testing problem can be formally defined in the following way. Problem 1.1 (Ideal primality testing). Given an algebraic circuit of size s computing polynomials with integer coefficients f1, . . . , fm ∈Z[x1, . . . , xn], decide if the ideal (f1, . . . , fm) · C[x1, . . . , xn] prime."
  },
  {
    "prompt": "Understanding the computational aspects of problem, testing, absolutely.",
    "response": "The ideal primality testing problem is the direct generalization of the problem of testing whether a single polynomial is (absolutely) irreducible. More precisely, the absolute irreducibility testing problem for polynomials corresponds to Problem 1.1 with m = 1. A geometric version of the ideal primality testing problem was posed in [BC04, Problem 8.5]: what is the complexity to check whether a given algebraic set is irreducible over C?"
  },
  {
    "prompt": "Read and understand this section about Algebraically, problem, cast from a research paper.",
    "response": "Algebraically, the latter problem can be cast as: given polynomials f1, . . . , fm ∈ Z[x1, . . . , xn], is rad (f1, . . . , fm) prime? The m = 1 case of Problem 1.1, that is, the absolute irreducibility test of polynomials, can be done in randomized polynomial time, and by several different methods, due to the works [Kal85; Baj+93; Kal95; Gao03] (and references therein). However, when m ⩾2, Problem 1.1 seems to be notoriously more difficult, even for natural classes of ideals."
  },
  {
    "prompt": "This is a technical discussion about ideal, algorithms, general in theoretical computer science.",
    "response": "Currently, the only algorithms to test whether a (general) ideal is prime are algorithms which compute a primary decomposition of the input ideal [Sei78; GTZ88; EHV92] (and references therein). Since these algorithms require the computation of Gr¨obner bases as a subroutine, the best complexity class which upper bounds the ideal primality testing problem is EXPSPACE, by applying the algorithm in [GTZ88] with the degree bounds for Gr¨obner basis obtained in [Dub90]."
  },
  {
    "prompt": "Understanding the computational aspects of Problem, cases, Due.",
    "response": "Due to the challenges posed by the general version of Problem 1.1 (and more generally for the primary decomposition problem), special cases of the problem(s) have been considered. In particular, natural classes of ideals have been studied, such as the cases where the ideal is a complete intersection1, or when the given ideal is radical, or ideals with either constant dimension or constant codimension."
  },
  {
    "prompt": "This is a technical discussion about Dic, Prob, ideal in theoretical computer science.",
    "response": "When the input to Prob- lem 1.1 is a complete intersection, the primary decomposition algorithm of [EHV92], combined with the Gr¨obner basis degree bounds of [Dic+91] yields an exponential time algorithm. When the input polynomi- als form a radical ideal, the works of [Chi86; Gri86] give an exponential time algorithm for computing the minimal primes of the given radical ideal, and [BS07, Theorem 4.1] gives a PSPACE-algorithm for counting the number of minimal primes of a given radical ideal."
  },
  {
    "prompt": "Read and understand this section about Theorem, Problem, RNC from a research paper.",
    "response": "Thus, for the class of radical ideals, Problem 1.1 is in PSPACE.2 For the case of radicals of ideals generated by constantly many polynomials, [BS10, Theorem 1.1] gives an algorithm in FRNC for computing the number of minimal primes, therefore solving the primality testing problem in RNC. Lastly, the dimension-dependent Gr¨obner basis degree bounds of [MT17], when combined with the primary decomposition algorithm of [GTZ88], yields a PSPACE-algorithm for primary 1An ideal I := (f1, . . ."
  },
  {
    "prompt": "Explain a concept involving dimension, generators, complete in computational complexity.",
    "response": ", fm) is a complete intersection when the codimension of I equals the number of generators (in this case m). That is, each polynomial fi “cuts” the dimension of the algebraic set by one, just as linearly independent linear forms would cut the dimension of the space by one. This is the algebraic generalization of the fact that a set of m linearly independent forms defines a space of dimension n −m."
  },
  {
    "prompt": "Understanding the computational aspects of Problem, geometric, works.",
    "response": "2The works [Chi86; Gri86; BS07] work on the geometric setting of the question, as stated in [BC04, Problem 8.5]. When the input ideal is already promised to be radical, then the geometric setting and Problem 1.1 become the same problem. 3 decomposition of ideals of constant dimension, thereby also providing a PSPACE-algorithm for the ideal primality testing problem in this case."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Problem, mality.",
    "response": "Apart from the above described algorithms, an important approach to Problem 1.1 is to provide sufficient conditions/criteria on certain classes of ideals that will ensure pri- mality. One prominent sufficient condition is given by the combination of Serre’s criterion ([Eis13, Theorem 11.5], [Sta24, Tag 031O]) with the Jacobian criterion [Eis13, Theorem 16.19], which works for the important class of connected Cohen-Macaulay ideals3 (see [Eis13, Theorem 18.15])."
  },
  {
    "prompt": "This is a technical discussion about problem, bounds, testing in theoretical computer science.",
    "response": "Given the seemingly weak upper bounds for the ideal primality testing problem even for the natural classes of ideals above, one can ask if there are any complexity-theoretic lower bounds for the ideal primal- ity testing problem. An easy argument shows that even for the natural class of zero-dimensional radical ideals, Problem 1.1 is coNP-hard. We prove this result in Proposition 2.7."
  },
  {
    "prompt": "Read and understand this section about GRH, Hypothesis, Problem from a research paper.",
    "response": "In this work, assuming the Generalized Riemann Hypothesis (GRH), we substantially improve the complexity-theoretic gap between the upper and lower bounds for Problem 1.1 for two important classes of ideals: radical ideals and equidimensional Cohen-Macaulay ideals. Our main theorem is the following. Theorem 1.2 (Interactive protocols for primality). Let C be an algebraic circuit of size s with integer constants that computes f1, . . . , fm ∈Z [x1, . . . , xn]. If I := (f1, . . ."
  },
  {
    "prompt": "This is a technical discussion about GRH, theorem, equidimensional in theoretical computer science.",
    "response": ", fm) is either radical or equidimensional Cohen-Macaulay, and if the dimension of I is given, then the complexity of testing if I is prime lies in coAM, assuming GRH. Combining the above theorem with [Koi97, Theorem 4.1], we obtain that Problem 1.1 for radical ideals and for equidimensional Cohen-Macaulay ideals are in the third level of the Polynomial Hierarchy (PH). Theorem 1.3 (Primality testing in PH). Let C be an algebraic circuit of size s with integer constants that computes f1, . . ."
  },
  {
    "prompt": "Read and understand this section about radical, ideal, bound from a research paper.",
    "response": ", fm ∈Z [x1, . . . , xn]. If the ideal I := (f1, . . . , fm) is either radical or equidimensional Cohen-Macaulay then the complexity of testing if I is prime lies in Σp 3 ∩Πp 3 , assuming GRH. In the case when the given ideal is radical, Theorem 1.3 makes substantial progress on [BC04, Problem 8.5], and improves on the previous best upper bound of [BS07] from PSPACE to Σp 3 ∩Πp 3 . Note that the coNP lower bound from Proposition 2.7 also applies to radical ideals."
  },
  {
    "prompt": "Read and understand this section about bound, GRH, substan from a research paper.",
    "response": "Thus, our upper bound gets substan- tially close to the known lower bound for the problem. Two important distinctions must be made here: our protocols only work when the input ideal is radical, and we need to assume the GRH; whereas the algo- rithm from [BS07] computes the number of irreducible components for the radical of the ideal generated by f1, . . . , fm, and their work is unconditional on the GRH."
  },
  {
    "prompt": "This is a technical discussion about Theorem, Problem, Proposition in theoretical computer science.",
    "response": "In the case of equidimensional Cohen-Macaulay ideals, which contains as special cases the important classes of zero dimensional ideals and of complete intersection ideals, Theorem 1.3 yields a nearly tight gap on the complexity of Problem 1.1, since the coNP lower bound of Proposition 2.7 also applies to zero- dimensional radical ideals. Note that our result for zero-dimensional ideals improves upon the previous upper bound of PSPACE."
  },
  {
    "prompt": "Scientific context on case, input, intersection and their role in graph theory.",
    "response": "If our input is a complete intersection, then the dimension is implicitly given to us, since the codimension of the ideal in this case is simply given by the number of input polynomials. Thus, in this case Theorem 1.2 tells us that Problem 1.1 is in coAM. This improves upon the previous best upper bound of exponential time. The above results show that the most computationally expensive step in our protocols is to determine the dimension of the input ideal."
  },
  {
    "prompt": "Explain a concept involving ideal, testing, Theorem in computational complexity.",
    "response": "Thus, any improved protocol to compute exactly the dimension of a given ideal would lead to improvements to our primality testing protocols. Testing the assumptions of Theorem 1.2 is a hard computational problem. Determining if an ideal is radical involves deciding if the ideal contains embedded primes."
  },
  {
    "prompt": "Explain a concept involving Sec, worst, tion in computational complexity.",
    "response": "As is lucidly explained in [BM93, Sec- tion 4], these embedded primes seem to be responsible for the worst case double exponential behaviour of many problems in commutative algebra and algebraic geometry. The current best known algorithms for computing the radical of an ideal are essentially the same as the algorithms for computing primary decom- position [GTZ88; EHV92], and therefore take double exponential time in the worst case."
  },
  {
    "prompt": "Explain a concept involving decision, version, easier in computational complexity.",
    "response": "It is possible that the decision version of the problem is easier, however we are not aware of any algorithms that solve the decision version faster than in double exponential space. Deciding if an ideal is equidimensional Cohen- Macaulay is similarly difficult, current methods rely on testing non-vanishing of Ext modules, which in turn requires the computation of projective resolutions. The complexity of this task is double exponential."
  },
  {
    "prompt": "Scientific context on Chapter, Tag, ideals and their role in graph theory.",
    "response": "3The definition of Cohen-Macaulay ideals is somewhat technical, as can be seen in [Eis13, Chapter 18.2] and in [Sta24, Tag 00N7]. However, as discussed in [Eis13, Chapter 18.5], Cohen-Macaulay ideals form a rich class of ideals, and are central objects in commu- tative algebra. In particular, zero dimensional ideals and complete intersections are Cohen-Macaulay."
  },
  {
    "prompt": "Understanding the computational aspects of primality, testing, problem.",
    "response": "4 This discussion also sheds some light on what makes the primality testing problem tractable in the cases we consider: we assume that our ideals do not have any embedded primes. As mentioned above, previous works solved the primality testing problem by either computing a pri- mary decomposition of the input ideal, or by using de Rham cohomology to count the number of irreducible components of the associated algebraic set (for the case of radical ideals)."
  },
  {
    "prompt": "Explain a concept involving approach, Problem, approaches in computational complexity.",
    "response": "Our approach to solve Problem 1.1 is different from the previous approaches. Our strategy is inspired by Koiran’s approach [Koi96] to decide whether a system of polynomial equations has a solution, which we now describe. Koiran, in his seminal work [Koi96], proposed a fundamentally different approach to the problem of deciding whether a system of polynomial equations with integer coefficients has a solution over C (known as the Hilbert’s nullstellensatz problem)."
  },
  {
    "prompt": "Scientific context on idea, approach, problem and their role in graph theory.",
    "response": "His approach to this problem was based on the following idea: let F := {f1 = 0, . . . , fm = 0}, where fi ∈Z[x1, . . . , xn] with deg fi ⩽d, be our system of polynomial equations. By Hilbert’s nullstellensatz, F is unsatisfiable if, and only if, 1 ∈(f1, . . . , fm). In other words, F is unsatisfiable iff there are polynomials g1, . . . , gm ∈C[x1, . . . , xn] such that 1 = f1g1 + · · · + fmgm."
  },
  {
    "prompt": "Explain a concept involving Note, entries, system in computational complexity.",
    "response": "Note that this last equation is simply a linear system of equations with integer entries, where the variables are the coefficients of the gi’s. Thus if there is a solution for this system over C, there must be a solution over Q. Hence, by clearing denominators, there are polynomials h1, . . . , hm ∈Z[x1, . . . , xn] and a ∈Z>0 such that a = f1h1 + · · · + fmhm."
  },
  {
    "prompt": "Explain a concept involving fmhm, unsatisfiable, mod in computational complexity.",
    "response": "In particular, if F is unsatisfiable, given any prime p ∈N which does not divide a, we have 1 ≡a−1 · (f1 · h1 + · · · + fmhm) mod p, which implies that F, when seen as a system over Fp[x1, . . . , xn], does not have any solutions over the field Fp. In particular, F has no solutions over Fn p."
  },
  {
    "prompt": "This is a technical discussion about theorem, solutions, primes in theoretical computer science.",
    "response": "The main conceptual insight in [Koi96] is to ask (and answer) the question: if F is satisfiable, would we be able to find ”simple” solutions (that is, solutions in Fn p, as opposed to solutions in F n p) for enough primes p?4 His main theorem [Koi96, Theorem 1] proves (assuming the GRH) a quantitative version of the following fact: if F is unsatisfiable, then there are ”very few” primes p such that F has a solution over Fn p; if F is satisfiable, then there are ”enough” (small enough) primes p such that F has a solution over Fn p."
  },
  {
    "prompt": "This is a technical discussion about Koiran, Hilbert, Equipped in theoretical computer science.",
    "response": "Equipped with the above theorem, Koiran can now distinguish between the two cases by using a set lower bound protocol, hence putting the Hilbert’s nullstellensatz problem in coAM. It is important to emphasize that the main technical challenge with the above approach, as pointed out by Koiran (see [Koi96, Page 275]), is to prove such quantitative bounds on the number of ”good primes,” when one changes the base ring from Z to Fp (or geometrically, from C to Fp)."
  },
  {
    "prompt": "Read and understand this section about object, change, base from a research paper.",
    "response": "The principle of changing the base field where an object of interest lives (in our case our algebraic set over Cn) to extract some information of our geometric object based on properties of its image is called a base change theorem. Such results are well studied in algebraic geometry and number theory, as can be seen in [GD65] and [Poo08, Appendix C].5 Often times, such base change theorems are non-constructive, which suffices for their mathematical purposes."
  },
  {
    "prompt": "Explain a concept involving change, base, computation in computational complexity.",
    "response": "However, for such a base change theorem to be useful for computation, as is the case in [Koi96], one needs to prove effective versions of such base change theorems. As usual in mathematics and computer science, making such theorems effective often requires a considerable amount of work."
  },
  {
    "prompt": "Understanding the computational aspects of ideal, Koiran, input.",
    "response": "In this work, we use the same high-level strategy developed by Koiran to give an AM protocol for proving that a given input ideal (from one of our classes of ideals) is not prime, assuming that we are given the dimension of our input ideal."
  },
  {
    "prompt": "This is a technical discussion about minimal, prime, primes in theoretical computer science.",
    "response": "We begin by noticing that in the classes of ideals that we study, there are three ways in which our ideal can fail to be prime: the ideal can have two minimal primes of different dimensions, or all the minimal primes of the ideal have the same dimension but there are at least two of them, or the ideal can have a single minimal prime with ”multiplicity.” For radical ideals, only the first and the second cases can happen, whereas for equidimensional Cohen-Macaulay ideals only the second and third cases can happen ([Eis13, Corollary 18.14])."
  },
  {
    "prompt": "Understanding the computational aspects of Jacobian, case, dimensions.",
    "response": "In the case where we have two minimal primes with different dimensions (which can only happen in the radical case) we can identify that the given ideal is not prime via the Jacobian. This allows us to reduce this case to an instance of the Hilbert’s nullstellensatz problem, which can be handled by the protocol of [Koi96]. This is done in Section 5.1."
  },
  {
    "prompt": "Understanding the computational aspects of criterion, part, Serre.",
    "response": "In the case where we have a single minimal prime with multiplicity, (which can only happen in the equidimensional Cohen-Macaulay case), we can use Serre’s criterion, combined with the Jacobian criterion 4We would like to emphasize that the part which takes a lot of work in this claim is the part that says that satisfiable systems will be satisfiable in Fn p."
  },
  {
    "prompt": "Explain a concept involving Checking, easy, system in computational complexity.",
    "response": "Checking that a system remains satisfiable over Fn p is also easy, as it can be solved by a similar argument as the one given for the unsatisfiability case. 5Sometimes base change theorems are also called spreading out theorems, as is done in the references given here. 5 (as done in [Eis13, Theorem 18.15]) to identify the multiplicity via the codimension of the Jacobian ideal inside our ideal. Thus, in this case we can use the protocol to compute dimension lower bounds given by [Koi97]."
  },
  {
    "prompt": "Understanding the computational aspects of Section, ideal, equidimensional.",
    "response": "This is done in Section 5.2. Now, the only case we have left is when our ideal is equidimensional, but has more than one irreducible component (this can happen for both the radical and the equidimensional Cohen-Macaulay cases). This turns out to be the most challenging case for us. The base change theorems of [GD65] tell us that if our ideal is prime over C[x1, . . . , xn], then, for many choices of primes p, our ideal over Fp[x1, . . . , xn] will remain prime over Fp[x1, . . . , xn]."
  },
  {
    "prompt": "This is a technical discussion about Combining, ideal, equidimensional in theoretical computer science.",
    "response": "Combining this fact with [Sei74], one also obtains that for many choices of primes p, if our ideal is equidimensional and not prime, then our ideal over Fp[x1, . . . , xn] will remain equidimensional and not prime over Fp[x1, . . . , xn]. Since we are only interested in distinguishing whether our ideal has more than one component of top dimension, by an effective version of Bertini’s theorem (Corollary 2.28), we can assume that we are working with ideals of dimension 1."
  },
  {
    "prompt": "This is a technical discussion about degree, curve, effective in theoretical computer science.",
    "response": "Thus, we have reduced our main problem to the task of distinguishing whether an algebraic curve (of potentially exponential degree) is absolutely irreducible or not. While the above gives us hope to be able to distinguish this case by going modulo p (for a ”good prime” p), there are two problems that we need to address: can we make this base change theorem effective? And in case we can make it effective, how can we witness the distinction between an absolutely irreducible curve (i.e."
  },
  {
    "prompt": "Scientific context on ideal, curve, non-prime and their role in graph theory.",
    "response": "a prime ideal) and a reducible curve (i.e., non-prime ideal) in a ”simple way”? Could we do the latter simply by looking at points in Fn p, as is done by Koiran? As it turns out, the answer to the last question is affirmative, due to effective versions of the celebrated Lang-Weil theorem (Theorem 2.20). We have thus far reduced our primality testing problem to the problem of proving an effective base change theorem which preserves equidimensional components of given algebraic sets."
  },
  {
    "prompt": "This is a technical discussion about Lang-Weil, theorem, care in theoretical computer science.",
    "response": "However, some care needs to be taken here: the Lang-Weil theorem only guarantees enough solutions over Fn p of algebraic varieties which are defined over Fp (that is, the polynomials defining the variety must have coefficients in Fp). Thus, if we are to use the Lang-Weil theorem, we must ensure that for an equidimensional non-prime ideal, at least two of its minimal primes will be Fp-definable for enough ”good primes.” At this point, one may ask:6 why not just preserve every minimal prime?"
  },
  {
    "prompt": "Understanding the computational aspects of Pages, base, change.",
    "response": "As discussed in [BM93, Pages 47 and 48], this is a very bad idea, since to do so would cause us to have to work over polynomials defined over extremely large extension fields, and therefore kill any attempt to get an effective base change theorem, since the coefficients become ”too complex.” With the above in mind, we have now finally arrived at the precise effective base change theorem that we need to prove: given an equidimensional ideal I := (f1, . . . , fm) with fi ∈Z[x1, . . ."
  },
  {
    "prompt": "This is a technical discussion about good, primes, dim in theoretical computer science.",
    "response": ", xn] and dim I = 1, we need to ensure that there are enough ”good primes” such that at least two of its irreducible components are defined over Fp. To prove such an effective base change theorem, we combine Kaltofen’s factoring algorithm over the algebraic closure [Kal95] with results from algebraic number theory and elimination theory."
  },
  {
    "prompt": "Scientific context on theorems, Sections, technical and their role in graph theory.",
    "response": "As we have mentioned before, this is the crucial technical result that we need, and it is carried out in Sections 3 and 4, which culminate in our main technical theorems, Theorems 4.5 and 4.6. We now present a summary of the above high-level discussion of our results, and in the following subsection we present a more complete overview of our proof, where we discuss in a more precise form all the issues that need to be overcome to prove correctness of our interactive protocols."
  },
  {
    "prompt": "Explain a concept involving problems, Problem, Summary in computational complexity.",
    "response": "Summary of contributions. 1. In this work, we give AM protocols for non-primality for natural classes of ideals, assuming the Generalized Riemann Hypothesis. This significantly tightens the complexity gap of these problems. More precisely, we have: • For the class of radical ideals, we prove that Problem 1.1 can be solved in coAM, whereas Propo- sition 2.7 shows that Problem 1.1 is coNP-hard for this class of problems."
  },
  {
    "prompt": "Understanding the computational aspects of Problem, class, ideals.",
    "response": "• For the class of equidimensional Cohen-Macaulay ideals (which includes the class of dimension zero ideals, and the class of complete intersection ideals), we prove that Problem 1.1 can be solved in coAM, whereas Proposition 2.7 shows that Problem 1.1 is also coNP-hard for this class of ideals. 6As mentioned in [BM93, pages 47 and 48], ”one who has never done any calculations.” 6 2."
  },
  {
    "prompt": "Explain a concept involving theorems, change, reducibility in computational complexity.",
    "response": "On the technical side, our main contribution is to prove effective base change theorems for geometric irreducibility (and reducibility) of algebraic sets with equations defined over the integers. This is the content of Theorems 4.5 and 4.6. • One important remark is that our base change theorems, together with the effective Chebotarev density theorem, yields nearly optimal density bounds for the good primes preserving (ir)reducibility. This result is interesting on its own right."
  },
  {
    "prompt": "Explain a concept involving algebraic, theory, techniques in computational complexity.",
    "response": "• The proof of our base change theorem involves the combination of (effective) techniques from algebraic geometry, elimination theory, efficient algorithms for factorization over the algebraic closure of a field (and its corresponding absolute irreducibility test), as well as techniques from algebraic number theory (to control the bit complexity of the algebraic numbers involved in the operations that we need to consider)."
  },
  {
    "prompt": "This is a technical discussion about Proof, overview, ideal in theoretical computer science.",
    "response": "1.1 Proof overview We now give a more detailed overview of the proof of our main technical theorem. Recall that we are given polynomials f1, . . . , fm ∈Z[x1, . . . , xn], defining the ideal I := (f1, . . . , fm) over C[x1, . . . , xn]. Let V := Z(I) ⊆Cn be the zero set of the ideal I, and r := dim V be the dimension of the ideal."
  },
  {
    "prompt": "This is a technical discussion about height, logarithmic, denote in theoretical computer science.",
    "response": "Since we will be interested in the bit-complexity of the polynomials and (algebraic) integers involved, we denote the logarithmic height of an integer a ∈Z by ht (a) := ⌈log(|a| + 1)⌉(i.e., its bit complexity) and extend this definition to denote the logarithmic height of a polynomial by the maximum logarithmic height of any of its coefficients. As stated earlier, for the types of ideals we consider, there are only a few ways in which the ideal I can fail to be prime."
  },
  {
    "prompt": "Scientific context on dimension, consists, prime and their role in graph theory.",
    "response": "If I is radical, then it can fail to be prime because V consists of at least two components, either both of dimension r, or one of dimension r and one of dimension less than r. If I is equidimensional CM, it can fail to be prime either because V consists of two components of dimension r, or because V consists of a single component of dimension r, but this component occurs with ”multiplicity”, that is, the unique primary component corresponding to the minimal prime of I is not itself prime."
  },
  {
    "prompt": "Explain a concept involving case, happen, dimension in computational complexity.",
    "response": "Handling the case where V has a component of dimension less than r (which can happen only in the radical case), or the case where V has a single component of dimension r but with multiplicity (which can happen only in the CM case) is straightforward, as we have discussed in the previous section. Thus, we focus on the case when I is equidimensional, where deciding primality reduces to deciding if V has only one component of dimension r."
  },
  {
    "prompt": "Read and understand this section about Bertini, theorem, mod from a research paper.",
    "response": "Also, as mentioned before, by an effective version of Bertini’s theorem, we can further assume that r = 1. As discussed above, our approach involves studying how the ideal I and the zeroset V behave when the coefficients are changed from Z to Fp for some prime p. Let Ip be the ideal generated by f1 (mod p), · · · , fm (mod p), and Vp := Z(Ip) ⊆F n p. We need to show that three properties are preserved. The first is that for all but a small number of primes, dim V = dim Vp."
  },
  {
    "prompt": "Scientific context on irreducible, primes, reducible and their role in graph theory.",
    "response": "The second is that if V is irreducible, then for all but a small number of primes, Vp is irreducible. The last is that if V is reducible, then for sufficiently many primes, Vp is also reducible, and crucially, Vp has at least two distinct components that are Fp-definable. Before we sketch our proof techniques for the above facts, we show that we cannot hope to get much stronger statements than the ones we claim."
  },
  {
    "prompt": "Explain a concept involving simple, behavior, dim in computational complexity.",
    "response": "While the examples we will give here may appear simple, it is easy to generalize their main idea to produce examples with more complex behavior. For the first property: dimension. Consider for example I = (x1, x1 + ax2) for some a ∈N. We have dim V = n −2. However, for any prime p | a, we have Ip = (x1) therefore dim Vp = n −1. Thus, we cannot hope that dim Vp remains unchanged for every base change from Z to Fp. For the second property: irreducibility. In this case, let I = (x1(x1 −1) −ax2)."
  },
  {
    "prompt": "Scientific context on reducible, polynomial, prime and their role in graph theory.",
    "response": "Here, we have that V is irreducible, but for any prime p|a, the zeroset Vp is reducible. And now for the third property: reducibility. Let’s assume n = 2. Let g ∈Z [x1] be a monic polynomial of degree d, with Galois group Sd (the full symmetric group on d elements). Note that a random polynomial satisfies this property. Let f be the polynomial obtained by homogenizing g with respect to x2, and let I = (f). Then dim V = 1, and V has d irreducible factors."
  },
  {
    "prompt": "Scientific context on extensions, factors, degree and their role in graph theory.",
    "response": "The smallest extension of Q that contains all the factors of f is exactly the splitting field of g, which has degree d!, which is exponential in d. However, there 7 are extensions of degree just d2 that contain two irreducible factors, namely extensions generated by any pair of roots of g. Now that we are aware of the pitfalls along the way, we can give a more precise idea of the quantitative bounds that we obtain."
  },
  {
    "prompt": "This is a technical discussion about size, logarithmic, set in theoretical computer science.",
    "response": "As is evident by the examples, the size of the set of bad primes for each of the state- ments will depend on the logarithmic height of the given polynomials. We will show that the size of the set of bad primes for the first two properties is polynomial in the degrees and logarithmic heights of f1, . . . , fm, and exponential in the number of variables. As the logarithmic heights are themselves exponential in s (the size of the circuit), the bounds we obtain are exponential in the input size."
  },
  {
    "prompt": "Understanding the computational aspects of number, primes, bound.",
    "response": "With the above in mind, in order for the set size lower bound protocols to work, when V is reducible, the number of good primes (that is, the number of primes p for which there are at least two Fp definable components) has to also be exponential, within a bound A of primes up to which we can check roots. In Remark 5.3 we show that the best one could hope to do in terms of the density of good primes is inverse exponential."
  },
  {
    "prompt": "Read and understand this section about show, density, theorem from a research paper.",
    "response": "Our main theorem will show that we can indeed achieve this inverse exponential density. This will show that there are sufficiently many good primes. There are three main ideas/tools we use to prove the above properties. The first is the fact that ideal membership can be written as a linear system, if bounds are known for the degrees of the witnesses. Here, by a witness to the membership of g ∈I we mean polynomials h1, . . . , hm such that g = P fihi."
  },
  {
    "prompt": "Understanding the computational aspects of size, ideals, bounds.",
    "response": "In general these bounds can be doubly exponential on the input size [Her26]. However in the special cases of radical ideals and ideals of constant dimension, single exponential bounds are known ([Jel05], [MR13]). This allows us to show that certain memberships (and non-memberships) in ideals continue to hold when going modulo p, for all but a small number of primes. The second tool is an effective Bertini-Noether theorem [Kal95]."
  },
  {
    "prompt": "Read and understand this section about Bertini-Noether, theorem, states from a research paper.",
    "response": "The Bertini-Noether theorem states that if a polynomial f ∈Z [x1, . . . , xn] is absolutely irreducible, then for all but finitely many primes p it remains absolutely irreducible in Fp. The effective version of this theorem states gives an upper bound on the number of bad primes for f that depends on the logarithmic height of f. The third main tool we use is the factoring algorithm of [Kal95]. This algorithm gives a bound on the logarithmic heights of the coefficients of factors of polynomials."
  },
  {
    "prompt": "Scientific context on Sections, number, work and their role in graph theory.",
    "response": "In this work, we combine the above ideas with a number of standard tools from commutative algebra and algebraic number theory, which can be found in Sections 2 and 3. We now return to the three statements we want to prove. Dimension of Vp. We want to show that dim Vp = 1 for all but a small number of primes p (recall that we are assuming dim V = 1). Since dim V = 1, we have I ∩Z [xi, xj] ̸= (0) for all pairs i ̸= j."
  },
  {
    "prompt": "Explain a concept involving corresponds, projection, dimension in computational complexity.",
    "response": "This is because I ∩Z [xi, xj] corresponds to projection to coordinates i, j, and such projections also have dimension at least 1. Further, if dim V = 1, then I ∩Z [xi] = (0) for some i, say i = 1. This is because projection to every coordinate cannot be finite, since V is not finite. These two properties characterise the fact that dim V = 1."
  },
  {
    "prompt": "Explain a concept involving gij, Nullstellensatz, hijk in computational complexity.",
    "response": "By the effective Nullstellensatz, if I∩Z [xi, xj] ̸= (0), then it contains a polynomial gij with deg gij ⩽dn, and such that gij = P fkhijk with hijk ∈Q[x1, . . . , xn] having deg hijk ⩽dn. Moreover, by the effective nullstellensatz, a similar linear-algebraic condition can be derived (with the same degree bounds) which established that there is no non-zero polynomial g1 ∈I ∩Z [x1]."
  },
  {
    "prompt": "Explain a concept involving hijk, system, linear in computational complexity.",
    "response": "Since the degrees of the potential hijk are bounded, we can write the condition for existence of gij, g1 as a linear system, where the unknowns are the coefficients of hijk. Using standard height bounds, we can deduce that for all but exponentially many primes, the existence and non-existence of solutions is preserved mod p, since these facts only depend on the vanishing and non vanishing of certain minors of the matrix of this linear system."
  },
  {
    "prompt": "Scientific context on irreducible, Lemma, exponentially and their role in graph theory.",
    "response": "Therefore, for all but exponentially many primes we have dim Vp = 1. This result is formally stated and proved in Lemma 4.1. Irreducibility of Vp. We want to show that if V is irreducible, then Vp is irreducible for all but a small number of primes p. Suppose n = 2, and dim V = 1. Then V is irreducible if and only if g ∈rad (I) for some absolutely irreducible polynomial g. Further, any such g must divide every generator of I."
  },
  {
    "prompt": "Explain a concept involving Nullstellensatz, deg, bound in computational complexity.",
    "response": "This easily allows us to get a bound on the coefficients of g, and by the effective Nullstellensatz, we can deduce that ge = P fihi with deg ge, deg hi ⩽dn. By Cramer’s rule, we can upper bound the common denominator of the polynomials hi. For any prime p that does not divide the common denominator, we have ge ∈Ip. Further, by an effective Bertini-Noether theorem, for all but a few primes p, the polynomial g (mod p) remains absolutely irreducible."
  },
  {
    "prompt": "This is a technical discussion about dim, Finally, irreducible in theoretical computer science.",
    "response": "Finally, for all but a few primes, dim Vp = dim V. If p is any prime that passes all the above conditions, then Vp is irreducible. 8 The general case, where n is arbitrary, is slightly more involved. It is no longer true that V is irreducible if and only if g ∈rad (I) for some absolutely irreducible polynomial g. For example, V could consist of two curves that both lie on the same irreducible hypersurface."
  },
  {
    "prompt": "Understanding the computational aspects of irreducible, subspace, call.",
    "response": "To overcome this, we project to a random two dimensional linear subspace, call this projection π. Since V is irreducible, π(V) is irreducible, and π(V) = Z(g) for an absolutely irreducible polynomial g. A technical (and somewhat involved) argument gives us bounds on the coefficients of g in this more general setting (Lemma 3.14). Proceeding as above, we can show that for all but a few primes p, we have dim Vp = 1 and π(Vp) is irreducible."
  },
  {
    "prompt": "Scientific context on irreducible, reducible, conclude and their role in graph theory.",
    "response": "However, the above is not enough to conclude that Vp is irreducible, since Vp could have been reducible, and all of its components could have been mapped to π(Vp) under π. To fix this, instead of just considering a single random projection π, we consider a large number of random projections π1, . . . , πk, and repeat the above argument."
  },
  {
    "prompt": "This is a technical discussion about irreducible, projections, components in theoretical computer science.",
    "response": "If Vp has two components, πi(Vp) can only be irreducible for a small fraction of these projections, and we can use this to bound all primes p such that Vp is not irreducible. This result is formally stated and proved in Theorem 4.5. Reducibility of Vp and Fp-definability of some of its components. We want to show that if V is reducible, then Vp is reducible, and has at least two Fp-definable components for sufficiently many primes p. Again, let us start by assuming n = 2."
  },
  {
    "prompt": "This is a technical discussion about rad, irreducible, components in theoretical computer science.",
    "response": "If V has k irreducible components, then V = Z(g) for a polynomial g ∈ rad (I) with exactly k absolutely irreducible factors. As before, g must divide the generators of I, which allows us to get a bound on the coefficients of g. By the effective Nullstellensatz, we can deduce that ge = P fihi with deg ge, deg hi ⩽dn, and we can deduce that for all but a few primes p, we have ge ∈Ip."
  },
  {
    "prompt": "This is a technical discussion about reducible, mod, imply in theoretical computer science.",
    "response": "However, this does not imply that Vp is reducible: it might be the case that Vp = Z(g′) for some g′ that is an absolutely irreducible factor of g (mod p). Therefore, the strategy we used to show that irreducibility is preserved does not work here. To prove that the above cannot happen (that is, that Vp = Z(g′)), we try and find defining equations for some components of V. In the case when n = 2, these are exactly the factors of g. Suppose g = Qk i=1 gi."
  },
  {
    "prompt": "Scientific context on extension, Lemma, coefficients and their role in graph theory.",
    "response": "Each gi has coefficients in some algebraic extension of Q. The smallest extension of Q that contains every gi might be of prohibitively high degree, but from the factoring algorithm of Kaltofen (Lemma 3.15) we can deduce that there is a small enough extension of Q that contains g1, g2. Now we consider the ideals I1 := I1 + g1 and I2 := I + g2. These are ideals of dimension 1 that are irreducible."
  },
  {
    "prompt": "Explain a concept involving curves, base, change in computational complexity.",
    "response": "Therefore, we can use our base change and irreducibility arguments to deduce that I1,p and I2,p are irreducible curves, which will be components of Ip. Of course we have to be careful here: g1 and g2 have coefficients in Q (α) for some algebraic number α, therefore it is not even clear at first what it means to go modulo p. Here, we need to invoke some standard results from algebraic number theory."
  },
  {
    "prompt": "Understanding the computational aspects of mod, root, monic.",
    "response": "If G is the monic equation of α, then for any p such that G (mod p) has a root, the operation of going mod p is well defined on Z [α] [x1, . . . , xn]. This allows us to not only make sense of the mod p operation and invoke our previous result, but it also guarantees that the components of Vp that we find this way are Fp definable. It is important to note here that there are a number of technical challenges in extending our earlier statements to this general algebraic number theoretic setting."
  },
  {
    "prompt": "Understanding the computational aspects of Section, challenges, dealt.",
    "response": "These challenges are dealt with in Section 3. Now suppose n is arbitrary. We again find equations that define the components of V. As before we do this by random projection to a two dimensional affine subspace, and then we apply (some of) our arguments from the case of n = 2. It turns out that a single extra polynomial g1 is not enough to define a component of V. However, we show any component of V can be defined by adding at most two equations to I."
  },
  {
    "prompt": "Scientific context on equations, reducible, definable and their role in graph theory.",
    "response": "As before, we give bounds on the coefficients of these equations, and then invoke our earlier arguments to deduce that Vp is reducible, and for sufficiently many primes it has at least two components that are Fp definable. This result is formally stated and proved in Theorem 4.6. 1.1.1 Putting it all together We now show how all the results mentioned above are put together to obtain our main protocol."
  },
  {
    "prompt": "Read and understand this section about case, brevity, similar from a research paper.",
    "response": "For brevity, we only consider the case when I is radical and dim V = 1, as the other cases are very similar. We now sketch an AM protocol that shows that I is not prime. In the radical case, I can fail to be prime for 2 reasons: V may have a component of strictly smaller dimension (hence in this case V will have at least one isolated point as a solution), or V is equidimensional with at least two components. 9 Case 1: V has an isolated point as a solution."
  },
  {
    "prompt": "This is a technical discussion about Jij, Jacobian, ifj in theoretical computer science.",
    "response": "Consider the Jacobian matrix J, whose entries are Jij = ∂ifj. If α is an isolated point of V, then the rank of J(α) is n. Merlin first sends Arthur the index of n columns of J that are linearly independent. If this submatrix is J′, then Arthur simply has to check if the system f1 = 0, . . . , fm = 0, det J′ ̸= 0 is satisfiable. This can be rewritten as f1 = 0, . . . , fm = 0, y det J′ −1 = 0 where y is a new variable."
  },
  {
    "prompt": "Scientific context on protocol, prime, solution and their role in graph theory.",
    "response": "Now, the satisfiability protocol of [Koi96] can be used to decide I is not prime, as in this case the protocol of [Koi96] will return that the system that we constructed will have a solution (which must correspond to an isolated point). On the other hand, if V only has components of dimension 1, then at every point on V, the rank of J is at most n −1. Therefore, no matter what choice of columns Merlin picks, the above protocol will fail to accept with high probability."
  },
  {
    "prompt": "This is a technical discussion about Case, components, dimension in theoretical computer science.",
    "response": "Case 2: V has two components of dimension 1. For every prime p such that Vp has at least two irreducible components, the Lang-Weil theorem says that the number of Fp points in Vp is at least 2p (up to an error term). If V has two components, then we have shown that there are enough primes with this property."
  },
  {
    "prompt": "This is a technical discussion about irreducible, number, points in theoretical computer science.",
    "response": "On the other hand, if V is irreducible, for all except a finite number of primes, Vp is irreducible, and therefore the Lang-Weil theorem says that the number of Fp points in Vp is at most p (up to an error term). Therefore, Merlin can prove to Arthur that V is reducible, by performing the set lower bound protocol on the set of primes for which the number of Fp points in Vp is at least 2p."
  },
  {
    "prompt": "Scientific context on Arthur, Merlin, fact and their role in graph theory.",
    "response": "Merlin also has to convince Arthur that there are in fact 2p points in Vp, since the primes p are too large for Arthur to check this himself. For this, the set lower bound protocol is used again, on the set of Fp points of Vp. 1.2 Acknowledgements N.S. is grateful to the University of Waterloo for an invite to the distinguished lecture series in Jun’24. The discussions done in Waterloo’s conducive environment led to this work. In addition, N.S."
  },
  {
    "prompt": "Explain a concept involving JCB, Chair, Rao in computational complexity.",
    "response": "thanks the funding support from DST-SERB (JCB/2022/57) and N.Rama Rao Chair. A.G. would like to thank TIFR for very generously hosting him during the later part of this project. Part of this work took place while R.O. and N.S. participated in the workshop on ”Algebraic and Analytic Methods in Computational Complexity” in Dagstuhl (Sep’24). The authors would like to thank Schloss Dagstuhl for providing excellent hospitality and conditions to conduct research discussions."
  },
  {
    "prompt": "This is a technical discussion about Dagstuhl, Peter, Pascal in theoretical computer science.",
    "response": "The authors would like to thank Peter B¨urgisser and Pascal Koiran for helpful conversations about this project during the workshop at Dagstuhl. 2 Preliminaries In this section we establish the notation that will be used throughout the paper, along with the background results that we will need in the later sections. 2.1 Notation Let R := Z [x1, . . . , xn] denote the n-variate polynomial ring with integer coefficients, and let S := Q [x1, . . ."
  },
  {
    "prompt": "Scientific context on denote, ideals, ideal and their role in graph theory.",
    "response": ", xn] denote the extension of scalars to the algebraic closure of Q. All the ideals we discuss are ideals of S, un- less stated otherwise. The same holds for any property of the ideals we discuss, for example the property of an ideal being prime, or Cohen-Macaulay, among others. We use An to denote the affine space in n- dimensions, with underlying field Q. We use Z(I) to denote the zeroset of an ideal I."
  },
  {
    "prompt": "Explain a concept involving proofs, polynomials, deal in computational complexity.",
    "response": "In the course of our proofs, we will have to deal with polynomials with coefficients in finite extensions of Q. Given a monic irreducible polynomial q ∈Z [z], and a root α ∈Q of q, we use A to denote the ring Z [α] [x1, . . . , xn]. Note that Z [α] is not necessarily the ring of integers of Q (α). Elements of A will be represented as polynomials in R [z], with z-degree less than deg q. This representation of elements of A is unique."
  },
  {
    "prompt": "Understanding the computational aspects of context, notation, ring.",
    "response": "The ring A depends on q, however q will always be clear from context, and therefore we suppress it from notation. In this setting, by deg f we mean the degree of f in the x-variables, unless stated otherwise. The logarithmic height of an integer c, denoted by ht (c), is the bit-complexity of c. This notion of logarithmic heights extends naturally to polynomials."
  },
  {
    "prompt": "Read and understand this section about height, logarithmic, deg from a research paper.",
    "response": "Given a polynomial f ∈R, the logarithmic height of f, also denoted by ht (f), is the maximum logarithmic height of the coefficients of f. If f ∈A, then ht (f) is 10 defined to be the logarithmic height of f treated when written as a polynomial in R [z] with z-degree less than deg q. Given polynomials f1, . . . , fm, g with deg fi, deg g ⩽d, the condition g ∈(f1, . . . , fm) is equivalent to the existence of polynomials h1, . . . , hm such that g = P fihi."
  },
  {
    "prompt": "Explain a concept involving priori, case, degrees in computational complexity.",
    "response": "If a bound on the degrees of hi is known a priori (which is usually the case), then the above condition can be written as a linear system where the coefficients of h1, . . . , hm are the unknowns. If this bound on the degrees of hi is D, we use MD(f1, . . . , fm) to denote the matrix corresponding to this linear system. The entries of this matrix are coefficients of f1, . . . , fm. When f1, . . . , fm are clear from context we denote this matrix by just MD."
  },
  {
    "prompt": "Read and understand this section about number, total, system from a research paper.",
    "response": "Observe that the total number of columns of MD, that is, the number of unknowns in the system is at most m · \u0000D+n n \u0001 . The total number of equations is at most \u0000D+d+n n \u0001 . Both these estimates easily follow from counting the number of monomials of given degrees. When reasoning using this linear system, we will slightly overload notation, and use the same symbols to refer to both polynomials and their coefficient vectors. For example, we write the condition g ∈(f1, . . ."
  },
  {
    "prompt": "Explain a concept involving MDv, unknowns, vector in computational complexity.",
    "response": ", fm) as g = MDv where v is a vector of unknowns. We further say that h1, . . . , hm are a solution to the system. 2.2 Results from complexity theory We begin by describing an AM protocol that lower bounds the size of sets that have an efficient membership test. The protocol is due to [GS86], and the following statement is from [AB09, Section 8.4.1]. Lemma 2.1. Suppose S ⊂{0, 1}n is a set such that the problem of membership in S is in NP."
  },
  {
    "prompt": "This is a technical discussion about Suppose, satisfy, number in theoretical computer science.",
    "response": "Suppose further that a number K is known, and S is guaranteed to either satisfy |S| ⩾2K or |S| < K. Then the problem of deciding if |S| ⩾2K is in AM. We sketch the protocol here, since it will make it easier to explain the generalisation we require. How- ever, we omit proofs. We also assume for convenience that K is a power of 2."
  },
  {
    "prompt": "Scientific context on Input, Boolean, Algorithm and their role in graph theory.",
    "response": "Algorithm 1: Goldwasser-Sipser set lower bound protocol Input : Boolean formula ϕS(x, y) such that x ∈S if and only if there exists y satisfying ϕS(x, y), and an integer K. Arthur : Let k := log2(2K), and pick a random hash function h : {0, 1}n →{0, 1}k+1 from a pairwise independent hash function collection, and pick u ∈{0, 1}k+1 uniformly at random. Send h, u to Merlin. Merlin : Find x, y such that h(x) = u and ϕS(x, y) is true. Send x, y to Arthur."
  },
  {
    "prompt": "Explain a concept involving Accept, Arthur, true in computational complexity.",
    "response": "Arthur : Accept if and only if h(x) = u and ϕS(x, y) is true. We now state a slight generalisation of this protocol, to the case when membership in S itself can be verified by an AM protocol. Corollary 2.2. Suppose for each x ∈{0, 1}n there exists a subset Sx ⊂{0, 1}m(x), and an integer K(x), such that m(x), K(x) are polynomially bounded functions of x."
  },
  {
    "prompt": "Read and understand this section about Suppose, returns, true from a research paper.",
    "response": "Suppose further that there is a uniform algorithm that runs in polynomial time that given input x, returns the number K(x) and also returns a boolean formula ϕx, such that z ∈Sx if and only if there exists y such that ϕx(z, y) is true. Suppose further than an integer K is known. If S ⊂{0, 1}n is the set of elements x such that |Sx| ⩾2K(x), and if S is promised to either satisfy |S| ⩾2K or |S| ⩽K, then the problem of deciding if |S| ⩾2K is in AM."
  },
  {
    "prompt": "Read and understand this section about proof, protocol, decides from a research paper.",
    "response": "We give a four round protocol that decides |S| ⩾2K. The result then follows from the fact that AM = AM[c] for all constants c ⩾2. The proof of correctness of the protocol is the exact same as the proof of correctness of Lemma 2.1, therefore we omit the proof. Remark 2.3. The choice of the constant 2 in the above protocol is arbitrary, and 2 can be replaced by a slightly smaller constant, say 1.9."
  },
  {
    "prompt": "Scientific context on Koiran, problem, main and their role in graph theory.",
    "response": "We now state the main theorems of Koiran, giving interactive protocols for the problem of deciding whether a system of polynomial equations has a solution, and for the problem of estimating the dimension of an algebraic set. We first state the main theorem of [Koi96], on the Hilbert nullstellensatz problem."
  },
  {
    "prompt": "Explain a concept involving Algorithm, Input, Corollary in computational complexity.",
    "response": "11 Algorithm 2: Goldwasser-Sipser set lower bound protocol with membership in AM Input : An integer K, and an algorithm that on input x outputs the integers K(x) and the circuit ϕx described in Corollary 2.2 Arthur : Let k := log2(2K), and pick a random hash function h : {0, 1}n →{0, 1}k+1 from a pairwise independent hash function collection, and pick u ∈{0, 1}k+1 uniformly at random. Send h, u to Merlin. Merlin : Find x such that h(x) = u and |Sx| ⩾2K(x). Send x to Arthur."
  },
  {
    "prompt": "Scientific context on Reject, Arthur, Merlin and their role in graph theory.",
    "response": "Arthur : Reject if h(x) ̸= u. If h(x) = u, let kx := log2(2K(x)), and pick a random hash function hx : {0, 1}m(x) →{0, 1}kx+1 from a pairwise independent hash function collection, and pick ux ∈{0, 1}kx+1 uniformly at random. Send hx, ux to Merlin. Merlin : Find y, z such that hx(z) = ux and such that ϕx(z, y) is true. Send z, y to Arthur. Arthur : Accept if and only if hx(z) = ux and ϕx(z, y) is true. Theorem 2.4. Assume GRH. Given f1, . . ."
  },
  {
    "prompt": "Explain a concept involving theorem, protocol, decides in computational complexity.",
    "response": ", fm ∈R, there is an AM protocol that decides if Z(f1, . . . , fm) ̸= ∅. The above theorem was further generalised by Koiran in [Koi97, Theorem 4.1], where now one wants to decide a lower bound on the dimension of the given algebraic set. Theorem 2.5. Assume GRH. Given f1, . . . , fm ∈R, and an integer r, there is an AM protocol that decides if dim Z(f1, . . . , fm) ⩾r. Remark 2.6."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, polynomials, results.",
    "response": "We make two remarks on the proofs of Theorem 2.4 and Theorem 2.5 that will be crucial to the way we invoke these results. The first of these is regarding the representation of input polynomials. The AM protocols for the above problems involve evaluating the polynomials f1, . . . , fm (mod p) at points in Fp, where p is a prime of bit complexity polynomial in the input. Therefore, the polynomials f1, . . ."
  },
  {
    "prompt": "This is a technical discussion about size, circuits, circuit in theoretical computer science.",
    "response": ", fm are allowed to be given either as white box circuits of polynomial size (here the size includes the bit complexity of the constants in the circuit), or more generally as black-boxes that allow mod p queries. The second remark is regarding the parameters. Suppose deg fi ⩽d and ht (fi) ⩽h. The length of the messages and the computation done by Arthur in the protocols in Theorem 2.4 and Theorem 2.5 is polynomial in log \u0010 h · 2(n log σ)c\u0011 , for a universal constant c, where σ := dm + 2."
  },
  {
    "prompt": "This is a technical discussion about Theorem, polynomials, create in theoretical computer science.",
    "response": "Therefore, if we create a system of polynomials g1, . . . , gm with deg gi ⩽dn and ht \u0010 h · 2(n log σ)c\u0011 , then the protocols in Theorem 2.4 and Theorem 2.5 applied to g1, . . . , gm still run in time poly (n, m, d, h) as long as we ensure that g1, . . . , gm have circuits of size poly (n, m, d, h). This will be crucial in our proofs. Lastly, we show that the ideal primality testing problem is coNP-hard, even for the special case of equidimensional Cohen-Macaulay ideals."
  },
  {
    "prompt": "Explain a concept involving Proposition, coNP-hardness, testing in computational complexity.",
    "response": "Proposition 2.7 (coNP-hardness of ideal primality testing). Given a 3CNF Φ on n variables, there exists a polynomial time algorithm whose output is a circuit CΦ computing polynomials f1, . . . , fm ∈R, along with their partial derivatives ∂ifj, such that the ideal I := (f1, . . . , fm)S ⊂S has the following properties: • I is a radical, equidimensional Cohen-Macaulay ideal • I is prime if and only if Φ is unsatisfiable. Proof. Let x2 1−x1, x2 2−x2, . . . , x2 n−xn, g1, . . ."
  },
  {
    "prompt": "Read and understand this section about system, solution, arithmetization from a research paper.",
    "response": ", gr be the arithmetization of Φ, where r is the number of clauses in Φ. The instance Φ is satisfiable if and only if the arithmetized system has a solution. The reduction algorithm proceeds as follows. First check if (0, . . . , 0) is a solution to the above system. If it is, then the algorithm returns the system f1 = x1 (x1 −1), clearly there is a constant-sized circuit that computes f1, ∂if1. Suppose (0, . . . , 0) is not a solution to the above system."
  },
  {
    "prompt": "Scientific context on xjgi, set, polynomials and their role in graph theory.",
    "response": "Consider the set of polynomials \b x2 i −xi i∈[n]∪ {xjgi}i∈[r],j∈[n]. Since each gi has a small circuit, there is a small circuit that computes all the above polyno- mials and all their derivatives. Further, the set of zeroes of the above system is exactly those points on the boolean hypercube that correspond to a satisfying assignment of ϕ, and also the point (0, . . . , 0). 12 In each of the above cases, by the Lemma in [Sei74, Page 310], the ideal constructed is radical."
  },
  {
    "prompt": "Explain a concept involving ideal, case, complete in computational complexity.",
    "response": "In the first case the ideal is a complete intersection therefore CM. In the second case the ideal is zero dimensional, therefore CM. If Φ is satisfiable, then in either case the system created has at least two components and is therefore not prime. If Φ is not satisfiable, the system created has unique solution (0, . . . , 0), thus the ideal is prime. 2.3 Algebraic circuits The inputs to our algorithms are given as algebraic circuits."
  },
  {
    "prompt": "This is a technical discussion about marked, nodes, Algebraic in theoretical computer science.",
    "response": "Algebraic circuits are natural models for com- puting polynomials. They consist of a directed acyclic graph, with nodes marked with + and ×. The source nodes are marked either with variables xi or with the constant 1. Each internal node computes the natural polynomial: nodes marked with + compute the sums of their inputs, and nodes marked with × compute the product."
  },
  {
    "prompt": "Understanding the computational aspects of constants, constant, αfu.",
    "response": "The edges are marked with constants, if (u, v) is an edge with constant α, then the input to v corresponding to u is αfu, where fu is the polynomial computed at the node u. We refer the reader to the excellent survey [SY10] for more background on algebraic circuits. Circuits are studied both as uniform and as non-uniform models of computation."
  },
  {
    "prompt": "Scientific context on setting, constants, circuit and their role in graph theory.",
    "response": "Further, they are studied both in the setting where all constants are considered to have size 1, and also in the setting where the logarithmic height of the constants is part of the size of the circuit. Since we are using circuits as inputs to a problem in the Turing machine model of computation, we are naturally in the setting where the size of circuit includes the logarithmic heights of the constants of the circuit. We assume that the constants in the circuit are all integers."
  },
  {
    "prompt": "Scientific context on circuit, Size, edges and their role in graph theory.",
    "response": "Size of a circuit: The size of an algebraic circuit is the sum of the logarithmic heights of the constants on the edges of circuits. We assume that every edge has a constant (potentially 1), therefore the size of a circuit is a lower bound on the number of edges of the circuit. Evaluating a circuit modulo p: Given a circuit C, a prime p, and a point α ∈Fn p, there is a polynomial time algorithm that computes the evaluations at α of all the mod p reductions of all polynomials computed by C."
  },
  {
    "prompt": "Read and understand this section about evaluation, circuit, computes from a research paper.",
    "response": "The algorithm recursively computes the evaluation (mod p) of the inputs to a given node in the circuit, and then performs arithmetic in Fp to compute the evaluation of the gate itself. Structural results: We need the following technical result [B¨ur13, Lemma 4.16] that bounds the loga- rithmic heights and degrees of polynomials computed by circuits of size s. The result follows by induction on the circuit, and using the fact that every coefficient in the circuit has logarithmic height at most s."
  },
  {
    "prompt": "Understanding the computational aspects of circuit, size, Lemma.",
    "response": "Lemma 2.8. If C is a circuit of size s computing polynomials f1, . . . , fm ∈R then ht (fi) ⩽22s and deg fi ⩽2s. We also need the following result from [BS83] that proves that given a circuit C, there is a circuit C′ of similar size that also computes the partial derivatives of the polynomials computed by C. Lemma 2.9. If C is a circuit of size s computing polynomials f1, . . . , fm ∈R there is a circuit C′ of size 5sm that computes f1, . . . , fm and all the partial derivatives ∂ifj."
  },
  {
    "prompt": "Read and understand this section about Results, Cramer, linear from a research paper.",
    "response": "2.4 Results from linear algebra Cramer’s rule is a well known explicit formula for the solution of a linear system of the form Ax = b, when A is a n × n matrix with det(A) ̸= 0. Under these conditions, the unique solution to the above system is given by xi = det Ai det A , where Ai is the matrix obtained by replacing the ith column of A with the vector b."
  },
  {
    "prompt": "Explain a concept involving det, solution, provided in computational complexity.",
    "response": "The following easy consequence shows that a similar formula exists for under and overdetermined systems, provided a solution is promised to exist. Note that the lemma applies for matrices with coefficients in any domain. Lemma 2.10. Suppose A is an n×m matrix, and suppose the linear system Ax = b is guaranteed to have a solution. Then there exists a solution where each xi is either 0 or of the form det Mi det N , where det Mi, det N are minors of the augmented matrix A|b. Proof."
  },
  {
    "prompt": "Understanding the computational aspects of columns, rank, column.",
    "response": "Let r := rank A, so r ⩽m, n. The fact that Ax = b has a solution is equivalent to the fact that the augmented matrix [A|b] also has rank r. After rearranging the columns, we can assume that the first r columns of A are linearly independent. Since b lies in the column span of A, it also lies in the column span 13 of the first r columns of A. Equivalently, if we set the variables xm−r+1, . . . , xm to 0, the resulting system still has a solution."
  },
  {
    "prompt": "Read and understand this section about rows, assume, system from a research paper.",
    "response": "It suffices to show the result for the resulting system, therefore we can assume that A has full column rank, so r = m. After further rearranging rows, we can assume that the first r rows of A are linearly independent, and the remaining rows are linear combinations of the first r rows. The augmented matrix [A|b] also has rank r, therefore the last n −r rows of [A|b] are linear combinations of the first r rows."
  },
  {
    "prompt": "Scientific context on A′x, solution, algebraic and their role in graph theory.",
    "response": "If we write A′x = b′ for the linear system consisting of the first r rows of Ax = b, then any solution to A′x = b′ is also a solution to Ax = b. Applying Cramer’s rule to A′x = b′ gives us the desired result. 2.5 Results from algebraic geometry In this subsection, we collect some useful basic results from algebraic geometry which we will need in the later sections. We begin by stating a characterisation of the dimension of an algebraic set. Lemma 2.11."
  },
  {
    "prompt": "Understanding the computational aspects of ideal, Suppose, zeroset.",
    "response": "Suppose I ⊂S is an ideal, and V is the zeroset of I. Then V has dimension r if and only if the following two conditions hold. • There is some subset U ⊂[n] of size r such that the elimination ideal IU is zero. • For every subset U′ ⊂[n] of size r + 1, the elimination ideal IU′ is nonzero. The first of the above conditions is equivalent to dim V ⩾r, and the second condition is equivalent to dim V ⩽r. We now state some standard facts about tangent spaces, which can be found in [SR94, Chapter 2]."
  },
  {
    "prompt": "This is a technical discussion about Chapter, Section, Theorem in theoretical computer science.",
    "response": "Let I be a radical ideal generated by f1, . . . , fm. Let J be the Jacobian of I, which is defined to be the matrix with Jij = ∂fi/∂xj. At any point x ∈V, the tangent space Tx(V) is isomorphic to the kernel of J(x), where J(x) is the Jacobian entrywise evaluated at the point x [SR94, Chapter 2, Section 1, Theorem 2.1]. At every nonsingular point x, the dimension of the tangent space is exactly equal to dimension of the component of V passing through x [SR94, Chapter 2, Section 1, Theorem 2.3]."
  },
  {
    "prompt": "Scientific context on dimension, singular, point and their role in graph theory.",
    "response": "At every singular point x, the dimension of the tangent space is greater than the dimension of the components of V passing through x. The singular locus of V is a proper algebraic subset of V, and does not contain any irreducible component of V. We deduce that there is a point x0 ∈V such that dim Tx0(V) < r if and only if V has an irreducible com- ponent of dimension less than r. The following lemma is an equivalent formulation of the above statement. Lemma 2.12."
  },
  {
    "prompt": "Explain a concept involving Theorem, Jacobian, dimension in computational complexity.",
    "response": "If V is a algebraic set of dimension r, and if J is the Jacobian of a radical ideal I such that V = Z(I), then rank J(x0) > n −r for some x0 ∈V if and only if V has an irreducible component of dimension less than r. The following statement is [Hei83, Theorem 1], and is referred to as B´ezout’s inequality. Theorem 2.13 (B´ezout’s inequality). Let K be an algebraically closed field and X, Y ⊆An K be locally closed subsets. Then, we have deg(X ∩Y) ⩽deg X · deg Y."
  },
  {
    "prompt": "Explain a concept involving B´ezout’s, inequality, Proposition in computational complexity.",
    "response": "A careful inductive application of the B´ezout’s inequality implies the following inequality, which we also refer to as B´ezout’s inequality. The statement given below is a special case of [HS80, Proposition 2.3]. Theorem 2.14 (Degree bounds for closed sets). Let K be an algebraically closed field and f1, . . . , fm ∈K[x1, . . . , xn] be polynomials of degree at most d. Then, we have deg V(f1, . . . , fm) ⩽dmin(m,n)."
  },
  {
    "prompt": "Scientific context on lemma, pro, jective and their role in graph theory.",
    "response": "The following lemma gives effective bounds on hyperplane sections that reduce the dimension of pro- jective varieties, a proof can be found in [GSS19, Lemma 19] Lemma 2.15. Suppose V ⊂Pn is a projective zeroset of dimension r and degree D. If ℓis a linear form where each coefficient is chosen uniformly and independently from a set B ⊂N, then dim V ∩Z(ℓ) = r −1 with probability at least 1 −D/ |B|. If ℓ1, . . . , ℓr+1 are linear forms chosen the same way then V ∩Z(ℓ1, . . ."
  },
  {
    "prompt": "Explain a concept involving probability, Suppose, linear in computational complexity.",
    "response": ", ℓr+1) = ∅with probability at least 1 −(r + 1)D/ |B|. 14 Suppose W ⊂An is an affine zeroset of dimension r and degree D. If ℓis a linear polynomial where each coefficient is chosen uniformly and independently from a set B ⊂N, then dim W ∩Z(ℓ) = r −1 with probability at least 1 −2D/ |B|. If ℓ1, . . . , ℓr+1 are linear polynomials chosen the same way then W ∩Z(ℓ1, . . . , ℓr+1) = ∅with probability at least 1 −2(r + 1)D/ |B|."
  },
  {
    "prompt": "Understanding the computational aspects of Noether, Theorem, Chapter.",
    "response": "The following is a sufficient condition for a linear map to be a Noether normalising map. This statement, and its proof can be found in [SR94, Chapter 1, Section 5, Theorem 1.15]. Lemma 2.16. Suppose V ⊂Pn is a projective variety disjoint from a k-dimensional linear subspace E ⊂Pn. Then the projection π : X →Pn−k−1 with center E defines a finite map X →π(X). Combining the two lemmas above gives us the following effective Noether Normalisation theorem."
  },
  {
    "prompt": "Read and understand this section about Effective, Normalisation, Noether from a research paper.",
    "response": "Lemma 2.17 (Effective Noether Normalisation). Suppose V ⊂Pn is a projective zeroset of dimension r and degree D. If ℓ1, . . . , ℓr+1 are linear forms where each coefficient is chosen uniformly and independently from a set B ⊂N, then the map π : V →Pr with coordinate functions ℓi is a well defined finite map with probability at least 1 −(r + 1)D/ |B|. Suppose W ⊂An is an affine zeroset of dimension r and degree D. If ℓ1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, map, probability.",
    "response": ", ℓr are linear polynomials where each coefficient is chosen uniformly and independently from a set B ⊂N, then the map π : W →Ar with coordinate functions ℓi is a well defined finite map with probability at least 1 −2(r + 1)D/ |B|. Proof. For the projective case, by Lemma 2.15, the zeroset Z(ℓ1, . . . , ℓr+1) is a linear space disjoint from V with probability at least 1 −(r + 1)D/ |B|. If this holds, then π is a finite and well defined map by Lemma 2.16."
  },
  {
    "prompt": "Scientific context on Theorem, case, affine and their role in graph theory.",
    "response": "The affine case follows by applying the projective case to the closure of Wp, and picking ℓr+1 = x0. Finally we state a necessary and sufficient condition for equidimensional Cohen-Macaulay rings to be reduced. The following statement is a special case of [Eis13, Item a, Theorem 18.15]. For the readers conve- nience, we isolate the ingredients in the proof here. Theorem 2.18. Let I = (f1, . . . , fm) ⊂C [x1, . . . , xn] be a codimension c ideal such that C [x1, . . ."
  },
  {
    "prompt": "Explain a concept involving equidi, mensional, Cohen-Macaulay in computational complexity.",
    "response": ", xn] /I is equidi- mensional Cohen-Macaulay. Let J be the Jacobian of I, that is, the m × n matrix with entries Jij = ∂fi/∂xj. Let J be the ideal generated by the c × c minors of J in the ring C [x1, . . . , xn] /I. Then I is radical if and only if J has codimension at least 1. Proof. For any Noetherian ring, being reduced is equivalent to the properties R0 and S1 [Sta24, Tag 031R]."
  },
  {
    "prompt": "Understanding the computational aspects of Tag, Theorem, Cohen-Macaulay.",
    "response": "Cohen-Macaulay rings satisfy Sk for every k [Sta24, Tag 0342], therefore it suffices to show that R0 is equiv- alent to J having codimension at least 1. By the Jacobian criterion [Eis13, Theorem 16.19], for any prime p of C [x1, . . . , xn] /I, regularity of the localisation at p is equivalent to J ̸⊂p. Finally, J has codimension at least 1 if and only if it is not contained in any minimal prime, which by the above happens if and only the localisation at every minimal prime of C [x1, . . ."
  },
  {
    "prompt": "Read and understand this section about regular, condition, Results from a research paper.",
    "response": ", xn] /I is regular, which is precisely the condition R0. 2.6 Results from number theory We state some algebraic number theory facts that we will need. We do not provide any proofs since these facts are standard, [Mil20, Chapter 2, 3] is an excellent exposition of all of these results. Let q ∈Z [z] be a monic irreducible polynomial with deg q = e, and let α be a root of q. Let F := Q (α) be the algebraic extension of Q generated by α, we have F ∼= Q [z] / (q)."
  },
  {
    "prompt": "Read and understand this section about ring, integers, set from a research paper.",
    "response": "Let OF be the ring of integers of F, that is, the set of elements β ∈F such that β satisfies a monic equation with coefficients in Z. We have Z ⊂OF, and OF is integrally closed in F. The discriminant of q, denoted discz (q) is defined to be resz (q, ∂q). Usually the ring of integers OF is different from Z [α]. We have Z [α] ⊂OF, but this inequality might be strict. However, it holds that OF ⊂(1/ discz (q))Z [α]."
  },
  {
    "prompt": "Explain a concept involving prime, discz, represent in computational complexity.",
    "response": "Therefore we can represent any algebraic integer as an element of Q [z], where the coefficients have common denominator discz (q). The ring OF is a Dedekind domain, and every nonzero prime ideal of OF is maximal. For any prime p ⊂OF, there is a unique prime p ∈N such that p ∩Z = (p). If this happens, then we say p lies above p. While OF is not factorial in general, it admits unique factorisation of ideals. Every ideal I ⊂OF can be the written uniquely as a product of primes, I = Q pei i ."
  },
  {
    "prompt": "Explain a concept involving ideals, prime, ideal in computational complexity.",
    "response": "In particular, the ideal (p) for a prime p ∈N itself can be factored as (p) = Q pei i . The set of primes ideals of OF occurring in this factorisation of (p) are exactly 15 the prime ideals of OF that lie above p. When p ̸ | discz (q), ei = 1 for all i, if this holds we say that p is unramified. For any unramified p, and p lying above p, the quotient map OF →OF/p is well structured."
  },
  {
    "prompt": "Explain a concept involving map, maximal, quotient in computational complexity.",
    "response": "Since p is maximal, and since p ∈p, the quotient OF/p is a field of characteristic p, and is in fact a finite extension of Fp. The prime p corresponds to an irreducible factor q1 of q (mod p), such that p = pOF + q1(α)OF. It follows that OF/p ∼= Fp [z] /q1(z). Composing with the inclusion map Z [α] →OF gives us a map Z [α] → Fp [z] /q1(z). We usually apply this to the case when q1 is a linear polynomial, and therefore Fp [z] /q1(z) = Fp. This map therefore extends the usual map Z →Fp."
  },
  {
    "prompt": "Scientific context on lemma, Gauss, state and their role in graph theory.",
    "response": "We now state a corollary of Gauss’s lemma. The following formulation is from [WR76, Lemma 7.1], we continue to refer to it as Gauss’s lemma. Lemma 2.19 (Gauss’s lemma). Suppose δ ∈OF and f ∈(1/δ)OF is a monic polynomial with factorisation f = gh in F. If g, h are monic then g, h ∈(1/δ)OF. We will need three more technical results from number theory. The first of these is an effective Lang- Weil bound."
  },
  {
    "prompt": "Read and understand this section about bounds, Theorem, terms from a research paper.",
    "response": "The Lang-Weil bounds are upper and lower bounds on the number of Fp points on irreducible varieties that are Fp definable. Classically these bounds have error terms, and effective versions of such bounds give bounds on the coefficients of the error terms. The following is a version of such a bound from [CM06, Theorem 7.1]. While we state the bound in generality, we only require it for curves. Theorem 2.20 (Effective Lang-Weil bound)."
  },
  {
    "prompt": "Scientific context on absolutely, irreducible, affine and their role in graph theory.",
    "response": "Let V ⊂Fn p be an absolutely irreducible affine variety defined over Fp of dimension r and degree D. Suppose also that p > 2(r + 1)D2. Then |V(Fp) −pr| < (D −1)(D −2)pr−1/2 + 5D13/3pr−1. The second number theoretic statement we need is a lower bound on the number of primes p for which q (mod p) has a root in Fp."
  },
  {
    "prompt": "Understanding the computational aspects of fraction, Galois, Chebotarev.",
    "response": "Observe that the Chebotarev density theorem states that this fraction of primes is the same as the fraction of the Galois group of q that has at least one fixed point, and this fraction is at least 1/e. We require an effective version of this statement. The following statement is from [Koi96, Corollary 1]. The proof itself is based on a bound by [AO83] which in turn is based on an effective version of the Chebotarev density theorem [LO77]."
  },
  {
    "prompt": "This is a technical discussion about conditional, result, GRH in theoretical computer science.",
    "response": "We note that this last result is conditional, and assumes the GRH, and this is also what makes our result conditional. Theorem 2.21. Assuming the GRH holds, there exists an absolute constant c such that πq(x) ⩾1 e \u0010 π(x) −log discz (q) −cx1/2 log (discz (q) xe) \u0011 , where π(x) is the prime counting function and πq(x) is the number of primes p ⩽x such that q (mod p) has a root in Fp. Finally, we need an unconditional lower bound on the prime counting function π(x)."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Nullstellensatz, Corollary.",
    "response": "One such estimate is the following classical bound. Theorem 2.22 ([RS62, Corollary 1]). For x ⩾17 we have π(x) > x/ ln x. 2.7 Degree bounds for polynomial ideals We now recollect some important complexity results for polynomial ideals. We begin with Jelonek’s effec- tive Nullstellensatz, from [Jel05, Theorem 1.1]. Theorem 2.23 (Effective Nullstellensatz). Let K be an algebraically closed field, and X ⊂Km be an affine n- dimensional variety of degree D. Let f1, . . ."
  },
  {
    "prompt": "Explain a concept involving non-constant, common, polynomials in computational complexity.",
    "response": ", fm ∈K[X] be non-constant polynomials without common zeros, where di := deg fi, and d1 ⩾· · · ⩾dm. Lastly, let N(d1, . . . , dm; n) :=      Qm i=1 di if n > 1 and n ⩾m dm · Qn−1 i=1 di if m > n > 1 d1 if n = 1 16 There exist polynomials gi ∈K[X] with deg(figi) ⩽ \u000e D · N(d1, . . . , dm; n), if m ⩽n 2D · N(d1, . . . , dm, n) −1, if m > n such that 1 = Pm i=1 figi. We now state some useful bounds on the complexity of representation of the reduced Gr¨obner basis of any ideal."
  },
  {
    "prompt": "Read and understand this section about Krull, Theorem, sections from a research paper.",
    "response": "The bounds that we state below are dependent on the Krull dimension of the given ideal7, and this will be crucial in our applications in the later sections. These bounds are from [MR13, Theorem 4]. Theorem 2.24 (Gr¨obner basis complexity). Let K be an infinite field and I = (f1, . . . , fm) ⊆K[x1, . . . , xn] be an ideal of dimension r, where dj := deg(fj) and d1 ⩾· · · ⩾dm. Let B := 2 · \u00121 2 \u0010 (d1 · · · dn−r)2(n−r) + d1 \u0011\u00132r . Given any admissible monomial ordering, if G = {g1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of obner, reduced, basis.",
    "response": ", gt} is the reduced Gr¨obner basis of I, then we have: deg(gi) ⩽B ∀i ∈[t]. Moreover, for each i ∈[t], there are polynomials hi1, . . . , him ∈K[x1, . . . , xn] satisfying deg(fj · hij) ⩽B such that gi = m X j=1 fjhij. The above bounds, when combined with the division algorithm, allow us to derive the following upper bound on the dimension of the linear system (over the base field) needed to check membership in polyno- mial ideals. Corollary 2.25 (Representation degree)."
  },
  {
    "prompt": "Read and understand this section about deg, infinite, field from a research paper.",
    "response": "Let K be an infinite field and I = (f1, . . . , fm) ⊆K[x1, . . . , xn] be an ideal of dimension r, where dj := deg(fj) and d1 ⩾· · · ⩾dm. Let B := 2 · \u00121 2 \u0010 (d1 · · · dn−r)2(n−r) + d1 \u0011\u00132r . Given any polynomial f ∈K[x1, . . . , xn] with deg(f) = D, we have that f ∈I if, and only if, there exist g1, . . . , gm ∈ K [x1, . . . , xn] such that deg(fjgj) ⩽max (B, D) and f = m X j=1 fjgj."
  },
  {
    "prompt": "Read and understand this section about Bertini, Theorem, irreducible from a research paper.",
    "response": "2.8 An Effective Bertini Theorem Bertini’s second theorem states that the intersection of an irreducible variety of dimension r ⩾2 with a random hyperplane of dimension n −r + 1 is an irreducible curve. This statement allows us to reduce the problem of irreducibility testing of arbitrary high dimensional varieties to that of curves and points."
  },
  {
    "prompt": "Scientific context on Section, theorem, algebra and their role in graph theory.",
    "response": "The theorem plays an important role in many routines in computer algebra, a comprehensive survey of its applications can be found in [Dic05, Section 9.1.3]. The effective versions of the theorem are usually stated for hypersurfaces, reduction from the general case to hypersurfaces follows from a projection argument. In order to apply the theorem to arbitrary varieties, a preprocessing projection step is applied."
  },
  {
    "prompt": "Scientific context on Bertini, results, Proposition and their role in graph theory.",
    "response": "The following two results are from [CM06], where the effective Bertini theorem is used to obtain improved Lang-Weil bounds. While the results in [CM06] are stated for varieties over Fq, the following two results also hold for varieties over any perfect field, with the same proofs. 7as is standard in commutative algebra, the Krull dimension of an ideal I is the Krull dimension of the quotient ring K [X] /I. 17 Lemma 2.26 ([CM06, Proposition 6.1, Proposition 6.3])."
  },
  {
    "prompt": "Scientific context on variables, equidimensional, affine and their role in graph theory.",
    "response": "Let V be an equidimensional affine variety of dimension r and degree D. Let Λ be a (r + 1) × n matrix of variables, and let Γ be a r + 1 dimensional vector of variables. There exists a nonzero polynomial G ∈Q [Λ, Γ] of degree at mot 2(r + 1)D2 such that for any values λ, γ of the variables Λ, Γ satisfying G(λ, γ) ̸= 0, the projection π defined by ℓi = P λijxj + γj is a birational map between V and its image π(V)."
  },
  {
    "prompt": "This is a technical discussion about polynomial, degree, Corollary in theoretical computer science.",
    "response": "Further, there is a polynomial g of degree D such that π(V) \\ Z(g) and V \\ π−1(Z(g)) are isomorphic. Lemma 2.27 ([CM06, Corollary 3.4]). Suppose f ∈Z [x1, . . . , xn] is an absolutely irreducible polynomial of degree D. For a point (v, w, z) ∈Q n × Q n−1 × Q n−1, let fv,w,z(x, y) := f(x + v1, w2x + z2y + v2, . . . , wnx + zny + vn). There exists a polynomial H of degree at most 10D5 such that for any v, w, z satisfying H(v, w, z) ̸= 0, the polynomial fv,w,z remains absolutely irreducible."
  },
  {
    "prompt": "Read and understand this section about Effective, Theorem, Bertini from a research paper.",
    "response": "Corollary 2.28 (Effective Bertini Theorem). Suppose V is an equidimensional affine variety of dimension r and degree D. There is a randomised algorithm that returns linear equations ℓ1, . . . , ℓr−1 with ht (ℓi) ⩽(nD)c such that the following holds with high probability: V ∩Z(ℓ1, . . . , ℓr−1) is an equidimensional affine curve, with the same number of irreducible components as V. Proof."
  },
  {
    "prompt": "Understanding the computational aspects of end, irreducible, focus.",
    "response": "We focus on each irreducible component of V and apply a union bound at the end, therefore assume without loss of generality that V is irreducible. Fix set B := \u0002 10D7\u0003 , a subset of N. Pick linear polynomials h1, . . . , hr+1 with coefficients from B, and let π be the projection map with coordinate functions hi. By Lemma 2.26, π(V) is birational with its image with probability at least 1 −2(r + 1)D2/ |B|. The image is an irreducible hypersurface of degree D, say Z(f)."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, point, linear.",
    "response": "Now pick a point (v, w, z) from Bn × Bn−1 × Bn−1. By Lemma 2.27, fv,w,z is absolutely irreducible with probability at least 1 −10D5/ |B|. Pick a linear subspace L ⊂Ar+1 such that fv,w,z is isomorphic to f|L, this can be performed by elementary linear algebra. By construction, L ∩π(V) is an irreducible curve. Since L is a random linear subspace, (π(V) ∩Z(g)) ∩Z(L) is a finite set, where g is the polynomial guaranteed by Lemma 2.27 8. Now we show that π−1(L) is the required subspace."
  },
  {
    "prompt": "Read and understand this section about poly, logarithmic, height from a research paper.",
    "response": "First observe that π−1(L) has defining equations of logarithmic height poly (D, n), this is because the equations are obtained by inverting matrices with entries of logarithmic height poly (D, n). Now we know that π(V)∩Z(L) is an irreducible curve. If ψ is the inverse of the birational map π, then π−1(L)∩V is exactly ψ(π(V) ∩Z(L)), which is irreducible and dimension 1."
  },
  {
    "prompt": "Scientific context on Height, operations, polynomial and their role in graph theory.",
    "response": "3 Height bounds In this section we recall some height bounds for certain operations over polynomial rings over the integers, as well as of certain operations over polynomial rings over certain algebraic numbers. Then, we proceed to prove some height bounds for membership problems in given ideals, which is done in Section 3.3."
  },
  {
    "prompt": "This is a technical discussion about Kaltofen, Height, polynomials in theoretical computer science.",
    "response": "We then conclude the section with some lemmas on absolutely irreducible factors of bivariate polynomials, where we discuss the work of Kaltofen and its implications to our setting. 3.1 Height bounds for elementary operations The following are some elementary height bounds on the results of basic arithmetic operations performed on polynomials. The next two statements are from [KPS01, Lemma 1.2]. Lemma 3.1. Let f1, . . . , fm ∈R be polynomials with deg fi ⩽d and ht (fi) ⩽h. Then 1."
  },
  {
    "prompt": "This is a technical discussion about log, Mij, matrix in theoretical computer science.",
    "response": "ht (P fi) ⩽h + log (m). 2. ht (Q fi) ⩽hm + md log (n + 1). 3. If g ∈Z [y1, . . . , ym] then ht (g(f1, . . . , fm)) ⩽ht (g) + deg g (h + log (m + 1) + d log (n + 1)). The above lemma allows us to get height bounds for the determinant of a matrix with ring elements. Corollary 3.2. Let M be a m×m matrix with Mij ∈R such that deg(Mij) ⩽d and ht (Mij) ⩽h for all i, j. Then ht (det M) ⩽m (h + log (m) + d log (n + 1))."
  },
  {
    "prompt": "Read and understand this section about Lemma, height, bounds from a research paper.",
    "response": "8Note that Lemma 2.15 does not apply since the distribution of L is different, but the same proof works 18 Once we have the above height bounds, the next corollary yields height bounds on the resultant and on the cofactors of the resultant identity. Corollary 3.3. Let f, g ∈Z [y] be coprime polynomials with deg f, deg g ⩽d and ht (f) , ht (g) ⩽h."
  },
  {
    "prompt": "Scientific context on log, deg, Proof and their role in graph theory.",
    "response": "Then there exists a, b ∈Z [y] with deg a < deg g and deg b < deg f such that resy (f, g) = af + bg, where ht (a) , ht (b) , ht (resy (f, g)) ⩽2d (h + log (2d + 1)) . In particular, if ∆is the discriminant of f then ht (∆) ⩽2d (h + log (d) + log (2d + 1)) . Proof. In Q [y] we have the B´ezout identity sf + tg = 1, with deg s < deg g and deg t < deg f. We can write this as a linear system of at most 2d equations in at most 2d variables."
  },
  {
    "prompt": "This is a technical discussion about Lemma, Cramer, coefficients in theoretical computer science.",
    "response": "The solution to this system is given by Cramer’s rule (Lemma 2.10): each coefficient of s, t has numerator given by the determinant of a matrix of size 2d×2d with entries the coefficients of f, g. The denominator is common among all the coefficients, and is similarly given by such a determinant. In fact this denominator is exactly resy (f, g). Clearing common denominators from the equation gives us the claimed identity with the claimed bounds."
  },
  {
    "prompt": "Understanding the computational aspects of resy, log, root.",
    "response": "The last statement follows from the fact that the discriminant is exactly resy (f, f′), and ht (f′) ⩽h + log d. We also need to bound the logarithmic height of any rational root of an integral polynomial. Lemma 3.4. Let f ∈Z [y] be a degree d polynomial and suppose a/b is a rational root of f in minimal form. Then ht (a) , ht (b) ⩽ht (f) d + 1. Proof. Suppose f = P fiyi, where fi ∈Z."
  },
  {
    "prompt": "This is a technical discussion about monic, integer, Multiplying in theoretical computer science.",
    "response": "Multiplying throughout by fd−1 d , and replacing fdy with a variable x gives us a monic integer polynomial g = xd + Pd−1 i=1 gixi with deg g = d and ht (g) ⩽ht (f) d. Since a/b is a root of f, we have tht c := fda/b is a root of g. Further, c ∈Z since g is a monic integer polynomial. By [Mig83, Theorem 2] we have ht (c) ⩽1 + ht (g) ⩽ht (f) d + 1. We can obtain a/b by putting c/fd in minimal form, therefore ht (a) , ht (b) ⩽ht (c) , ht (fd) ⩽ht (f) d + 1."
  },
  {
    "prompt": "Understanding the computational aspects of monic, proposition, shows.",
    "response": "We can also bound the heights of the quotient and the remainder upon division by a monic polynomial, as the following proposition shows. Proposition 3.5. Let f, g ∈Z[y] be polynomials where d := deg f ⩾deg g =: e, ht (f) , ht (g) ⩽h, and g is a monic polynomial. If f = g·q+r, where g, r ∈Z[y] with deg r < e, then we have ht (g) , ht (r) ⩽(d+1)·(h+2 log (d + 1)). Proof."
  },
  {
    "prompt": "Read and understand this section about matrix, Note, equations from a research paper.",
    "response": "Note that g, r are the unique solution to the following system of linear equations: N · \u0012g r \u0013 = \u0000f\u0001 , where N is a (d + 1) × (d + 1) matrix of the form N = \u0000Mq P\u0001 and P = \u0012 0 Ie \u0013 . Since q is monic, we know that N is a unipotent lower triangular matrix. Thus, det(N) = 1, and by Cramer’s rule (Lemma 2.10) we have that each coefficient of g and r is given by the determinant of a matrix in Z(d+1)×(d+1) with height ⩽h. By Corollary 3.2 we have the desired bound."
  },
  {
    "prompt": "Explain a concept involving earlier, nite, polynomials in computational complexity.",
    "response": "As discussed earlier, we will often have to work with polynomials with coefficients lying in some fi- nite extension generated by an algebraic integer α. If α has monic minimal polynomial q(z) ∈Z [z] with deg q(z) ⩽e then we will represent elements of A = Z [α] [x1, . . . , xn] ≃R[z]/(q) as polynomials in R[z] of degree less than e in z. Thus, elements of A inherit the logarithmic height function from R[z]."
  },
  {
    "prompt": "Scientific context on lemma, poly, nomial and their role in graph theory.",
    "response": "After some algebraic operations, for example taking products of such polynomials, the resulting poly- nomial will no longer have degree less than e in the variable z, and we must perform a step of going modulo q(z). The following lemma bounds the change in logarithmic height by this operation. A sharper version of this lemma can be found in [Kal95, Lemma 1]. Lemma 3.6. Let q(z) ∈Z [z] be a monic polynomial with deg q = e. Suppose f ∈R [z] is a polynomial with degz f = d ⩾e."
  },
  {
    "prompt": "Explain a concept involving mod, Lastly, log in computational complexity.",
    "response": "Lastly, suppose ht (f) , ht (q) ⩽h. Then ht (f mod q) ⩽(d + 1) (h + 2 log (d + 1)). Proof. Let Mf be the set of exponent vectors of f, when viewed as a polynomial in Z[z][x1, . . . , xn]. Hence, we have f = X a∈Mf fa(z) · xa. Therefore, f mod q = X a∈Mf (fa(z) mod q) · xa and thus ht (f mod q) = max a∈Mf ht (fa(z) mod q) ⩽(d + 1) · (h + 2 log (d + 1)). 19 Where the last inequality follows from Proposition 3.5."
  },
  {
    "prompt": "Explain a concept involving Lemma, Corollary, Combining in computational complexity.",
    "response": "Combining Corollary 3.2 with Lemma 3.6 we are able to obtain the following height bounds for deter- minants with entries in rings of the form Z[α], where α is an algebraic integer. Corollary 3.7. Let q(z) ∈Z[z] be monic with deg q = e, ht (q) ⩽h and let α ∈Q be a root of q. If M ∈Z[α]m×m is such that ht (Mij) ⩽h for all i, j ∈[m], we have that ht (det M) ⩽e · m · (m · (e + h + log m) + 2 log(em)). Proof."
  },
  {
    "prompt": "Read and understand this section about log, Mij, det from a research paper.",
    "response": "By regarding M ∈Z[z]m×m, with deg Mij < e and ht (Mij) ⩽h for all i, j ∈[m] Corollary 3.2 implies that deg det M ⩽(e−1)·m and ht (det M) ⩽m·(h+log(m)+(e−1)). Now, applying Lemma 3.6 to det M with quotient q(z), we have that ht ((det M) mod q) ⩽((e −1) · m + 1) · (m · (h + log(m) + (e −1)) + 2 log((e −1)m + 1)) ⩽e · m · (m · (e + h + log(m)) + 2 log(em)) . The following corollary obtains height bounds for basic operations over the ring A. Corollary 3.8."
  },
  {
    "prompt": "Scientific context on log, deg, monic and their role in graph theory.",
    "response": "Let q(z) ∈Z[z] be monic with deg q = e, ht (q) ⩽h and let α ∈Q be a root of q. Let A := Z[α][x1, . . . , xn]. Let f1, . . . , fm ∈A be polynomials with deg fi ⩽d and ht (fi) ⩽h. Then 1. ht (Pm i=1 fi) ⩽h + log (m). 2. ht (Qm i=1 fi) ⩽em · (hm + dm log(n + 2) + 2 log(em)). 3. If g ∈Z[α] [y1, . . . , ym] with dg := deg(g) then ht (g(f1, . . . , fm)) ⩽2edg · [ht (g) + dg · (h + log(m + 1) + max {e, d} · log(n + 2)) + 2 log(2edg)] Proof."
  },
  {
    "prompt": "This is a technical discussion about item, degree, Lemma in theoretical computer science.",
    "response": "For item 1, note that the coefficients of Pm i=1 fi are still given by polynomials in Z[z] with degree less than e, since addition does not increase the degree. Thus, the bounds follow from Lemma 3.1 item 1. For item 2, let p = Qm i=1 fi. In this case, we have that p = ˆp mod q, where ˆp ∈R[z] is the product of the polynomials fi when seen as elements of R[z]. By Lemma 3.1 item 2, ht (ˆp) ⩽h · m + md log(n + 2). If degz(ˆp) < e, we have ht (p) = ht (ˆp) and we are done."
  },
  {
    "prompt": "Read and understand this section about log, Lemma, bound from a research paper.",
    "response": "Else, by Lemma 3.6 and the bound degz(ˆp) ⩽em−1, we have ht (p) = ht (ˆp mod q) ⩽h · em2 + dem2 log(n + 2) + 2em log(em). For item 3, consider the representation ˆg ∈Z[y1, . . . , ym][z] and the representation of bfi ∈R [z]. Let p := ˆg(bf1, . . . , c fm) ∈R[z]. Thus, degz(p) ⩽(e −1) · (dg + 1) and by Lemma 3.1 item 3, ht (p) ⩽ht (g) + dg · (h + log(m + 1) + max {e −1, d} · log(n + 2)) Since g(f1, . . . , fm) = p mod q, by Lemma 3.6 we have ht (g(f1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of log, Height, bounds.",
    "response": ", fm)) ⩽2edg · [ht (g) + dg · (h + log(m + 1) + max {e, d} · log(n + 2)) + 2 log(2edg)] . 3.2 Height bounds for primitive elements In this section we gather height bounds for representations of primitive elements. The following theorem is stated in [Koi96, Theorem 4]. Theorem 3.9 (Complexity of primitive element). There is a universal constant c ⩾1 such that the following holds. Let α1, α2, . . . , αm be m algebraic numbers which are roots of polynomials Pi ∈Z [z] with deg(Pi) ⩽d and ht (Pi) ⩽h."
  },
  {
    "prompt": "Read and understand this section about Theorem, deg, dmc from a research paper.",
    "response": "There is a primitive element γ for α1, . . . , αm which is a root of an irreducible polynomial q ∈Z [z] with deg(q) ⩽dm and ht (q) ⩽h · dmc. Moreover, there are non-zero integers ai and polynomials Qi ∈Z [z] with ht (ai) ⩽h · dmc, deg(Qi) < deg(q) and ht (Qi) ⩽h · dmc such that αi = Qi(γ)/ai for every i ∈[m]. We now state [Koi96, Theorem 7]. Theorem 3.10. There is an absolute constant c > 0 such that if the system f1, . . . , fm ∈R has a solution over Q, then there is a solution α = (α1, . . ."
  },
  {
    "prompt": "Explain a concept involving deg, log, Nullstellensatz in computational complexity.",
    "response": ", αn) such that each αi is a root of a polynomial Pi ∈Z[z] satisfying deg(Pi) ⩽2(n log σ)c and ht (Pi) ⩽h · 2(n log σ)c, where σ := 2 + Pm i=1 deg(fi). 20 3.3 Height bounds for membership in ideals We now state some height bounds on the polynomials occurring in the Nullstellensatz over Z and Z [α], where α is an algebraic integer. Results of this nature are known as effective arithmetic Nullstellensatz, and the current best known bounds can be found in [KPS01]."
  },
  {
    "prompt": "Understanding the computational aspects of bounds, polynomials, optimal.",
    "response": "While the optimal bounds for polynomials with integer coefficients are easy to state, the optimal bounds for the more general case of polynomials with algebraic integers are significantly more technical. Their statements involve extending the notion of logarithmic height to algebraic number fields, which is fairly intricate. We choose to forego these optimal bounds in the interest of simplicity, as the more elementary (and weaker) bounds suffice for our purposes. Subsection setup."
  },
  {
    "prompt": "Understanding the computational aspects of subsection, rest, monic.",
    "response": "For the rest of this subsection, we let q ∈Z [z] be a monic irreducible polynomial with deg q = e and ht (q) ⩽h, and let α ∈Q be a root of q. Let A := Z [α] [x1, . . . , xn]. By our convention, we represent polynomials in A as elements of R [z] with z-degree less than e. In the statements that follow, we will consider a degree parameter d ⩾3, and polynomials f1, . . . , fm ∈A with deg fi ⩽d and ht (fi) ⩽h. We will denote by I := (f1, . . . , fm) ⊆S the ideal generated by f1, . . ."
  },
  {
    "prompt": "Explain a concept involving deg, Lemma, Theorem in computational complexity.",
    "response": ", fm and by V := Z(I) ⊆Q n. Lemma 3.11. If V(I) = ∅, then there exists a ∈Z [α] \\ {0} and g1, . . . , gm ∈A satisfying a = f1g1 + · · · + fmgm, (1) with deg gi ⩽2dn and ht (gi) , ht (a) ⩽h · e5 · d5n2. Proof. Since V(f1, . . . , fm) = ∅, the effective Nullstellensatz Theorem 2.23 shows that there are polynomials h1, . . . , hm ∈S with with deg hi ⩽2dn such that 1 = X fihi. (2) Let M := MD(f1, . . . , fm) be the matrix of the linear system Eq. (2) (as defined in Section 2) with D := 2dn."
  },
  {
    "prompt": "Read and understand this section about elements, solution, Note from a research paper.",
    "response": "Note that the entries of M are elements of Z[α] (hence given by elements of Z [z] of degree less than e). Theorem 2.23 implies that the linear system 1 = Mv has a solution with coefficients in Q, and since the co- efficients of M lie in Q [α], this implies the existence of a solution with coefficients in Q [α]. By Lemma 2.10, there exists a solution where each unknown vj is either 0, or of the form detNj det N , where det Nj, det N are minors of [M|1]."
  },
  {
    "prompt": "Read and understand this section about det, Note, unknowns from a research paper.",
    "response": "Note that the denominator is common among all the unknowns. We will take our polyno- mials g1, . . . , gm to be the polynomials in A corresponding to the vector det N · v (note that this is a vector with entries in A). By the above, this choice gives us a solution to Eq. (1) with gi ∈A and deg gi ⩽2dn. We now need to bound ht (gi). Since ht (gi) ⩽maxj{ht (det Nj)}, it is enough to bound ht (det N) and ht (det Nj)."
  },
  {
    "prompt": "Read and understand this section about log, Corollary, eD′ from a research paper.",
    "response": "Each square submatrix N, Nj has entries in Z[α] and dimension upper bounded by D′ := \u0000D+d+n n \u0001 ⩽dn2+n, which is the number of rows of M. As ht (Mij) ⩽h for all i, j, Corollary 3.7 yields ht (det N) , ht (det Nj) ⩽eD′ (D′ (e + h + log D′) + 2 log (eD′)) ⩽edn2+n \u0010 dn2+n \u0000e + h + (n2 + n) · log d \u0001 + 2 log \u0010 edn2+n\u0011\u0011 ⩽h · e5 · d5n2 The following easy corollary extends this result to arbitrary membership in radical ideals. Corollary 3.12. Let f ∈A be such that deg f ⩽d and ht (f) ⩽h."
  },
  {
    "prompt": "Explain a concept involving rad, exist, fmgm in computational complexity.",
    "response": "If f ∈rad (I) then there exist t ∈N, a ∈Z [α] \\ {0}, and g1, . . . , gm ∈A satisfying aft = f1g1 + · · · + fmgm, with t ⩽2(d + 1)n+1, deg gi ⩽2(d + 1)n+2, and ht (gi) , ht (a) ⩽12 · h · e5 · (d + 1)6(n+1)2. Proof. Let y be a new variable, and consider the polynomials f1, . . . , fm, 1−yf ∈A[y]. As f ∈rad (f1, . . . , fm), we have Z(f1, . . . , fm, 1 −yf) = ∅, and thus Lemma 3.11 implies that there exist a ∈Z [α] \\ {0} and h, h1, . . . , hm ∈A such that a = P fihi + h(1 −yf)."
  },
  {
    "prompt": "Scientific context on Lemma, deg, implies and their role in graph theory.",
    "response": "Moreover, Lemma 3.11 implies that deg(hi), deg(h) ⩽ 21 D := 2(d + 1)n+1. Substituting y = 1/f in the above equation and multiplying by ft to clear denom- inators, where t ⩽D (due to the degree bounds), we obtain equation aft = P figi, where we have gi = ft · hi(x1, . . . , xn, 1/f). We now show the bounds on the heights and degrees. Let h′ i(x1, . . . , xn, y) := ydegy(hi)·hi(x1, . . . , xn, 1/y). Since hi and h′ i have the same coefficient set, we have ht (h′ i) = ht (hi)."
  },
  {
    "prompt": "Understanding the computational aspects of deg, Corollary, bounds.",
    "response": "Moreover, deg h′ i = deg(hi). By Lemma 3.11 we also have that ht (hi) , ht (a) ⩽h · e5 · (d + 1)5(n+1)2, and also deg hi ⩽D. Since gi = ft−degy(hi) · h′ i(x1, . . . , xn, f), we have that deg(gi) ⩽t · deg(f) + deg(hi) ⩽D · (d + 1). Moreover, Corollary 3.8 items 2 and 3 imply the desired height bounds. We will use the above lemmas to show bounds on the defining equations for the image of Z (f1, . . . , fm) under certain projections."
  },
  {
    "prompt": "Explain a concept involving Lemma, albeit, assumptions in computational complexity.",
    "response": "This is an arithmetic version of Lemma 2.11, albeit with stronger assumptions. Lemma 3.13. Suppose I is radical and dim I = 1. Define π : An →A2 to be projection to the first two coordinates. If dim π(V) = 1 and π(V) is equidimensional, then π(V) = Z(g) for some g ∈Z [α] [x1, x2]. Further, there exists a ∈Z [α] \\ {0}, and h1, . . . , hm ∈A satisfying ag = X fihi, with deg hi ⩽d4n2 and ht (g) , ht (hi) ⩽h · e2 · (d)12n3. Proof. The ideal of π(V) is exactly the elimination ideal I ∩Q [x1, x2]."
  },
  {
    "prompt": "Understanding the computational aspects of equidi, mensional, principal.",
    "response": "By the assumption that π(V) is equidi- mensional with dim π(V) = 1, this ideal is principal. Let G be the reduced Gr¨obner basis of I with respect to the lexicographic order induced by the variable order xn ≻xn−1 ≻· · · ≻x1. Since I is generated in A, we have G ⊂Q [α] [x1, . . . , xn]. Thus, I∩Q [x1, x2] is generated by the elements of G∩Q [x1, x2]. Since I∩Q [x1, x2] is principal, we must have G ∩Q [x1, x2] = {ˆg}. Since ˆg ∈G we have ˆg ∈Q(α)[x1, x2] and LT (ˆg) = 1."
  },
  {
    "prompt": "Explain a concept involving theorem, deg, common in computational complexity.",
    "response": "Let b ∈Z be the common denominator of ˆg and g := b · ˆg ∈Z[α][x1, x2]. By the above, π(V) = Z(g). By B´ezout’s theorem (Theorem 2.13) we have deg g ⩽dn. By the above paragraph, g is the unique (up to scalar multiplication) lowest degree polynomial in I ∩Q [x1, x2]. Let B := d4n2. Since dim(I) = 1 and B is larger than deg g and also larger than the representation bound from Corollary 2.25, there are forms hi ∈Q [α] [x1, . . . , xn] with deg (fihi) ⩽B such that g = P fihi. Let M := MB(f1, . . ."
  },
  {
    "prompt": "Read and understand this section about Section, bhi, matrix from a research paper.",
    "response": ", fm) be the matrix corresponding to the linear system as described in Section 2. Let M′ be the matrix obtained by dropping the rows of M that correspond to monomials in x1, x2 that are smaller than LM (g) in the monomial order. Any solution c h1, . . . , c hm of the linear system M′v = LM (g) is such that P fi bhi ∈I ∩Q [x1, x2], and LM \u0010P fi bhi \u0011 = LM (g). Therefore, for any such solution c h1, . . . , c hm we have P fi bhi = 1 LT(g) · g = b−1 · g."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, Cramer, det.",
    "response": "By Cramer’s rule (Lemma 2.10), there is a solution ehi whose coefficients are of the form det Mj/ det N, where Mj, N are submatrices of the augmented matrix [M′| LM (g)]. Let B′ := \u0000B+n n \u0001 , that is, B′ is the number of monomials in the variables x1, . . . , xn of degree at most B. By our upper bound on B, we have B′ ⩽(4d)4n3. Note that B′ is an upper bound on the number of rows of the matrix [M′| LM (g)]. Thus, by Corollary 3.7, we have ht (det Mj) , ht (det N) ⩽e2 · h · (4d)9n3."
  },
  {
    "prompt": "Scientific context on det, ehi, Multiplying and their role in graph theory.",
    "response": "Multiplying the equation P fi ehi = 1 b · g throughout by det N, we get that det N b · g = (det N) · P fi ehi ∈ Z[α][x1, x2] \\ {0}. Let hi := det(N) · ehi. As the coefficients of hi are given by the minors det(Mj), we have ht (hi) ⩽maxj ht (det(Mj)) ⩽e2 · h · (4d)9n3. Since det N b · g ∈Z[α][x1, x2], and by the definition of g, we must have a := det N b ∈Z[α] \\ {0}. In particular, this implies b | det N and therefore we have ht (a) , ht (b) ⩽ht (det N) ⩽e2 · h · (4d)9n3."
  },
  {
    "prompt": "Scientific context on lemma, dim, radical and their role in graph theory.",
    "response": "As ht \u0000b−1 · g \u0001 ⩽e2 · h · (4d)9n3 and b ∈Z, we have that ht (g) ⩽2 · e2 · h · (4d)9n3. We will need the following extension of the above lemma, where we drop the assumption that I is radical. Lemma 3.14. Suppose dim I = 1. Define π : An →A2 to be projection to the first two coordinates. If dim π(V) = 1 and π(V) is equidimensional, then π(V) = Z(g) for some g ∈Z [α] [x1, x2]. Moreover, there exist an absolute constant c > 0, t ∈N, a ∈Z [α], and h1, . . ."
  },
  {
    "prompt": "Explain a concept involving fihi, deg, dcnc in computational complexity.",
    "response": ", hm ∈A such that agt = P fihi, with t ⩽4d2n, deg (fihi) ⩽5d3n, and ht (a) , ht (hi) ⩽h · ec · dcnc. 22 Proof. Since π(V) is equidimensional and dim π(V) = 1, we have that I(π(V)) = (ˆg) for some ˆg ∈Q[x1, x2]. Let G be the reduced Gr¨obner basis of I with respect to the lexicographic order induced by the variable order xn ≻xn−1 ≻· · · ≻x1. Since I is generated in A, we have G ⊂Q [α] [x1, . . . , xn]. Thus, J := I∩Q [x1, x2] is generated by the elements of G ∩Q [x1, x2]."
  },
  {
    "prompt": "Explain a concept involving rad, theorem, Chapter in computational complexity.",
    "response": "Since rad (J) = rad (I) ∩Q[x1, x2], by the closure theorem [CLO97, Chapter 3.2, Theorem 3] we have that rad (J) = (ˆg). Now, let J′ := J ∩Q[α] [x1, x2] and let rad (J′) be the radical ideal of J′ in Q[α] [x1, x2]. Since dim J′ = 1, we have that rad (J′) = (g′), for some g′ ∈Q[α] [x1, x2]. Since Q is a perfect field, by [Sta24, Tag 030U], we know that rad (J) = rad (J′) ⊗Q. Thus, we have that rad (J) = (g′)Q [x1, x2]."
  },
  {
    "prompt": "This is a technical discussion about theorem, Letting, rad in theoretical computer science.",
    "response": "Letting g ∈Z [α] [x1, x2] be the polynomial obtained by clearing the common denominator of g′, we have rad (J) = (g) as we wanted. By B´ezout’s theorem (Theorem 2.13) we have deg g ⩽dn. Further, by the effective nullstellensatz (Theorem 2.23) and the Rabinowitsch trick, there exists t ∈N and h′ i ∈Q [α] [x1, . . . , xm] such that gt = P fih′ i. Moreover, we have t ⩽4d2n and deg (fih′ i) ⩽5d3n. We assume that t is the smallest power for which we can write gt with these degree bounds."
  },
  {
    "prompt": "Understanding the computational aspects of system, linear, obtained.",
    "response": "Consider the linear system M := MD(f1, . . . , fm) with D = 5d3n. Let M′ be the system obtained by dropping those rows of M corresponding to monomials that are smaller than LM (g)t = LM (gt). Any solution to M′v = LM (g)t is a monic polynomial in Q [α] [x1, x2] with leading coefficient 1, and leading monomial LM (g)t. Let h′ 1, . . . , h′ m be a solution to this system obtained by applying Cramer’s rule (Lemma 2.10)."
  },
  {
    "prompt": "This is a technical discussion about det, Corollary, coefficient in theoretical computer science.",
    "response": "Thus, there is a submatrix N of h M′ | LM (g)ti such that det N is the common de- nominator of every coefficient of h′ i for i ∈[m]. Moreover, there are submatrices Mj of h M′ | LM (g)ti such that each coefficient det N · h′ i is given by some det Mj. In particular, Corollary 3.7 implies that ht (det N · h′ i) ⩽e · D · (2ehD + D log D + 2 log(eD)) ⩽4e2hD3 ⩽h · e2 · d10n. Let g1 := P fi · (det N · h′ i). Hence, we have g1 ∈A, and Corollary 3.8 implies ht (g1) ⩽h · e4 · d14n."
  },
  {
    "prompt": "Read and understand this section about Corollary, leading, coefficient from a research paper.",
    "response": "Let γ ∈Z [α] be the leading coefficient of g1. Treating γ as an element of Z [z] of degree at most e, we see that γ and q are relatively prime. By Corollary 3.3, we can find an element δ ∈Z [z] with deg(δ) < e, and b := resz (γ, q) ∈N∗such that γδ ≡b mod q. Moreover, Corollary 3.3 implies ht (δ) , ht (b) ⩽h · e4 · d18n. Replacing g1 by δg1, we can assume that the leading coefficient of g1 is b, and ht (g1) ⩽h · e6 · d22n. Since g1 ∈I ⊂rad (I) = (g), we have g | g1 in Q [α] [x1, x2]."
  },
  {
    "prompt": "Scientific context on write, lemma, assume and their role in graph theory.",
    "response": "Let us write g1 = gg2. We use this to deduce a bound on ht (g). Let OL be the ring of integers of Q [α]. Thus, we have Z [α] ⊂OL ⊂Z [α]∆, where ∆:= discz (q). In the ring (OL)b [x1, x2], the polynomial g1 is monic. By Gauss’s lemma Lemma 2.19 we can assume that g, g2 ∈(OL)b [x1, x2]. Further, after rescaling, we can assume that g′, g′ 2 are the monic forms of g, g2 in (OL)b [x1, x2]. By [Len84, pp."
  },
  {
    "prompt": "Scientific context on constant, universal, Multiplying and their role in graph theory.",
    "response": "64-67], there is a universal constant c1 > 0 such that every coefficient of g′, when written as a monic element of Z [α]Db, uses rational numbers of absolute value ⩽2h·ec1·dc1nc1 . Multiplying by ∆b shows that there is a universal constant c2 > 0 such that ht (g) ⩽h · ec2 · dc2nc2. Now that we have control on ht (g), by Cramer’s rule applied to Mv = gt, there are a ∈Z [α] and h1, . . ."
  },
  {
    "prompt": "Read and understand this section about fihi, agt, dcnc from a research paper.",
    "response": ", hm ∈A such that agt = P fihi, with deg fihi ⩽D, and ht (a) , ht (hi) ⩽h · ec · dcnc for some universal constant c > 0. This height bound comes from noticing that, by Cramer’s rule, a and every coefficient of hi is a minor of the matrix [M | gt], and we have upper bounds on ht (M) and ht (gt). 3.4 Absolutely irreducible factors of bivariate polynomials In [Kal95], Kaltofen gives an algorithm to factor bivariate polynomials with coefficients in a field F over the algebraic closure F."
  },
  {
    "prompt": "Understanding the computational aspects of algorithm, algorithms, usual.",
    "response": "The algorithm uses the usual template of factoring algorithms: given f(x, y), the algorithm begins by picking a root α ∈F of f(x, 0) in F, which is the same as the linear factor x −α. This factorisation is then lifted, using Newton iteration, modulo yt for increasing powers of t. After some steps of lifting, a reconstruction step is performed and a factor f1 of f, over F, is obtained."
  },
  {
    "prompt": "Scientific context on Newton, factor, coefficients and their role in graph theory.",
    "response": "A key observation made in [Kal95] is that the lifting and reconstruction steps only use arithmetic in the field F, even though the factor (x −α) that the Newton iteration begins with has coefficients in F. In particular this implies that the factor f1 has coefficients in F (α). Since the minimal polynomial of α is a factor of f(x, 0), the result can be interpreted as the fact that factors of f can be found in low complexity extensions of F."
  },
  {
    "prompt": "Scientific context on irreducible, factor, complexity and their role in graph theory.",
    "response": "Further, every irreducible factor of f can be obtained this way, therefore every irreducible factor of f lies in some low complexity extension of F. While each factor lies in a low complexity extension of F, since the number of factors of f can be large, the compositum of all these extensions might be very large. 23 This is a common phenomenon in computational algebraic geometry, see [BM93, Section 4]."
  },
  {
    "prompt": "Understanding the computational aspects of principle, required, extensions.",
    "response": "Following the principle, we will only factor as much as is required, in order to ensure that we can work with low complexity extensions. The next lemmas are from [Kal95], and they formalise the discussion in the previous two paragraphs. Lemma 3.15. Given polynomial f ∈F [x, y] that monic in x, such that deg f = d, and such that the resultant resx (f(x, 0), ∂f(x, 0)/∂x) ̸= 0, there exists an algorithm that returns a list of absolutely irreducible factors f1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of factors, root, algorithm.",
    "response": ", fr of f, with each fi ∈F(αi) [x, y], where αi is a root of f(x, 0). While the result gives an actual algorithm that computes the factors, we are just interested in the fact that every factor of f is of this form. The list of factors returned by the algorithm is not necessarily distinct, and computing the set of distinct factors is a non-trivial task depending on the model used to represent the extensions F(αi). We now state bounds on the logarithmic heights of the factors."
  },
  {
    "prompt": "Scientific context on Corollary, Theorem, setting and their role in graph theory.",
    "response": "The following is [Kal95, Corollary 1, Theorem 3], although we state a simplified statement specialised to our setting. Lemma 3.16. Suppose f ∈Z [x, y] is a squarefree polynomial with deg(f) = d and ht (f) = h that is monic in x, and such that f(x, 0) is squarefree. Suppose f1 is an absolutely irreducible factor of f."
  },
  {
    "prompt": "Scientific context on hdc, exists, irreducible and their role in graph theory.",
    "response": "There exists g(x) an irreducible (in Z [x]) factor of f(x, 0), and polynomials ef1, f2, h ∈Z [x, y, z] and integers ∆1, ∆2 with ht (∆1) , ht (∆2) , ht \u0010 ef1 \u0011 = O(hdc) such that ∆1∆2f = ef1f2 + g(z)h. Further, there exists a root α of g such that f1(x, y) = ef1(x, y, α). Proof. The algorithm of Lemma 3.15 shows the existence of g, α, and also monic polynomials bf1, bf2 ∈ Q [x, y, z] such that f = bf1bf2 + g(z)h for some h ∈Q [x, y, z]."
  },
  {
    "prompt": "This is a technical discussion about Theorem, Corollary, coefficients in theoretical computer science.",
    "response": "By [Kal95, Corollary 1], the coefficients of bf1 are the solutions of a certain linear system, and by [Kal95, Theorem 3] the minors of this system have logarithmic height O (h · dc1). Using Corollary 3.2 to invert the denominators, we can write the coefficients of bf1 as elements of (1/∆1)Z [z], where ht (∆1) = O (h · dc2). Further, the coefficients themselves also have logarithmic height O (h · dc2)."
  },
  {
    "prompt": "Explain a concept involving discg, integers, similar in computational complexity.",
    "response": "No similar bounds are provided on the logarithmic height of bf2, however since f, bf1, bf2 are monic we can use the fact that the coefficients of bf2 are algebraic integers, in particular they are contained in (1/∆2)Z [z] where ∆2 = discg (z). Writing bf2 in this form and clearing denominators from the equation f = bf1bf2 + g(z)h gives us the required equation. Recall the notation of the number theory preliminaries."
  },
  {
    "prompt": "Read and understand this section about Theorem, absolutely, irreducible from a research paper.",
    "response": "As a corollary of the factoring algorithm, [Kal95, Theorem 8] obtains a bound on primes p for which an absolutely irreducible polynomial remains absolutely irreducible after applying the map Z →OF/p for some prime p above p. Theorem 3.17. Let A = Z [α] [x1, . . . , xm] for an algebraic integer α with minimal polynomial q satisfying deg q = e and ht (q) ⩽h. Suppose f ∈A is absolutely irreducible with ht (f) ⩽h and deg f ⩽d."
  },
  {
    "prompt": "Explain a concept involving log, prime, integer in computational complexity.",
    "response": "There exists an integer ∆with ht (∆) ⩽c \u0000ed6 log (h) + ed6n log (d) \u0001 for a universal constant c, such that for any prime p ̸ |∆and any prime p above p, the image of f under the map A →OF/p remains absolutely irreducible. 4 Geometric Irreducibility and base change In this section, we prove our technical base change results. We first show that certain mod p reductions pre- serve dimensions of zerosets Lemma 4.1."
  },
  {
    "prompt": "Understanding the computational aspects of dimension, base, varieties.",
    "response": "We then show that irreducibility of dimension 0 and dimension 1 varieties is also preserved under such base changes. Finally, we show that reducible zerosets of dimension 0 and 1 remain reducible under base changes, and further, for sufficiently many primes, there are irreducible components of the zeroset after base change that are definable over Fp itself. This last fact will be crucial when we apply the Lang-Weil bounds Theorem 2.20 in a subsequent section."
  },
  {
    "prompt": "Read and understand this section about dimension, Theorem, Corollary from a research paper.",
    "response": "We split the study of irre- ducibility and reducibility into two parts, based on the dimension. The dimension 0 case (Corollary 4.3 and Lemma 4.4) is relatively straightforward, and uses ideas from [Koi96] The dimension 1 case (Theorem 4.5 and Theorem 4.6) is significantly more involved. We show Corollary 4.3, Lemma 4.4, and Theorem 4.6 for zerosets that are defined over Z."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Lemma, section.",
    "response": "Our proof of Theorem 4.6 invokes Lemma 4.1 and Theorem 4.5 for zerosets that are defined not just over Z, but also 24 over small algebraic extensions of Q. Therefore, we prove Lemma 4.1 and Theorem 4.5 in this more general setting. With all of the above in mind, we now set up the notation for this section. We recall the preliminaries discussed in Section 2, especially the number theory preliminaries in Section 2.6."
  },
  {
    "prompt": "Read and understand this section about monic, root, irreducible from a research paper.",
    "response": "Let q ∈Z [z] be a monic irreducible polynomial with deg q = e and ht (q) ⩽h, and let α be a root of q. Let F := Q [α], and let OF be the ring of integers of F, we have Z [α] ⊂OF. Recall that for each prime p above p with p ̸ | discz (q), we have a quotient map Z [α] →OF/p ∼= Fp [z] /q1, where q1 is an irreducible factor of q (mod p). We define A := Z [α] [x1, . . . , xn]. Suppose f1, . . . , fm ∈A are polynomials with deg fi ⩽d with d ⩾3 and ht (fi) ⩽h. Let I := (f1, . . . , fm) and V := Z(I)."
  },
  {
    "prompt": "Read and understand this section about prime, discz, factor from a research paper.",
    "response": "For a prime p ∈N such that p ̸ | discz (q), and for any prime p ⊂OF that lies above p corresponding to a factor q1 of q (mod p), let Ip be the ideal of (Fp [z] /q1) [x1, . . . , xn] generated by the images of f1, . . . , fm under the map Z [α] →OF/p described above. Let Vp denote the algebraic subset of Ip in Fp [x1, . . . , xn]. Define σ := dm + 2. Lemma 4.1."
  },
  {
    "prompt": "Scientific context on dim, integer, primes and their role in graph theory.",
    "response": "There exists a universal constant c and an integer ∆with ht (∆) ⩽h · ec · 2(n log σ)c such that for any prime p ̸ |∆and for every p above p we have dim(V) = dim(Vp). Proof. Let r be the dimension of V. We first control the primes p for which dim Vp < r, and then control the primes p for which dim Vp > r. By Lemma 2.11, there is a subset of r variables such that I does not contain any polynomial supported on these r variables. Without loss of generality, suppose these variables are x1, . . ."
  },
  {
    "prompt": "Explain a concept involving Set, fihi, system in computational complexity.",
    "response": ", xr. Set D := 4d3n, and Dr := rD. Set Db := Dr + 5d3n. Let g := xD 1 xD 2 · · · xD r , so deg g = Dr. Let M := MDb(f1, . . . , fm) be the linear system corresponding to membership in I as defined in Section 2. Let M′ be the system obtained by dropping all the rows of M corresponding to monomials in x1, . . . , xr that are smaller than g in the graded lexicographic order. If h1, . . . , hm is a solution to M′v = g, then P fihi ∈Q [x1, . . . , xr], with LM (P fihi) = g."
  },
  {
    "prompt": "Explain a concept involving M′v, variables, rank in computational complexity.",
    "response": "The system M′v = g is unsatisfiable since I does not contain any polynomial supported on these r variables. Therefore rank M′ < rank [M′|g]. Let δ0 ∈Z [α] be any nonzero minor of [M′|g] of size rank [M′|g], the size of the submatrix corresponding to δ0 is at most 1 + \u0000Db+Dr+n n \u0001 . By Corollary 3.2 and Lemma 3.6 we can deduce that ht (δ0) ⩽h · ec1 · 2(n log σ)c1. Observe that δ0 and q are coprime as elements of Z [z], since q is irreducible."
  },
  {
    "prompt": "Read and understand this section about Corollary, discz, resz from a research paper.",
    "response": "By Corollary 3.3 we can deduce that ∆0 := discz (q) · resz (q, δ0) satisfies ht (∆0) ⩽h · ec2 · 2(n log σ)c2. We show that for every p ̸ |∆0, and for every p above p we have dim Vp ⩾r. Let Jp := IpFp [x1, . . . , xn]. Suppose towards contradiction that Vp has dimension less than r. By Lemma 2.11, the ideal of Vp, that is rad (Jp), contains a polynomial supported on x1, . . . , xr."
  },
  {
    "prompt": "Scientific context on Theorem, degree, bounded and their role in graph theory.",
    "response": "By Theorem 2.13, the degree of Vp is bounded by dn, therefore the degree the projection of Vp to the coordinates x1, . . . , xr is bounded by dn. By [Sch07, Corollary 1.9], the ideal rad (Jp) contains a polynomial f in the variables x1, . . . , xr of degree at most dn. By Theorem 2.23 and the Rabinowitsch trick, there exist e ⩽4d2n and gi with deg gi ⩽5d3n such that fe = P figi. The degree of fe is at most D = 4d3n."
  },
  {
    "prompt": "Scientific context on monomial, matrix, potentially and their role in graph theory.",
    "response": "By potentially multiplying the equation throughout by a monomial, we can assume that there is a polynomial f′ with leading monomial xD 1 xD 2 · xD r such that f′ = P fig′ i, where each g′ i has degree at most 5d3n + rD, which is exactly Db. Observe that the matrix of the linear system corresponding to membership in Ip is exactly the matrix M, where we apply the map Z [α] →Fp [z] /q1 to each coefficient. Denote this by matrix by Mp."
  },
  {
    "prompt": "This is a technical discussion about similarly, drop, rows in theoretical computer science.",
    "response": "If we similarly drop the rows of Mp corresponding to monomials in x1, . . . , xr that are larger than g in the graded lexicographic order, the matrix obtained is exactly the reduction of M′ under the same map, denote this matrix by M′ p. We have rank M′ p ⩽rank M′, since any zero minor of M′ remains zero after reduction. Since p ̸ |∆0, in particular p ̸ | resz (q, δ0). Therefore, the polynomials δ0 and q remain relatively prime in Fp [z]."
  },
  {
    "prompt": "This is a technical discussion about rank, minor, remains in theoretical computer science.",
    "response": "In particular, the minor δ0 remains nonzero under the map Z [α] →Fp [z] /q1. The augmented matrix \u0002 M′ p|g \u0003 satisfies rank \u0002 M′ p|g \u0003 = rank [Mp|g] = rank M′ + 1. In particular this implies that M′ pv = g is unsolvable, which is in contradiction to the existence of f′. This completes the first part of the proof, that is controlling the primes p for which dim Vp < r. We now move on to the second part of the proof, that is, controlling the primes p for which dim Vp > r."
  },
  {
    "prompt": "Explain a concept involving variables, Similar, proof in computational complexity.",
    "response": "Similar to the proof above, since V has dimension r, for every subset of r + 1 variables, say for example x1, . . . , xr+1, the ideal I contains a polynomial f′ in x1, . . . , xr+1 with leading monomial g := xD 1 xD 2 · · · xD r+1. Further, we can write f′ = P figi with deg gi ⩽5d3n + (r + 1)D. Set Db := 5d3n + (r + 1)D. If we 25 consider the linear system MDb, and drop the rows corresponding to monomials in x1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of M′v, Lemma, satisfiable.",
    "response": ", xr+1 that are smaller than xD 1 xD 2 · · · xD r+1 and call this new matrix M′, then M′v = g is now satisfiable. In other words, rank M′ = rank [M′|g]. By Lemma 2.10, there is a solution to this system, where the entries of v have common denominator a minor of [M′|g]. Call this minor δ[r+1], by Corollary 3.2 and Lemma 3.6 we can deduce that ht \u0000δ[r+1] \u0001 ⩽h · ec3 · 2(n log σ)c3."
  },
  {
    "prompt": "Understanding the computational aspects of Set, resz, discq.",
    "response": "Set ∆[r+1] := resz \u0000q, δ[r+1] \u0001 , for any p ̸ |∆[r+1] and p ̸ | discq (z), the image of this denominator is nonzero in Fp [z] /q1, and the ideal Ip contains a polynomial in x1, . . . , xr+1. Similarly, we can construct δU, ∆U for every U ⊂[n] with |U| = r + 1. If we let ∆1 := discq (z) Q ∆U, then for every prime p ̸ |∆1 we must have dim Vp ⩽r. We have ht (∆1) ⩽h · ec4 · 2(n log σ)c4. This controls all primes for which dim Vp > r. Defining ∆:= ∆1 · ∆2 completes the proof."
  },
  {
    "prompt": "This is a technical discussion about Dimension, Note, dim in theoretical computer science.",
    "response": "4.1 Dimension zero Note that when dim I = 0, the zeroset V(I) is a finite set of points. In this subsection, we assume that f1, . . . , fm ∈R. We first show that if V consists of a single point, then the logarithmic height of this point is not too large. Lemma 4.2. Suppose V = {(α1, . . . , αn)}. Then αi ∈Q and there is a universal constant c > 0 such that ht (αi) ⩽ h · 2(n log σ)c for all i ∈[n]. Proof. Let K be the smallest Galois extension of Q that contains α1, . . . , αn."
  },
  {
    "prompt": "Explain a concept involving Suppose, assume, loss in computational complexity.",
    "response": "Suppose K ̸= Q, and without loss of generality assume α1 ̸∈Q. Then, there exists some element τ of the Galois group of K/Q that is not identity on α1. Since τ fixes fi, for i ∈[m], as fi ∈R, we must have (τ(α1), · · · , τ(αn)) ∈V, contradicting the fact that V = {(α1, . . . , αn)}. We now obtain the bound on ht (αi). By Theorem 3.10, there is a universal constant c1 > 0 and a polynomial Pi with deg(Pi) ⩽2(n log σ)c1, and ht (Pi) ⩽h · 2(n log σ)c1 such that αi is a root of Pi."
  },
  {
    "prompt": "Explain a concept involving Lemma, universal, constant in computational complexity.",
    "response": "By Lemma 3.4, there is a universal constant c > 0 such that ht (αi) ⩽h · 2(n log σ)c. With the above lemma at hand, we now prove that if |V| = 1, then for all but finitely many primes p, we must have that Vp consists of exactly one point, and this point lies in Fn p. Corollary 4.3. Suppose V = {(α1, . . . , αn)}. There is a universal constant c and ∆∈Z with ht (∆) ⩽h · 2(n log σ)c, such that for any prime p with ∆̸∈(p), Vp ⊂Fn p and |Vp| = 1. Proof."
  },
  {
    "prompt": "This is a technical discussion about Lemma, log, hij in theoretical computer science.",
    "response": "By Lemma 4.2, there is a universal constant c1 such that ht (αi) ⩽h · 2(n log σ)c1, for all i ∈[n]. Let αi = βi/γi with βi, γi ∈Z in reduced form, and let gi := γixi −βi. By the Nullstellensatz gi ∈I for all i ∈[n]. By Corollary 3.12, there are: a universal constant c2 > 0, positive integers ai, ei and polynomials hij ∈R such that aigei i = P j fjhij, where ei ⩽4(n + 1)(d + 1)n+1, deg(hij) ⩽8(n + 1)(d + 1)n+2 and lastly we have ht (ai) , ht (hij) ⩽h · 2(n log σ)c1 (d + 1)n (nd log m)c2."
  },
  {
    "prompt": "Understanding the computational aspects of Define, aiγi, log.",
    "response": "Define ∆:= Q i aiγi. For any prime p such that ∆̸∈(p), the equation aigei i = P j fjhij reduced mod p implies that gi ∈rad (Ip) over Fp[x1, . . . , xn]. Therefore, Vp = { \u0000β1γ−1 1 , . . . , βnγ−1 n \u0001 }. From the bounds on ht (ai) , ht (γi) we have ht (∆) ⩽(n + 1) · h · 2(n log σ)c1 (d + 1)n (nd log m)c2, which can be bounded by h · 2(n log σ)c for an appropriate universal constant c > c2."
  },
  {
    "prompt": "Read and understand this section about log, show, density from a research paper.",
    "response": "We now show that if |V| ⩾2, then the density of primes p such that the set Vp has at least two points in Fn p is large enough. Our argument is an extension of the main argument of [Koi96]. Lemma 4.4. Suppose |V| ⩾2. There exist: a universal constant c, a polynomial G ∈Z [z] with deg(G) ⩽2(n log σ)c and ht (G) ⩽h · 2(n log σ)c, and ∆∈Z with ht (∆) ⩽h · 2(n log σ)c, such that for any prime p satisfying ∆̸∈(p) and G mod p has a root in Fp, we have |Vp ∩Fn p| ⩾2. Proof. Let (α1, . . ."
  },
  {
    "prompt": "This is a technical discussion about Pij, Lemma, root in theoretical computer science.",
    "response": ", αn) and (β1, . . . , βn) be two points in V. By [Koi96, Lemma 2], there are a universal constant c1 > 0 and polynomials Pij ∈Z[x1, . . . , xn], where i ∈[2], j ∈[n], with deg(Pij) ⩽2(n log σ)c1 and ht (Pij) ⩽h · 2(n log σ)c1 such that αj is a root of P1j and βj is a root of P2j. 9 9Note than while Theorem 7 in [Koi96] is stated in terms of a single root, Lemma 2 of [Koi96] holds for every root. 26 Let c2 ⩾1 be the universal constant from Theorem 3.9."
  },
  {
    "prompt": "Explain a concept involving Theorem, log, Qij in computational complexity.",
    "response": "Applying Theorem 3.9 to the 2n elements α1, . . . , αn, β1, . . . , βn with the polynomials Pij gives us a primitive element γ, with minimal polynomial G, where deg(G) ⩽2(n log σ)c2 and ht (G) ⩽h · 2(n log σ)c2. Further, we also obtain Qij ∈Z [z] with deg(Qij) < deg(G) and ht (Qij) ⩽h · 2(n log σ)c2, and aij ∈Z∗with log(|aij|) ⩽h · 2(n log σ)c2 such that αj = Q1j(γ)/a1j and βj = Q2j(γ)/a2j. Let p be a prime such that G mod p has a root x0 in Fp and that Q i,j aij ̸∈(p)."
  },
  {
    "prompt": "This is a technical discussion about points, proof, coordinate in theoretical computer science.",
    "response": "Then, the points ((Q11(x0) /a11, . . . , Q1n(x0)/a1n) and ((Q21(x0) /a21, . . . , Q2n(x0)/a2n) are in Vp. To complete the proof, it suffices to ensure that these two points differ on at least one coordinate, so we can claim that Vp has at least two distinct points with coefficients in Fp. To this end, assume without loss of generality that α1 ̸= β1, and therefore Q11/a11 ̸= Q21/a21. Consider the resultant resz (a21Q11 −a11Q21, G)."
  },
  {
    "prompt": "Scientific context on deg, log, resz and their role in graph theory.",
    "response": "Since G is irreducible and deg(a21Q11 −a11Q21) ⩽max(deg(Q11), deg(Q21)) < deg(G), we can deduce resz (Q11 −Q21, G) ∈Z∗. Further, ht (G) , ht (a21Q11 −a11Q21) ⩽h · 2(n log σ)c2. Since resz (Q11 −Q21, G) ∈Z is the determinant of a matrix in the coefficients of G, Q11 −Q21, it has logarithmic height at most h · 2(n log σ)c3. Define ∆:= a·resz (Q11 −Q21, G), so ∆has logarithmic height at most h·2(n log σ)c. Suppose p is a prime such that p ̸ |∆and such that G has a root in Fp."
  },
  {
    "prompt": "Explain a concept involving resz, polynomials, remain in computational complexity.",
    "response": "Since p ̸ | resz (Q11 −Q21, G), the polynomials Q11 −Q21 and G remain relatively prime in Fp [z]. Therefore, for any root x0 of G, we must have Q11(x0) ̸= Q21(x0). Combined with the arguments above, this shows that for any such p, the algebraic set Vp has at least two distinct points with coefficients in Fp. 4.2 Dimension one In the following theorem, we assume once again the more general setting where f1, . . . , fm ∈A. Theorem 4.5. Suppose V is an irreducible curve."
  },
  {
    "prompt": "Read and understand this section about integer, irreducible, maps from a research paper.",
    "response": "There exists an integer ∆with ht (∆) ⩽h · ec · 2(n log σ)c such that for any prime p ̸ |∆and for every p above p the zeroset Vp is irreducible. Proof. Let D := dn so deg(V) ⩽D by Theorem 2.13. Define B := D2. Let Φ be the set of all linear maps An →A, where each coefficient is picked from the set [B]. The total number of such linear maps is Bn+1. For maps ϕi, ϕj ∈Φ, define ϕij : An →A2 to be the map with coordinate functions ϕi, ϕj."
  },
  {
    "prompt": "This is a technical discussion about projective, closure, hyperplane in theoretical computer science.",
    "response": "Let V be the projective closure of V, and let VP := V∩{x0 = 0} be the intersection of V with the hyperplane at infinity. Since deg(V) ⩽D, and since V is defined by polynomials in R, the zeroset VP is finite and consists of at most D points. By [SR94, Theorem 1.15], for any ϕi such that the projective closure of the hyperplane ϕi = 0 does not contain a point in Vp, the map ϕi : V →A1 is a finite map."
  },
  {
    "prompt": "Read and understand this section about ϕij, irreducible, finite from a research paper.",
    "response": "Define Φ′ ⊂Φ to be the set of maps that are finite, the above argument shows that |Φ′| ⩾(1 −D/B) |Φ| = (1 −1/D) |Φ|. Consider the pairs i, j such that ϕi ∈Φ′, ϕj ∈Φ′. Since ϕi is a finite map, in particular it is surjective. Since ϕi is a projection of ϕij, we can deduce that ϕij(V) has dimension 1. Further, ϕij(V) is irreducible since V is irreducible. For all such ϕij, define gij to be the generator of I(ϕij(V)). Each gij is a form of degree at most D, and is absolutely irreducible."
  },
  {
    "prompt": "Scientific context on prime, ideal, satisfies and their role in graph theory.",
    "response": "The ideal I is prime, and satisfies dim I = 1. For every i, j we can pick an invertible linear map An →An such that ϕij is projection to the first two coordinates after applying the linear map. Therefore, we can apply Lemma 3.14 to deduce that gij ∈A, and we obtain hijk ∈A, and tij ∈N, and aij ∈Z [α] such that aijgtij ij = P k fkhijk. Further, deg hi, t ⩽8dn2, and ht (a) , ht (hi) , ht (gij) ⩽h · log m · ec1 · dc1nc1."
  },
  {
    "prompt": "Explain a concept involving Theorem, Define, discq in computational complexity.",
    "response": "By Theorem 3.17, for each such ij there exists an integer ∆ij with ht (∆ij) ⩽h · ec2 · dcnc2 such that for any p ̸ |∆ij and p ̸ | discq (z), and for any p above p, the image of polynomial gij under the map Z [α] →Fp [z] /q1 remains absolutely irreducible. Define ∆′ := Q i,j resz (aij, q) · ∆ij, where the product is over i, j such that ϕi, ϕj ∈Ψ′. Finally, define ∆:= ∆′ · ∆′′ · (B + 1)!, where ∆′′ is the integer obtained by applying Lemma 4.1 to V."
  },
  {
    "prompt": "Understanding the computational aspects of log, maps, Pick.",
    "response": "We have ht (∆′) ⩽ h · ec2 · dcnc2 · D(2n+2) ⩽h · ec3 · 2(n log σ)c3. Further ht (∆′′) ⩽h · ec4 · 2(n log σ)c4, and ht ((B + 1)!) ⩽dn3. Therefore, ht (∆) ⩽h · ec · 2(n log σ)c. Pick a prime p ̸ |∆and p above p, we will show that Vp is irreducible, which will complete the proof. Since p ̸ |∆′′, Vp has dimension 1. Further, since p ̸ |(S + 1)!, we have p > S + 1. This ensures that the set of maps in Φ are distinct, even when considered as maps from F n p →F 2 p."
  },
  {
    "prompt": "Understanding the computational aspects of Fix, irreducible, ϕij.",
    "response": "Fix any i, j such that ϕi ∈Φ′, ϕj ∈Φ′. The equation aijgtij ij = P k fkhijk holds in Fp [z] /q1, and aij ̸= 0, therefore gtij ij ∈Ip for every such p. Further, since p ̸ |∆ij, the polynomial gij remains absolutely 27 irreducible. These facts together imply that ϕij(Vp) is either empty or irreducible, here we treat ϕij as a map from F n p →F 2 p. The total number of such maps is (1 −1/D)2 |Φ|2. Assume towards a contradiction that Vp is reducible."
  },
  {
    "prompt": "This is a technical discussion about deg, ϕij, components in theoretical computer science.",
    "response": "Let C1, C2 be two components of Vp, we have deg(C1), deg(C2) ⩽D. Define Φp ⊂Φ such that ϕi ∈Φp if ϕi : C1 →F 1 p and ϕi : C2 →F 1 p are both finite maps. Similar to the argument for Φ′, we can deduce that |Φp| ⩾(1 −2/D) |Φ| and we can deduce that for any i, j with ϕi ∈Φp we have dim ϕij(C1) = 1 and dim ϕij(C2) = 1. We bound the number of maps ϕij such that ϕij(C1) = ϕij(C2). Fix i, and pick a point α ∈C1\\C2."
  },
  {
    "prompt": "Understanding the computational aspects of finite, points, set.",
    "response": "Since ϕi : C2 →F 1 p is finite, in particular it is surjective and therefore the curve C2 is not contained in the hyperplane defined by ϕi = ϕi(α). Therefore, C2 ∩{ϕi = ϕi(α)} is a finite set consisting of at most D points, call these points β1, . . . , βD. The set of points α −β1, . . . , α −βD is a finite set of nonzero points, therefore at most (D/S) |Φ| many ϕj are such that ϕj is zero at one of these points."
  },
  {
    "prompt": "Read and understand this section about ϕij, empty, remaining from a research paper.",
    "response": "For every remaining ϕj, the intersection of C2 with the hyperplanes ϕi = ϕi(α) and ϕj = ϕj(α) is empty, whence ϕij(C1) ̸= ϕij(C2). Since the maps ϕij are finite on C1, C2, this implies ϕij(C1) ̸= ϕij(C2). Therefore, the total number of pairs i, j such that ϕi, ϕj ∈Φp and ϕij(C1) ̸= ϕij(C2) is at least (1 −2/D)(1 −3/D) |Φ|2. Recall however that for at least (1 −1/D)2 |Φ|2 maps, ϕij(Vp) was either irreducible or empty, in particular ϕij(C1) = ϕij(C2)."
  },
  {
    "prompt": "Understanding the computational aspects of reducible, proof, Theorem.",
    "response": "This is a contradiction to our assumption that Vp is reducible, and completes the proof. The following result again assumes f1, . . . , fm ∈R. The proof will invoke Lemma 4.1 and Theorem 4.5 in the general setting on a new system of polynomials that extends f1, . . . , fm. We note that this is a limitation of our proof, and we expect the result to also hold if f1, . . . , fm ∈A. Theorem 4.6. Suppose f1, . . . , fm ∈R."
  },
  {
    "prompt": "Understanding the computational aspects of irreducible, components, equidimensional.",
    "response": "Suppose dim I = 1, that V is equidimensional, and that V has at least two irreducible components. There exists an integer ∆with ht (∆) ⩽h · 2(n log σ)c and a polynomial G ∈Z [z] with deg G ⩽D4 and ht (G) ⩽h · 2(n log σ)c such that for any prime p ̸ |∆and such that G (mod p) has a root in Fp, the zeroset Vp has at least two irreducible components that are Fp-definable. Proof. Let k be the number of irreducible components of V, and suppose V = V1 ∪· · · Vk is the irreducible decomposition of B."
  },
  {
    "prompt": "Understanding the computational aspects of theorem, B´ezout’s, components.",
    "response": "By B´ezout’s theorem (Theorem 2.13) we have k ⩽D := dn. As a first step, we will find defining equations for two components of V. 10 The components of V might not be Q-definable, therefore the defining equations we find will potentially lie in some algebraic extension of Q. Pick a linear map ϕ : An →A2, where the coefficients of each coordinate map are picked from the set \u0002 D6\u0003 , such that π|V : V →ϕ(V) is finite, and such that each of the k components of V have distinct images under ϕ."
  },
  {
    "prompt": "Understanding the computational aspects of map, properties, exists.",
    "response": "A random linear map has these properties, and therefore such a map exists. Since dim I = 1 we have dim ϕ(V) = 1. Further, since I is equidimensional, π(V) has exactly k components that are each of codimension 1, therefore ϕ(V) = Z(g1) for some g1. We can apply Lemma 3.14 to deduce that g1 ∈R, and the existence of polynomials h1k ∈R, and a1, t1 ∈Z such that a1gt1 1 = P k fkh1k. Further ht (g1) , ht (h1k) ⩽ h · dc1nc1."
  },
  {
    "prompt": "Explain a concept involving factors, irreducible, absolutely in computational complexity.",
    "response": "Now g1 has exactly k absolutely irreducible factors, g1 = g11 · g12 · · · · · g1k. Each of these factors are coprime, and occur with multiplicity 1. Further, each factor g1i corresponds to the irreducible component Vi. Consider I1 := I + g11 and I2 := I + g12. We have V1 ⊂Z(I1), and V2 ⊂Z(I2). None of the components V2, . . . , Vk vanish on g11. Therefore (V2 ∪· · · ∪Vk) ∩Z(g11) has dimension 0."
  },
  {
    "prompt": "Scientific context on deg, B´ezout’s, consists and their role in graph theory.",
    "response": "By B´ezout’s inequality, and the fact that deg g11 ⩽deg g1 ⩽dn, we deduce that (V2 ∪· · · ∪Vk) ∩Z(g11) consists of at most d2n points. Similarly, (V1 ∪V3 ∪· · · ∪Vk) ∩Z(g12) consists of at most d2n points. In order to define V1 (respectively V2), we need to add equations to I1 (respectively I2) that vanish on V1 (respectively V2) but not on the above points."
  },
  {
    "prompt": "Understanding the computational aspects of map, points, pick.",
    "response": "Towards this, pick a second linear map ψ : An →A2 where the coefficients of each coordinate map are picked from the set \u0002 D6\u0003 , such that ψ|V : V →ψ(V) is finite, and such that each of the k components of V have distinct images under ψ. We further require that none of the d2n points of (V2 ∪· · · ∪Vk) ∩Z(g11) are mapped to ψ(V1), and that none of the d2n points of (V1 ∪V3 ∪· · · ∪Vk) ∩Z(g12) are mapped to ψ(V2). Such a map exists, since a random map satisfies these properties."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, polynomials, existence.",
    "response": "As before, we have g2 such that ψ(V) = Z(g2), and by Lemma 3.14 we have g2 ∈R, and the existence of polynomials h2k ∈R, and a1, t1 ∈Z such that a2gt2 2 = P k fkh2k. Further ht (g2) , ht (h2k) ⩽ h · dc1nc1. We can also factor g2 as g2 = g21 · g22 · · · · · g2k with g2i corresponding to Vi. 10A reviewer pointed out to us that the existence of such equations also follows from [Sch10, Theorem 1.1]. 28 We now set J1 := I1 + g21 = I + g11 + g21 and J2 := I2 + g22 = I + g12 + g22."
  },
  {
    "prompt": "Understanding the computational aspects of construction, ments, linear.",
    "response": "By construction, Z(J1) = V1 and Z(J2) = V2. The next step is to control the heights of g11, g12, g21, g22, in a number field that contains all these ele- ments. To this end, we treat g1, g2 as polynomials in two variables each, say y1, y2. We can assume that the leading coefficients of g1, g2 in y1 belong to Z: if not then this can be achieved by a random linear transformation, and we could have applied this transformation to the linear maps defining g1, g2 them- selves."
  },
  {
    "prompt": "Read and understand this section about adeg, write, leading from a research paper.",
    "response": "If a is the leading coefficient of g1, then we write eg1 := adeg g1−1g(y1/a, y2 + b), for a random b with ht (b) ⩽·2(n log σ)c2. After this transformation, the polynomial eg1 is monic in y1, and eg1(y1, 0) is squarefree. Further, factors of g1, in particular g11, g12 are in bijection with factors of eg1. We now focus on eg11, the factor corresponding to g11 under this bijection."
  },
  {
    "prompt": "Read and understand this section about Lemma, degz, integers from a research paper.",
    "response": "By Lemma 3.16, there is an irreducible factor q11(z) of eg1(z, 0), integers ∆11, ∆′ 11, and polynomials u11, v11, w11 ∈Z [y1, y2, z] such that ∆11∆′ 11eg1(y1, y2) = u11(y1, y2, z)v11(y1, y2, z) + w11(y1, y2, z)q11(z). Further, degz(u11), degz(v11) < e. Finally, there is a root α11 of q11 such that g11 = u11(x, y, α11), potentially after scaling. By Gelfond’s inequality [HS13, B.7.3] we have ht (q11) ⩽h · 2(n log σ)c3, and we also have deg q11 ⩽deg g1 = D."
  },
  {
    "prompt": "Explain a concept involving Lemma, qij, αij in computational complexity.",
    "response": "We also have ht (∆11) , ht (∆′ 11) , ht (u11) ⩽h · 2(n log σ)c3 by Lemma 3.16. We similarly construct qij, αij, uij, vij, wij, ∆ij, ∆′ ij. We now construct an extension of Q that contains every αij. To this end, we construct a primitive element of α11, α12, α21, α22. Using Theorem 3.9 we can deduce the existence of a polynomial G with deg G ⩽D4 and ht (G) ⩽h·2(n log σ)c4, and a root γ of q, along with Qij, r such that αij = Qij(γ)/r. We also have ht (Qij, r) ⩽h · 2(n log σ)c4."
  },
  {
    "prompt": "Explain a concept involving equation, multiply, obtain in computational complexity.",
    "response": "In equation ∆11∆′ 11eg1(y1, y2) = u11(y1, y2, z)v11(y1, y2, z) + w11(y1, y2, z)q11(z) we substitute z = Q11/r, multiply throughout by rD2, and reduce the coefficients mod G to obtain ∆11∆′ 11rD2eg1(y1, y2) = eu11(y1, y2, z)ev11(y1, y2, z) + ew11(y1, y2, z)G(z). We now have eu(y1, y2, γ) = eg11, potentially after scaling. Up to scaling by a, the polynomial g11 is exactly eu(ay1, y2 + b, γ). This shows that ht (g11) ⩽h · 2(n log σ)c5."
  },
  {
    "prompt": "This is a technical discussion about gij, factor, bounds in theoretical computer science.",
    "response": "Therefore, we have obtained the height bounds on g11 that we were looking for, and the same bound holds for gij. Recall that J1, J2 correspond to different irreducible components of I, and that J1 + J2 has dimension 0. Pick any prime p ̸ | discz (G) such that G (mod p) has a linear factor. Let p be a prime of the ring of integers of Q [z] /G(z) that corresponds to the linear factor. The quotient map Z [γ] →Z [γ] /p has image in Fp."
  },
  {
    "prompt": "This is a technical discussion about Lemma, Theorem, ideals in theoretical computer science.",
    "response": "We now apply Lemma 4.1 and Theorem 4.5 to the ideals J1, J2, and J1 +J2. We deduce that there is an integer ∆0 such that for any prime p ̸ |∆0, the images of J1, J2 remain irreducible, and the dimensions of I, J1, J2, J1 + J2 are one, one, one, zero respectively. We also have Z((J1)p) ⊂Z(Ip). Therefore, Z((J1)p) is an Fp-definable irreducible component of Z(Ip). The same holds for Z((J2)p). Since dim(J1+J2)p = 0, these two components are different from each other."
  },
  {
    "prompt": "Scientific context on Interactive, section, testing and their role in graph theory.",
    "response": "5 Interactive proofs of primality testing In this section, we prove our main theorem: interactive protocols for testing non-primality of natural classes of ideals. As we have discussed in Section 1, we first give some protocols for certain special cases, as the protocol in the main theorem will invoke these special cases as subroutines."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, theorem, size.",
    "response": "The setting of this section is that of our main theorem: we assume that we have an algebraic circuit C′ of size 5sm with integer coefficients that computes polynomials f1, . . . , fm ∈Z [x1, . . . , xm], and the partial derivatives ∂ifj. The existence of C′ is guaranteed by Lemma 2.9 applied to the input circuit C. By Lemma 2.8 also applied to the input circuit C, we have ht (fi) ⩽22s and deg fi ⩽2s. Define d := max deg fi. Define σ := dm + 2. Let I := (f1, . . ."
  },
  {
    "prompt": "This is a technical discussion about algebraic, denote, set in theoretical computer science.",
    "response": ", fm), and let V := Z(I) denote the algebraic set corresponding to I. For any prime p, define Ip to be the ideal in Fp [x1, . . . , xn] generated by the images of f1, . . . , fm under the quotient map R →Fp [x1, . . . , xn], and let Vp denote the algebraic set of Ip in Fp [x1, . . . , xn]. We begin with a protocol to decide if a zero dimensional ideal has at least two points in its zeroset. 29 Lemma 5.1. Assume GRH."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, Πred, Corollary.",
    "response": "If I is promised to be zero dimensional, then there is an AM protocol that verifies |V| ⩾2. Proof. The proof uses Corollary 4.3, Lemma 4.4 and the Goldwasser-Sipser protocol (Lemma 2.1). Let h := maxi ht (fi), by Lemma 2.8 we have h ⩽22s. Define Πred(x) to be the set of primes p ⩽x such that Vp has at least two points in Fp, define πred(x) := |Πred(x)|. By Corollary 4.3, if |V| = 1 then πred(x) ⩽h · 2(n log σ)c1 for all x. Suppose |V| ⩾2. Let G be the polynomial guaranteed by Lemma 4.4."
  },
  {
    "prompt": "Understanding the computational aspects of log, theorem, Gxdeg.",
    "response": "By the effective Chebotarev density theorem (Theorem 2.21) we have πG(x) ⩾ 1 deg G \u0010 π(x) −log ∆G −c2x1/2 log \u0010 ∆Gxdeg(G)\u0011\u0011 , where ∆G is the discriminant of G, and πG(x) is the number of primes p ⩽x such that G has a root in Fp. By Lemma 4.4 deg G ⩽2(n log σ)c3, and ht (G) ⩽h · 2(n log σ)c4. Bounding the discriminant using Corollary 3.3, we have πG(x) ⩾ π(x) 2(n log σ)c3 −c5hx1/22(n log σ)c6 −c7x1/2 log x. By Lemma 4.4 we have πred(x) ⩾πG(x) −h · 2(n log σ)c8."
  },
  {
    "prompt": "Scientific context on Theorem, Finally, log and their role in graph theory.",
    "response": "Finally, by Theorem 2.22 we have π(x) > x loge x for all x larger than 17. For x0 = hc9 · 2(n log σ)c10 for a large enough constants c9, c10, we see that πred(x0) ⩾ 2h · 2(n log σ)c1. Note that ht (x0) is polynomial in the input. We can now invoke the Goldwasser-Sipser protocol (Lemma 2.1) on the set Πred(x0), with K = h · 2(n log σ)c1."
  },
  {
    "prompt": "Scientific context on Membership, roots, πred and their role in graph theory.",
    "response": "Membership in this set is in NP: the certificate for membership are two distinct roots, which has polynomial bit complexity, and we can evaluate the input polynomials on these roots to verify them. As shown above, |V| ⩾2 if and only if πred(x0) ⩾2K and |V| = 1 if and only if πred(x0) ⩽K. Therefore, the protocol correctly verifies |V| ⩾2. The second special case is a protocol that decides if a zeroset has at least two components of highest dimension. Lemma 5.2. Assume GRH."
  },
  {
    "prompt": "Scientific context on dimension, promised, equidimensional and their role in graph theory.",
    "response": "If I is promised to be equidimensional and have dimension r, then there is an AM protocol that verifies that V has at least two irreducible components of dimension r. Proof. Let D := dn. Apply the algorithm of Corollary 2.28 in order to obtain a linear subspace H. The space H is defined by linear equations h1, . . . , hr−1, with ht (hi) ⩽(nD)c1. Let J := I + (h1, . . . , hr−1), and let W := Z(J)."
  },
  {
    "prompt": "Scientific context on number, irreducible, components and their role in graph theory.",
    "response": "By Corollary 2.28, with high probability we have dim(W) = 1, and the number of irreducible components of dimension 1 of W is the same as the number of irreducible components of dimension r in V. For the rest of the proof, we assume that this event occurs, this will only change the soundness and completeness of our protocol by a small constant, which does not change the complexity class AM. Let Jp, Wp be defined the same way as Ip, Vp."
  },
  {
    "prompt": "Read and understand this section about Πred, max, maxj from a research paper.",
    "response": "Let h := max (maxj ht (hj) , maxi ht (fi)), we have h ⩽(nD)c1 + 22s. Let Πred(x) be the set of primes (150)2D8 < p < x such that Wp ∩Fn p ⩾2p −10D4p1/2, and let πred(x) := |Πred(x)|. By an effective Lang- Weil bound (Theorem 2.20), p ∈Πred(x) if and only if (150)2D8 < p < x and Wp has at least two irreducible components of dimension 1 defined over Fp. Suppose W has only one irreducible component of dimension 1 (this component is then automatically defined over Fp)."
  },
  {
    "prompt": "Understanding the computational aspects of log, Theorem, irreducible.",
    "response": "By Theorem 4.5, for all x we have πred(x) ⩽h · 2(n log σ)c, since for all but h · 2(n log σ)c primes Wp has at most one irreducible component defined over Fp. On the other hand, suppose W has at least two irreducible components of dimension 1. Let G be the polynomial guaranteed by Theorem 4.6. By the effective Chebotarv density theorem Theorem 2.21 we have πG(x) ⩾ π(x) 2(n log σ)c2 −c3hx1/22(n log σ)c4 −c5x1/2 log x."
  },
  {
    "prompt": "Understanding the computational aspects of log, Πred, Theorem.",
    "response": "By Theorem 4.6, for all but h · 2(n log σ)c6 primes in πG(x), the zeroset Wp has at least two components, therefore every such prime larger than (150)2D8 is in Πred(x). For x0 = hc7 · 2(n log σ)c8 for a large enough constants c7, c8, we see that πred(x0) ⩾2h · 2(n log σ)c. We can now invoke the extension of the Goldwasser-Sipser protocol (Corollary 2.2) on the set Πred(x0), with K = h · 2(n log σ)c."
  },
  {
    "prompt": "Understanding the computational aspects of Membership, note, dimension.",
    "response": "Membership in this set is itself in AM: note that Wp ∩Fn p is either at most p + 30 5D4p1/2 + D or at least 2p −10D4p1/2 −D. Since p > (150)2D8 the ratio of these sizes is at least 1.9. If the protocol accepts, then with high probability W, and therefore V has at least two irreducible components of top dimension. If not, then with high probability V has only one component of top dimension. Remark 5.3. We give an example that shows that Theorem 4.6 is tight in some sense."
  },
  {
    "prompt": "Understanding the computational aspects of theorem, Chebotarev, density.",
    "response": "The theorem, combined with the effective Chebotarev density theorem Theorem 2.21 shows that the density of primes for which Fp has two Fp definable components is inverse exponential. We give an example to show that this inverse exponential behaviour is unavoidable. Consider I = \u0000x2 1 −2, x2 2 −3, . . . , x2 n−1 −pn−1 \u0001 , where pj is the jth prime. The zeroset V consists of 2n−1 components, each of dimension 1."
  },
  {
    "prompt": "This is a technical discussion about components, Similarly, dimension in theoretical computer science.",
    "response": "Similarly, for any prime p > pn−1, the zeroset Vp also consists of 2n−1 components, each of dimension 1. However, these components might not be defined over Fp itself. For any prime p, the components will be defined over the smallest extension of Fp that contains square roots of 2, 3, · · · , pn−1. The only primes p for which there exist Fp definable components of Vp are those where 2, 3, . . . , pn−1 are all quadratic residues."
  },
  {
    "prompt": "Read and understand this section about Interactive, Dirichlet, radical from a research paper.",
    "response": "By quadratic reciprocity and Dirichlet’s theorem, the density of such primes within all primes is 2−n+1. 5.1 Interactive proof for radical ideals We now give an AM protocol for deciding non primality of radical ideals, using the above two protocols as subroutines. Algorithm 3: AM protocol for non-primality of radical ideals Input : A circuit C′ that computes polynomials f1, . . . , fm ∈R along with the derivatives ∂ifj, with deg fi ⩽d such that I := (f1, . . ."
  },
  {
    "prompt": "Understanding the computational aspects of Merlin, ideal, Arthur.",
    "response": ", fm) is a radical ideal, and the dimension r of I. Arthur : If r = 0, then perform the protocol in Lemma 5.1, and accept if and only if the protocol accepts. If r > 0, then send the empty string to Merlin. Merlin : Compute the Jacobian matrix Jij = ∂fi/∂xj. Check if there is a minor M of J of size at least n −r + 1 such that M ̸= 0, f1 = 0, . . . , fm = 0 is a satisfiable system. If such a minor exists, send the rows and columns that correspond to M to Arthur."
  },
  {
    "prompt": "Explain a concept involving Arthur, protocol, Merlin in computational complexity.",
    "response": "If not, then send the empty string to Arthur. Arthur : If Merlin sends the empty string then perform the protocol in Lemma 5.2, and accept if and only if the protocol accepts. If not, then construct an algebraic circuit for 1 −yM, where M is the minor of J corresponding to the rows and columns received from Merlin. Perform the protocol in Theorem 2.4 on the inputs f1, . . . , fm, 1 −yM. Accept if and only if this protocol accepts. Theorem 5.4. Assume GRH."
  },
  {
    "prompt": "Explain a concept involving Algorithm, prime, ideal in computational complexity.",
    "response": "If I is a radical ideal of dimension r then Algorithm 3 is a valid AM protocol for deciding I is not prime. Proof. Suppose I is not prime. Since I is radical, it has at least two irreducible components. If dim I = 0, then |V| ⩾2, and the protocol in Lemma 5.1 correctly accepts with high probability, and therefore Algorithm 3 correctly accepts with high probability in the first step."
  },
  {
    "prompt": "This is a technical discussion about dimension, component, equidimensional in theoretical computer science.",
    "response": "If r > 0, then either V has at least one component of dimension at most r −1, or V is equidimensional and has at least two components of dimension r. If V has a component of dimension less than r, then by Lemma 2.12, there is a point x0 ∈V such that rank J(x0) > n −r. Therefore Merlin can always find a minor of J of size greater than n −r that makes the polynomial system created by Arthur in round 3 satisfiable."
  },
  {
    "prompt": "Scientific context on Observe, maxi, computes and their role in graph theory.",
    "response": "Observe that the system has degree at most nd + 1, and logarithmic height at most h · (dn)c, where h := maxi ht (fi). Further, given the circuit C′ that computes f1, . . . , fm and ∂jfi, Arthur can easily produce a circuit C′′ that also computes 1−yM. In this case, the protocol in Theorem 2.4 correctly accepts with high probability. If V has two components of dimension r, then the protocol in Lemma 5.2 correctly accepts with high probability. Suppose I is a prime ideal."
  },
  {
    "prompt": "This is a technical discussion about Lemma, Algorithm, correctly in theoretical computer science.",
    "response": "If r = 0, then |V| = 1, and the protocol in Lemma 5.1 correctly rejects with high probability, and therefore Algorithm 3 correctly fails to accept with high probability in the first step. If r ⩾0, then V has only one irreducible component."
  },
  {
    "prompt": "This is a technical discussion about round, probability, Theorem in theoretical computer science.",
    "response": "The Jacobian J has rank at most n −r at every point 31 in V, therefore no matter what choice of rows and columns Merlin picks in round 2, the system created by Arthur in round 3 is unsatisfiable, and therefore the protocol in Theorem 2.4 fails to accept with high probability. If Merlin sends the empty string, then the protocol in Lemma 5.2 fails to accept with high probability. Therefore, when I is a prime ideal, the protocol in Algorithm 3 correctly fails to accept with high probability."
  },
  {
    "prompt": "Read and understand this section about Equidimensional, protocol, ideals from a research paper.",
    "response": "The above protocol is in AM[4], the theorem now follows from the fact that AM[4] = AM. 5.2 Equidimensional Cohen-Macaulay ideals We now give an AM protocol for deciding non primality of equidimensional Cohen-Macaulay ideals. Algorithm 4: AM protocol for non-primality of equidimensional CM ideals Input : A circuit C′ that computes polynomials f1, . . . , fm ∈R along with the derivatives ∂ifj, with deg fi ⩽d such that I := (f1, . . ."
  },
  {
    "prompt": "This is a technical discussion about protocol, Lemma, perform in theoretical computer science.",
    "response": ", fm) is an equidimensional Cohen-Macaulay ideal, and the dimension r of I. Arthur : If r = 0, then perform the protocol in Lemma 5.1, and accept if the protocol accepts. If r > 0, then perform the protocol in Lemma 5.2, and accept if the protocol accepts. If the above protocols fail to accept, let Y, Z be (n −r) × m and n × (n −r) symbolic matrices (in new variables), and perform the protocol in Theorem 2.5 on the system f1, . . . , fm, det (YJZ), with parameter (n −r)(m + n) + r."
  },
  {
    "prompt": "Scientific context on Accept, accepts, prime and their role in graph theory.",
    "response": "Accept if and only if this protocol accepts. Theorem 5.5. Assume GRH. If I is a equidimensinal Cohen-Macaulay ideal of dimension r then Algorithm 4 is a valid AM protocol for deciding I is not prime. Proof. Suppose I is not prime. Then either I has at least two components of dimension r, or I has exactly one irreducible component but is not radical."
  },
  {
    "prompt": "Explain a concept involving Lemma, protocol, correctly in computational complexity.",
    "response": "If I has at least two irreducible components of dimension r, then depending on r either the protocol in Lemma 5.1 or the protocol in Lemma 5.2 correctly accepts with high probability. Suppose now that I has a unique minimal prime, call this prime p. Since S is an affine domain of dimension n, by [Eis13, Cor 13.4] we have codim (I) = codim (p) = n−r. Let J be the ideal generated by the (n −r) × (n −r) minors of J."
  },
  {
    "prompt": "Understanding the computational aspects of YJZ, Theorem, det.",
    "response": "Since S/I is Cohen-Macaulay and I is not radical, by Theorem 2.18 we deduce that J has codimension 0 in S/I, equivalently every generator of J lies in p. The polynomial det (YJZ) lies in the ideal J⊗Q [Y, Z], therefore det (YJZ) ∈p⊗Q [Y, Z]. The ideal p⊗Q [Y, Z] has dimension r+(n−r)(m+n), and is the unique minimal prime of I ⊗Q [Y, Z], therefore dim det (YJZ) + I ⊗Q [Y, Z] = r + (n −r)(m + n). Therefore the protocol in Theorem 2.5 correctly accepts with high probability. Now suppose I is prime."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, YJZ, protocol.",
    "response": "Since I has only one irreducible component, depending on r either the protocol in Lemma 5.1 or the protocol in Lemma 5.2 fails to accept with high probability. Since S/I is reduced, Theorem 2.18 implies that J has codimension at least 1 in S/I. Equivalently, there is at least one generator of J that does not lie in the ideal I. Therefore, det (YJZ) ̸∈I ⊗Q [Y, Z], and dim det (YJZ) + I ⊗Q [Y, Z] = r + (n −r)(m + n) −1. The protocol in Theorem 2.5 correctly fails to accept with high probability."
  },
  {
    "prompt": "This is a technical discussion about main, theorems, convenience in theoretical computer science.",
    "response": "5.3 Proof of main theorems We now use the above protocols to prove our main theorems, which we restate here for convenience. Theorem 1.2 (Interactive protocols for primality). Let C be an algebraic circuit of size s with integer constants that computes f1, . . . , fm ∈Z [x1, . . . , xn]. If I := (f1, . . . , fm) is either radical or equidimensional Cohen-Macaulay, and if the dimension of I is given, then the complexity of testing if I is prime lies in coAM, assuming GRH. Proof."
  },
  {
    "prompt": "Explain a concept involving Theorem, complexity, deciding in computational complexity.",
    "response": "Since we know the dimension of I, by Theorem 5.4 and Theorem 5.5 (depending on whether I is rad- ical or equidimensional Cohen-Macaulay), the complexity of deciding if I is not prime lies in AM, therefore the complexity of deciding if I is prime lies in coAM. Theorem 1.3 (Primality testing in PH). Let C be an algebraic circuit of size s with integer constants that computes f1, . . . , fm ∈Z [x1, . . . , xn]. If the ideal I := (f1, . . ."
  },
  {
    "prompt": "Scientific context on GRH, dimension, Theorem and their role in graph theory.",
    "response": ", fm) is either radical or equidimensional Cohen-Macaulay then the complexity of testing if I is prime lies in Σp 3 ∩Πp 3 , assuming GRH. 32 Proof. We first compute the dimension of I. By Theorem 2.5, there exists an AM protocol for checking if dim I ⩾r for any r, and since AM ⊂ΠP 2 , the complexity of computing the dimension of I exactly lies in PΠp 2 ⊂Σp 3 ∩Πp 3 . With the dimension at hand, we can apply Theorem 1.2 and decide whether I is prime in PΠp 2 ."
  },
  {
    "prompt": "This is a technical discussion about Conclusion, ideals, open in theoretical computer science.",
    "response": "Thus, the complexity of deciding if I is prime is in Σp 3 ∩Πp 3 . 6 Conclusion & open problems In this work, we proved that the ideal primality testing problem is in the third level of the polynomial hierarchy for the natural classes of ideals comprised of radical ideals and equidimensional Cohen-Macaulay ideals."
  },
  {
    "prompt": "Explain a concept involving primality, problem, testing in computational complexity.",
    "response": "This significantly tightens the complexity-theoretic gap for the primality testing problem for these classes of ideals, given that the primality testing problem is already coNP-hard for such classes. Prior to our work, the best upper bounds for testing whether a radical ideal or a zero-dimensional ideal is prime was PSPACE, whereas for complete intersections the best upper bound was EXP."
  },
  {
    "prompt": "This is a technical discussion about embedded, issue, primes in theoretical computer science.",
    "response": "• A common nice feature of the two classes of ideals that we studied is that any associated prime must be a minimal prime, and therefore we avoid the issue of having to detect embedded primes. The issue of embedded primes, which causes non-reduced behavior in ”very small” parts of our algebraic set, is in general a very hard problem to identify (and therefore to handle). We leave it as an open question, to detect embedded primes either ”via zeros” or algorithmically better than EXPSPACE."
  },
  {
    "prompt": "Scientific context on field, primes, studied and their role in graph theory.",
    "response": "• We studied the problems over the field of C (or Q). This allows us to go modulo several primes p, leading to the application of arithmetic-geometry results over finite fields (e.g. Lang-Weil points count and Chebotarev primes count). The ideal primality testing problem over an input field Fq, for prime q, rules out the idea of going modulo other primes p ̸= q. This makes it an open question to extend Koiran’s result [Koi96] to input field Fq."
  },
  {
    "prompt": "Explain a concept involving EXPSPACE, Similarly, question in computational complexity.",
    "response": "Similarly, our paper leaves it as an open question, to improve ideal primality testing beyond EXPSPACE, for natural classes of ideals over an algebraically closed field of positive characteristic. (E.g. what about the dimension ⩽1 case here?) References [AB09] Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge Univer- sity Press, 2009 (cit. on p. 11). [AO83] Leonard M Adleman and Andrew M Odlyzko. “Irreducibility testing and factorization of poly- nomials”."
  },
  {
    "prompt": "Read and understand this section about Mathematics, cit, Baj from a research paper.",
    "response": "In: Mathematics of Computation 41.164 (1983), pp. 699–709 (cit. on p. 16). [Baj+93] Chanderjit Bajaj et al. “Factoring rational polynomials over the complex numbers”. In: SIAM Journal on Computing 22.2 (1993), pp. 318–331 (cit. on p. 3). [BC04] Peter B¨urgisser and Felipe Cucker. “Variations by complexity theorists on three themes of Euler, B´ezout, Betti, and Poincar´e”. In: Complexity of computations and proofs 13 (2004), pp. 73–152 (cit. on pp. 3, 4). [BM93] Dave Bayer and David Mumford."
  },
  {
    "prompt": "Read and understand this section about Peter, Scheiblechner, cit from a research paper.",
    "response": "“What can be computed in algebraic geometry?” In: arXiv preprint alg-geom/9304003 (1993) (cit. on pp. 3, 4, 6, 24). [BS07] Peter B¨urgisser and Peter Scheiblechner. “Differential forms in computational algebraic geome- try”. In: Proceedings of the 2007 international symposium on Symbolic and algebraic computation. 2007, pp. 61–68 (cit. on pp. 3, 4). [BS10] Peter B¨urgisser and Peter Scheiblechner. “Counting irreducible components of complex alge- braic varieties”."
  },
  {
    "prompt": "Read and understand this section about cit, computational, complexity from a research paper.",
    "response": "In: computational complexity 19 (2010), pp. 1–35 (cit. on p. 3). [BS83] Walter Baur and Volker Strassen. “The complexity of partial derivatives”. In: Theoretical computer science 22.3 (1983), pp. 317–330 (cit. on p. 13). [B¨ur13] Peter B¨urgisser. Completeness and reduction in algebraic complexity theory. Vol. 7. Springer Science & Business Media, 2013 (cit. on p. 13). 33 [Chi86] AL Chistov."
  },
  {
    "prompt": "Explain a concept involving time, cit, polynomial in computational complexity.",
    "response": "“Algorithm of polynomial complexity for factoring polynomials and finding the components of varieties in subexponential time”. In: Journal of Soviet Mathematics 34 (1986), pp. 1838–1882 (cit. on p. 3). [CLO97] David Cox, John Little, and Donal O’shea. Ideals, varieties, and algorithms. Vol. 3. Springer, 1997 (cit. on p. 23). [CM06] Antonio Cafure and Guillermo Matera. “Improved explicit estimates on the number of solutions of equations over a finite field”."
  },
  {
    "prompt": "This is a technical discussion about Finite, Applications, cit in theoretical computer science.",
    "response": "In: Finite Fields and Their Applications 12.2 (2006), pp. 155–185 (cit. on pp. 16–18). [Dic05] Alicia Dickenstein. Solving polynomial equations. Springer, 2005 (cit. on p. 17). [Dic+91] Alicia Dickenstein et al. “The membership problem for unmixed polynomial ideals is solvable in single exponential time”. In: Discrete Applied Mathematics 33.1-3 (1991), pp. 73–94 (cit. on p. 3). [Dub90] Thomas W Dub´e. “The structure of polynomial ideals and Gr¨obner bases”."
  },
  {
    "prompt": "Read and understand this section about SIAM, Computing, Journal from a research paper.",
    "response": "In: SIAM Journal on Computing 19.4 (1990), pp. 750–773 (cit. on p. 3). [EHV92] David Eisenbud, Craig Huneke, and Wolmer Vasconcelos. “Direct methods for primary decom- position”. In: Inventiones mathematicae 110.1 (1992), pp. 207–235 (cit. on pp. 3, 4). [Eis13] David Eisenbud. Commutative algebra: with a view toward algebraic geometry. Vol. 150. Springer Science & Business Media, 2013 (cit. on pp. 3–6, 15, 32). [Gao03] Shuhong Gao."
  },
  {
    "prompt": "Read and understand this section about Factoring, cit, Math from a research paper.",
    "response": "“Factoring multivariate polynomials via partial differential equations”. In: Math- ematics of computation 72.242 (2003), pp. 801–822 (cit. on p. 3). [GD65] Alexandre Grothendieck and Jean A. Dieudonn´e. “´El´ements de g´eom´etrie alg´ebrique IV”. In: Publ. Math. IHES, Ibid. 24 (1965) (cit. on pp. 5, 6). [Gri86] Dima Grigoriev. “Factorization of polynomials over a finite field and the solution of systems of algebraic equations”. In: Journal of Soviet Mathematics (1986) (cit. on p. 3)."
  },
  {
    "prompt": "This is a technical discussion about Shafi, Sipser, Goldwasser in theoretical computer science.",
    "response": "[GS86] Shafi Goldwasser and Michael Sipser. “Private coins versus public coins in interactive proof systems”. In: Proceedings of the eighteenth annual ACM symposium on Theory of computing. 1986, pp. 59–68 (cit. on p. 11). [GSS19] Zeyu Guo, Nitin Saxena, and Amit Sinhababu. “Algebraic dependencies and PSPACE algo- rithms in approximative complexity over any field”. In: Theory of Computing 15.1 (2019), pp. 1–30 (cit. on p. 14). [GTZ88] Patrizia Gianni, Barry Trager, and Gail Zacharias."
  },
  {
    "prompt": "This is a technical discussion about obner, ideals, cit in theoretical computer science.",
    "response": "“Gr¨obner bases and primary decomposition of polynomial ideals”. In: Journal of Symbolic Computation 6.2-3 (1988), pp. 149–167 (cit. on pp. 3, 4). [Hei83] Joos Heintz. “Definability and fast quantifier elimination in algebraically closed fields”. In: The- oretical Computer Science 24.3 (1983), pp. 239–277 (cit. on p. 14). [Her26] Grete Hermann. “Die Frage der endlich vielen Schritte in der Theorie der Polynomideale: Unter Benutzung nachgelassener S¨atze von K. Hentzelt”."
  },
  {
    "prompt": "Understanding the computational aspects of Mathematische, Annalen, cit.",
    "response": "In: Mathematische Annalen 95.1 (1926), pp. 736– 788 (cit. on pp. 3, 8). [HS13] Marc Hindry and Joseph H Silverman. Diophantine geometry: an introduction. Vol. 201. Springer Science & Business Media, 2013 (cit. on p. 29). [HS80] Joos Heintz and Claus-Peter Schnorr. “Testing polynomials which are easy to compute”. In: Pro- ceedings of the twelfth annual ACM Symposium on Theory of Computing. 1980, pp. 262–272 (cit. on p. 14). [HS81] Joos Heintz and Malte Sieveking."
  },
  {
    "prompt": "Understanding the computational aspects of Heidelberg, Automata, variables.",
    "response": "“Absolute primality of polynomials is decidable in random polynomial time in the number of variables”. In: Automata, Languages and Programming. Ed. by Shimon Even and Oded Kariv. Berlin, Heidelberg: Springer Berlin Heidelberg, 1981, pp. 16–28. ISBN: 978-3-540-38745-9 (cit. on p. 3). [Jel05] Zbigniew Jelonek. “On the effective Nullstellensatz”. In: Inventiones mathematicae 162.1 (2005), pp. 1–17 (cit. on pp. 8, 16). 34 [Kal85] Erich Kaltofen. “Fast parallel absolute irreducibility testing”."
  },
  {
    "prompt": "Understanding the computational aspects of Computation, Journal, Symbolic.",
    "response": "In: Journal of Symbolic Computation 1.1 (1985), pp. 57–67 (cit. on p. 3). [Kal95] E Kaltofen. “Effective Noether Irreducibility Forms and Applications”. In: Journal of Computer and System Sciences 50.2 (1995), pp. 274–295 (cit. on pp. 3, 6, 8, 19, 23, 24). [Koi96] Pascal Koiran. “Hilbert’s Nullstellensatz is in the polynomial hierarchy”. In: Journal of complexity 12.4 (1996), pp. 273–286 (cit. on pp. 5, 10, 11, 16, 20, 24, 26, 33). [Koi97] P. Koiran."
  },
  {
    "prompt": "Explain a concept involving Randomized, varieties, Computer in computational complexity.",
    "response": "“Randomized and deterministic algorithms for the dimension of algebraic varieties”. In: Proceedings of the 38th Annual Symposium on Foundations of Computer Science. FOCS ’97. USA: IEEE Computer Society, 1997, p. 36. ISBN: 0818681977 (cit. on pp. 4, 6, 12). [KPS01] Teresa Krick, Luis Miguel Pardo, and Martin Sombra. “Sharp estimates for the arithmetic Null- stellensatz”. In: Duke Mathematical Journal 109.3 (2001), pp. 521 –598. DOI: 10.1215/S0012- 7094- 01- 10934- 4."
  },
  {
    "prompt": "Scientific context on cit, URL, Emanuel and their role in graph theory.",
    "response": "URL: https://doi.org/10.1215/S0012- 7094- 01- 10934- 4 (cit. on pp. 18, 21). [Las05] Emanuel Lasker. “Zur Theorie der moduln und Ideale”. In: Mathematische Annalen 60.1 (1905), pp. 20–116 (cit. on p. 3). [Len84] Arjen K. Lenstra. Polynomial-time algorithms for the factorization of polynomials. 1984. URL: https: //infoscience.epfl.ch/handle/20.500.14299/51045 (cit. on p. 23). [LO77] Jeffrey C Lagarias and Andrew M Odlyzko. “Effective versions of the Chebotarev density the- orem”."
  },
  {
    "prompt": "Understanding the computational aspects of Durham, L-functions, Proc..",
    "response": "In: Algebraic number fields: L-functions and Galois properties (Proc. Sympos., Univ. Durham, Durham, 1975). Vol. 7. 1977, pp. 409–464 (cit. on p. 16). [Mig83] Maurice Mignotte. “Some Useful Bounds”. In: Computer Algebra: Symbolic and Algebraic Compu- tation. Springer, 1983, pp. 259–263 (cit. on p. 19). [Mil20] James S. Milne. Algebraic Number Theory (v3.08). Available at www.jmilne.org/math/. 2020 (cit. on p. 15). [MR13] Ernst W Mayr and Stephan Ritscher."
  },
  {
    "prompt": "Scientific context on Dimension-dependent, obner, poly and their role in graph theory.",
    "response": "“Dimension-dependent bounds for Gr¨obner bases of poly- nomial ideals”. In: Journal of Symbolic Computation 49 (2013), pp. 78–94 (cit. on pp. 8, 17). [MT17] Ernst W. Mayr and Stefan Toman. “Complexity of Membership Problems of Different Types of Polynomial Ideals”. In: Algorithmic and Experimental Methods in Algebra, Geometry, and Num- ber Theory. Cham: Springer International Publishing, 2017, pp. 481–493. ISBN: 978-3-319-70566-8. DOI: 10.1007/978-3-319-70566-8_20."
  },
  {
    "prompt": "Understanding the computational aspects of URL, cit, Mathematics.",
    "response": "URL: https://doi.org/10.1007/978-3- 319-70566-8_20 (cit. on p. 3). [Poo08] Bjorn Poonen. “Rational points on varieties”. In: Graduate Studies in Mathematics 186 (2008), p. 337 (cit. on p. 5). [RS62] J Barkley Rosser and Lowell Schoenfeld. “Approximate formulas for some functions of prime numbers”. In: Illinois Journal of Mathematics 6.1 (1962), pp. 64–94 (cit. on p. 16). [Sch07] Peter Scheiblechner."
  },
  {
    "prompt": "This is a technical discussion about Varieties, Complexity, Counting in theoretical computer science.",
    "response": "“On the Complexity of Counting Irreducible Components and Computing Betti Numbers of Algebraic Varieties”. PhD thesis. University of Paderborn, 2007 (cit. on p. 25). [Sch10] Peter Scheiblechner. “On a generalization of Stickelberger’s theorem”. In: Journal of Symbolic Computation 45.12 (2010), pp. 1459–1470 (cit. on p. 28). [Sei74] Abraham Seidenberg. “Constructions in algebra”. In: Transactions of the American Mathematical Society 197 (1974), pp. 273–313 (cit. on pp. 3, 6, 13)."
  },
  {
    "prompt": "This is a technical discussion about Abraham, Seidenberg, cit in theoretical computer science.",
    "response": "[Sei78] Abraham Seidenberg. “Constructions in a polynomial ring over the ring of integers”. In: Ameri- can Journal of Mathematics 100.4 (1978), pp. 685–703 (cit. on p. 3). [SR94] Igor Rostislavovich Shafarevich and Miles Reid. Basic algebraic geometry. Vol. 2. Springer, 1994 (cit. on pp. 14, 15, 27). [Sta24] The Stacks project authors. The Stacks project. https://stacks.math.columbia.edu. 2024 (cit. on pp. 4, 15, 23). 35 [SY10] Amir Shpilka and Amir Yehudayoff."
  },
  {
    "prompt": "Explain a concept involving Arithmetic, cit, circuits in computational complexity.",
    "response": "“Arithmetic circuits: A survey of recent results and open questions”. In: Foundations and Trends® in Theoretical Computer Science 5.3–4 (2010), pp. 207–388 (cit. on p. 13). [WR76] Peter J. Weinberger and Linda Preiss Rothschild. “Factoring polynomials over algebraic number fields”. In: ACM Transactions on Mathematical Software (TOMS) 2.4 (1976), pp. 335–350 (cit. on p. 16). 36"
  },
  {
    "prompt": "Read and understand this section about Linear, Ordering, Principle from a research paper.",
    "response": "arXiv:2503.19188v1 [cs.CC] 24 Mar 2025 Upper and Lower Bounds for the Linear Ordering Principle Edward A. Hirsch∗ Ilya Volkovich† March 26, 2025 Abstract Korten and Pitassi (FOCS, 2024) deﬁned a new1 complexity class LP 2 as the polynomial- time Turing closure of the Linear Ordering Principle. They asked whether a Karp–Lipton– style collapse can be proven for LP 2. We answer this question aﬃrmatively by showing that PprMA ⊆LP 2."
  },
  {
    "prompt": "This is a technical discussion about Roy, Computational, Complexity in theoretical computer science.",
    "response": "As a byproduct, we also answer an open question of Chakaravarthy and Roy (Computational Complexity, 2011) whether PprMA ⊆SP 2. We complement this result by providing a new upper bound for LP 2, namely LP 2 ⊆PprSBP. Thus we are placing LP 2 between PprMA and PprSBP. One technical ingredient of this result is an algorithm that approximates the number of satisfying assignments of a Boolean circuit using a prSBP oracle (i.e. in FPprSBP), which could be of independent interest."
  },
  {
    "prompt": "Explain a concept involving Roy, Chakaravarthy, Lipton in computational complexity.",
    "response": "Finally, we prove that PprOP 2 ⊆OP 2, which implies that the Karp–Lipton–style collapse to PprOMA is actually better than both known collapses to PprMA due to Chakaravarthy and Roy (Computational Complexity, 2011) and to OP 2 also due to Chakaravarthy and Roy (STACS, 2006). 1 Introduction The seminal theorem of Richard M. Karp and Richard J."
  },
  {
    "prompt": "Scientific context on Polynomial, Hierarchy, Boolean and their role in graph theory.",
    "response": "Lipton [KL80] connected non-uniform and uniform complexity by demonstrating a collapse of the Polynomial Hierarchy assuming NP has polynomial-size Boolean circuits. This collapse has since been very instrumental in transferring lower bounds against Boolean circuits of ﬁxed-polynomial2 size to smaller classes of the Polynomial Hierarchy. Since then, these results were strengthened in many ways yielding “minimal” classes that have such lower bounds and to which the Polynomial Hierarchy collapses."
  },
  {
    "prompt": "Scientific context on Symmetric, Alternation, Background and their role in graph theory.",
    "response": "1.1 Background 1.1.1 Classes Based on Symmetric Alternation An important notion in this context is that of symmetric alternation. Namely, one of the best col- lapses was based on the following idea ([Cai07], attributed to Sengupta): if polynomial-size circuits ∗Department of Computer Science, Ariel University, Israel. This research was conducted with the support of the State of Israel, the Ministry of Immigrant Absorption, and the Center for the Absorption of Scientists."
  },
  {
    "prompt": "Explain a concept involving Email, Computer, Department in computational complexity.",
    "response": "Email: edwardh@ariel.ac.il †Computer Science Department, Boston College, Chestnut Hill, MA. Email: ilya.volkovich@bc.edu 1Note that this notation had been used in the past [Sch83] for a very diﬀerent class, which has been apparently forgotten after that. 2That is, for any k ∈N, the class contains a language that cannot be computed by Boolean circuits of size nk, i.e. a language outside of Size[nk]."
  },
  {
    "prompt": "Read and understand this section about no’, SAT, exist from a research paper.",
    "response": "1 for SAT exist, two provers (defending the answers ‘yes’ and ‘no’, respectively) send such circuits to a polynomial-time bounded veriﬁer who can use them to verify membership in any language in PH. The corresponding class SP 2 [Can96, RS98] was thus shown to have ﬁxed-polynomial circuit lower bounds. Indeed, since NP ⊆SP 2, if SAT requires super-polynomial circuits, we are done."
  },
  {
    "prompt": "This is a technical discussion about Hierarchy, Polynomial, Kannan in theoretical computer science.",
    "response": "Otherwise, the Polynomial Hierarchy, which is known to contain “hard” languages3 by Kannan’s theorem [Kan82], collapses to SP 2 and so do these hard languages. This technique has been known as a win-win argument in the literature [Kan82, BCG+96, KW98, Vin05, Cai07, San09, CR11, Vol14, IKV23]. Upon further inspection, one can observe that the presumed polynomial-size circuits for SAT do not actually depend on the input itself, but rather on its length."
  },
  {
    "prompt": "Understanding the computational aspects of Based, observation, called.",
    "response": "Based on this observation, the collapse was deepened to the input-oblivious version of SP 2, called OP 2 [CR06]. Yet, since OP 2 is not known and, in fact, not believed to contain NP, the ﬁxed-polynomial lower bounds do not (immediately) carry over to OP 2. This state of aﬀairs remained unchanged for about ﬁfteen years until a signiﬁcant progress was made when Kleinberg et al. [KKMP21] initiated the study of total functions beyond TFNP."
  },
  {
    "prompt": "Scientific context on Karp, Lipton, Size and their role in graph theory.",
    "response": "That is, while Karp–Lipton’s theorem has not been improved, lower bounds against Size[nk] were pushed down to OP 2 [GLV24] and LP 2 [KP24], a new1 important class which we describe in more detail below. An important feature of these new results was that they were based on reducing ﬁnding a hard function to a total search problem. Namely, the works of Korten [Kor21] and Li [Li24] reduced the question to the so-called Range Avoidance problem."
  },
  {
    "prompt": "This is a technical discussion about Boolean, represented, circuit in theoretical computer science.",
    "response": "That is, given a function f : {0, 1}n →{0, 1}m with m > n, represented by a Boolean circuit, ﬁnd a point outside its image. In [CHR24, Li24], the range avoidance problem has been reduced to symmetric alternation. Subsequently, Korten and Pitassi [KP24] reduced the range avoidance problem to the Linear Ordering Principle: given an implicitly described ordering relation, either ﬁnd the smallest element or report a breach of the linear order axioms."
  },
  {
    "prompt": "This is a technical discussion about polynomial-time, Turing, length in theoretical computer science.",
    "response": "A polynomial-time Turing closure of this principle gave rise to a new class LP 2 ⊆SP 2: a version of SP 2 where the two provers provide points of a polynomial-time veriﬁable linear order on binary strings of a certain length (each point starting with the corresponding answer 0 or 1), and the prover that provides the smaller element wins."
  },
  {
    "prompt": "Understanding the computational aspects of Santhanam, Based, Classes.",
    "response": "1.1.2 Classes Based on Merlin-Arthur Protocols In a parallel line of research, the same questions were considered for classes based on Merlin– Arthur proofs: Santhanam [San09] has shown ﬁxed-polynomial lower bounds for promise problems possessing such proofs (i.e. the class prMA). In [CR11], Chakaravarthy and Roy have shown a Karp–Lipton–style collapse and thus ﬁxed-polynomial size lower bounds for the class PprMA."
  },
  {
    "prompt": "Explain a concept involving PprAM, PprMA, presented in computational complexity.",
    "response": "In particular, they presented a new upper bound for SP 2 by showing that SP 2 ⊆PprAM. Combining this with a result of [AKSS95], that NP ⊆P/poly implies an “internal collapse” MA = AM4, they concluded that the Polynomial Hierarchy collapses all the way to PprMA. Subsequently, by applying the win-win argument, they obtained ﬁxed-polynomial bounds for PprMA, which (unlike prMA) is a class of languages."
  },
  {
    "prompt": "Read and understand this section about prMA, PprMA, languages from a research paper.",
    "response": "It is to be noted though that since prMA is not a class of languages — while PprMA is, there is no immediate way to carry any lower bound against prMA over to PprMA. Nonetheless, the relationship between PprMA and the classes of symmetric alternation (including LP 2, OP 2, and SP 2) remained unknown. 3For every k ∈N : PH ̸⊆Size[nk]. 4The internal collapse goes through for the promise versions of the classes as well."
  },
  {
    "prompt": "Explain a concept involving protocols, SBP, Approximate in computational complexity.",
    "response": "2 1.1.3 SBP, Approximate Counting, and Set Size Estimation There is a class that sits between the two basic classes based on Arthur–Merlin protocols, yet its deﬁnition is not based on these protocols. The class SBP, which stands for small bounded-error polynomial-time computations, was introduced in [BGM06] by B¨ohler et al. as a relaxation of the class BPP to the case when the acceptance probability is not required to be bounded away from 0."
  },
  {
    "prompt": "This is a technical discussion about SBP, result, class in theoretical computer science.",
    "response": "Indeed, this relaxation gives more power and as a result, as was shown in [BGM06], MA ⊆SBP ⊆AM. Furthermore, SBP is the only known natural class that lies between MA and AM. Computing the number of accepting paths of a given non-deterministic Turing machine is a fundamental problem captured by the “counting” class #P. Yet, this class appears to be too powerful since, by Toda’s Theorem [Tod91], even a single query to it suﬃces to decide any language in the polynomial hierarchy, PH ⊆P#P[1]!"
  },
  {
    "prompt": "Read and understand this section about approximations, natural, explore from a research paper.",
    "response": "Given that, it is natural to explore approximations. To this end, one can consider the problem of Approximate Counting which refers to the task of approximating the number of accepting paths (within a constant factor). Equivalently, this problem can be framed as approximating the size of a set S represented as the set of satisfying assignments of a Boolean circuit C."
  },
  {
    "prompt": "Explain a concept involving FBPPNP, Previously, FPprAM in computational complexity.",
    "response": "Previously, it was shown that this task could be carried out by a randomized algorithm using an NP oracle (FBPPNP) [Sto85, JVV86] and by a deterministic algorithm using a prAM oracle (FPprAM) [Sip83, GS86]. Shaltiel and Umans [SU06] show how to accomplish this task in FPNP, yet under a derandomization assumption. All of these algorithms can be implemented using parallel oracle queries. The decision version of the problem is to choose between two constant-factor estimates of the set size."
  },
  {
    "prompt": "Explain a concept involving SSE, Boolean, concreteness in computational complexity.",
    "response": "For concreteness, consider the following problem called set-size estimation (or SSE for short): Given a set S (via a Boolean circuit C) and an integer m with a promise that either |S| ≥m or |S| ≤m/2, our goal is to decide which case holds. Interestingly, a result of [BGM06] implies that this problem is complete for the corresponding class prSBP of promise problems (and thus it is hard for SBP)!"
  },
  {
    "prompt": "Scientific context on Goldwasser, Arthur, Merlin and their role in graph theory.",
    "response": "In terms of upper bounds, in a seminal paper [GS86], Goldwasser and Sipser have exhibited an Arthur–Merlin protocol not only for this problem, but also for the case when the set S is represented by a non-deterministic circuit5! This more general version of the problem (WSSE, where the set S is given via a non-deterministic circuit), is complete for the class prAM."
  },
  {
    "prompt": "Explain a concept involving Deﬁnition, fact, problems in computational complexity.",
    "response": "(See Deﬁnition 5.2 for the formal deﬁnition of the problems; in fact, the factor-of-two gap in the estimates is arbitrary and can be replaced by any positive constant. .) In fact, Goldwasser–Sipser’s protocol proves the containment both for languages (SBP ⊆AM) and for promise problems (prSBP ⊆prAM). At the same time, it is important to highlight the distinction between the two versions of the problem — i.e."
  },
  {
    "prompt": "This is a technical discussion about SSE, WSSE, Boolean in theoretical computer science.",
    "response": "for the “standard” (SSE) vs non-deterministic (WSSE) Boolean circuits — which appears to be (at the very least) non-trivial. Notably, the work of [BGM06] established an oracle separation between SBP and AM. 1.2 Promise Problems as Oracles An important note is due on the use of a promise problem as an oracle, because the literature contains several diﬀerent notions for this. The collapse result of Chakaravarthy and Roy that we use follows the loose oracle access mode adopted in [CR11]."
  },
  {
    "prompt": "Explain a concept involving promise, queries, oracle in computational complexity.",
    "response": "Namely, oracle queries outside of the promise set are allowed and no particular behaviour of the computational model deﬁning the promise class is expected on such queries. At the same time, the answer of the (base) machine using 5A non-deterministic circuit C(x, w) accepts x if there exists a witness w for which C(x, w) = 1."
  },
  {
    "prompt": "This is a technical discussion about promise, answers, oracle in theoretical computer science.",
    "response": "3 this oracle must be correct irrespective of the oracle’s answers to such queries and no assumption is made on the internal consistency of the answers outside of the promise set. For deterministic polynomial-time oracle machines this approach is equivalent to querying any language consistent with the promise problem, that is, a language that contains all the “yes” instances and does not contain the “no” instances of the promise problem."
  },
  {
    "prompt": "Understanding the computational aspects of promise, class, consistent.",
    "response": "One can also extend it to the classes of promise problems: A class C of languages is consistent with a class D of promise problems if for every problem Π ∈D there is a language L ∈C consistent with Π. The equivalence between the approaches follows from the works of [GS88, BF99], where the latter approach has been adopted. Nonetheless, we include a formal proof of this equivalence in Appendix C for the sake of completeness."
  },
  {
    "prompt": "This is a technical discussion about Karp, Lipton, Contribution in theoretical computer science.",
    "response": "1.3 Our Contribution 1.3.1 A New Lower Bound for LP 2 and the Strongest Non–Input-Oblivious Karp– Lipton Collapse The containment PprMA ⊆LP 2 and its consequences. Two open questions regarding sym- metric alternation have been stated explicitly: • whether a Karp–Lipton–style theorem holds for LP 2 [KP24], • whether PprMA is contained in SP 2 [CR11]. In this paper we resolve both these questions aﬃrmatively by showing the following containment. (Recall that LP 2 ⊆SP 2.) Theorem 1. PprMA ⊆LP 2."
  },
  {
    "prompt": "Scientific context on Lipton, Roy, Karp and their role in graph theory.",
    "response": "Combining this theorem with a result of Chakaravarthy and Roy [CR11] that NP ⊆P/poly implies the collapse PH = PprMA, we obtain a Karp–Lipton–style collapse theorem for LP 2, thus solving an open question posed in [KP24]. Corollary 2. If NP ⊆P/poly, then PH = LP 2 = PprMA. Which Karp–Lipton–style collapse is stronger? Theorem 1 shows that PprMA is currently the smallest non–input-oblivious class for which a Karp–Lipton–style collapse is known."
  },
  {
    "prompt": "Scientific context on hand, input-oblivious, PprMA and their role in graph theory.",
    "response": "On the other hand, such a collapse is known for OP 2 [CR06], which is input-oblivious. However, the exact relationship between OP 2 and PprMA remains unknown. We demonstrate how to unify these results by observing that the collapse can be extended to PprOMA ⊆PprMA, where prOMA is the input- oblivious version of prMA and therefore is contained in prMA. At the same time, we show that PprOMA is also contained in OP 2, thus making PprOMA smaller than both PprMA and OP 2!"
  },
  {
    "prompt": "This is a technical discussion about PprOP, intermediate, step in theoretical computer science.",
    "response": "As an intermediate step we show that PprOP 2 ⊆OP 2. Theorem 3. PprOMA ⊆PprOP 2 ⊆OP 2. We note that the “non-promise” version of the latter inclusion, i.e. POP 2 ⊆OP 2, was already es- tablished in [CR06]. Yet, this result does not carry over to the promise case. A similar phenomenon occurs with the non–input-oblivious analogue of this question: while we know that PSP 2 ⊆SP 2 [CR06], it still remains open whether PprSP 2 ⊆SP 2. 4 1.3.2 A New Upper Bound for LP 2 Containment."
  },
  {
    "prompt": "Scientific context on Theorem, PprSBP, upper and their role in graph theory.",
    "response": "Our second main result is a new upper bound on LP 2. In particular, we prove that LP 2 ⊆PprSBP. The best known upper bound prior to our result followed from [KP24, CR11]: LP 2 ⊆SP 2 ⊆PprAM. Theorem 4. LP 2 ⊆PprSBP. In summary, our two new inclusions yield the following picture: MA SBP AM PprMA LP 2 PprSBP SP 2 PprAM Theorem 1 Theorem 4 Implications for “hard” functions."
  },
  {
    "prompt": "Understanding the computational aspects of Korten, Pitassi, bound.",
    "response": "In [KP24], Korten and Pitassi have shown that LE 2 — the exponential version of LP 2 — contains a language of circuit complexity 2n/n. By translation, our upper bound scales as LE 2 ⊆EprSBP. As a corollary we obtain new circuit lower bounds for EprSBP. To the best of our knowledge, the strongest previously established bound for this class was a “half-exponential” bound that followed from the bound on MAEXP [BFT98, MVW99]. Corollary 5. EprSBP contains a language of circuit complexity 2n/n."
  },
  {
    "prompt": "Explain a concept involving Aydinlio, Corollary, glu in computational complexity.",
    "response": "It is to be noted that Corollary 5 could be viewed as an unconditional version of a result of Aydinlio˜glu et al. [AGHK11] as it recovers and strengthens their conclusion. In particular, [AGHK11] have shown the following6: if PNP is consistent with prAM or even prSBP, then ENP contains a language of circuit complexity 2n/n. Indeed, given the premises we obtain that EprSBP ⊆EPNP = ENP from which the claim follows directly by Corollary 5."
  },
  {
    "prompt": "Explain a concept involving Techniques, Range, Korten in computational complexity.",
    "response": "1.4 Our Techniques 1.4.1 Range Avoidance In [KP24], Korten and Pitassi show that MA ⊆LP 2. The inclusion PprMA ⊆LP 2 essentially follows their argument with the additional observation that since LP 2 is a syntactic class, not only it contains MA (as was shown) but it is also consistent with prMA. Thus one can ﬁrst construct a pseudorandom generator using an LP 2 oracle [Kor21, KP24] and then leverage it to fully derandomize the prMA oracle not just in prNP, but actually in NP ⊆LP 2."
  },
  {
    "prompt": "Explain a concept involving PLP, Lipton, Karp in computational complexity.",
    "response": "Therefore, PprMA ⊆PLP 2 = LP 2. 1.4.2 Input-Oblivious Symmetric Alternation A Karp–Lipton–style collapse to PprOMA follows from [CR11] by combining several previously known techniques. However, is this collapse stronger than the known collapse to OP 2 [CR06]? The 6The corresponding claims in [AGHK11] were stated using slightly diﬀerent terminology."
  },
  {
    "prompt": "Read and understand this section about prOP, inclusion, prOMA from a research paper.",
    "response": "5 inclusion prOMA ⊆prOP 2 can be transferred from a somewhat similar statement that was proven in [CR06]; however, in order to prove PprOMA ⊆OP 2 we need also the inclusion PprOP 2 ⊆OP 2, which seems novel."
  },
  {
    "prompt": "Explain a concept involving witnesses, oracle, algorithm in computational complexity.",
    "response": "The main idea is that the two provers corresponding to the oracle give their input-oblivious witnesses prior to the whole computation, and the veriﬁcation algorithm performs a cross-check not only between the witnesses of diﬀerent provers but also between the witnesses of the same prover, which allows us to simulate all oracle queries to prOP 2 in a single OP 2 algorithm."
  },
  {
    "prompt": "Understanding the computational aspects of queries, model, advance.",
    "response": "Indeed, our approach is made possible by the input-oblivious nature of the computational model: while the oracle queries may be adaptive and not known in advance (due to potential queries outside of the promise set), the witnesses are universal for the whole computation and nothing else is required."
  },
  {
    "prompt": "Understanding the computational aspects of Approximate, Order, Rank.",
    "response": "1.4.3 Approximate Counting and the Order Rank Approximation The upper bound LP 2 ⊆PprSBP is obtained by developing a process that, given an arbitrary element in a linearly ordered set, rapidly converges to the set’s minimum. Approximate counting using a prSBP oracle. We show how to deterministically approxi- mate the number of satisfying assignments of a Boolean circuit, given oracle access to prSBP (i.e. in FPprSBP), using parallel queries."
  },
  {
    "prompt": "This is a technical discussion about SBP, times, algorithm in theoretical computer science.",
    "response": "Our algorithm is based on SBP ampliﬁcation that was used in [BGM06, Vol20]. A crucial observation is that, as we need a multiplicative approximation (up to the factor 1+ε), it suﬃces to place the desired number between two consecutive powers of two; the correct place then could be found by either querying a prSBP oracle O(n/ε) times in parallel or (using binary search) O(log2(n/ε)) times sequentially. This result could be on independent interest. See Lemma 5.6 for the formal statement."
  },
  {
    "prompt": "Understanding the computational aspects of rank, Approximating, order.",
    "response": "Approximating the rank w.r.t. a linear order. The rank of an element α of a linearly ordered set U is the number of elements in this set that are strictly less than α (in particular, α is the minimum if and only if rank(α) = 0). We can extend this deﬁnition to non-empty subsets S ⊆U, where rank(S) is the average rank of elements in S. We reduce the problem of approximately comparing the average ranks of two sets to approximate counting."
  },
  {
    "prompt": "Understanding the computational aspects of size, Boolean, order.",
    "response": "To see how, consider a strict linear order <E implicitly deﬁned on U = {0, 1}n using a Boolean circuit E, and observe that for a non-empty subset S ⊆U, the average rank of S is exactly the size of the set of pairs {(υ, α) ∈U × S | υ <E α} divided by the size of S. Hence, this task can be carried out using a prSBP oracle. An upper bound for the Linear Ordering Principle."
  },
  {
    "prompt": "This is a technical discussion about rank, set, element in theoretical computer science.",
    "response": "As was mentioned, we develop a process that, given an arbitrary element in a linearly ordered set U = {0, 1}n, rapidly converges to the set’s minimum. Given an element α ∈U, we ﬁrst deﬁne the set S as the set of all the elements less or equal to α. Formally, S := {x | x ≤α}. Observe that rank(S) = rank(α)/2. We then iteratively partition S into two disjoint sets, starting from i = 1: S0 = {x ∈S | xi = 0} and S1 = {x ∈S | xi = 1}. By averaging argument, min {rank(S0), rank(S1)} ≤rank(S)."
  },
  {
    "prompt": "This is a technical discussion about rank, subset, smaller in theoretical computer science.",
    "response": "We then take S to be the subset (S0 or S1) with the smaller rank and continue to the next value of i. That is, we ﬁx the bits of the 6 elements of S one coordinate at a time. Therefore, once i = n, our “ﬁnal” set S contains exactly one element β and thus at that point rank(S) = rank(β). On the other hand, as the rank of the “initial” S was rank(α)/2 and the overall rank could only decrease, we obtain that rank(β) ≤rank(α)/2. We can then invoke the same procedure this time with β as its input."
  },
  {
    "prompt": "Explain a concept involving times, elements, element in computational complexity.",
    "response": "As the there are 2n elements in U, this process will converge to the set’s minimum after invoking the procedure at most n times, given any initial element. The algorithm described above requires computing (or at least comparing) the average ranks of two sets. Our analysis demonstrates that a procedure for approximate comparison, developed before, is suﬃcient for the implementation of this idea (though the factor at each step will be a little bit less than 2)."
  },
  {
    "prompt": "Understanding the computational aspects of Section, Theorem, Organization.",
    "response": "1.5 Organization of the Paper In Section 2 we give the necessary deﬁnitions. Section 3 contains the proof of Theorem 1 answering the open questions of Korten and Pitassi [KP24] and Chakaravarthy and Roy [CR11]. Section 4 contains the proof of Theorem 3 which implies that collapse to PprOMA is better than both collapses to PprMA and to OP 2. Section 5 contains the proof of Theorem 4. In Section 6 we outline further directions for the research."
  },
  {
    "prompt": "Explain a concept involving Appendix, proofs, Promise in computational complexity.",
    "response": "In Appendix C we discuss oracle access modes in a formalistic way. The two other Appendices A and B contain formal proofs of statements (used in Section 4) that essentially follow from the proofs of [AKSS95, CR06, CR11] (namely, the collapse to PprOMA and the inclusion prOMA ⊆prOP 2), we provide these proofs for the sake of completeness. 2 Deﬁnitions 2.1 Promise Classes as Oracles A promise problem is a relaxation of (the decision problem for) a language. Deﬁnition 2.1 (promise problem)."
  },
  {
    "prompt": "Scientific context on ΠYES, ΠNO, promise and their role in graph theory.",
    "response": "Π = (ΠYES, ΠNO) is a promise problem if ΠYES ∩ΠNO = ∅. We say that a language O is consistent with Π, if ΠYES ⊆O and ΠNO ⊆O. Similarly to [CR11], when an oracle is described as a promise problem, we use loose access to the oracle. The outer Turing machine is allowed to make queries outside of the promise set, and the oracle does not need to conform to the deﬁnition of the promise oracle class for such queries."
  },
  {
    "prompt": "Scientific context on Turing, oracle, Avoid and their role in graph theory.",
    "response": "However, the outer Turing machine must return the correct answer irrespective of oracle’s behavior for queries outside of the promise set in particular, the oracle does not need to be consistent in its answers to the same query. Refer to Appendix C for formalistic deﬁnitions and further discussion. 2.2 Problems Avoid and LOP Deﬁnition 2.2 (Avoid, Range Avoidance, [KKMP21, Kor21]). Avoid is the following total search problem. Input: circuit C with n inputs and m > n outputs."
  },
  {
    "prompt": "Read and understand this section about stretch, PNP, Output from a research paper.",
    "response": "Output: y ∈{0, 1}m \\ Im C. Korten [Kor21] proved that for a stretch of n + 1 this problem is equivalent to a stretch of O(n) (and, of course, vice versa) using PNP reductions. 7 The following deﬁnition is due to Korten and Pitassi [KP24]: Deﬁnition 2.3 (LOP, Linear Ordering Principle). LOP is the following total search problem. Input: ordering relation <E given as a Boolean circuit E with 2n inputs."
  },
  {
    "prompt": "Explain a concept involving Output, ample, counterex in computational complexity.",
    "response": "Output: either the minimum for <E (that is, x such that ∀y ∈{0, 1}n \\ {x} x <E y) or a counterex- ample, if <E is not a strict linear order. A counterexample is either a pair satisfying x <E y <E x or a triple satisfying x <E y <E z <E x. 2.3 Complexity Classes The following two deﬁnitions have been suggested by Korten and Pitassi who also proved their equivalence [KP24]. Deﬁnition 2.4 (LP 2 via reductions). A language L ∈LP 2 if it can be reduced to LOP using a PNP- Turing reduction."
  },
  {
    "prompt": "Understanding the computational aspects of Polynomial-time, Turing, reductions.",
    "response": "(Polynomial-time Turing reductions and polynomial-time many-one reductions have the same eﬀect, as proved in [KP24].) The following is an alternative deﬁnition of LP 2, which was shown in [KP24] to be equivalent. Deﬁnition 2.5 (LP 2 via symmetric alternation)."
  },
  {
    "prompt": "Understanding the computational aspects of bit, order, minimal.",
    "response": "A language L ∈LP 2 if there is a ternary relation R computable in time s(n), where s is a polynomial, R ⊆{0, 1}n × {0, 1}s(n) × {0, 1}s(n), denoted Rx(u, v) for x ∈{0, 1}n, u, v ∈{0, 1}s(n), such that, for every ﬁxed x, it deﬁnes a linear order on s(|x|)-size strings such that: • for every x ∈L, the minimal element of this order starts with bit 1, • for every x /∈L, the minimal element of this order starts with bit 0."
  },
  {
    "prompt": "Understanding the computational aspects of deﬁnition, exists, obvious.",
    "response": "It is obvious that the latter version of the deﬁnition is a particular case of the deﬁnition of SP 2 [Can96, RS98]: Deﬁnition 2.6. A language L ∈SP 2 if there is a polynomial-time computable ternary relation R ⊆{0, 1}n × {0, 1}s(n) × {0, 1}s(n), denoted Rx(u, v) for x ∈{0, 1}n, u, v ∈{0, 1}s(n), such that: • for every x ∈L, there exists w(1) such that ∀v Rx(w(1), v) = 1, • for every x /∈L, there exists w(0) such that ∀u Rx(u, w(0)) = 0."
  },
  {
    "prompt": "This is a technical discussion about version, formally, deﬁne in theoretical computer science.",
    "response": "We now formally deﬁne the class OP 2 [CR06], which is the input-oblivious version of SP 2. Since we will need also a promise version of it, we start with deﬁning this generalization. Deﬁnition 2.7."
  },
  {
    "prompt": "Read and understand this section about ΠYES, ΠNO, Turing from a research paper.",
    "response": "A promise problem Π = (ΠYES, ΠNO) belongs to prOP 2 if there is a polynomial-time deterministic Turing machine A such that for every n ∈N, there exist w(0) n , w(1) n (called irrefutable certiﬁcates) that satisfy: • If x ∈ΠYES, then for every v, A(x, w(1) n , v) = 1, • If x ∈ΠNO, then for every u, A(x, u, w(0) n ) = 0. No assumption on the behaviour of A is made outside the promise set except that it stops (accepts or rejects) in polynomial time."
  },
  {
    "prompt": "This is a technical discussion about ΠNO, ΠYES, languages in theoretical computer science.",
    "response": "OP 2 is the respective class of languages (that is, it corresponds to the case of ΠYES = ΠNO). 8 3 PprMA ⊆LP 2 In this section we prove Theorem 1 by, essentially, expanding the proof of MA ⊆LP 2 in [KP24]. We use the following statements from that paper and an earlier paper by Korten [Kor21]. Deﬁnition 3.1 ([Kor21, Deﬁnitions 7, 8]). PRG is the following search problem: given 1n, output a pseudorandom generator7 R = (x1, . . ."
  },
  {
    "prompt": "This is a technical discussion about strings, array, n-input in theoretical computer science.",
    "response": ", xm), that is, an array of strings xi ∈{0, 1}n such that for every n-input circuit C of size n: Pr x←U(R){C(x) = 1} − Pr y←U({0,1}n){C(y) = 1} ≤1 n. Korten demonstrates that such a generator containing m = n6 strings can be constructed with a single oracle query to Avoid8 (actually, the stretch is even larger than “plus 1 output wire”). Proposition 3.2 ([Kor21, Theorem 3]). PRG reduces in polynomial time to Avoid."
  },
  {
    "prompt": "Scientific context on LOP, Pitassi, Weak and their role in graph theory.",
    "response": "Korten and Pitassi demonstrate that Avoid (which they call Weak Avoid) can be solved with one oracle query to LOP. Proposition 3.3 ([KP24, Theorem 1]). Avoid is polynomial-time many-one reducible to LOP. The main result of this section is the following theorem. Theorem 3.4 (Theorem 1, restated). PprMA ⊆LP 2. Proof. We show how to replace the oracle prMA by LP 2. Since PLP 2 = LP 2 by Deﬁnition 2.4, the result follows."
  },
  {
    "prompt": "Explain a concept involving Arthur, assume, calls in computational complexity.",
    "response": "One can assume that calls to the prMA oracle are made for input lengths such that Arthur can be replaced by a circuit A(x, w, r) of size at most s(n) for a speciﬁc polynomial s. One can assume perfect completeness for A, that is, for x in the promise set “YES”, there is w such that ∀r A(x, w, r) = 1."
  },
  {
    "prompt": "Read and understand this section about pseudorandom, Turing, oracle from a research paper.",
    "response": "Before simulating the prMA oracle, our deterministic polynomial-time Turing machine will make oracle calls to LP 2 in order to build a pseudorandom generator suﬃcient to derandomize circuits of size s(n). By Proposition 3.2, such a pseudorandom generator G, which is a sequence G(1s(n)) of pseudorandom strings g1, . . . , gm ∈{0, 1}s(n) for m bounded by a polynomial in s(n), can constructed (for m = s(n)6 and error 1 s(n)) using a reduction to Avoid."
  },
  {
    "prompt": "Explain a concept involving Proposition, Avoid, LOP in computational complexity.",
    "response": "Subsequently, by Proposition 3.3, Avoid is reducible to LOP. As a result, {gi}m i=1 can be computed in deterministic polynomial time by querying an LP 2 oracle. After G is computed, each call to the prMA oracle can be replaced by an NP ⊆LP 2 query ∃w C(w) for the circuit C that computes the conjunction of the circuits A(x, w, gi) with hardwired x and gi, for every i."
  },
  {
    "prompt": "Explain a concept involving Arthur, queries, Note in computational complexity.",
    "response": "Note that such queries constructed for x outside of the promise set are still valid NP queries even if Arthur does not conform to the deﬁnition of MA in this case. These oracle answers are irrelevant, because the original PprMA machine must return the correct (in particular, the same) answer irrespectively of the oracle’s answer. 7Korten does not say that m has a polynomial dependence on n though we think it is assumed, and anyway his theorem provides a construction with m = n6."
  },
  {
    "prompt": "Explain a concept involving Empty, Lipton, Avoid in computational complexity.",
    "response": "8In the paper the Avoid problem is refered to as “Empty”. 9 4 Which Karp–Lipton–style Collapse is Better? Chakaravarthy and Roy proved two Karp–Lipton–style collapses: down to OP 2 [CR06] and down to PprMA [CR11]. These two classes seem to be incomparable thereby rising the question: which collapse result is stronger?"
  },
  {
    "prompt": "Understanding the computational aspects of PprOMA, prMA, classes.",
    "response": "We observe that the collapse to PprMA can actually be deepened to PprOMA, where prOMA is the oblivious version of prMA — and subsequently show that latter class is contained in both previous classes. That is, PprOMA ⊆PprMA ∩OP 2. Indeed, the “internal collapse” of prMA (and, in fact, even prAM) to prOMA, under the assumption that NP ⊆P/poly, is implicit in [AKSS95]. Nonetheless, we include a formal proof in Appendix A in order to present a self-contained argument. Proposition 4.1 (Prop. A.1)."
  },
  {
    "prompt": "Scientific context on prOMA, poly, PprOMA and their role in graph theory.",
    "response": "If NP ⊆ P/poly, then prAM ⊆ prOMA and PH = PprOMA. To show that this class is not only included in PprMA but also in OP 2, we use two inclusions: 1. prOMA ⊆prOP 2, 2. PprOP 2 ⊆OP 2. The ﬁrst inclusion is essentially proved in [CR06, Theorem 3], which says that MA ⊆NOP 2 (⊆OP 2). One needs to notice that the proof goes through for promise classes. Proposition 4.2 (Prop. B.1). prOMA ⊆prOP 2. We include a formal proof of this proposition in Appendix B."
  },
  {
    "prompt": "Explain a concept involving lemma, prOP, inclusion in computational complexity.",
    "response": "The second inclusion seems novel and is proved in the following lemma: Lemma 4.3. PprOP 2 ⊆OP 2. Remark: Formally, we show that PΠ ⊆OP 2 for every promise problem Π ∈prOP 2. Proof. Let L ∈PΠ and let M• be a deterministic oracle machine that decides L correctly given loose oracle access to Π (i.e. irrespective of the answers to its queries outside of the promise set), in time p(n) (for a polynomial p). Consider the polynomial-time deterministic veriﬁer A(q, u, v) from the deﬁnition of Π ∈prOP 2."
  },
  {
    "prompt": "This is a technical discussion about input, lengths, Deﬁne in theoretical computer science.",
    "response": "For n ∈N, let 1, . . . , p(n) be all possible lengths of oracle queries made by M given an input of length n. Deﬁne Wn := (w(0) 1 , . . . , w(0) p(n), w(1) 1 , . . . , w(1) p(n)) as a vector containing the irrefutable certiﬁcates (both “yes” and “no”) of A for the appropriate input lengths."
  },
  {
    "prompt": "Explain a concept involving constitutes, construct, polynomial-time in computational complexity.",
    "response": "We now construct a new polynomial-time deterministic veriﬁer A′(x, U, V ) that will demonstrate that L ∈OP 2 and will show that, for any x, the string9 W|x| constitutes an irrefutable certiﬁcate that can be used both as U = (u(0) 1 , . . . , u(0) p(n), u(1) 1 , . . . , u(1) p(n)) and as V = (v(0) 1 , . . . , v(0) p(n), v(1) 1 , . . . , v(1) p(n)). 9There might be diﬀerent versions of this string as the irrefutable certiﬁcates need not to be unique."
  },
  {
    "prompt": "Read and understand this section about input, simulate, oracle from a research paper.",
    "response": "10 Given (x, U, V ) as an input, A′ will simulate M. Whenever M makes an oracle query q to Π, A′ will compute four bits: a := A(q, u(1) |q| , v(0) |q| ), b := A(q, v(1) |q| , u(0) |q| ), c := A(q, u(1) |q| , u(0) |q| ), d := A(q, v(1) |q| , v(0) |q| ), and will proceed with the simulation of M as if the oracle answered ℓ:= (a ∧c) ∨(b ∧d). By deﬁnition, for any x, the machine M computes L(x) correctly given the correct answers to the queries in the promise set (i.e."
  },
  {
    "prompt": "Scientific context on ΠYES, ΠNO, oracle and their role in graph theory.",
    "response": "q ∈ΠYES ∪ΠNO) and irrespective of the oracle’s answers outside of the promise set. Thus it suﬃces to prove that the oracle’s answers to the queries in the promise set are computed correctly, which we show now by inspecting the four possible cases. • Suppose x ∈L and U = W|x|. – If q ∈ΠYES then u(1) |q| is a ‘yes’-irrefutable certiﬁcate and hence a = c = 1 =⇒ℓ= 1. – If q ∈ΠNO then u(0) |q| is a ‘no’-irrefutable certiﬁcate and hence b = c = 0 =⇒ℓ= 0. • Suppose x ̸∈L and V = W|x|."
  },
  {
    "prompt": "This is a technical discussion about ΠYES, Theorem, certiﬁcate in theoretical computer science.",
    "response": "– If q ∈ΠYES then v(1) |q| is a ‘yes’-irrefutable certiﬁcate and hence b = d = 1 =⇒ℓ= 1. – If q ∈ΠNO then v(0) |q| is a ‘no’-irrefutable certiﬁcate and hence a = d = 0 =⇒ℓ= 0. Theorem 4.4 (Theorem 3, restated). PprOMA ⊆OP 2. Proof. By Proposition 4.2 and Lemma 4.3. Remark 4.5. When a semantic class without complete problems is used as an oracle, it may be ambiguous. However, prAM does have a complete problem WSSE (see Deﬁnition 5.2). By inspecting the proof of the collapse (Prop."
  },
  {
    "prompt": "Explain a concept involving WSSE, Turing, poly in computational complexity.",
    "response": "4.1) one can observe that WSSE actually belongs to prOMA under NP ⊆P/poly, and the oracle Turing machine that demonstrates PH = PprOMA still queries a speciﬁc promise problem. For the inclusion of PprOMA in OP 2, the ﬁrst part (Prop. 4.2) transforms one promise problem into another promise problem, thus in the inclusion PprOMA ⊆PprOP 2 it is also the case that a single oracle is replaced by (another) single oracle. 5 LP 2 ⊆PprSBP In this section we prove Theorem 4."
  },
  {
    "prompt": "Scientific context on smaller, order, point and their role in graph theory.",
    "response": "Our proof strategy is as follows: Given a point in a linear order, we aim to move “down the order” (i.e. towards “smaller” points). At each stage we will skip over a constant fraction of the points remaining on our way to the minimum. In order to ﬁnd the next point, we will employ a binary-search-like procedure to determine the bits of the desired point, one coordinate at a time."
  },
  {
    "prompt": "Explain a concept involving bit, points, approximately in computational complexity.",
    "response": "Here is where our prSBP oracle comes into play: At each step, we look at the remaining set of points partitioned into two subsets: the points where the appropriate bit is 0 and where that bit is 1, and select the subset with the (approximately) smaller average rank. Before we proceed with the main algorithm, we show how to approximate the size of a set using a prSBP oracle. This procedure could be of independent interest."
  },
  {
    "prompt": "Understanding the computational aspects of Approximate, Counting, unary.",
    "response": "11 5.1 Approximate Counting For a unary relation R(x), we denote the number of elements in R as #xR(x) := |{x | x ∈R}|. We will also abbreviate this notation to #xR. For k ∈N, we deﬁne R⊗k — the k-th tensor power of R as R⊗k((x1, . . . , xk)) := R(x1) ∧R(x2) ∧. . . ∧R(xk), where x1, . . . , xk are k disjoint copies of the argument x of R, respectively. Observation 5.1. Then #x(R⊗k) = (#xR)k. Note that queries to prSBP can be thought of as queries to a speciﬁc promise problem."
  },
  {
    "prompt": "Understanding the computational aspects of Estimation, SSENO, WSSE.",
    "response": "Deﬁnition 5.2 (Set-Size Estimation, SSE and WSSE). Let C be a Boolean circuit and m ≥1 be an integer given in binary representation. Then SSE := (SSEYES, SSENO), where SSEYES = {(C, m) | #xC(x) ≥m}, SSENO = {(C, m) | #xC(x) ≤m/2}. If C is a non-deterministic circuit, we denote the corresponding problem by WSSE. These two promise problems are complete for promise classes prSBP and prAM, respectively. This is what is proved essentially in [BGM06] and [GS86] and formulated explicitly in [Vol20]."
  },
  {
    "prompt": "This is a technical discussion about Implicit, Lemma, SSE in theoretical computer science.",
    "response": "Lemma 5.3 (Implicit in [BGM06]). SSE is prSBP-complete. Lemma 5.4 (Implicit in [GS86]). WSSE is prAM-complete. In particular, these complete problems showcase that prSBP ⊆prAM. Observation 5.5 (Useful inequalities). Let x, y ≥0 and 0 ≤ε ≤1. Then 1. 4ε/3 ≤1 + ε (Bernoulli’s inequality), 2. x+εy x+y ≥ε. Next, we show how to approximate deterministically the number of satisfying assignments for a Boolean circuit using a prSBP oracle (i.e. FPprSBP)."
  },
  {
    "prompt": "Scientific context on FBPPNP, algorithm, Previously and their role in graph theory.",
    "response": "Previously, it was shown that this task could be carried out by a randomized algorithm using an NP oracle (FBPPNP) [Sto85, JVV86] and by a deterministic algorithm using a prAM oracle (FPprAM) 10 [Sip83, GS86]. Lemma 5.6. There exists a deterministic algorithm that given a Boolean circuit C on n variables and a rational number ε > 0 outputs an integer number t satisfying #xC ≤t ≤4ε/3 · #xC ≤(1 + ε)#xC in time polynomial in n, the size of C and 1 ε, given oracle access to SSE. Proof."
  },
  {
    "prompt": "Read and understand this section about SSE, language, consistent from a research paper.",
    "response": "Let O be a language consistent with SSE. Set k := \u0006 3 ε \u0007 and deﬁne ˆC := C⊗k. Consider the following algorithm: 10Essentially using an oracle access to WSSE. 12 1. If (C, 1) ̸∈O return t = 0 // #xC = 0 2. Find i as the smallest j between 1 and nk + 1 such that \u0010 ˆC, 2j\u0011 ̸∈O 3. return t = \u0004 2i/k\u0005 If #xC = 0 then (C, 1) ̸∈O and the algorithm outputs t = 0. Otherwise, #x ˆC ≥1 and hence \u0010 ˆC, 1 \u0011 ∈O. On the other hand, by deﬁnition #x ˆC ≤2nk and hence \u0010 ˆC, 2nk+1\u0011 ̸∈O."
  },
  {
    "prompt": "Scientific context on well-deﬁned, time, times and their role in graph theory.",
    "response": "Therefore, i is well-deﬁned and we have that: • \u0010 ˆC, 2i\u0011 ̸∈O =⇒#x ˆC < 2i =⇒(#xC)k < 2i =⇒#xC < 2i/k =⇒#xC ≤t, • \u0010 ˆC, 2i−1\u0011 ∈O ⇒#x ˆC > 2i−1 2 ⇒(#xC)k > 2i−2 ⇒41/k · #xC > 2i/k ⇒41/k · #xC > t. In conclusion, #xC ≤t ≤41/k · #xC ≤4ε/3 · #xC ≤(1 + ε)#xC. For the running time, the algorithm ﬁnds i by either querying the oracle O at most nk+2 times in parallel or (using binary search) O(log2(nk)) times sequentially, and thus the computation of t can be carried out in time poly(nk)."
  },
  {
    "prompt": "Understanding the computational aspects of Estimating, Rank, Linear.",
    "response": "5.2 Estimating the Average Rank w.r.t. a Linear Order Let U = {0, 1}n. A single-output Boolean circuit E with 2n inputs induces an ordering relation <E on U as x <E y ⇐⇒E(x, y) = 1. If <E is a strict linear order, we call E a linear order circuit. Observation 5.7."
  },
  {
    "prompt": "Understanding the computational aspects of outputs, satisfying, circuit.",
    "response": "There exists a deterministic Turing machine with SAT oracle that, given a circuit E on 2n variables, stops in time polynomial in n and the size of E and does the following: if E is a linear order circuit, it outputs “yes”; otherwise, it outputs a counterexample: a pair satisfying x <E y <E x or a triple satisfying x <E y <E z <E x. Fix any strict linear order < on U. Deﬁnition 5.8. For an element α ∈U we deﬁne its rank as rank(α) := |{x ∈U | x < α}|."
  },
  {
    "prompt": "This is a technical discussion about rank, non-empty, deﬁne in theoretical computer science.",
    "response": "We can extend this deﬁnition to non-empty subsets S ⊆U of U by taking the average rank: deﬁne rank(S) := P x∈S rank(x) |S| . If S = {x ∈U | C(x) = 1} is described by a circuit C, we use the same notation: rank(C) = rank(S). Below are some useful observations that we will use later. Observation 5.9. • For a non-empty subset S ⊆U, |{(υ, α) ∈U × S | υ < α}| = |S| · rank(S). • For any α ∈U, rank{υ | υ ≤α} = rank(α)/2. 13 • Let S0, S1 ⊆U be two non-empty disjoint subsets of U."
  },
  {
    "prompt": "This is a technical discussion about rank, order, lemma in theoretical computer science.",
    "response": "Then rank(S0 ∪S1) = |S0| · rank(S0) + |S1| · rank(S1) |S0| + |S1| . In the following lemma the rank is deﬁned w.r.t. the order <E described by a linear order circuit E. This lemma allows us to estimate the rank of a set using a prSBP oracle. Lemma 5.10."
  },
  {
    "prompt": "Understanding the computational aspects of variables, SSE, rank.",
    "response": "There exists a deterministic algorithm that given a Boolean circuit C on n variables, a linear order circuit E on 2n variables, and an ε > 0, outputs a rational number r satisfying: 4−ε · rank(C) ≤r ≤4ε · rank(C) in time polynomial in n, the sizes of C and E, and in 1 ε, given oracle access to SSE. Proof. Consider a circuit D(x, y) := C(y) ∧E(x, y). That is, y is accepted by C and x <E y. By Observation 5.9, #(x,y)D = #xC ·rank(C)."
  },
  {
    "prompt": "This is a technical discussion about Lemma, rank, numbers in theoretical computer science.",
    "response": "By Lemma 5.6 we can compute integers tC and tD that approximate the numbers #xC and #(x,y)D, respectively. Formally, #xC ≤tC ≤4ε · #xC and #(x,y)D ≤tD ≤4ε · #(x,y)D. Therefore we obtain: 4−ε · rank(C) ≤#(x,y)D 4ε · #xC ≤tD tC ≤4ε · #(x,y)D #xC = 4ε · rank(C). 5.3 Finding the Minimum Using a prSBP Oracle We use the approximation algorithms developed above in order to ﬁnd an element that is much closer to the minimum than a given element."
  },
  {
    "prompt": "Read and understand this section about Back, procedure, element from a research paper.",
    "response": "The following lemma describes the procedure Back that given an element α ﬁnds another element β whose rank is smaller by a constant factor. We will use this procedure afterwards in order to ﬁnd the minimum in a polynomial number of iterations. The procedure proceeds by determining the bits of the new element, one coordinate at a time, using a prSBP oracle. The rank is w.r.t. the order <E described by a linear order circuit E. Lemma 5.11."
  },
  {
    "prompt": "Read and understand this section about rank, SSE, Back from a research paper.",
    "response": "There exists a deterministic algorithm Back that given a linear order circuit E on 2n variables and an element α ∈{0, 1}n, outputs an element β ∈{0, 1}n such that rank(β) ≤ rank(α) √ 2 , in time polynomial in n and the size of E, given oracle access to SSE. Proof. Consider the following procedure: Back(E, α) : 1. Deﬁne C(x) := E(x, α) ∨x = α. // The set of all elements that are <E than or equal to α 2. Set ε = 1/(8n). 3."
  },
  {
    "prompt": "Read and understand this section about choose, Lemma, set from a research paper.",
    "response": "For i = 1 to n: (a) For b ∈{0, 1}: deﬁne Cb := C|xi=b 14 (b) For b ∈{0, 1}: if #xCb = 0 then set C := C1−b, βi := 1 −b; continue to the next i // If one of the sets is empty, we choose the other one (c) For b ∈{0, 1}: use Lemma 5.10 to approximate rank(Cb) with ε into rb (d) If r1 ≥r0 then C := C0, βi := 0 else C := C1, βi := 1 // Choose the set with smaller approximate order After each iteration one more variable xi gets its value βi and is substituted into C, that is, in the current circuit C variables x1, ."
  },
  {
    "prompt": "Scientific context on rank, iteration, Observation and their role in graph theory.",
    "response": ". . , xi are replaced by the corresponding constants β1, . . . , βi. We claim that after each iteration the rank of the resulting circuit is bounded from the above: rank(C) ≤42εi · rank(α) 2 . Indeed, by Observation 5.9, before the ﬁrst iteration, we have that rank(C) = rank(α) 2 . Now consider any iteration. If C1 or C0 are empty, then rank(C) remains the same and 42εi ≤42ε(i+1). Otherwise, by Lemma 5.10, for b ∈{0, 1} : 4−ε · rank(Cb) ≤rb ≤4ε · rank(Cb)."
  },
  {
    "prompt": "Explain a concept involving rank, Observations, Equivalently in computational complexity.",
    "response": "If r1 ≥r0 then rank(C1) ≥r1 · 4−ε ≥r0 · 4−ε ≥rank(C0) · 4−2ε and therefore by Observations 5.9 and 5.5(2) rank(C) = #xC0 · rank(C0) + #xC1 · rank(C1) #xC0 + #xC1 ≥#xC0 · rank(C0) + #xC1 · rank(C0) · 4−2ε #xC0 + #xC1 ≥rank(C0) · 4−2ε. Equivalently, rank(C0) ≤rank(C) · 42ε. Similarly, if r1 < r0 then rank(C1) ≤rank(C) · 42ε. Therefore, at each step the rank is multiplied at most by 42ε."
  },
  {
    "prompt": "Read and understand this section about rank, element, iteration from a research paper.",
    "response": "Consequently, after the n-th iteration, C represents the set that contains only the element β and we have that rank(β) ≤42εn · rank(α)/2 ≤ √ 2 · rank(α)/2 = rank(α)/ √ 2. For the runtime, all the steps can be carried out in time polynomial in n and 1 ε = O(n). Note that the procedure Back has a unique ﬁxed point, namely, the minimal element. Observation 5.12. Back(E, α) = α if and only if α is the minimal element in E. Proof. rank(α) = 0 ⇐⇒rank(α) ≤rank(α)/ √ 2."
  },
  {
    "prompt": "This is a technical discussion about section, Observation, ready in theoretical computer science.",
    "response": "We are now ready to prove the main result of this section. Theorem 5.13. LP 2 ⊆PprSBP. Proof. It suﬃces to construct a deterministic polynomial-time algorithm that solves LOP given oracle access to prSBP. Let E be a circuit on 2n inputs. We describe an algorithm for LOP. 1. Check that E is indeed a linear order using Observation 5.7. 15 2. Let α := 0n, β := 1n. 3. While α ̸= β repeat: α := β, β := Back(E, α). 4. Output α. Given Observation 5.7, we can assume w.l.o.g."
  },
  {
    "prompt": "Understanding the computational aspects of circuit, iterations, rank.",
    "response": "that E is a linear order circuit. We claim that the algorithm will output the minimal element after at most 2n iterations. Indeed, by Lemma 5.11, the rank of the element α after 2n iterations satisﬁes rank(α) ≤rank(1n) √ 2 2n ≤2n−1 2n < 1, thus the “While” cycle will be terminated before that. 6 Further Research 1. Can one strengthen our inclusion LP 2 ⊆PprSBP to SP 2 ⊆PprSBP? One can try combining our techniques with the proof of SP 2 ⊆PprAM by Chakaravarthy and Roy [CR11]."
  },
  {
    "prompt": "Read and understand this section about SBP, Chakaravarthy, Roy from a research paper.",
    "response": "Note that in the other direction it is open even whether SBP ⊆SP 2. 2. Chakaravarthy and Roy [CR11] asked whether PprMA and PprSP 2 are contained in SP 2. While we resolved the ﬁrst question, the second one remains open."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Lemma, predicted.",
    "response": "We note that, while in the input- oblivious world both inclusions hold (PprOMA ⊆PprOP 2 ⊆OP 2, Theorem 4.4), the proof of the latter inclusion (Lemma 4.3) is essentially input-oblivious (one needs to give all the certiﬁcates for the oracle non-adaptively, and queries cannot be predicted because oracle answers cannot be predicted for promise problems). 3."
  },
  {
    "prompt": "Explain a concept involving Proposition, PprOMA, bounds in computational complexity.",
    "response": "While Proposition 4.1 shows a collapse of PH to PprOMA, it does not immediately imply ﬁxed-polynomial size lower bounds for PprOMA, since this class is unlikely to contain NP, so in the case that NP ̸⊆P/poly it may indeed miss hard languages. One can observe that Santhanam’s proof [San09] of Size[nk] lower bounds for promise problems in prMA is input- oblivious. Indeed, the presented hard promise problems are actually in prOMA!"
  },
  {
    "prompt": "Scientific context on Size, class, research and their role in graph theory.",
    "response": "However, it does not yield a language in PprOMA that is hard for Size[nk], and we leave this question for further research. 4. Similarly to LP 2, one could deﬁne a class of languages reducible to Avoid. A similar class of search problems, APEPP, has been deﬁned by [KKMP21, Kor21] (and Korten [Kor21] proved that constructing a hard truth table is a problem that is complete for this class under PNP-reductions); however, we are asking about a class of languages."
  },
  {
    "prompt": "Explain a concept involving Turing, Pitassi, Korten in computational complexity.",
    "response": "Korten and Pitassi have shown that LP 2 can be equivalently deﬁned using many-one, Turing, or PNP-reductions, thus there are several options. One can observe that the containment PprMA ⊆LP 2 (Theorem 3.4) is essentially proved via the intermediate class PAvoid,NP that uses both an oracle for Range Avoidance (a single-valued or an essentially unique [KP24] version) and an oracle for SAT. Can one prove that one of the containments in PprMA ⊆PAvoid,NP ⊆LP 2 is in fact an equality? 5."
  },
  {
    "prompt": "Understanding the computational aspects of queries, non-adaptive, approximate.",
    "response": "As was mentioned, the FPprSBP procedure for approximate counting can be implemented using parallel (i.e. non-adaptive) oracle queries. On the other hand the containment LP 2 ⊆ PprSBP, which uses approximate counting as a black-box subroutine, seems to require sequen- tial, adaptive queries. Could one implement the latter containment using parallel queries?"
  },
  {
    "prompt": "Explain a concept involving Turing, prSBP, reductions in computational complexity.",
    "response": "16 In particular, as PP is consistent with prSBP [BGM06] and is closed under non-adaptive Turing reductions [FR96], this would imply that LP 2 ⊆PP. Note that it is unknown even whether PNP ⊆PP, moreover, there is an oracle separating these two classes [Bei94]. 6. One can deﬁne an input-oblivious version OLP 2 of LP 2 in diﬀerent ways."
  },
  {
    "prompt": "Scientific context on POLP, PprMA, PprOMA and their role in graph theory.",
    "response": "One of the ways will provide a class that contains hard functions similarly to SP 2 [Li23], OP 2 [GLV24], and LP 2 [KP24], and one can extend the proof of PprMA ⊆LP 2 to show PprOMA ⊆POLP 2 . We have not veriﬁed it so far and leave it as another conjecture for the moment. If true, it will also prove the Karp–Lipton–style theorem for OLP 2, which does not follow from known results otherwise."
  },
  {
    "prompt": "Understanding the computational aspects of discussing, Yaroslav, Alekseev.",
    "response": "Acknowledgement The authors are grateful to Yaroslav Alekseev for discussing and to Dmitry Itsykson for discussing and proofreading a preliminary version of this paper. References [AGHK11] B. Aydinlio˜glu, D. Gutfreund, J. M. Hitchcock, and A. Kawachi. Derandomizing arthur- merlin games and approximate counting implies exponential-size lower bounds. Com- put. Complex., 20(2):329–366, 2011. [AKSS95] V. Arvind, J. K¨obler, U. Sch¨oning, and R. Schuler."
  },
  {
    "prompt": "Explain a concept involving circuits, Sci., Comput in computational complexity.",
    "response": "If NP has polynomial-size circuits, then MA=AM. Theor. Comput. Sci., 137(2):279–282, 1995. [BCG+96] N. H. Bshouty, R. Cleve, R. Gavald`a, S. Kannan, and C. Tamon. Oracles and queries that are suﬃcient for exact learning. J. Comput. Syst. Sci., 52(3):421–433, 1996. [Bei94] R. Beigel. Perceptrons, PP, and the polynomial hierarchy. Computational Complexity, 4:339–349, 1994. [BF99] H. Buhrman and L. Fortnow. One-sided versus two-sided error in probabilistic compu- tation."
  },
  {
    "prompt": "Read and understand this section about STACS, pages, Fortnow from a research paper.",
    "response": "In STACS, pages 100–109, 1999. [BFT98] H. Buhrman, L. Fortnow, and T. Thierauf. Nonrelativizing separations. In Proceedings of the 13th Annual IEEE Conference on Computational Complexity (CCC), pages 8–12, 1998. [BGM06] E. B¨ohler, C. Glaßer, and D. Meister. Error-bounded probabilistic computations be- tween MA and AM. J. Comput. Syst. Sci., 72(6):1043–1076, 2006. [Cai07] J.-Y. Cai. S2P ⊆ZPP NP . Journal of Computer and System Sciences, 73(1):25–35, 2007. [Can96] R. Canetti."
  },
  {
    "prompt": "This is a technical discussion about BPP, hierarchy, Roy in theoretical computer science.",
    "response": "More on BPP and the polynomial-time hierarchy. Inf. Process. Lett., 57(5):237–241, 1996. 17 [CHR24] L. Chen, S. Hirahara, and H. Ren. Symmetric exponential time requires near-maximum circuit size. In Proceedings of the 56th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2024, to appear. Association for Computing Machinery, 2024. [CR06] V. T. Chakaravarthy and S. Roy. Oblivious symmetric alternation. In STACS, pages 230–241, 2006. [CR11] V. T. Chakaravarthy and S. Roy."
  },
  {
    "prompt": "Understanding the computational aspects of Merlin, Arthur, oracles.",
    "response": "Arthur and Merlin as oracles. Comput. Complex., 20(3):505–558, 2011. [FR96] L. Fortnow and N. Reingold. PP is closed under truth-table reductions. Inf. Comput., 124(1):1–6, 1996. [GLV24] K. Gajulapalli, Z. Li, and I. Volkovich. Oblivious classes revisited: Lower bounds and hierarchies. Electron. Colloquium Comput. Complex., TR24-049, 2024. [GM15] O. Goldreich and O. Meir. Input-oblivious proof systems and a uniform complexity perspective on P/poly."
  },
  {
    "prompt": "Scientific context on TOCT, Transactions, Computation and their role in graph theory.",
    "response": "ACM Transactions on Computation Theory (TOCT), 7(4):1– 13, 2015. [GS86] S. Goldwasser and M. Sipser. Private coins versus public coins in interactive proof systems. In Proceedings of the 18th Annual ACM Symposium on Theory of Computing (STOC), pages 59–68, 1986. [GS88] J. Grollmann and A. L. Selman. Complexity measures for public-key cryptosystems. SIAM J. Comput., 17(2):309–335, 1988. [HS15] E. A. Hirsch and D. Sokolov. On the probabilistic closure of the loose unambiguous hierarchy. Inf."
  },
  {
    "prompt": "Explain a concept involving Process, circuit, Lett. in computational complexity.",
    "response": "Process. Lett., 115(9):725–730, 2015. [IKV23] R. Impagliazzo, V. Kabanets, and I. Volkovich. Synergy between circuit obfuscation and circuit minimization. In Nicole Megow and Adam D. Smith, editors, Approxi- mation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM 2023, September 11-13, 2023, Atlanta, Georgia, USA, volume 275 of LIPIcs, pages 31:1–31:21. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2023. [JVV86] M. Jerrum, L. G. Valiant, and V. V."
  },
  {
    "prompt": "Understanding the computational aspects of Vazirani, Theor, Random.",
    "response": "Vazirani. Random generation of combinatorial structures from a uniform distribution. Theor. Comput. Sci., 43:169–188, 1986. [Kan82] R. Kannan. Circuit-size lower bounds and non-reducibility to sparse sets. Information and Control, 55(1-3):40–56, 1982. [KKMP21] R. Kleinberg, O. Korten, D. Mitropolsky, and C. Papadimitriou. Total Functions in the Polynomial Hierarchy. In James R."
  },
  {
    "prompt": "This is a technical discussion about ITCS, Germany, Innovations in theoretical computer science.",
    "response": "Lee, editor, 12th Innovations in Theoretical Computer Science Conference (ITCS 2021), volume 185 of Leibniz International Pro- ceedings in Informatics (LIPIcs), pages 44:1–44:18, Dagstuhl, Germany, 2021. Schloss Dagstuhl–Leibniz-Zentrum f¨ur Informatik. 18 [KL80] R. M. Karp and R. J. Lipton. Some connections between nonuniform and uniform complexity classes."
  },
  {
    "prompt": "This is a technical discussion about California, USA, Annual in theoretical computer science.",
    "response": "In Proceedings of the 12th Annual ACM Symposium on Theory of Computing, April 28-30, 1980, Los Angeles, California, USA, pages 302–309, 1980. [Kor21] O. Korten. The hardest explicit construction. CoRR, abs/2106.00875, 2021. [KP24] O. Korten and T. Pitassi. Strong vs. Weak Range Avoidance and the Linear Ordering Principle. Electron. Colloquium Comput. Complex., TR24-076, 2024. Extended abstract published in FOCS-2024. [KW98] J. K¨obler and O. Watanabe."
  },
  {
    "prompt": "Understanding the computational aspects of IEEE, SIAM, Computer.",
    "response": "New collapse consequences of NP having small circuits. SIAM J. Comput., 28(1):311–324, 1998. [Li23] X. Li. Two source extractors for asymptotically optimal entropy, and (many) more. In 2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS), pages 1271–1281, Los Alamitos, CA, USA, nov 2023. IEEE Computer Society. [Li24] Z. Li. Symmetric exponential time requires near-maximum circuit size: Simpliﬁed, truly uniform."
  },
  {
    "prompt": "Scientific context on STOC, Annual, Proceedings and their role in graph theory.",
    "response": "In Proceedings of the 56th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2024, to appear. Association for Computing Machinery, 2024. [LR94] K.-J. Lange and P. Rossmanith. Unambiguous polynomial hierarchies and exponential size. In Structure in Complexity Theory Conference, pages 106–115. IEEE Computer Society, 1994. [MVW99] P. B. Miltersen, N. V. Vinodchandran, and O. Watanabe. Super-polynomial versus half-exponential circuit size in the exponential hierarchy."
  },
  {
    "prompt": "This is a technical discussion about COCOON, pages, Sundaram in theoretical computer science.",
    "response": "In COCOON, pages 210–220, 1999. [RS98] A. Russell and R. Sundaram. Symmetric alternation captures BPP. Comput. Complex., 7(2):152–162, 1998. [San09] R. Santhanam. Circuit lower bounds for Merlin–Arthur classes. SIAM J. Comput., 39(3):1038–1061, 2009. [Sch83] U. Sch¨oning. A low and a high hierarchy within NP. Journal of Computer and System Sciences, 27:14–28, 1983. [Sip83] M. Sipser. A complexity theoretic approach to randomness."
  },
  {
    "prompt": "Read and understand this section about Boston, Massachusetts, USA from a research paper.",
    "response": "In Proceedings of the 15th Annual ACM Symposium on Theory of Computing, 25-27 April, 1983, Boston, Massachusetts, USA, pages 330–335. ACM, 1983. [Sto85] L. J. Stockmeyer. On approximation algorithms for #P. SIAM J. Comput., 14(4):849– 861, 1985. [SU06] R. Shaltiel and C. Umans. Pseudorandomness for approximate counting and sampling. Comput. Complex., 15(4):298–341, 2006. [Tod91] S. Toda. PP is as hard as the polynomial time hierarchy. SIAM J. on Computing, 20(5):865–877, 1991. 19 [Vin05] N. V."
  },
  {
    "prompt": "Understanding the computational aspects of Vinodchandran, Volkovich, pages.",
    "response": "Vinodchandran. A note on the circuit complexity of PP. Theor. Comput. Sci., 347(1-2):415–418, 2005. [Vol14] I. Volkovich. On learning, lower bounds and (un)keeping promises. In Proceedings of the 41st ICALP, pages 1027–1038, 2014. [Vol20] I. Volkovich. The untold story of SBP. In Henning Fernau, editor, The 15th Interna- tional Computer Science Symposium in Russia, CSR, volume 12159 of Lecture Notes in Computer Science, pages 393–405. Springer, 2020."
  },
  {
    "prompt": "Read and understand this section about Lipton, Collapse, Karp from a research paper.",
    "response": "A A Karp–Lipton–style Collapse to PprOMA The following proposition shows that the collapse of [CR11] can be actually pushed down to PprOMA. For this, we observe that by standard techniques (e.g. [AKSS95]) prAM ⊆prOMA under NP ⊆P/poly. Proposition A.1. If NP ⊆P/poly, then prAM ⊆prOMA and PH = PprOMA. Proof. If NP ⊆P/poly, then Merlin’s proof in prMA can be made input-oblivious. Let us show it ﬁrst for a larger class: prAM."
  },
  {
    "prompt": "Explain a concept involving ΠYES, ΠNO, Turing in computational complexity.",
    "response": "If (ΠYES, ΠNO) ∈prAM, then there is a polynomial-time Turing machine A such that ∀x ∈ΠYES ∀r ∃w A(x, r, w) = 1, ∀x ∈ΠNO Pr r [∃w A(x, r, w) = 1] < 1/2. Let us ask Merlin to send a family Sn of CircuitSAT circuits of appropriate input sizes (slightly abusing the notation: Sn contains circuits for a polynomial range of input sizes, not just n); one can assume that they compute a correct satisfying assignment or say “no” (they can lie only if they say “no” for a satisﬁable formula)."
  },
  {
    "prompt": "Explain a concept involving Ax,r, Boolean, Merlin in computational complexity.",
    "response": "Consider the Boolean circuit Ax,r obtained by embedding x and r into A, its variables are the bits of the witness w. Now Arthur can use this circuit instead of Merlin’s original proof: ∀n ∈N ∃Sn ∀x ∈ΠYES ∀r A(x, r, Sn(Ax,r)) = 1, ∀x ∈ΠNO Pr r [∃Sn A(x, r, Sn(Ax,r)) = 1] < 1/2. The ﬁrst condition is true because Merlin can send correct CircuitSAT circuits. The second condition is true because if such a Sn existed, then the original Merlin could have sent w = Sn(Ax,r)."
  },
  {
    "prompt": "This is a technical discussion about Ax,r, Merlin, Note in theoretical computer science.",
    "response": "Note that in this case although Sn may depend on x and r, A will able to catch a cheating Merlin. If we relax it by pulling the existential quantiﬁer over Sn outside, we get exactly the deﬁnition of prOMA: ∀n ∈N ∃Sn ∀x ∈ΠYES ∀r A(x, r, Sn(Ax,r)) = 1, ∀x ∈ΠNO ∀Sn Pr r [A(x, r, Sn(Ax,r)) = 1] < 1/2. Therefore, if NP ⊆P/poly, then prAM ⊆prOMA. In [CR11], it is shown that under the same premises PH = PprAM and hence the claim follows by combining these results."
  },
  {
    "prompt": "Understanding the computational aspects of Merlin, Theorem, NOP.",
    "response": "20 B Promise Oblivious Merlin–Arthur Protocols are in Promise OP 2 The following proof essentially repeats the proof of [CR06, Theorem 3], which says that MA ⊆ NOP 2, we need to verify that it holds for promise problems as well, and Merlin’s advice remains oblivious if it was oblivious before, that is, prOMA ⊆prOP 2. Proposition B.1. prOMA ⊆prOP 2. Proof. Consider a promise problem Π = ΠY ˙∪ΠN ∈prOMA."
  },
  {
    "prompt": "This is a technical discussion about ΠYES, ΠNO, polynomial-time in theoretical computer science.",
    "response": "There is a polynomial-time deterministic machine A such that ∀n ∈N ∃wn ∀x ∈ΠYES ∀r A(x, r, wn)) = 1, (1) ∀x ∈ΠNO ∀w Pr r [A(x, r, w) = 1] < 1/2."
  },
  {
    "prompt": "Understanding the computational aspects of Condition, NOP, ONP.",
    "response": "(2) Condition (2) can be replaced by ∀n ∈N ∃r′ n ∀x ∈ΠN ∩{0, 1}n ∀w ∈{0, 1}p(n) A(x, w, r′ n) = 0 (2’) because of the Adleman’s trick (similarly to the treatment of RP ⊆ONP in [GM15] or MA ⊆ NOP 2 in [CR06]): given x and w, the new veriﬁer can apply the old one as A(x, w, ri) for np(n) independent random strings ri ∈{0, 1}t(n) in order to reduce the error from 1 2 to 1 2 · 1 2np(n) ."
  },
  {
    "prompt": "Understanding the computational aspects of tnp, Access, pair.",
    "response": "Since for every pair (x, w) there are at most a 1 2 · 1 2np(n) fraction of strings r′ n ∈{0, 1}tnp(n) results in an error, there exists r′ that satisﬁes (2’). It does not harm the condition (1) as well. Therefore, Merlin’s proof w remains input-oblivious, whereas Arthur’s universal random string r′ n is input-oblivious and w-oblivious. C Loose Oracle Access vs Access Through a Consistent Language One must be very careful when arguing about oracle access to promise classes."
  },
  {
    "prompt": "Explain a concept involving access, loose, language in computational complexity.",
    "response": "Several modes of access have been considered in the literature including guarded access [LR94], loose access (e.g., [CR11, HS15]), and access through a consistent language [GS88, BF99]. In this paper we use the same mode of access as in [CR11]. Fortunately, when the underlying computational model is a polynomial-time Turing machine [with a speciﬁc polynomial-time alarm clock], loose access is equivalent to access through a consistent language."
  },
  {
    "prompt": "Read and understand this section about ΠNO, clear, equivalence from a research paper.",
    "response": "In order to make this connection very clear, below we deﬁne both of them rigorously and show the equivalence. Note that for other computational devices, such as those corresponding to OP 2 • or SP 2 •, the connection between the modes is much less clear. Access through a consistent language. The following deﬁnition is from [GS88]. Deﬁnition C.1 (promise problem, consistency). Consider a promise problem Π = (ΠYES, ΠNO), where ΠYES ∩ΠNO = ∅."
  },
  {
    "prompt": "Read and understand this section about ΠYES, ΠNO, language from a research paper.",
    "response": "We say that a language O is consistent with Π if ΠYES ⊆O and ΠNO ⊆O. The containment of O outside of ΠYES ∪ΠNO can be arbitrary. Let us denote it O ⊂ ∼Π. In one line of the literature [GS88, BF99], when an oracle is described as a promise problem, the following mode of access is used (access through a consistent language): the outer Turing machine queries a language L consistent with this promise problem. In particular, it is allowed to make 21 queries outside of the promise set."
  },
  {
    "prompt": "This is a technical discussion about Turing, Deﬁnition, machine in theoretical computer science.",
    "response": "However, the outer Turing machine must return a correct answer for every possible choice of L. Let us formalize it in the simplest case of PΠ: Deﬁnition C.2. A language L ∈PΠ according to the consistent language mode if there is a deterministic oracle Turing machine M• with polynomial-time alarm clock stopping it in time p(n) such that for every O ⊂ ∼Π and for every input x, MO(x) = L(x)."
  },
  {
    "prompt": "Explain a concept involving deﬁnition, observe, language in computational complexity.",
    "response": "One can observe that for every such language O, PΠ ⊆PO, therefore the following holds as well, and in fact can be considered as an alternative deﬁnition. Deﬁnition C.3 (promise problems as oracles). PΠ := T ΠYES⊆O ΠNO⊆O PO. For more details and discussion see e.g. [GS88, BF99]. Loose access. In the loose access mode, one considers oracle as a black box (physical device) that is only guaranteed to work correctly on the promise set."
  },
  {
    "prompt": "Read and understand this section about diﬀerent, query, computation from a research paper.",
    "response": "Outside this set it can produce any answer, in particular, it can give diﬀerent answers to the same query, either during one deterministic computation, or for diﬀerent witnesses if the computational model uses them, or for diﬀerent inputs. Let us deﬁne it rigorously in the simplest case of PΠ: Deﬁnition C.4."
  },
  {
    "prompt": "This is a technical discussion about queries, Turing, oracle in theoretical computer science.",
    "response": "A language L ∈PΠ according to the loose access mode if there is a deterministic oracle Turing machine M• with polynomial-time alarm clock stopping it in time p(n) such that for every input x M•(x) produces the answer L(x) if the oracle answers “yes” on queries in ΠY and answers “no” on queries in ΠN. (No hypothesis is made on its behaviour outside ΠY ∪ΠN, in particular, its answers may be inconsistent for the same query.) Equivalence for PΠ."
  },
  {
    "prompt": "This is a technical discussion about language, consistent, Fortunately in theoretical computer science.",
    "response": "Fortunately, it is very easy to verify the equivalence of these two deﬁnitions in the case of PΠ. Proposition C.5. Let Π be a promise problem. A language L belongs to PΠ according to the loose access mode if and only if it belongs to PΠ according to access through a consistent language. Proof. The “only if” direction is trivial since a consistent language is a particular case of a black box. For the “if” direction, consider M• from the deﬁnition of access through a consistent language."
  },
  {
    "prompt": "This is a technical discussion about table, mode, maintain in theoretical computer science.",
    "response": "To make it tolerant to the loose access mode, maintain a table of oracle answers, and if M• attempts to repeat the same query, use the answer from the table instead. The deﬁnition of the machine will not change if its oracle is a language, thus it will still produce the same answer."
  },
  {
    "prompt": "Scientific context on oracle, behaviour, blackbox and their role in graph theory.",
    "response": "On the other hand, its behaviour on input x when given a blackbox oracle is the same as its behaviour when given the following language Lx as an oracle: Lx = ΠY ∪Yx, where Yx is the set of all queries for which the blackbox oracle gave the answer “yes” when asked for the ﬁrst time. Since Lx is consistent with Π, MLx(x) = L(x) according to the deﬁnition of access through a consistent language. 22"
  },
  {
    "prompt": "Scientific context on Algebraic, Equations, March and their role in graph theory.",
    "response": "A Galois-Theoretic Complexity Measure for Solving Systems of Algebraic Equations Timothy Duff March 25, 2025 Abstract Motivated by applications of algebraic geometry, we introduce the Ga- lois width, a quantity characterizing the complexity of solving algebraic equations in a restricted model of computation allowing only field arith- metic and adjoining polynomial roots."
  },
  {
    "prompt": "Explain a concept involving optimization, statistics, Galois in computational complexity.",
    "response": "We explain why practical heuristics such as monodromy give (at least) lower bounds on this quantity, and dis- cuss problems in geometry, optimization, statistics, and computer vision for which knowledge of the Galois width either leads to improvements over standard solution techniques or rules out this possibility entirely. Contents 1 Introduction 1 2 Galois Width 3 2.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Algebraic Computation Model . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Understanding the computational aspects of Algebraic, Solving, Properties.",
    "response": ". . . 5 2.3 Properties of Galois Width . . . . . . . . . . . . . . . . . . 7 3 Solving Algebraic Equations 9 3.1 Branched Covers and Thin Sets . . . . . . . . . . . . . . . . 9 3.2 Homotopies and Optimal Solving . . . . . . . . . . . . . . . 13 4 Applications 17 4.1 Metric Algebraic Geometry . . . . . . . . . . . . . . . . . . 17 4.2 Algebraic Statistics . . . . . . . . . . . . . . . . . . . . . . . 21 4.3 Algebraic Vision . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "prompt": "Read and understand this section about Introduction, Galois, theory from a research paper.",
    "response": "24 1 Introduction Galois theory, since its inception, has provided a powerful toolkit for an- swering questions of computability and complexity in algebra. Building on the classical impossibility theorems, more recent works span a diverse ar- ray of applications, including complex dynamics [24], computer vision [72], enumerative geometry [38], graph drawing [6], and optimization [5]."
  },
  {
    "prompt": "Scientific context on Mar, arXiv, math.AG and their role in graph theory.",
    "response": "1 arXiv:2503.17884v1 [math.AG] 22 Mar 2025 In this paper, our main motivation is the increasingly important role of Galois theory in applications of algebraic geometry—especially for prob- lems where the goal is to compute solutions of a system of algebraic equa- tions."
  },
  {
    "prompt": "This is a technical discussion about practice, solutions, system in theoretical computer science.",
    "response": "In practice, these equations are multivariate polynomials or rational functions with coefficients in Q, and the number of complex solutions (the degree of the system) has often been proposed as a measure of a problem’s complexity. Section 4 recalls such problems in detail. The purpose of this paper is to introduce a Galois-theoretic refinement of the degree that better reflects the practical complexity of solving."
  },
  {
    "prompt": "This is a technical discussion about solving, alge, complexity in theoretical computer science.",
    "response": "In contrast to existing theories, which focus on upper bounds for the com- plexity of solving, our framework naturally facilitates complexity lower bounds on the complexity of solving in a simplified, yet plausible, alge- braic model of computation. At the same time, for some highly-structured problems, the Galois width is smaller than a priori upper bounds suggest, which can be exploited to develop more efficient algorithms."
  },
  {
    "prompt": "Scientific context on Galois, Section, work and their role in graph theory.",
    "response": "Section 2 contains the definitions and results at the core of this work. We define the Galois width gw(α) of an algebraic number α ∈Q, a min- imax measure of how much a solution costs in our computational model. A similar cost gw(G) may be defined for any finite group G. Theorem 2 states that the Galois width of α and the Galois group of its minimal poly- nomial coincide."
  },
  {
    "prompt": "Explain a concept involving Theorem, computed, explicitly in computational complexity.",
    "response": "The latter can be computed explicitly using composition series and minimal permutation degrees of simple groups (Theorem 3.) Before turning to illustrate our theory on examples, we first describe in Section 3.1 a general setup to which they all adhere. We propose to study gw(•) for concrete systems of equations via Galois group compu- tation, including heuristics based in both exact computation (eg. Frobe- nius elements) and inexact numerical methods (eg."
  },
  {
    "prompt": "This is a technical discussion about Galois, monodromy, group in theoretical computer science.",
    "response": "monodromy.) For the latter, Corollary 2 uses Hilbert’s irreducibility theorem [76, Ch. 3] to precisely state the following principle: the Galois/monodromy group of a parametrized system of algebraic equations provides a lower bound on the Galois width of its solutions for a dense subset of rational parameters. We note two limitations of the aforementioned Galois group heuristics: (1) they might only identify a subgroup of the full Galois group, and (2) the numerical methods are not certified."
  },
  {
    "prompt": "Read and understand this section about heuristics, inaccessible, Limitation from a research paper.",
    "response": "Nevertheless, heuristics offer insight into examples that might otherwise be inaccessible. On the other hand, as long as our goal is to obtain lower bounds on gw(•), then its monotonicity (Theorem 4) suggests that Limitation (1) need not be a concern in practice. Limitation (2) can also be addressed with rigorous, certified numerical path-tracking—see eg. [42, 33] and references therein."
  },
  {
    "prompt": "Explain a concept involving optimal, Section, Galois in computational complexity.",
    "response": "Our work is close in spirit to recent studies of decomposable branched covers, and various proposals for “optimal” numerical homotopy contin- uation methods. Section 3.2 reviews this related work in the context of Galois width. We must also mention the work [72], discussed further in Section 4.3, which inspired our computational model. To the best of our knowledge, Galois width has not been explicitly studied in the literature."
  },
  {
    "prompt": "This is a technical discussion about Section, algebraic, Galois in theoretical computer science.",
    "response": "The paper concludes by discussing three topics where Galois width has, at least in hindsight, already played an important role: metric algebraic geometry (Section 4.1), algebraic statistics (Section 4.2), and algebraic vision (Section 4.3.) We hope the perspective and tools developed here will aid readers pursuing their own applications. 2 2 Galois Width 2.1 Preliminaries We first establish notation and state (mostly standard) results that will play a key role in what follows."
  },
  {
    "prompt": "This is a technical discussion about field, number, extension in theoretical computer science.",
    "response": "We denote by Q ⊂C the usual field of algebraic numbers. An intermediate field Q ⊂K ⊂Q is called a number field if K/Q is a finite extension. Many of our results will remain intact after replacing Q/Q with more a general field extension. However, this level of generality is not necessary for the applications given later. In our setting, the compositum KL of two number fields K and L is the intersection of all subfields of Q containing both K and L."
  },
  {
    "prompt": "Read and understand this section about L-linear, written, combination from a research paper.",
    "response": "Any element of KL may be written as an L-linear combination of elements in K. Proposition 1. Let F, K, L be number fields with F ⊂K. Then [K : F] ≥[KL : FL]. Proof. Let α be a primitive element for K/F, so that the powers αi, for i = 0, . . . , [K : F] −1, form a F-basis for K. Promoting these powers under the inclusion map K ,→KL produces a spanning set for KL over FL. We now state a useful group-theoretic fact. The argument relates to the notion of a stabilizer chain (see eg."
  },
  {
    "prompt": "Read and understand this section about subgroups, chain, Lemma from a research paper.",
    "response": "[15, §1.13].) Lemma 1. Let G ≤Sd be a transitive permutation group. Then there exists a chain of subgroups G = H0 ≥H1 ≥H2 ≥. . . ≥Hd = id such that d = [H0 : H1] > [Hi : Hi+1] for all i = 1, . . . , d −1. Proof. Set W0 = Sd, and for i = 1 . . . , d, define subgroups Wi = i\\ j=1 StabSd(i) ≤Sd, (1) Hi = Wi ∩G ≤G. (2) We claim that the chain G = H0 ≥H1 ≥. . . ≥Hd = id (3) satisfies the desired conclusion. First note [H0 : H1] = d by the orbit- stabilizer theorem."
  },
  {
    "prompt": "Understanding the computational aspects of comparing, corre, sponding.",
    "response": "Then, comparing the chain in G against the corre- sponding chain in Sd, namely Sd = W0 ≥W1 ≥. . . ≥Wd = id, (4) we bound the remaining indices for i = 1, . . . , d: [Hi : Hi+1] = |Wi ∩G| |Wi+1 ∩G| = |Wi+1G| |WiG| · |Wi||G| |Wi+1||G| = |Wi+1G| |WiG| · [Wi : Wi+1] ≤[Wi : Wi+1] = (d −i)! (d −i −1)! = d −i < d. 3 Corollary 1. Let G be a finite simple group with a subgroup H ⊂G. Then there exists a chain of subgroups G = H0 ≥H = H1 ≥H2 ≥. . ."
  },
  {
    "prompt": "Read and understand this section about conclusion, Proof, homomorphism from a research paper.",
    "response": "≥Hd = id such that [H0 : H1] > [Hi : Hi+1] for all i = 1, . . . , d −1. Remark 1. More generally, although we will not need it, the conclusion of Corollary 1 holds when H is core-free in G, meaning ∩g∈gg−1Hg = id . Proof. Set d = [G : H1], and note that G acts transitively on left cosets of H, giving a group homomorphism G →Sd. Since G is simple, we may identify it with its image under this homomorphism, and H1 with the stabilizer of a point. The conclusion now follows as in Lemma 1."
  },
  {
    "prompt": "Understanding the computational aspects of permutation, work, interplay.",
    "response": "The interplay between abstract finite groups and their permutation representations is a reoccurring theme in this work. In particular, we will need to collect some useful results on the well-studied topic of minimum faithful permutation degrees. Definition 1. For any finite group G, we define µ(G) to be the minimum degree of a faithful permutation representation of G. That is, µ(G) = min {d ∈Z≥0 | ∃φ ∈Hom(G, Sd) w/ ker φ = id} . Such a representation is minimal if its degree is µ(G)."
  },
  {
    "prompt": "Understanding the computational aspects of Faithful, permutation, subgroup.",
    "response": "Faithful permutation representations remain faithful when restricted to subgroups: thus, if H ≤G is any subgroup, we have µ(H) ≤µ(G). (5) One should bear in mind that a minimal faithful permutation representa- tion φ : G →Sd need not give a transitive subgroup of Sd. For example, if G = Z/2Z × Z/3Z, then µ(G) = 5, and if φ : G →S5 is minimal then φ(G) is generated by disjoint cycles of length 2 and 3. Fortunately, when G is simple, minimality implies transitivity. Proposition 2."
  },
  {
    "prompt": "Scientific context on min, Proposition, Theorem and their role in graph theory.",
    "response": "If G is a finite simple group with µ(G) = d, then µ(G) = min M<G maximal[G : M] = min H<G [G : H], and any minimal faithful representation of G is transitive. A short proof of Proposition 2 is given in [52, Proposition 1 (i)]. In contrast to the case of subgroups, the behavior of µ(•) for quotients is more subtle. This behavior was studied by Kov´acs and Praeger, who proved the following result. Theorem 1. ([56, Theorem 1]) Let H be a finite group with a normal subgroup N ⊴H."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, simple, nontrivial.",
    "response": "If H/N has no nontrivial abelian normal subgroup (in particular, if H/N is simple), then µ(H/N) ≤µ(H). Theorem 1 is key to our proof of Theorem 4. Finally, we will use the following criterion recognizing when a transitive group is full-symmetric. Lemma 2. Let G ⊂Sd act transitively on {1, . . . , d}. If Sk × Sd−k ⊂G, where 1 ≤k < d/2, then G = Sd. 4 Proof. Maximal subgroups of Sd which are intransitive are precisely those of the form Sk × Sd−k, where 1 ≤k < d/2. See eg. [15, Theorem 4.8]."
  },
  {
    "prompt": "This is a technical discussion about Remark, Lemma, Computation in theoretical computer science.",
    "response": "Remark 2. The conclusion of Lemma 2 fails when n is even, k = d/2, and G = Sk ≀S2 is the wreath product containing all permutations in Sd that preserve the partition {1, . . . , k} ⊔{k + 1, . . . , d}. 2.2 Algebraic Computation Model We now address the question of how to define the complexity of an alge- braic number α ∈Q. To answer this question, we must define a model of computation and a class of admissible algorithms. We begin with a slightly informal description."
  },
  {
    "prompt": "This is a technical discussion about field, execute, finite in theoretical computer science.",
    "response": "Our algorithms will execute a finite number of steps m, and maintain a working field Fk ⊂Q for each step k = 1, . . . , m. We initialize F0 ←Q. At each step k ≥1, an algorithm can perform one of the following two “oracle steps”: 1. Perform arithmetic in the field Fk−1 (and set Fk ←Fk−1), or 2. Compute a root αk ∈Q of some polynomial pk(x) ∈Fk−1[x], and extend the working field Fk ←Fk−1(αk). Finally, the output of the algorithm is given by αm."
  },
  {
    "prompt": "Explain a concept involving type, operation, definition in computational complexity.",
    "response": "We note that any arithmetic operation of type 1 is in fact a special case of the root-finding operation of type 2; indeed, for any operation op ∈{+, −, ×, ÷}, the polynomial x −(α op β) ∈Fk−1[x] has a unique root given by α op β. The next definition, although more abstract, captures all of the essen- tial data needed to analyze the complexity of an algorithm in this model. It is exactly the same as in [72, Definition 2.2]. Definition 2. Let α ∈Q."
  },
  {
    "prompt": "Explain a concept involving field, algorithm, tower in computational complexity.",
    "response": "An algorithm is tower of field extensions Q = F0 ⊂F1 ⊂F2 ⊂· · · ⊂Fm ∋α. (6) We will measure the cost of the k-th step in algorithm by the degree of the working field extension [Fk : Fk−1]."
  },
  {
    "prompt": "Read and understand this section about working, field, cost from a research paper.",
    "response": "We point out out that, in order to obtain an interesting theory, it is crucial to allow the working field to depend on the results of previous computations; if we were to work in the simpler setting where the working field is always the rational numbers, then the cost of an algebraic number would just be its degree over Q. We now propose measuring the total cost of an algorithm in this model by its most costly step(s)."
  },
  {
    "prompt": "This is a technical discussion about stand, step, definition in theoretical computer science.",
    "response": "We would like, then, for any α ∈Q, to under- stand which algorithms computing α minimize the most costly step. This leads to the following definition. Definition 3. The Galois width of α ∈Q is the quantity gw(α) = min algorithms Q=F0⊂···⊂Fm∋α \u0012 max 0≤i<m [Fi+1 : Fi] \u0013 . We now define a parallel notion for a finite group G. We define the cost of a chain of subgroups G ≥H0 ≥. . . ≥Hn 5 to be the maximum value of the indices [Hi : Hi+1] for i = 0, . . . , n −1."
  },
  {
    "prompt": "Understanding the computational aspects of chain, strict, subgroups.",
    "response": "To compute the cost of such a subgroup chain, we may assume without loss of generality that all inclusions are strict, Hi > Hi+1. The length of the chain (7) is defined to be n, and we say this chain is maximal if it cannot be refined to a longer chain by inserting strict subgroups. Definition 4. The Galois width of a finite group G is the minimum cost taken over all maximal chains of subgroups G = H0 > . . . > Hn = id . (7) We denote this quantity by gw(G). We now relate Definitions 3 and 4."
  },
  {
    "prompt": "This is a technical discussion about Theorem, Definition, Galois in theoretical computer science.",
    "response": "Theorem 2. If f ∈Q[x] is an irreducible polynomial with Galois group G, and α is any root of f, then gw(α) = gw(G). Remark 3. Although the Galois correspondence makes Theorem 2 plau- sible, some care is required. Note that the minimum in Definition 4 is taken over a finite set, whereas the set of algorithms in Definition 3 is infinite. Moreover, the final field Fm in an algorithm computing α need not be inclusion-wise comparable with the splitting field of f. Proof."
  },
  {
    "prompt": "Scientific context on Galois, exten, sion and their role in graph theory.",
    "response": "Throughout, we let F ⊂Q denote the Galois closure of the exten- sion Q(α)/Q. First, we observe that gw(α) ≤gw(G). Indeed, under the Galois correspondence, a maximal chain of subgroups of G attaining the minimum cost gw(G) corresponds to a chain of fields Q = F0 ⊂F1 ⊂· · · ⊂Fm = F (8) such that [Fi : Fi−1] ≤gw(G) for all i. It remains to prove gw(α) ≥gw(G). Consider any tower of fields Q = F′ 0 ⊂F′ 1 ⊂· · · ⊂F′ m ∋α (9) attaining the minimum cost gw(α)."
  },
  {
    "prompt": "Explain a concept involving Galois, subgroup, chain in computational complexity.",
    "response": "Let F′ denote the Galois closure the extension F′ m/Q, with G′ its Galois group. Since F ⊂F′, the Galois correspondence implies that G ∼= G′/N ′ for some normal subgroup of N ′ ⊴G′. Corresponding to (9) are both the subgroup chain in G′ given by G′ = H′ 0 ≥· · · ≥H′ m (10) as well as the subgroup chain in G given by G = H0 ≥· · · ≥Hm, (11) where Hi := N ′H′ i/N ′. The latter chain corresponds to the tower of fields Q = eF0 ⊂eF1 ⊂· · · ⊂eFm, (12) where eFi = F′ i ∩F."
  },
  {
    "prompt": "Read and understand this section about eFm, chain, suffice from a research paper.",
    "response": "Since Hi ∼= H′ i/(H′ i ∩N), we have [Hi : Hi+1] = [H′ i : H′ i+1] [H′ i ∩N ′ : H′ i+1 ∩N ′] ≤[H′ i : H′ i+1] ≤gw(α), 6 and it will suffice to prove that the subgroup chain (11) has cost at least gw(G). To do so, we extend the tower (12) as follows: let β be a primitive element for the extension eFm/Q, and γ = g·β one of its Galois conjugates. We may then form the tower of composite extensions Q(β) = eFm ⊂eFm \u0010 g · eF1 \u0011 ⊂· · · ⊂eFm \u0010 g · eFm \u0011 = Q(β, γ)."
  },
  {
    "prompt": "Scientific context on eFi, eFm, Proposition and their role in graph theory.",
    "response": "(13) By Proposition 1, we have h eFm \u0010 g · eFi+1 \u0011 : eFm \u0010 g · eFi \u0011i ≤ h g · eFi+1 : g · eFi i = h eFi+1 : eFi i . If Q(β, γ) ⊊F, we construct yet another chain as in (13), but starting from Q(β, γ). Continuing this process, we eventually obtain a tower of fields from Q to F, corresponding to an extension of the subgroup chain (11), G = H0 ≥· · · ≥Hm ≥· · · ≥Hl = id, (14) with the property that [Hi : Hi+1] ≤gw(α) for all i ≥m."
  },
  {
    "prompt": "Explain a concept involving cost, chains, Galois in computational complexity.",
    "response": "Thus, the cost of both subgroup chains (11) and (14) is at most gw(α). Since the cost of (14) is at least gw(G), this completes the proof. 2.3 Properties of Galois Width Having now established the Galois width as the principal quantity of in- terest in our computation model, we turn to the problem of computing this quantity."
  },
  {
    "prompt": "Understanding the computational aspects of Definition, formulation, optimization.",
    "response": "Since Definition 4 is essentially a minimax formulation, it would be natural to consider methods from combinatorial optimization— and indeed, a subgroup chain in G attaining the minimum cost gw(G) may in principal be found using network flow on the lattice of subgroups. However, the graph of all subgroups (or even conjugacy classes thereof) is much too cumbersome to work with in practice. Thus, it is desirable to express gw(G) in terms of group-theoretic primitives."
  },
  {
    "prompt": "Scientific context on Theorem, presses, permu and their role in graph theory.",
    "response": "Theorem 3 ex- presses gw(G) in terms as the maximum over all minimal faithful permu- tation degrees of the composition factors of G. We also use this section to prove Theorem 4, a monotonicity property for the Galois width. We begin with a key splitting property. Proposition 3. For any normal subgroup N ⊴G, we have that gw(G) = max (gw(G/N), gw(N)) . (15) Proof. First, we argue that the left-hand-side in (15) is bounded by the right-hand-side."
  },
  {
    "prompt": "Explain a concept involving chain, maximal, cost in computational complexity.",
    "response": "To see this, note that the lattice isomorphism theorem identifies any maximal chain of subgroups in G/N with a chain of sub- groups G > . . . > N. Such a chain attaining the minimum cost gw(G/N) may be prepended to a maximal chain of subgroups of N which attains the minimum cost gw(N). This produces a maximal chain in G whose cost equals max (gw(G/N), gw(N)). Thus gw(G) ≤max (gw(G/N), gw(N)) . We now prove the reverse inequality."
  },
  {
    "prompt": "Explain a concept involving chain, cost, maximal in computational complexity.",
    "response": "Consider a maximal chain (7) whose 7 cost attains the minimum gw(G). We may “reroute” this chain through N using intersections and subgroup products as follows: G = NH0 ≥. . . ≥NHn = N = H0 ∩N ≥. . . ≥Hn ∩N = id. The cost of this rerouted chain is at least max (gw(G/N), gw(N)) . From the product formula [Hi : Hi+1] = [Hi ∩N : Hi+1 ∩N][NHi : NHi+1], and the bound xy ≥max(x, y), valid for any positive integers x and y, it follows that the rerouted chain also attains the minimum cost gw(G)."
  },
  {
    "prompt": "This is a technical discussion about Galois, max, group in theoretical computer science.",
    "response": "Thus max (gw(G/N), gw(N)) ≤gw(G). Proposition 3 shows that the Galois width is determined by its values on simple groups. Moreover, when the group G is simple, it turns out that gw(G) equals the minimum (faithful) permutation degree µ(G). This in turn implies a clean formula for the Galois width of any finite group G."
  },
  {
    "prompt": "Understanding the computational aspects of tors, composition, Recall.",
    "response": "Recall that the Jordan-H¨older Theorem states that G always has a composition series: that is, a chain G = N0 ⊵N1 · · · ⊵Nn = id (16) such that all successive quotient groups Ni/Ni+1, called composition fac- tors, are simple. The chain (16) is subnormal: ie., Nj need not be normal in Ni for i < j −1. Multiple composition series for a group may exist; however, the multiset of composition factors is uniquely determined. Theorem 3."
  },
  {
    "prompt": "Read and understand this section about max, series, subgroup from a research paper.",
    "response": "If G is any finite group with composition series (16), then gw(G) = max 0≤i≤n−1 µ(Ni/Ni+1). (17) Proof. It is enough to prove the result when G is simple—the general case then follows by repeated application of Proposition 3. Let M < G be a maximal subgroup. Applying Corollary 1, we have that gw(G) ≤[G : M]. (18) Moreover, the reverse inequality must hold for any maximal subgroup M which appears in a chain attaining the minimum cost gw(G). Such an M is clearly a subgroup of minimum index in G."
  },
  {
    "prompt": "This is a technical discussion about Proposition, conclude, groups in theoretical computer science.",
    "response": "Thus, using Proposition 2, we conclude that gw(G) = [G : M] = µ(G). Theorem 3 allows us to easily evaluate gw(G) for many familiar families of groups. For example, if G is solvable, then gw(G) equals the largest prime dividing the group order |G|. For the symmetric and alternating groups, we have the composition series Sn ⊵An ⊵id. Recall that, when n ≥5, the group An is simple and has no subgroup of index less than n."
  },
  {
    "prompt": "Scientific context on hand, chain, roots and their role in graph theory.",
    "response": "For A4, on the other hand, we have the subgroup chain A4 ≥Z/2Z × Z/2Z ≥Z/2Z ≥id, 8 which attains the minimum cost of 3. Altogether, this gives gw(Sn) = gw(An) = ( 3 if n = 4, n else. (19) For a Galois-theoretic interpretation of formula (19), recall that the roots of a general univariate quartic equation may be expressed in terms of the roots of its resolvent cubic and additional square roots thereof. For larger values of n, no such reduction is generally possible."
  },
  {
    "prompt": "Understanding the computational aspects of inclusion, prove, monotonic.",
    "response": "We now prove that gw(•) is monotonic with respect to inclusion. Theorem 4. gw(H) ≤gw(G) for any subgroup H ≤G. (20) Proof. We induct twice: first on the order of G, then on the order H. The base cases where G or H are trivial follow trivially. Consider first the case where H contains a nontrivial normal subgroup of G, say N ⊴G. Then H/N ≤G/N, and we have gw(H) = max (gw(H/N), gw(N)) (by Proposition 3) ≤max (gw(G/N), gw(N)) (by induction) = gw(G)."
  },
  {
    "prompt": "This is a technical discussion about case, Theorem, generally in theoretical computer science.",
    "response": "More generally, suppose N ′ ⊴G is a nontrivial normal subgroup of G, and set N = N ′ ∩H. Either N ̸= id, in which case the previous argument works, or H ∼= N ′H/N ′. The latter case yields gw(H) = gw(N ′H/N ′) ≤gw(N ′H) ≤gw(G), where the last inequality follows from the case previously considered. It remains to treat the case where G is simple. Taking now N ⊴H to be a proper normal subgroup with H/N simple, we then have gw(H/N) = µ(H/N) (Theorem 3) ≤µ(H) (Theorem 1) ≤µ(G) (eq. (5)) = gw(G)."
  },
  {
    "prompt": "Explain a concept involving Theorem, induction, Equations in computational complexity.",
    "response": "(Theorem 3) By induction, gw(N) ≤gw(G). Thus, combining Proposition 3 and the inequality above, we conclude that gw(H) = max(gw(N), gw(H/N)) ≤gw(G). 3 Solving Algebraic Equations 3.1 Branched Covers and Thin Sets In this section, we pass from univariate polynomials to multivariate sys- tems of equations. For a more detailed exposition of Galois groups arising in polynomial system solving, we recommend the recent survey [78]."
  },
  {
    "prompt": "This is a technical discussion about points, pairs, instance in theoretical computer science.",
    "response": "9 Consider an algebraic variety X ⊂Cm × Cn whose points (p, x) are problem-solution pairs. Solutions to a particular problem instance p ∈Cm, then, may be identified with points in the fiber π−1(p), where π : X →Cm, (p, x) 7→p (21) projects X onto the space of problems. We assume: A1 For a generic problem instance p ∈Cm, there exists a finite, nonzero number of solutions, d = #π−1(p)."
  },
  {
    "prompt": "This is a technical discussion about Zariski-open, precisely, depending in theoretical computer science.",
    "response": "More precisely, for all p in some nonempty Zariski-open U ⊂Cm (depending on π), the set π−1(p) consists of d ≥1 distinct points. A2 The variety X is irreducible. In other words, the map π is a branched cover. More generally, we define a branched cover π : X 99K Z to be a dominant rational map between quasiprojective varieties of equal dimension over C. The degree of π, denoted deg(π), is the number of points #π−1(z) in a generic fiber."
  },
  {
    "prompt": "Scientific context on Assumption, essentially, well-posed and their role in graph theory.",
    "response": "Assumption A1 essentially requires that problems are well-posed. As- sumption A2, although strong, is often satisfied in practice. A common theme among the examples to be presented is that problems may be writ- ten as rational functions of their solutions, in which case X would be the graph of a rational map Cm 99K Cm. Much of our ensuing discussion could be adapted to reducible varieties by working component-by-component."
  },
  {
    "prompt": "This is a technical discussion about Gal, group, irreducible in theoretical computer science.",
    "response": "Since X is irreducible, we may consider a primitive element f for the extension of function fields C(X)/C(p1, . . . , pm), f(y; p) = yd + c1(p)yd−1 + . . . + cd(p). (22) The Galois/monodromy group Gal(π) may be defined as the Galois group of the polynomial f over C(p1, . . . , pm). Alternatively, one may define Gal(π) via the monodromy action of Gal(π) on a fiber π−1(p) of size deg(π). This is a group homomorphism π1(U; p) →Sd, where π1(•) de- notes the topological fundamental group."
  },
  {
    "prompt": "Read and understand this section about Gal, permutations, obtained from a research paper.",
    "response": "The permutations in Gal(π) are obtained by lifting paths in U based at p up to X. Numerical homotopy continuation methods that compute approximations of these lifts are the basis of popular heuristics for computing Gal(π)—see eg. [29, 69, 63, 11, 46]. Assumption A2 implies that Gal(π) acts transitively on π−1(p). A third assumption often holding in practice is that the variety X is defined by polynomials with rational coefficients: A3 The variety X is defined over Q."
  },
  {
    "prompt": "Scientific context on case, computed, principle and their role in graph theory.",
    "response": "In this case, a suitable f may be computed (at least in principle) using Gr¨obner bases [40]. Let I ⊂R = Q(p)[x1, . . . , xn] be the ideal obtained from the vanishing ideal of X by extension of scalars. This is a polynomial ideal of dimension 0 and degree d. For generic (a1, . . . , an) ∈Qn, consider the linear form y = a1x1 +. . .+anxn. We endow the polynomial ring R = Q(p)[x1, . . . , xn−1, y] with the lexicographic order x1 > · · · > xn−1 > y."
  },
  {
    "prompt": "This is a technical discussion about Galois, form, group in theoretical computer science.",
    "response": "The elimination ideal I ∩Q(p)[y] is then generated by f of the form (22). From our preceding discussion, it becomes natural to consider three types of Galois group: Gal(π), the Galois group of the univariate poly- nomial f(y; p) ∈Q(p)[y], and Galois groups of specializations f(y; ˜p) for 10 particular problem instances ˜p ∈Qm. When needed, we denote the latter two groups by GalQ(p)(f) and GalQ(f(y; ˜p)), respectively."
  },
  {
    "prompt": "This is a technical discussion about Section, veloped, applies in theoretical computer science.",
    "response": "The theory de- veloped in Section 2 applies most directly to the last of these types. In general, the three types of groups may all be different. Example 1. Consider the branched cover π : X →C1 given by X = {(p, x) ∈C1 × C1 | f(x; p) = 0}, f(x; p) = x3 −p. We have Gal(π) ∼= A3, yet the Galois group of f ∈Q(p)[x] is S3."
  },
  {
    "prompt": "Explain a concept involving Galois, case, group in computational complexity.",
    "response": "Although S3 is also typically the Galois group after specialization, this is not the case for certain unlucky specializations like ˜p = 0, in which case the Galois group is trivial. This example shows that Gal(π) and the specialized Galois groups are generally not comparable inclusion-wise. The notion of a thin set defined in [76, Ch. 3] captures the essence of “unlucky” specializations such as in the example above."
  },
  {
    "prompt": "Read and understand this section about Zariski-closed, Q-valued, set from a research paper.",
    "response": "We say a ∆⊂Qm has type C1 if it is contained in a proper Zariski-closed subset of Cm, and type C2 if there is a branched cover π : X →Cm with deg(π) ≥2 and ∆⊂π(XQ) (here XQ denotes the set of Q-valued points in X.) Definition 5. A set ∆⊂Qm is said to be thin if it is contained in a finite union of sets of types C1 or C2. Example 2. Continuing with Example 1, we show that the set ∆= {p ∈Q | GalQ(x3 −p) ̸= S3}. is thin."
  },
  {
    "prompt": "Read and understand this section about Galois, root, GalQ from a research paper.",
    "response": "If ˜p is chosen such that x3 −˜p has a rational root, then the Galois group GalQ(x3 −˜p) is contained in the subgroup S2: we define XS2 = {(p, a, b, c) ∈C4 | x3 −p = (x −a)(x2 + bx + c)}. The condition that p ∈XS2 Q may be found by equating coefficients in x, p = ac = a2b = a3. Furthermore, the map π : XS2 →C1 defined by (p, a, b, c) 7→p is a branched cover of degree 3. Thus, we see that π(XS2 Q ) ⊂Q, the set of rational perfect cubes, is thin."
  },
  {
    "prompt": "Scientific context on GalQ, discriminant, rule and their role in graph theory.",
    "response": "One may also rule out GalQ(x3 −˜p) ⊂A3 by computing the discriminant, 27p2 = 0 ⇒p = 0. Since A3 and S2 are the maximal subgroups of S3, we conclude that ∆= π(XS2 Q ) is thin. Example 2 generalizes significantly. Theorem 5. [76, Proposition 3.3.5] For f as in equation (22), there exists a thin set ∆⊂Qm such that if ˜p /∈∆, then ˜p is not a pole of any of the rational functions c1(p), . . ."
  },
  {
    "prompt": "Read and understand this section about Galois, width, group from a research paper.",
    "response": ", cd(p), the polynomial f(y; ˜p) ∈Q[y] is irreducible over Q, and its Galois group equals that of f over Q(p). We define the Galois width gw(π) of a branched cover π in the obvious manner to be the Galois width of its Galois/monodromy group Gal(π) as in Definition 4. Using Theorem 5, we may relate the three different incarnations of Galois width studied thus far. 11 Corollary 2."
  },
  {
    "prompt": "This is a technical discussion about GalQ, assumptions, property in theoretical computer science.",
    "response": "With π : X →Cm and f satisfying assumptions A1–A3 above, there exists a thin set ∆⊂Qm with the following property: for any ˜p /∈∆and any α with f(α; ˜p) = 0, we have gw (π) ≤gw \u0000GalQ(p)(f) \u0001 = gw(α). (23) Furthermore, for any ˜p ∈Qm and any α with f(α; ˜p) = 0, we have gw(α) ≤gw(GalQ(p)(f)). (24) Proof. Taking ∆to be the thin set guaranteed by Theorem 5, we have for any ˜p /∈∆that gw(α) = gw \u0000GalQ(p)(f) \u0001 . Now, Gal(π) ≤GalQ(p)(f), as explained eg."
  },
  {
    "prompt": "This is a technical discussion about Gal, GalQ, fact in theoretical computer science.",
    "response": "in [78, §1.2]; in fact, Gal(π)⊴GalQ(p)(f), since the base of π is the rational variety Cm. Thus Theorem 4 gives us the needed inequality in (23). Inequality (24) may be deduced from the proof of [76, Proposition 3.1.1], which expresses ∆as a union of thin sets for which some proper subgroup H < GalQ(p)(f) occurs as the specialized Galois group. It is worth emphasizing that the thin set appearing in Corollary 2 is “small” in multiple senses of the word."
  },
  {
    "prompt": "This is a technical discussion about density, Theorem, number-theoretic in theoretical computer science.",
    "response": "Indeed, several number-theoretic results show that thin sets are of small natural density. For instance, [76, Theorem 3.4.4] provides an asymptotic count of integral points in ∆, # (∆∩Zm ∩[−N, N]) = O \u0010 N m−1 2 log N \u0011 ≪O(N m). (25) Corollary 2 also offers takeaways for practitioners of both symbolic and numerical computation. In the symbolic approach, a family of prob- lems may be studied by fixing a particular problem instance ˜p ∈Qm and computing GalQ(f(x; ˜p))."
  },
  {
    "prompt": "Explain a concept involving Theorem, Stauduhar, Galois in computational complexity.",
    "response": "When the latter can be carried out algo- rithmically (eg. using Stauduhar’s method [79]), either of the inequalities in Theorem 5 may then provide information; the Galois width for a spe- cific problem instance provides a lower bound for the generic case ˜p /∈∆. In other cases, heuristics such Frobenius elements may be used to show GalQ(p)(f) = Sd (see eg. Example 8.) Numerical approaches typically compute a subgroup of monodromy permutations H ≤Gal(π)."
  },
  {
    "prompt": "Read and understand this section about Gal, guarantees, certify from a research paper.",
    "response": "One may also hope for equality; the branch point method of [46] guarantees this, although it may be slower than other heuristics and more difficult to certify. Once more, Theorem 4 shows that even a partial computation of Gal(π) can provide a useful lower bound on the complexity of solving a generic problem instance. We note that inequality (24) may also be applied when the equality gw(GalQ(p)(f)) = gw(Gal(π)) is known a priori."
  },
  {
    "prompt": "Read and understand this section about Gal, Galois, deg from a research paper.",
    "response": "This often arises when deg(f) ≥5 and GalQ(p)(f) is full-symmetric; this forces Gal(π) to either be full-symmetric or full-alternating, and in either case the Galois widths are equal. Numerical methods have (at least) one decisive advantage over sym- bolic computation: complete equations describing X are not needed, much less the primitive element (22)."
  },
  {
    "prompt": "Read and understand this section about equa, tions, locally from a research paper.",
    "response": "All that we need in practice are n equa- tions cutting out X locally (these may be polynomial or rational) and some method for sampling generic points on X. 12 3.2 Homotopies and Optimal Solving In the homotopy continuation literature, there have been various propos- als of “optimal homotopies”. Since rigorous analysis of numerical path- tracking is very difficult, the usual objective is to minimize number of so- lution paths tracked."
  },
  {
    "prompt": "This is a technical discussion about complexity, satisfactory, coarse in theoretical computer science.",
    "response": "Even for such a coarse measure of complexity, none of the various proposals is completely satisfactory. Our present study was partially motivated by the author’s desire to settle the question, once- and-for-all, of what makes a homotopy optimal. This section is partly a reflection on his failure to find clear answers. Polyhedral homotopies constitute a well-studied class of homotopies sometimes considered to be optimal."
  },
  {
    "prompt": "This is a technical discussion about Laurent, equa, tions in theoretical computer science.",
    "response": "They arise when solving n equa- tions in n unknowns, where each equation is a (Laurent) polynomial with prescribed monomial support. We denote by A• = (A1, . . . , An) the sys- tem’s prescribed support sets, Ai ⊂Zn, and abuse notation by writing A• = #A1 + . . . + #An for the total number of monomials in the system. With some mild conditions on A•, we may consider the branched cover πA• : XA• →CA•, (p, x) 7→p, XA• =   (p, x) ∈CA• × (C∗)n | X α∈Ai pi,α xα = 0, 1 ≤i ≤n   ."
  },
  {
    "prompt": "Understanding the computational aspects of Newton, conv, theorem.",
    "response": "The Bernstein-Kushnirenko (BK) theorem [7, 55] writes the degree of πA• as the mixed volume of the n equations’ Newton polytopes, deg(πA•) = MV(A•) := MV(conv(A1), . . . , conv(An)). (26) Correspondingly, the polyhedral homotopy method, introduced by Huber and Sturmfels [51], involves tracking MV(A•) solution paths. This method is optimal in the sense that all paths lead to a solution for “most” systems p ∈CA•."
  },
  {
    "prompt": "Understanding the computational aspects of caveats, applications, branched.",
    "response": "However, there are two major caveats: (1) In many applications, we are led naturally to consider a branched cover (21) which is not of the form πA•. (2) Even if we are interested in πA•, it may be possible to track fewer than MV(A•)-many paths by exploiting symmetry or decomposabil- ity into branched covers of lower degree. In both of these scenarios, the BK theorem typically overestimates both the Galois width and the number of homotopy paths needed."
  },
  {
    "prompt": "Scientific context on Khovanskii, Similar, con and their role in graph theory.",
    "response": "Similar caveats apply to various other root-counting technologies used to con- struct start systems for homotopy continuation: namely, extensions of the BK theorem via methods based on Khovanskii bases [14, 8] or tropi- cal geometry [47, 64], and most of the classical approaches cataloged in [77, Ch. 8]. In contrast, the parameter homotopy paradigm of [77, Ch. 7] is well-adapted to the setting of this paper, and gw(•) may be viewed as a general complexity measure for this approach."
  },
  {
    "prompt": "Read and understand this section about ters, Caveat, geome from a research paper.",
    "response": "Caveats (1) and (2) above are well-known to applied algebraic geome- ters. For example, the maximum-likelihood estimation problems of Sec- tion 4.2 provide one notable illustration of Caveat (1), as shown by the 13 problems in [45, Table 1] where BK overcounts by multiple orders of mag- nitude. Caveat (2) was addressed in work of Brysiewicz et al. [12], which showed how polyhedral homotopy could be made more efficient by ex- ploiting special structure in the support sets A•."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, Gal, terov.",
    "response": "Prior to this work, Es- terov [36, Theorem 6] characterized the supports for which Gal(πA•) is full-symmetric, ie. the symmetric group acting on MV(A•) letters. We recall a reformulation of this result in the terminology of [12, 78]. The support sets A1, . . . , An ⊂Zn are said to be strictly lacunary if they span a proper sublattice of Zn of index less than deg(πA•)."
  },
  {
    "prompt": "Understanding the computational aspects of triangular, strictly, spanning.",
    "response": "We also say A• is triangular if there exists a proper subset of supports AI = {Ai | i ∈I}, |I| = k < n, spanning a sublattice in Zn of rank k; if this holds, then up to unimodular transformation, we may assume then that AI ⊂Zk, and say that A• is strictly triangular if 1 < MV(AI) < MV(A•). Theorem 6. (Esterov, [36]) If MV(A•) > 0, then Gal(πA) is full-symmetric if and only if A• is neither strictly triangular nor strictly lacunary. Example 3."
  },
  {
    "prompt": "Understanding the computational aspects of vector, conv, i-th.",
    "response": "Let Ai = {0, 2ei}, where 0, ei ∈Zn are the zero and i-th standard basis vector, for i = 1, . . . , n. This encodes the sparse system p1,1x2 1 + p1,2 = . . . = pn,1x2 n + pn,2 = 0. (27) conv(Ai) is a segment, and MV(A•) is the volume of the zonotope conv(A1) + . . . + conv(An) = [0, 2]×n, (28) as expected since deg(πA•) = 2n. In this case, the supports A• are both strictly lacunary and strictly triangular. The function field C(XA•) is a Galois extension of C(CA•), and Gal(πA•) ∼= (Z/Z2)×n."
  },
  {
    "prompt": "Understanding the computational aspects of Galois, deg, width.",
    "response": "For this example, the gap between Galois width and degree is exponential in n: gw(πA•) = 2 < 2n = deg(πA). The two conditions appearing in Theorem 6 reflect two birational in- variants of a general branched cover related to its Galois width: namely, the group of deck transformations and decomposability. Definition 6. Let π : X 99K Z be a branched cover. A rational map Ψ : X 99K X is said to be a deck transformation if π(Ψ(x)) = π(x) for all x in a common domain of definition."
  },
  {
    "prompt": "Explain a concept involving deck, Gal, decomposition in computational complexity.",
    "response": "If π1 : X 99K Y and π2 : Y 99K Z are branched covers of degree strictly less than deg(π) with π = π2 ◦π1 on a common domain of definition, we say that π is decomposable, and say that the πi give a decomposition of π. The deck transformations form a group Deck(π) with respect to com- position; if this group is nontrivial, then X 99K X/ Deck(π) 99K Z is a decomposition unless Gal(π) is a cyclic group of prime order, in which case also Gal(π) ∼= Deck(π)."
  },
  {
    "prompt": "Scientific context on Gal, Galois, Deck and their role in graph theory.",
    "response": "When Gal(π) ∼= Deck(π), we say that π is a Galois cover. Many previous works have utilized either deck transforma- tions or decompositions, at least implicitly, to speed up numerical homo- topy continuation methods. One may consult [77, §7.6] for a discussion in 14 the context of parameter homotopy. Recent articles such as [12, 32, 31, 2] provide further applications."
  },
  {
    "prompt": "Scientific context on Gal, Proposition, Galois and their role in graph theory.",
    "response": "Much like Proposition 3, there is a clean splitting principle that relates the Galois widths of a branched cover and its various decompositions. Proposition 4. If π = π2 ◦π1 is a decomposition, then gw (π) = max (gw (π1) , gw (π2)) . Proof. First, note that Gal(π2) is a homomorphic image of Gal(π) arising from the action of Gal(π) on fibers π−1 2 (z). Thus, Proposition 3 implies gw (Gal(π2)) ≤gw (Gal(π))."
  },
  {
    "prompt": "Understanding the computational aspects of Gal, Theorem, Proposition.",
    "response": "Similarly, fixing y ∈π−1 2 (z), Gal(π1) is a homomorphic image of the subgroup of Gal(π) that stabilizes π−1 1 (y) setwise. Thus, applying now both Proposition 3 and Theorem 4, we obtain that gw (Gal(π1)) ≤gw (Gal(π)) . Finally, since Gal(π) is a subgroup of the wreath product Gal(π1) ≀Gal(π2) = Gal(π1)deg(π2) ⋊Gal(π2), we may use Proposition 3 and Theorem 4 to deduce that gw(Gal(π)) ≤max (gw (Gal(π1), Gal(π2))) ."
  },
  {
    "prompt": "Understanding the computational aspects of Proposition, Esterov, Theorem.",
    "response": "Now, briefly returning to the setting of sparse systems, we note a corollary of Esterov’s Theorem 6 and Proposition 4. Corollary 3. The equivalent conditions of Theorem 6 are satisfied if and only if gw (πA•) = MV(A•). This follows since if A• is either strictly lacunary or strictly triangular, then πA• is decomposable, as observed in [12]. Galois groups in the de- composable cases are not known, despite continued attention and partial results [37]."
  },
  {
    "prompt": "Explain a concept involving Galois, Determining, manageable in computational complexity.",
    "response": "Determining Galois widths may be more manageable. Our study of Galois width falls in line with a general philosophy that intrinsic complexity of solving algebraic equations is best measured through birational invariants of the associated branched covers. Degree, Galois width, decomposability, and deck transformations may thus all be viewed as intrinsic measures of complexity for parametric systems of polynomial equations, whereas various bounds on the degree like BK generally cannot be."
  },
  {
    "prompt": "Read and understand this section about interesting, intrinsic, studies from a research paper.",
    "response": "This can be contrasted with several interesting, yet fundamentally less intrinsic, studies of when the BK bound is tight (eg. [17, 10, 67].) From a practical point of view, many of the problems presented in Sec- tion 4, decompose into solving one very costly “bottleneck” subproblem and other, smaller, subproblems, and it is tempting to conclude that gw(•) measures not only the complexity of optimal algebraic algorithms as in Section 2, but also an optimal parameter homotopy."
  },
  {
    "prompt": "Read and understand this section about conclusion, resistance, tempting from a research paper.",
    "response": "This tempting conclusion, however, should be met with some resistance. The proof of Theorem 2 can be adapted into a complete procedure for optimally decomposing a Galois cover. Unfortunately, this procedure is impractical for all but the simplest examples, as it relies on finding irreducible components of iterated fiber products. Given a branched cover π : X 99K Z and an integer k, we recall the k-th fiber product Xk Z = {(x1, . . . , xk, z) ∈X×k × Z | π(xi) = z, 1 ≤i ≤k}."
  },
  {
    "prompt": "Read and understand this section about Note, Equation, assumes from a research paper.",
    "response": "(29) 15 (Note: the notation in Equation (29) assumes that π is clear from the context.) When 1 ≤k ≤deg(π), there is a nonempty variety X(k) Z := Xk Z \\ ∆ (30) where ∆is the “big diagonal” containing all points where xi = xj for some i ̸= j. The variety X(k) Z is irreducible if and only if Gal(π) acts transitively on ordered k-tuples of solutions (x1, . . . , xk)."
  },
  {
    "prompt": "Scientific context on Galois, deg, top-dimensional and their role in graph theory.",
    "response": "When k = deg(π), each top-dimensional irreducible component of X(k) Z \\ ∆is a variety ˆ X whose function field C( ˆ X) is a Galois closure for the extension C(X)/C(Z). The Galois covers ˆπ : ˆ X →Z are quite unlike the branched covers one usually meets in practice. Note, for example, that the ability to sample a generic point on ˆ X is the same as computing an entire fiber of π, which would obviate the need for any solving."
  },
  {
    "prompt": "This is a technical discussion about Theorem, optimal, shows in theoretical computer science.",
    "response": "Although the argument in Theorem 2 shows that decomposing ˆπ along a chain of subcovers is theoretically optimal, an ad-hoc approach analyzing the original branched cover π is far more likely to yield practical results. On the other hand, decomposing π itself may not be optimal. We close with an example from enumerative geometry which reflects some of these difficulties. Example 4. We consider the problem of computing determinantal rep- resentations of cubic surfaces."
  },
  {
    "prompt": "Understanding the computational aspects of degree, homogeneous, polynomial.",
    "response": "Given a homogeneous polynomial of degree 3, f ∈C[x0, . . . , x3]3, can we find M0, . . . , M3 ∈C3×3 with f(x0, x1, x2, x3) ∝det (x0M0 + x1M1 + x2M2 + x3M3)? If f is generic, then up to the action of GL×2 3 defined by (H1, H2) · Mi = H1MiH−1 2 , we may restrict to determinantal representations of the form M0 = I, M1 = diag(x1, x2, x3), M2 =   x4 x5 x6 x5 x7 x8 x9 x8 x10  , M3 =   x11 x12 x13 x14 x15 x16 x17 x18 x19  ."
  },
  {
    "prompt": "Scientific context on cover, determines, branched and their role in graph theory.",
    "response": "This determines a branched cover, π : X →P19, ((M0, M1, M2, M3), f) 7→f. It is known classically that a smooth cubic surface f has exactly 72 distinct orbits of determinantal representations under the GL×2 3 -action. In our chosen parametrization, we have deg(π) = 1728 = 24·72. The deck trans- formation group Deck(π) has order 48; it is generated by the map sending Mi 7→M T i and a subgroup of order 24 which preserves GL×2 3 -orbits."
  },
  {
    "prompt": "Scientific context on Deck, Galois, deg and their role in graph theory.",
    "response": "Thus, we have a decomposition π = π2 ◦π1, where π1 : X 99K X/ Deck(π) is a Galois cover with gw (π1) = 4 and π2 : X/ Deck(f) 99K P19 with deg(π2) = 36. As it turns out, π1 is not decomposable, so we might rea- sonably guess that gw (π) = gw (π2) = 36. But, in fact, gw (π) = gw (π2) = 27, due to the fact that the determinantal representations of f are rational functions of the 27 lines on f (cf."
  },
  {
    "prompt": "Read and understand this section about Algebraic, Metric, Geometry from a research paper.",
    "response": "[23, 13].) With respect to the computa- tional model in Section 2.2, using these rational functions of the 27 lines is algebraically optimal for computing these determinantal representations. 16 4 Applications 4.1 Metric Algebraic Geometry Figure 1: Minimizing the distance from a point in R3 to the surface of Example 5. The emerging field of metric algebraic geometry [9] [86] studies inter- actions between algebraic varieties and metric structures on spaces they inhabit."
  },
  {
    "prompt": "Read and understand this section about problem, algebraic, variety from a research paper.",
    "response": "One of the foundational problems in this field is solving the nearest point problem for a real algebraic variety. A general measure of algebraic complexity for this problem is the Euclidean Distance (ED) de- gree. The systematic study of the ED degree was initiated in [25], and the subject has received significant attention since. As we will illustrate, the Galois width captures more refined information about the nearest- point problem and its variants."
  },
  {
    "prompt": "Explain a concept involving Galois, highly-structured, degree in computational complexity.",
    "response": "For certain, highly-structured problems, we will see how the Galois width characterizes the difficulty of solving the nearest-point problem more accurately than the ED degree. Let X ⊂Cn be a closed, irreducible subvariety. We assume that the set of real points XR = X ∩Rn contains a smooth point x ∈Xsm. Given a data point u = (u1, . . . , un) ∈Rn, the nearest-point problem is min x∈XR ∥u −x∥2. (31) 17 Here ∥• ∥is the usual 2-norm on Rn. Note that the minimum is attained since XR ⊂Rn is closed."
  },
  {
    "prompt": "Explain a concept involving points, critical, view in computational complexity.",
    "response": "From an algebraic point of view, the minimizer x in (31) is just one of many critical points. These points can be studied using the Euclidean Distance Correspondence EX = n (u, x) ∈Cn × Xsm | (x −u) ∈(Tx(X))⊥o (32) and an associated branched cover πEX : EX →Cn, (u, x) 7→u, (33) whose fibers π−1 EX (u) are the complex-valued critical points over u. The ED degree ED(X), then, is simply the number of points in the fiber for generic u ∈Cn."
  },
  {
    "prompt": "Read and understand this section about provide, real, global from a research paper.",
    "response": "When u ∈Rn, the real points in π−1 EX (u) provide candidates for the global minimizer of (31). An additional candidate that must be checked is the closest point to u on the singular locus Xsing R . Noting the general inequality gw (πEX ) ≤ED(X), (34) we now consider an example where this inequality is strict. Example 5. Consider the irreducible surface X = {x ∈C3 | f(x) = 0}, f(x1, x2, x3) = x4 3 −(x2 1 + x2 2)3."
  },
  {
    "prompt": "Scientific context on Euclidean, data, point and their role in graph theory.",
    "response": "(35) Given a data point u = (u1, u2, u3) ∈R3, we may may minimize the squared Euclidean distance function d(x; u) = (x1 −u1)2 + (x2 −u2)2 + (x3 −u3)2. (36) over x ∈XR by introducing a Lagrange multiplier λ and writing down the first-order conditions for optimality, ∇x,λ (d + λf) =   2(x1 −u1) −3λ(x2 1 + x2 2)2x1 2(x2 −u2) −3λ(x2 1 + x2 2)2x2 2(x3 −u3) + 4λx3 3 f(x1, x2, x3)  =   0 0 0 0  ."
  },
  {
    "prompt": "Understanding the computational aspects of plane, solution, equations.",
    "response": "(37) Using the first two equations in (37), we see for generic u that any solution (x1, x2, x3) must lie in the plane x2(x1 −u1) −x1(x2 −u2) = u2x1 −u1x2 = 0. (38) Intersecting this plane with X, as shown in Figure 1, we obtain a plane curve which is the union of two touching cuspidal cubics, \u0012 x2 3 − q\u00001 + (u2/u1)2\u0001 x3 1 \u0013 \u0012 x2 3 + q\u00001 + (u2/u1)2\u0001 x3 1 \u0013 = 0. (39) The ED cover π : EX →C3 decomposes as π = π2 ◦π1, where π1 : EX →Y, π2 : Y →C3, Y = {(s, u1, u2, u3) ∈C4 | u2 1(s2 −1) −u2 2 = 0}."
  },
  {
    "prompt": "Scientific context on Note, deg, Gal and their role in graph theory.",
    "response": "18 Note that deg(π2) = 2. The cuspidal cubics have ED degree 4—this may be verified directly from their usual monomial parametrizations. From this observation, it also follows that deg(π1) = 4. Thus, gw (Gal(πEX )) ≤gw(S4≀S2) = gw(S2 4⋊S2) = min(4, 2) = 4 < 8 = ED(X). A curious feature of this example is that Xsing R = {(0, 0, 0)} only attains the minimum in (31) when u = (0, 0, 0)."
  },
  {
    "prompt": "Understanding the computational aspects of Fig., Voronoi, half-plane.",
    "response": "For either one of the cuspidal cubics in (39), its cusp attains the minimum distance precisely when u belongs to a Voronoi cell in the opposite half-plane (see [19, Fig. 2].) The two opposite Voronoi cells intersect only in the origin. In summary, although ED(X) = 8, the Galois width reveals that nearest-point problem for X admits a closed-form solution in radicals."
  },
  {
    "prompt": "Understanding the computational aspects of Euclidean, Galois, varieties.",
    "response": "For real varieties with highly-structured Euclidean geometry, like the surface of revolution in Example 5, computing the Galois width may pro- vide a critical first clue towards a practical solution to the nearest-point problem. Our next two examples belong to another such special class of varieties, the orthogonally-invariant matrix varieties studied in [28, 27]. Example 6."
  },
  {
    "prompt": "Read and understand this section about EET, Xes, matrices from a research paper.",
    "response": "Consider the affine cone of essential matrices, Xes = {E ∈C3×3 | EET E −(1/2) tr(EET )E = 0, det E = 0}, (40) a much-studied object in computer vision. Over the reals, this variety has a well-known spectral characterization: a nonzero matrix E belongs to Xes R if it has rank 2 and two of its singular values are equal. In fact, given a generic matrix F ∈R3×3, one may compute the closest point E∗∈Xes R to F via its singular value decomposition (see eg."
  },
  {
    "prompt": "This is a technical discussion about diag, Theorem, Xes in theoretical computer science.",
    "response": "[68, Theorem 5.9]), F = U diag (σ1, σ2, σ3) V T ⇒ E∗= U diag \u0010σ1 + σ2 2 , σ1 + σ2 2 , 0 \u0011 V T . Furthermore, [27, Example 2.7] implies that all critical points are real: π−1 EXes (F) = n \u0010 F, U diag \u0010σ1 ± σ2 2 , σ2 ± σ1 2 , 0 \u0011 V T \u0011 , \u0010 F, U diag \u0010σ1 ± σ3 2 , 0, σ3 ± σ1 2 \u0011 V T \u0011 , \u0010 F, U diag \u0010 0, σ2 ± σ3 2 , σ3 ± σ2 2 \u0011 V T \u0011 o . Thus ED(Xes) = 6. However, gw (πEXes ) = 3."
  },
  {
    "prompt": "Read and understand this section about EXes, Galois, Unlike from a research paper.",
    "response": "Unlike in Example 5, the ED cover for this example has a nontrivial deck transformation, Ψ : EXes →EXes, (F, E) 7→(F, F −E). Here, again, the Galois width reflects the algebraic difficulty of obtaining a solution—namely, the cost of eigendecomposing a 3 × 3 matrix. As the next example shows, the gap between the Galois width and ED degree can be exponentially large. Example 7. Fix integers m ≥n ≥r ≥1, and let Xm,n,r ⊂Rm×n be the closed subvariety of m × n matrices whose rank is at most r."
  },
  {
    "prompt": "Read and understand this section about theorem, σiuivT, SVD from a research paper.",
    "response": "There 19 is an “enhanced” version of the celebrated Eckart-Young theorem for the Frobenius norm (see eg. [9, Theorem 2.9]), which implies that ED(Xm,n,r) = n r ! , and that all ED-critical points are real and may be obtained via SVD: A = n X i=1 σiuivT i ⇒ π−1 EXm,n,r (A) = (X i∈S σiuivT i | S ∈ [n] r !) . The following result establishes gw(πEXm,n,r ) = n, (41) an exponential improvement over the ED degree: for even n, gw(πEXm,n,n/2 ) = n ≪ED(Xm,n,n/2) = n n/2 ! ≈4n/√πn."
  },
  {
    "prompt": "Scientific context on Galois, problem, eigendecomposing and their role in graph theory.",
    "response": "Once more, the Galois width better reflects the algebraic cost of solving the nearest-point problem (eigendecomposing a n × n matrix.) Theorem 7. As a permutation group, Gal(πEXm,n,r ) is the symmetric group Sn acting on \u0000[n] r \u0001 , the set of all r-element subsets of [n] = {1, . . . , n}. Proof. Consider first the case r = 1, which we prove by induction on n. The base case n = 1 is immediate. Suppose the result result holds for matrices with fewer than n columns."
  },
  {
    "prompt": "This is a technical discussion about Gal, real, singular in theoretical computer science.",
    "response": "Fix A′ to be a real matrix of size (m −1) × (n −1) with distinct singular values. Let σ1 be a real number greater than the singular values of A′, and set A = \u0014σ1 0 0 A′ \u0015 ∈Cm×n. Our inductive hypothesis provides loops based at A which generate a subgroup Sn−1 × S1 ⊂Gal(πEm,n,r). Lemma 2 gives Gal(πEm,n,r) = Sn. For r > 1, consider the Galois closure (eq. (30)) for the rank-1 case, ˆEXm,n,1 = \u0000EXm,n,1 \u0001(n) Cm×n ."
  },
  {
    "prompt": "This is a technical discussion about map, matrices, factors in theoretical computer science.",
    "response": "(42) The map ˆπ : ˆEXm,n,1 →Cm×n factors through πEXm,n,r , for instance via the map that sends an ordered n-tuple of rank-1 matrices to the sum of the first r matrices. Thus, the monodromy action on EXm,n,r is induced by that of the group Gal(ˆπ), which is Sn by the r = 1 case. We point out that in Example 5 the number of real critical points for generic real-valued data may vary, whereas in Examples 6 and 7 they are always real."
  },
  {
    "prompt": "Scientific context on Galois, problems, nearest-point and their role in graph theory.",
    "response": "Overall, it is an interesting challenge to determine the Galois widths of nearest-point problems on varieties. The same is true for numerous variations of this problem found in the literature which arise from weighted nearest-point problems [73], p-norm optimization [59], or ED degrees with respect to general inner products [57]. All of these 20 problems correspond to branched covers, and it is interesting to ask when their Galois/monodromy group are full-symmetric."
  },
  {
    "prompt": "Understanding the computational aspects of case, degrees, answer.",
    "response": "In the case of weighted ED degrees, the surface X from Example 5 again provides the answer “not always.” To minimize the weighted distance function d(x; u, w) = w1(x1 −u1)2 + w2(x2 −u2)2 + w3(x3 −u3)2. (43) over X from Example 5, solving the critical equations again reduces to the case of two touching cuspidal cubics, x2 3 = ± q\u00001 + ((w2u2)/(w1u1))2\u0001 x3 1, (44) and once more we find s 8 critical points of Galois width 4."
  },
  {
    "prompt": "Understanding the computational aspects of Galois, case, hand.",
    "response": "On the other hand, for the case of low-rank matrix approximation, the generic weighted ED jumps dramatically—see eg. [57, eq. 3.11]. It seems plausible in this case that the Galois/monodromy group is not merely symmetric (as in Theorem 7), but full-symmetric. 4.2 Algebraic Statistics In this section, we further motivate the Galois width with applications to statistical parameter estimation, one of the primary topics in the ac- tive field of algebraic statistics [83]."
  },
  {
    "prompt": "Explain a concept involving Conjectures, expository, experts in computational complexity.",
    "response": "Our discussion is brief, and partly expository—for the experts, we highlight Conjectures 1 and 2. Much like the ED degree, the Maximum-Likelihood (ML) degree [48, 16] bounds the algebraic complexity of minimizing the log-likelihood ℓ(p; u) = n X i=0 ui log(pi), where u is a given data vector, over points [p0 : · · · : pn] of an irreducible projective variety X ⊂Pn which lie in the affine chart Pn i=0 pi = 1 and satisfy pi ̸= 0 for all i."
  },
  {
    "prompt": "Scientific context on con, cover, sider and their role in graph theory.",
    "response": "To construct the associated branched cover, con- sider first the vanishing ideal IX = ⟨g1, . . . , gk⟩and the rational map ΨX : X \\ \u0010 Xsing ∪Hn \u0011 × Pk →Pn ([p0 : · · · : pn], [λ0 : · · · : λk+1]) 7→ \" λ0 + k X i=1 λipi dgi dpj (p; u) | 0 ≤j ≤n # , where Hn = V (p0 · · · pn · (P pi)) is the union of the n + 2 hyperplanes. The image of the graph of ΨX under the coordinate projection Gr(ΨX) →Pn × Pn, (p, λ, u) 7→(u, p) is known as the likelihood correspondence LX."
  },
  {
    "prompt": "Understanding the computational aspects of πLX, deg, degree.",
    "response": "When the projection πLX : LX →Pn, (u, p) 7→u is a branched cover, its degree, ML(X) := deg(πLX ), is known as the ML degree of X. For generic u, this number counts the complex-valued critical points p ∈X \\ \u0000Xsing ∪Hn \u0001 for the log-likelihood ℓ(p; u). 21 Example 8. If V ⊂Pn is a generic linear space of codimension r, then ML(V ) = n r ! , (45) as shown in [16, Theorem 13], which states furthermore that for real data all critical points are real."
  },
  {
    "prompt": "Explain a concept involving Galois, real, incide in computational complexity.",
    "response": "Although the number of (real) critical points for this problem and low-rank matrix approximation from Example 7 co- incide, Galois widths imply that this problem is much harder. For exam- ple, if we take (n, r) = (4, 2), low-rank approximation has Galois width 4 < 6 = \u00004 2 \u0001 . In contrast, this problem for generic V ∈Gr(P2, P4) has Galois width 6 and cannot be solved in radicals."
  },
  {
    "prompt": "Explain a concept involving Solutions, matrix, drops in computational complexity.",
    "response": "We may take u = \u0002 6 4 5 3 12 \u0003 , V = V(ℓ1(p), ℓ2(p)), where ℓ1(p) = p1 + p2 + 2p3 −5p4 −2p0, ℓ2(p) = 3p1 + 2p2 −p3 −3p4 + 4p0. Solutions to this ML problem are the points on V \\ H4 where the matrix   u0 u1 u2 u3 u4 p0 p1 p2 p3 p4 p0 · ∂ℓ1 ∂p0 p1 · ∂ℓ1 ∂p1 p2 · ∂ℓ1 ∂p2 p3 · ∂ℓ1 ∂p3 p4 · ∂ℓ1 ∂p4 p0 · ∂ℓ2 ∂p0 p1 · ∂ℓ2 ∂p1 p2 · ∂ℓ2 ∂p2 p3 · ∂ℓ2 ∂p3 p4 · ∂ℓ2 ∂p4   drops rank."
  },
  {
    "prompt": "Scientific context on Gal, obtain, homogenized and their role in graph theory.",
    "response": "With a little help from Macaulay2 [41], we may obtain a homogenized primitive element f =1 126 944 p6 0 + 4 127 767 p5 0p1 −2 310 180 p4 0p2 1 −2 105 058 p3 0p3 1 + 434 717 p2 0p4 1 + 170 397 p0p5 1 + 5 670 p6 1, and check that Gal(f) = S6. This follows after reducing f modulo the primes 29 and 141 and factoring, thereby showing that Gal(f) contains Frobenius elements which are, respectively, a 5-cycle and a transposition."
  },
  {
    "prompt": "Explain a concept involving Galois, cases, case in computational complexity.",
    "response": "Although we expect the Galois groups for maximum-likelihood in many cases to be full-symmetric like in Example 8, this is not always the case. Example 9. Consider the projectivization Ym,n,r = P(Xm,n,r) of the variety of m×n rank ≤r matrices from Example 7. As before, we assume r ≤n ≤m. Using numerical homotopy continuation methods, the values ML(Ym,n,r) for m ≤6 were completely tabulated in [45]."
  },
  {
    "prompt": "Understanding the computational aspects of Zn,r, rank, matrices.",
    "response": "An analogous table produced ML degrees for rank-constrained symmetric matrices, Zn,r =          p ∈P(n+2)(n−1)/2 | rank   2p1,1 p1,2 · · · p1,n p1,2 2p2,2 · · · p2,n ... ... ... ... p1,n p2,n · · · pn,n  ≤r          , (46) for 1 ≤r ≤n ≤6, except the missing cases ML(Z6,3) = ML(Z6,4) = 68774 announced in [34, Figure 15]."
  },
  {
    "prompt": "Read and understand this section about Proposition, Galois, acting from a research paper.",
    "response": "For the particular case of Z3,2, it was noticed in [45, Proposition 1] that the Galois group was S4 acting on 6 = \u00004 2 \u0001 22 solutions corresponding to pairs 1 ≤i < j ≤4. As a consequence, the maximum-likelihood estimate in this case is solvable in radicals. This fact can be deduced immediately from the existence of an order-2 deck transformation for the branched cover πLZ3,2 ."
  },
  {
    "prompt": "Read and understand this section about diag, set, deck from a research paper.",
    "response": "If we set P =   2 p1,1 p1,2 p1,3 p1,2 2 p2,2 p2,3 p1,3 p2,3 2 p3,3  , U =   2 u1,1 u1,2 u1,3 u1,2 2 u2,2 u2,3 u1,3 u2,3 2 u3,3  , D(U) = diag (2u1,1 + u1,2 + u1,3, u2,1 + 2u2,2 + u2,3, u3,1 + u3,2 + 2u3,3) , then this deck transformation may be given explicitly as Ψ : LZ4,2 99K LZ4,2 (U, P) 7→        U, 32 (tr(U))3 · D(U) ·   u1,1 p1,1 u1,2 p1,2 u1,3 p1,3 u1,2 p1,2 u2,2 p2,2 u2,3 p2,3 u1,3 p1,3 u2,3 p2,3 u3,3 p3,3   · D(U)        ."
  },
  {
    "prompt": "Understanding the computational aspects of Theorems, Rodriguez, Draisma.",
    "response": "This is a special case of a maximum-likelihood duality result due to Draisma and Rodriguez, which has an analogue for rectangular, non- symmetric matrices ([26, Theorems 11 and 6, respectively]). In general, when n is odd and r = (n + 1)/2, these duality results specialize to for- mulas for deck transformations like Ψ given above. Based on this discussion, we propose the following conjecture. Conjecture 1."
  },
  {
    "prompt": "Understanding the computational aspects of odd, Zn,r, πLZn,r.",
    "response": "For any integers 1 ≤r ≤n ≤m, we have gw \u0010 πLXm,n,r \u0011 = ( ML(Ym,n,r)/2 for n odd, r = (n + 1)/2, ML(Ym,n,r) else, gw \u0010 πLZn,r \u0011 = ( ML(Zn,r)/2 for n odd, r = (n + 1)/2, ML(Z,r) else. We note that deck transformations in maximum-likelihood may also arise due to label-swapping symmetries—see eg. [82, §5]. We next recall how such symmetries arise in another statistical parameter estimation problem: the method-of-moments for mixture models."
  },
  {
    "prompt": "Scientific context on Pearson, concreteness, setting and their role in graph theory.",
    "response": "For concreteness, we consider the classical setting, pioneered by Pearson over a century ago [74], of univariate Gaussian mixture models. For a mixture of k Gaussians N(µ1, σ1), . . . , N(µk, σk), the moments (Mi,k(µ, σ, λ))i≥0 are a sequence of polynomials depending on the mean parameters µ = (µ1, . . . , µk), standard deviations σ = (σ1, . . . , σk), and mixing coefficients λ = (λ1, . . . , λk−1). These polynomials may be com- puted recursively by the rules Mi,k(µ, σ, λ) = λ1mi(µ1, σ1) + . . ."
  },
  {
    "prompt": "This is a technical discussion about parameters, moments, sample in theoretical computer science.",
    "response": "+ (1 −λ1 −. . . −λk−1)mi(µk, σk), mi(µ, σ) =    1 i = 0, µ mi−1(µ, σ) + σ2 d dµmi−1(µ, σ) i > 0. (47) 23 The method of moments hypothesizes that data x1, . . . , xn ∈R are sam- ples drawn from a mixture of Gaussians with unknown parameters (µ, σ, λ). To recover these parameters, a sequence of sample moments is computed, si = (1/n)(xi 1 + . . . + xi n), 1 ≤i ≤e, and one may expect Mi,k(µ, σ, λ) ≈si for all i if the sample size n is large enough."
  },
  {
    "prompt": "This is a technical discussion about Mi,k, Solving, πe,k in theoretical computer science.",
    "response": "Thus, we are led naturally to consider the map πe,k : C3k−1 →Ce, (µ, σ, λ) 7→(Mi(µ, σ, λ) | 1 ≤i ≤e) . (48) Solving the moment equations si −Mi,k(µ, σ, λ) for i = 1, . . . , e may be understood as inverting the map πe,k. An algebro-geometric study of these maps was initiated in the work [1], which also posed a conjecture consisting of three parts (cf. [1, Conjecture 3]): 1. (Algebraic identifiability) The map π3k−1,k is a branched cover."
  },
  {
    "prompt": "Read and understand this section about words, moments, possibilities from a research paper.",
    "response": "In other words, 3k−1 moments from a generic Gaussian mixture model suffice to recover the parameters up to finitely-many possibilities. This was later settled in the affirmative [3], where it was also dis- covered that generalizations to multivariate Gaussians may fail. 2. (Rational identifiability) The map π3k,k is generically a degree k! (Galois) branched cover over its image."
  },
  {
    "prompt": "Explain a concept involving words, exact, moments in computational complexity.",
    "response": "In other words, 3k exact moments suffice to recover the parameters of the mixture model up to the label-swapping action by Sk. This remains open, although the same result for 3k +2 moments is established in [66], with analogous results when some parameters are known or known to be equal. 3. (Degree of algebraic identifiability) The authors in [1] made a “wild guess” that deg(π3k,k) = k!"
  },
  {
    "prompt": "Understanding the computational aspects of based, incorrect, computations.",
    "response": "((2k −1)!!)2 based on computations for k = 2, 3, but numerical results for the case k = 4 in [2] provide strong evidence that this is incorrect. Although a degree formula remains elusive, we may loosely interpret the spirit of their conjecture to mean that the moment equations have no “hidden symmetries.” Conjecture 2. For all k ≥1, gw (π3k−1,k) = deg(π3k−1,k)/k! In summary, the Galois width provides a novel tool for studying the algebraic complexity of statistical parameter estimation problems."
  },
  {
    "prompt": "This is a technical discussion about study, identi, fiability in theoretical computer science.",
    "response": "We feel optimistic that it will find further applications in the study of identi- fiability questions and in concrete parameter estimation scenarios. 4.3 Algebraic Vision The computational model studied in Section 2 originates from work of Hartley, Nist´er, and Stew´enius [72], who pioneered the application of Ga- lois theory to the study of 3D reconstruction and related problems in com- puter vision."
  },
  {
    "prompt": "This is a technical discussion about alternating, Galois, specifically in theoretical computer science.",
    "response": "More specifically, their paper outlined a framework based on symbolic computation that could be applied to estimation problems with Galois groups that are full-symmetric/alternating. In many respects, our work is an extension of theirs: we have characterized complexity in our model for any finite group, and can obtain lower bounds on this complex- ity using more practical numerical heuristics."
  },
  {
    "prompt": "Understanding the computational aspects of SVD, hand, operation.",
    "response": "24 On the other hand, there are some notable differences between the setup of [72] and ours; for example, their work treats extracting n-th roots in the working field as a unit-cost operation, and possibly additional ora- cles like the SVD of an arbitrarily-large matrix with real entries. It would certainly be worthwhile to extend the computational model of Section 2 to such settings, but this is beyond our current scope."
  },
  {
    "prompt": "Scientific context on paper, problems, vision and their role in graph theory.",
    "response": "Rather, we see a discussion of motivating problems from computer vision as a fitting end to this paper. Although we can only offer a glimpse of these problems and their practical importance, we recommend the recent survey of Kileel and Kohn [54] as one entry point into the expanding field of algebraic vision."
  },
  {
    "prompt": "This is a technical discussion about Consensus, Sampling, Random in theoretical computer science.",
    "response": "Interest in algebraic complexity from the computer vision community stems largely (but not exclusively) from the use of minimal solvers [60, 61] in outlier-robust estimation schemes such as RanSaC (“Random Sampling and Consensus”) [39]. The rough idea behind these methods is this: un- known quantities are estimated multiple times using random subsamples of the data, and these estimates are aggregated so as to filter outliers and produce a more robust estimate."
  },
  {
    "prompt": "Explain a concept involving minimal, outliers, Algebraic in computational complexity.",
    "response": "To minimize the influence of outliers, the subsamples should ideally be as small as possible (“minimal”). Algebraic methods such as Gr¨obner bases are currently the favored approach for constructing minimal solvers. The Galois width measures a fundamental limit of what degree of algebraic simplification these solvers can achieve. Aside from minimal problems, the algebraic complexity of nonlinear least-squares problems in vision has also attracted significant interest."
  },
  {
    "prompt": "Understanding the computational aspects of covers, triangulation, revisit.",
    "response": "We revisit the problem of ℓ2-optimal triangulation in Example 10 below. In this paper, we adopt the point of view that estimation problems in vision, minimal or not, may be profitably studied via branched covers (eg. the joint camera map of [30].) The Galois width provides a general tool for measuring both the difficulty of these problems and optimality of proposed solutions which is more refined than the degrees of these branched covers."
  },
  {
    "prompt": "Understanding the computational aspects of Galois, width, large.",
    "response": "If the Galois width of a problem is large, then it is truly hard in the sense of [50]. If, however, the Galois width is smaller than the degree, then there is an opportunity for a more efficient algebraic solution. Example 10 (Triangulation). The standard model for the pinhole camera is a 3 × 4 matrix P of rank 3 [43, Ch. 6]."
  },
  {
    "prompt": "Explain a concept involving point, homogeneous, coordinates in computational complexity.",
    "response": "We model a world point X ∈P3 as a 4 × 1 vector of homogeneous coordinates, so that P3 99K P2, X 7→P · X associates each 3D point X ∈P3 \\ {[ker P]} with a 2D point in 3 × 1 homogeneous coordinates. In practice, of course, one must work with affine pixel coordinates in the image, and hence also the rational map P3 99K C2, X 7→Π(P · X), where Π(x, y, z) = (x/z, y/z). If we have an arrangement of multiple cameras P• = (P1, . . ."
  },
  {
    "prompt": "Explain a concept involving variety, Zariski, obtained in computational complexity.",
    "response": ", Pn), then one is naturally led to consider the affine multiview variety, a variety MP• ⊂C2n obtained as the Zariski closure of the image of P3 99K C2n, X 7→(Π(P1 · X), . . . , Π(Pn · X)) . In the computer vision literature, the ℓ2-optimal triangulation prob- lem [44, 80] is an instance of the Euclidean distance minimization problem, already studied in Section 4.1, for which the following result is known. 25 Theorem 8."
  },
  {
    "prompt": "This is a technical discussion about cameras, EMP, Galois in theoretical computer science.",
    "response": "[70] If P• is a generic arrangement of n ≥2 cameras, then ED (MP•) = 9 2n3 −21 2 n2 + 8n −4. (49) We propose a refinement of this result. Conjecture 3. With notation as in Theorem 8, the Euclidean distance cover π : EMP• →C2n has full-symmetric Galois/monodromy group, and thus its Galois width is thus also given by the formula (49)."
  },
  {
    "prompt": "Explain a concept involving Conjecture, heuristically, verified in computational complexity.",
    "response": "When n = 2, Conjecture 3 was verified in [72] by direct symbolic computation; using numerical homotopy continuation methods, we have verified it (heuristically) for a few larger values of n. We note that, due to different formulations of the problem, different root counts have been reported in the literature. For example, when n = 3, when the ED degree equals 47, but [18, Table 2] reports 94 = 2 · 47 solutions; nevertheless, the branched cover implicit in their work still has Galois width 47."
  },
  {
    "prompt": "Explain a concept involving Rydell, Analogously, approximation in computational complexity.",
    "response": "Analogously to [57] in the case of low-rank matrix approximation, recent work of Rydell et al. [75] shows that certain weighted variants of 2- view triangulation are solvable in radicals. Although we believe the Galois width may be useful for analyzing similar problems, our framework does not address the question of how the “best” weights might be chosen. Example 11 (Resectioning / Absolute Camera Pose)."
  },
  {
    "prompt": "Read and understand this section about point, observed, projections from a research paper.",
    "response": "The triangulation problem can be understood as fitting the best 3D point to n observed 2D projections under known cameras. Dually, the camera resectioning problem requires fitting an unknown camera to n known 3D-2D point correspondences. In general, n ≥6 correspondences are needed to solve the problem resectioning. ED degree conjectures for such problems, of a similar flavor to Theorem 8, appear in [20, 35]."
  },
  {
    "prompt": "This is a technical discussion about calibrated, Euclidean, Resectioning in theoretical computer science.",
    "response": "Resectioning can be solved with n < 6 correspondences if the camera is known to be calibrated; that is, when P is given up to scale as an element of the special Euclidean group, P ∝(R | t) ∈SE3 . Calibrated resectioning in the minimal case of n = 3 points is known as the Perspective-3-Point problem, or P3P. Early solutions to this problem date back to Lagrange in 1700s and Grunert (see [81] for a history), and involve computing the roots of a univariate quartic."
  },
  {
    "prompt": "Read and understand this section about Galois, holds, solutions from a research paper.",
    "response": "From (19), it follows that the solutions to this problem have Galois width at most 3, and in [72] it was verified that equality holds. We note that several solutions to P3P naturally reduce to the case of a cubic equation, without using resolvents (see eg. [22].) A Galois-theoretic study of camera resectioning with minimal combi- nations of point and line features was recently carried out in [32]."
  },
  {
    "prompt": "Scientific context on Gk,m, Grassmannian, Letting and their role in graph theory.",
    "response": "Letting p and l be nonnegative integers with p + l = 3, and Gk,m denote the Grassmannian of k-planes in Pm, then a minimal solution to the resec- tioning problem with p point-point correspondences and l line-line corre- spondences amounts to inverting a branched cover of the form πp,l : (P)×p (G1,3)×l × SE3 99K \u0000P3 × P2\u0001×p × (G1,3 × G1,2)×l . The main result in [32, §3] implies that gw(π3,0) = gw(π1,2) = 3, gw(π2,1) = 2, gw(π0,3) = 8."
  },
  {
    "prompt": "Explain a concept involving Practical, minimal, consequences in computational complexity.",
    "response": "(50) 26 Practical consequences of this result were recently realized in [49], in which the authors developed new solutions to the minimal cases with (p, l) ∈{(2, 1), (1, 2)}. These minimal solvers were vetted in experiments in which they outperformed the previous state-of-the-art for these prob- lems in terms of both accuracy and speed. Exploiting the gap between Galois width and degree was a key factor that made this possible. Example 12 (Reconstruction / Relative Camera Pose)."
  },
  {
    "prompt": "Understanding the computational aspects of points, cam, eras.",
    "response": "When both cam- eras and 3D points are unknown, and only 2D points are given, we are faced with the full 3D reconstruction problem, which has been extensively studied in both the uncalibrated and the calibrated cases. In either set- ting, there is an inherent ambiguity due to choice of coordinates; in other words, solutions are defined up to the action of the projective group PGL4 or its subgroup consisting of 3D similarity transformations, respectively."
  },
  {
    "prompt": "This is a technical discussion about obtain, ing, reconstructions in theoretical computer science.",
    "response": "Since knowledge of calibration is often available, and necessary for obtain- ing metrically-accurate reconstructions, we mostly focus on this case. In [30], minimal problems for reconstructing a configuration of points and lines in space were completely classified under the assumptions of cal- ibrated cameras and complete correspondences across multiple images."
  },
  {
    "prompt": "Understanding the computational aspects of configu, ration, projections.",
    "response": "A key player in this story was the joint camera map of such a configuration, a branched cover which sends an equivalence class of solutions (configu- ration + cameras) to multiple 2D projections. Recent work [53] extends this result to the uncalibrated case, and also suggests novel a novel strat- egy for checking that the joint camera map is decomposable in the sense of Definition 6."
  },
  {
    "prompt": "Read and understand this section about case, space, simplest from a research paper.",
    "response": "In the calibrated case, the simplest instance of this joint camera map arises for a configuration of five points in space. We define π5pp : \u0000P3\u0001×5 × C5pp 99K \u0000P2\u0001×10 (51) (X1, . . . , X5, P1, P2) 7→(P1 · X1, . . . , P1 · X5, P2 · X1, . . . , P2 · X5) , where C5pp is a variety whose points represent generic pairs of calibrated cameras up to world similarity transformation, specifically C5pp = {(P1, P2) ∈(SE3)×2 | P1 = \u0002 I | 0 \u0003 , P2 = \u0002 R | t \u0003 , t3 = 1}."
  },
  {
    "prompt": "Scientific context on Computing, data, fibers and their role in graph theory.",
    "response": "(52) Computing fibers of π5pp over generic data (x1, . . . , x5, y1, . . . , y5) ∈ \u0000P2\u0001×10 is the celebrated five-point relative pose problem. Work on this problem dates back to Kruppa in 1912 [58], whose work implies deg(π5pp) ≤22. This was sharpened much later by Demazure in 1988 [21], who showed that deg(π5pp) = 20."
  },
  {
    "prompt": "This is a technical discussion about Xes, map, based in theoretical computer science.",
    "response": "His proof implicitly relied on a decomposition of the joint camera map, π5pp = π2◦π1, based on a projectivization P(Xes) ⊂P8 of the essential matrix variety already discussed in Example 6. We take Y5pp = {(E, x1, . . . , x5,y1, . . . , y5) ∈P(Xes) × \u0000P2\u0001×10 | yT i Exi = 0}, (53) π1 : \u0000P3\u00015 × C5pp 99K Y, (X1, . . . , X5, P1, P2) 7→(E(P1, P2), P1 · X1, . . . , P1 · X5, P2 · X1, . . ."
  },
  {
    "prompt": "Scientific context on Xes, Gal, matrix and their role in graph theory.",
    "response": ", P2 · X5) , where E(P1, P2) ∈P(Xes) is a 3 × 3 matrix whose (i, j)-th entry equals ei,j = (−1)i+j det \u0014 P1[{1, 2, 3} \\ {i}, :] P2[{1, 2, 3} \\ {j}, :] \u0015 , (54) 27 and π2 : Y → \u0000P2\u00012 is the coordinate projection. We have deg(π1) = 10, and deg(π2) = 2, implying that Gal(π5pp) is a subgroup of the wreath product S2 ≀S10. In [32, §4], the authors obtained strong evidence that Gal(π5pp) ∼= S2 ≀S10 ∩A20 ∼= S9 2 ⋊S10 using numerical homotopy continu- ation methods."
  },
  {
    "prompt": "Read and understand this section about Gal, surjective, proved from a research paper.",
    "response": "They also proved that the natural map Gal(π5pp) →S10 is surjective. Thus, gw(π5pp) = 10. (55) This agrees with a result of [72], implying that general methods for solv- ing five-point relative pose, such as Nist´er’s popular solution [71], must invariably resort to computing the roots of a degree-10 polynomial. Figure 2: A 3D point (blue) and its 1-dimensional radial projection (red.) We conclude with a recent example of a minimal reconstruction prob- lem solved by Hruby et al."
  },
  {
    "prompt": "Explain a concept involving Galois, model, dramatic in computational complexity.",
    "response": "[50], where the gap between the degree of the branched cover and its Galois width is dramatic. The problem arises from a non-pinhole camera model known as the radial camera model [85, 84, 62, 50]. This model addresses the physical phenomenon of lens distor- tion, a common source of deviations between image measurements and the pinhole model."
  },
  {
    "prompt": "Understanding the computational aspects of lines, straight, Figure.",
    "response": "Figure 2 illustrates a case of “barrel distortion”; un- der pinhole projection, straight lines in P3 map to straight lines in P2, but 28 distortion causes these lines to bend outward. To remove the effects of distortion, a number of parametric distortion models have been proposed in the literature."
  },
  {
    "prompt": "Scientific context on model, point, center and their role in graph theory.",
    "response": "In contrast, the radial camera model may be seen as a relaxation of the pinhole model, under the assumption that distortion is radially symmetric about some point in the image (the center of dis- tortion.) With this assumption, the straight line through the center of distortion and perspective projection of a 3D point also passes through any point obtained by distorting that projection. This is the radial line illustrated in Figure 2."
  },
  {
    "prompt": "Explain a concept involving lines, mapping, projective-linear in computational complexity.",
    "response": "The pencil of lines through the center of distortion forms a P1, and the mapping P3 99K P1 from 3D points to radial lines is projective-linear, as we now explain. Consider a pinhole camera P. With- out loss of generality, we may assume that the image’s center of distortion is the point e3 = [0 : 0 : 1] ∈P2."
  },
  {
    "prompt": "Understanding the computational aspects of radial, camera, cameras.",
    "response": "For any 3D point X ∈P3, we then have spanP2 (P · X, e3) = span \u0012\u0014 ˜P 0 \u0015 · X, e3 \u0013 , (56) where ˜P = P[1:2, :] is a 2 × 4 matrix, representing the radial camera ˜P : P3 99K P1 associated to P. Given a radial line ℓ∈P1 and a radial camera ˜P : P3 99K P1, note that the fiber ˜P −1(ℓ) is a plane. Thus, in order to obtain a nontrivial constraint on a 3D point X, it should be seen by at least 4 radial cameras ˜P1, . . . , ˜P4. If we assume the original pinhole cameras P1, . . ."
  },
  {
    "prompt": "Scientific context on calibrated, Observe, assume and their role in graph theory.",
    "response": ", P4 are calibrated, then ˜Pi = \u0014rT i1 ti,1 rT i2 ti,2 \u0015 where rT i1ri1 = rT i2ri2 = 1, rT i1 ri2 = 0. (57) Up to similarity transformation in R3, we may assume A1 = \u00141 0 0 0 0 1 0 0 \u0015 , A2 = \u0014rT 21 0 rT 22 1 \u0015 , (58) and define C13pp = { ˜P1, . . . , ˜P4) ∈C2×4 | (57) and (58) hold}. (59) Observe that dim(C13pp) = 13. Thus, it is little surprise that the map πrad : \u0000P3\u000113 × C13pp 99K \u0000P1\u0001×52 (X1, . . . , X13, ˜P1, . . . , ˜P4) 7→( ˜P1 · X1, . . . , ˜P4 · X13) (60) is a branched cover."
  },
  {
    "prompt": "Scientific context on deg, Theorem, surprising and their role in graph theory.",
    "response": "What is more surprising is the following. Theorem 9. gw(π13pp) = 28 < deg(π13pp) = 3584. Proof. In [50], it was shown that π13pp has a decomposition of the form π13pp = π4 ◦π3 ◦π2 ◦π1, where deg(π1) = 16, deg(π2) = 4, deg(π3) = 2, and deg(π1) = 28. Now, applying Proposition 4 and Theorem 4, we have that gw(π4) = 28 ⇒ gw(π13pp) = 28. (61) 29 To see that gw(π4) = 28, consider the branched cover π′ = π3 ◦π2 ◦π1."
  },
  {
    "prompt": "Explain a concept involving Xquad, recall, description in computational complexity.",
    "response": "We recall, also from [50], a description of this map analogous to (62); π′ : \u0000P3\u00015 × C5pp 99K Y13pp, (62) Y13pp = {(T, ℓ1,1, . . . , ℓ13,4) ∈Xquad × \u0000P1\u0001×52 | T(ℓi,1, . . . , ℓi,4) = 0}, where Xquad ⊂P(C2×2×2×2) is the projective variety of binary tensors T whose entries are given by Ti,j,k,l = (−1)i+j+k+l det   ˜P1[{1, 2} \\ {i}, :] ˜P2[{1, 2} \\ {j}, :] ˜P3[{1, 2} \\ {k}, :] ˜P4[{1, 2} \\ {l}, :]  ."
  },
  {
    "prompt": "Understanding the computational aspects of map, variety, degree.",
    "response": "(63) This is a 13-dimensional variety of degree 28, which happens to coin- cide with the image of the 4 × 4 principal minor map studied in [65]. The map π4 : Y13pp → \u0000P1\u0001×52 is given by coordinate projection. Each 4-tuple T(ℓi,1, . . . , ℓi,4) gives rise to a linear form in the entries of the tensor T. Considering all such linear forms, we obtain a rational map \u0000P1\u000152 99K G13,15. A computatation in local coordinates shows that this map is dominant."
  },
  {
    "prompt": "Understanding the computational aspects of obtained, slicing, 11-12.",
    "response": "As such, we may consider the monodromy action of the G13,15 on points in Y13pp obtained by slicing, which is well-known to be S28 [4, pp. 11-12]. Thus, by Corollary 2, a generic instance of the 13-point problem with rational coordinates has Galois group S28. Since Gal(π4) ⊴S28, we deduce that gw(π4) = 28 as well. The decomposition used in the proof of Theorem 9 was also used in [50] to devise an efficient homotopy continuation solver for the minimal radial camera reconstruction problem."
  },
  {
    "prompt": "Read and understand this section about writing, sym, bolic from a research paper.",
    "response": "At the time of writing, a practical sym- bolic solution to this problem is still unknown. Naturally, tracking 28 representative solution paths is more efficient than tracking all 3584. This final example embodies the main takeaway from this paper: Don’t give up right away on solving a “hard” system of algebraic equations; first, determine the problem’s Galois width, then, plan accordingly."
  },
  {
    "prompt": "Scientific context on Jon, Hauenstein, Kathl´en and their role in graph theory.",
    "response": "Acknowledgements I wish to thank the following individuals for helpful conversations and comments during the preparation of this work: Jon Hauenstein, Kathl´en Kohn, Kisun Lee, Julia Lindberg, and Jose Israel Rodriguez. References [1] Carlos Am´endola, Jean-Charles Faug`ere, and Bernd Sturmfels. Mo- ment varieties of Gaussian mixtures. Journal of Algebraic Statistics, 7(1):14–28, 2016. [2] Carlos Am´endola, Julia Lindberg, and Jose Israel Rodriguez."
  },
  {
    "prompt": "Understanding the computational aspects of Solving, projections, IMRN.",
    "response": "Solving parameterized polynomial systems with decomposable projections. arXiv preprint arXiv:1612.08807, 2016. 30 [3] Carlos Am´endola, Kristian Ranestad, and Bernd Sturmfels. Alge- braic identifiability of Gaussian mixtures. International Mathematics Research Notices. IMRN, (21):6556–6580, 2018. [4] Enrico Arbarello, Maurizio Cornalba, Phillip Augustus Griffiths, and Joseph Harris. Geometry of algebraic curves. Vol. I. Springer-Verlag, New York, 1985. [5] Chanderjit Bajaj."
  },
  {
    "prompt": "This is a technical discussion about problems, Michael, Computational in theoretical computer science.",
    "response": "The algebraic degree of geometric optimization problems. Discrete & Computational Geometry. An International Journal of Mathematics and Computer Science, 3(2):177–191, 1988. [6] Michael J. Bannister, William E. Devanny, David Eppstein, and Michael T. Goodrich. The galois complexity of graph drawing: Why numerical solutions are ubiquitous for force-directed, spectral, and circle packing drawings. Journal of Graph Algorithms and Applica- tions, 19(2):619–656, 2015. [7] D. N. Bernstein."
  },
  {
    "prompt": "Explain a concept involving Paul, Breiding, number in computational complexity.",
    "response": "The number of roots of a system of equations. Funkcional. Anal. i Priloˇzen., 9(3):1–4, 1975. [8] Viktoriia Borovik, Paul Breiding, Javier del Pino, Mateusz Micha lek, and Oded Zilberberg. Khovanskii bases for semimixed systems of polynomial equations–approximating stationary nonlinear newtonian dynamics. Journal de Math´ematiques Pures et Appliqu´ees, 182:195– 222, 2024. [9] Paul Breiding, Kathl´en Kohn, and Bernd Sturmfels. Metric algebraic geometry, volume 53 of Oberwolfach Seminars."
  },
  {
    "prompt": "Explain a concept involving Springer, Cham, Birkh in computational complexity.",
    "response": "Birkh¨auser/Springer, Cham, 2024. [10] Paul Breiding, Frank Sottile, and James Woodcock. Euclidean dis- tance degree and mixed volume. Foundations of Computational Mathematics. The Journal of the Society for the Foundations of Com- putational Mathematics, 22(6):1743–1765, 2022. [11] Paul Breiding and Sascha Timme. HomotopyContinuation.jl: A Package for Homotopy Continuation in Julia. In James H."
  },
  {
    "prompt": "Understanding the computational aspects of ICMS, Cham, Manuel.",
    "response": "Dav- enport, Manuel Kauers, George Labahn, and Josef Urban, editors, Mathematical Software – ICMS 2018, pages 458–465, Cham, 2018. Springer International Publishing. [12] Taylor Brysiewicz, Jose Israel Rodriguez, Frank Sottile, and Thomas Yahl. Solving decomposable sparse systems. Numerical Algorithms, 88(1):453–474, 2021. [13] Anita Buckley and Tomaˇz Koˇsir. Determinantal representations of smooth cubic surfaces. Geometriae Dedicata, 125:115–140, 2007."
  },
  {
    "prompt": "Read and understand this section about Michael, Burr, Frank from a research paper.",
    "response": "[14] Michael Burr, Frank Sottile, and Elise Walker. Numerical homotopies from Khovanskii bases. Mathematics of Computation, 92(343):2333– 2353, 2023. [15] Peter J. Cameron. Permutation groups, volume 45 of London Math- ematical Society Student Texts. Cambridge University Press, Cam- bridge, 1999. [16] Fabrizio Catanese, Serkan Ho¸sten, Amit Khetan, and Bernd Sturm- fels. The maximum likelihood degree. American Journal of Mathe- matics, 128(3):671–697, 2006. 31 [17] Tianran Chen."
  },
  {
    "prompt": "Scientific context on BKK, index, geometric and their role in graph theory.",
    "response": "A geometric criterion on the equality between BKK bound and intersection index. arXiv preprint arXiv:1812.05408, 2018. [18] Chiang-Heng Chien, Hongyi Fan, Ahmad Abdelfattah, Elias Tsigari- das, Stanimire Tomov, and Benjamin Kimia. Gpu-based homotopy continuation for minimal problems in computer vision. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15765–15776, 2022. [19] Diego Cifuentes, Kristian Ranestad, Bernd Sturmfels, and Madeleine Weinstein."
  },
  {
    "prompt": "This is a technical discussion about Voronoi, varieties, cells in theoretical computer science.",
    "response": "Voronoi cells of varieties. J. Symbolic Comput., 109:351– 366, 2022. [20] Erin Connelly, Timothy Duff, and Jessie Loucks-Tavitas. Algebra and geometry of camera resectioning. Mathematics of Computation, 2024. [21] Michel Demazure. Sur deux problemes de reconstruction. Technical report, 1988. [22] Yaqing Ding, Jian Yang, Viktor Larsson, Carl Olsson, and Kalle ˚Astr¨om. Revisiting the P3P problem."
  },
  {
    "prompt": "Explain a concept involving IEEE, CVF, Recogni in computational complexity.",
    "response": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, pages 4872–4880, 2023. [23] Igor V. Dolgachev. Classical Algebraic Geometry: A Modern View. Cambridge University Press, Cambridge, 2012. [24] Peter Doyle and Curt McMullen. Solving the quintic by iteration. Acta Mathematica, 163(3-4):151–180, 1989. [25] Jan Draisma, Emil Horobet,, Giorgio Ottaviani, Bernd Sturmfels, and Rekha R. Thomas. The Euclidean distance degree of an algebraic variety. Found. Comput."
  },
  {
    "prompt": "This is a technical discussion about Math., Dmitriy, Drusvyatskiy in theoretical computer science.",
    "response": "Math., 16(1):99–149, 2016. [26] Jan Draisma and Jose Rodriguez. Maximum likelihood duality for determinantal varieties. International Mathematics Research Notices. IMRN, (20):5648–5666, 2014. [27] Dmitriy Drusvyatskiy, Hon-Leung Lee, Giorgio Ottaviani, and Rekha R. Thomas. The Euclidean distance degree of orthogonally invariant matrix varieties. Israel Journal of Mathematics, 221(1):291– 316, 2017. [28] Dmitriy Drusvyatskiy, Hon-Leung Lee, and Rekha R. Thomas."
  },
  {
    "prompt": "Explain a concept involving Timothy, Duff, Anton in computational complexity.",
    "response": "Counting real critical points of the distance to orthogonally invari- ant matrix sets. SIAM Journal on Matrix Analysis and Applications, 36(3):1360–1380, 2015. [29] Timothy Duff, Cvetelina Hill, Anders Jensen, Kisun Lee, Anton Leykin, and Jeff Sommars. Solving polynomial systems via homotopy continuation and monodromy. IMA Journal of Numerical Analysis, 39(3):1421–1446, 2019. [30] Timothy Duff, Kathl´en Kohn, Anton Leykin, and Tomas Pajdla."
  },
  {
    "prompt": "Read and understand this section about PLMP, Visi, Minimal from a research paper.",
    "response": "PLMP–[oint-Line Minimal Problems in Complete Multi-view Visi- bility. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 46(1):421–435, 2023. 32 [31] Timothy Duff, Viktor Korotynskiy, Tomas Pajdla, and Margaret Re- gan. Using monodromy to recover symmetries of polynomial systems. In Proceedings of the International Symposium on Symbolic & Alge- braic Computation (ISSAC 2023), pages 251–259. ACM, New York, [2023] ©2023."
  },
  {
    "prompt": "This is a technical discussion about Viktor, Korotynskiy, Tomas in theoretical computer science.",
    "response": "[32] Timothy Duff, Viktor Korotynskiy, Tomas Pajdla, and Margaret H. Regan. Galois/monodromy groups for decomposing minimal prob- lems in 3D reconstruction. SIAM Journal on Applied Algebra and Geometry, 6(4):740–772, 2022. [33] Timothy Duff and Kisun Lee. Certified homotopy tracking using the Krawczyk method. In ISSAC’24—Proceedings of the 2024 Inter- national Symposium on Symbolic and Algebraic Computation, pages 274–282. ACM, New York, [2024] ©2024."
  },
  {
    "prompt": "Understanding the computational aspects of Anton, Leykin, Rodriguez.",
    "response": "[34] Timothy Duff, Anton Leykin, and Jose Israel Rodriguez. u- generation: solving systems of polynomials equation-by-equation. Numer. Algorithms, 95(2):813–838, 2024. [35] Timothy Duff and Felix Rydell. Metric multiview geometry–a cata- logue in low dimensions. arXiv preprint arXiv:2402.00648, 2024. [36] Alexander Esterov. Galois theory for general systems of polynomial equations. Compositio Mathematica, 155(2):229–245, 2019. [37] Alexander Esterov and Lionel Lang."
  },
  {
    "prompt": "Scientific context on Galois, ucts, Sparse and their role in graph theory.",
    "response": "Sparse polynomial equations and other enumerative problems whose Galois groups are wreath prod- ucts. Selecta Mathematica (New Series), 28(2):Paper No. 22, 35, 2022. [38] Benson Farb and Jesse Wolfson. Resolvent degree, Hilbert’s 13th problem and geometry. L’Enseignement Math´ematique, 65(3-4):303– 376, 2019. [39] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography."
  },
  {
    "prompt": "Understanding the computational aspects of ACM, Communications, Patrizia.",
    "response": "Communications of the ACM, 24(6):381– 395, 1981. [40] Patrizia Gianni and Teo Mora. Algebraic solution of systems of poly- nomial equations using Groebner bases. In Applied algebra, algebraic algorithms and error-correcting codes (Menorca, 1987), volume 356 of Lecture Notes in Comput. Sci., pages 247–257. Springer, Berlin, 1989. [41] Daniel R. Grayson and Michael E. Stillman. Macaulay2, a software system for research in algebraic geometry. Available at http://www2. macaulay2.com."
  },
  {
    "prompt": "Scientific context on Alexandre, Lairez, Guillemot and their role in graph theory.",
    "response": "[42] Alexandre Guillemot and Pierre Lairez. Validated numerics for al- gebraic path tracking. In ISSAC’24—Proceedings of the 2024 Inter- national Symposium on Symbolic and Algebraic Computation, pages 36–45. ACM, New York, [2024] ©2024. [43] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 33 [44] Richard I Hartley and Peter Sturm. Triangulation. Computer vision and image understanding, 68(2):146–157, 1997."
  },
  {
    "prompt": "Read and understand this section about Sturmfels, Bernd, Jose from a research paper.",
    "response": "[45] Jonathan Hauenstein, Jose Israel Rodriguez, and Bernd Sturmfels. Maximum likelihood for matrices with rank constraints. J. Algebr. Stat., 5(1):18–38, 2014. [46] Jonathan D. Hauenstein, Jose Israel Rodriguez, and Frank Sottile. Numerical computation of Galois groups. Found. Comput. Math., 18(4):867–890, 2018. [47] Paul Alexander Helminck, Oskar Henriksson, and Yue Ren. A trop- ical method for solving parametrized polynomial systems. arXiv preprint arXiv:2409.13288, 2024."
  },
  {
    "prompt": "Scientific context on Serkan, Amit, Khetan and their role in graph theory.",
    "response": "[48] Serkan Ho¸sten, Amit Khetan, and Bernd Sturmfels. Solving the like- lihood equations. Foundations of Computational Mathematics. The Journal of the Society for the Foundations of Computational Mathe- matics, 5(4):389–407, 2005. [49] Petr Hruby, Timothy Duff, and Marc Pollefeys. Efficient solution of point-line absolute pose. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21316– 21325, 2024."
  },
  {
    "prompt": "Read and understand this section about Viktor, Petr, Hruby from a research paper.",
    "response": "[50] Petr Hruby, Viktor Korotynskiy, Timothy Duff, Luke Oeding, Marc Pollefeys, Tomas Pajdla, and Viktor Larsson. Four-view geometry with unknown radial distortion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8990–9000, 2023. [51] Birkett Huber and Bernd Sturmfels. A polyhedral method for solving sparse polynomial systems. Mathematics of Computation, 64(212):1541–1555, 1995. [52] David L. Johnson."
  },
  {
    "prompt": "This is a technical discussion about Kohn, Kathl´en, groups in theoretical computer science.",
    "response": "Minimal permutation representations of finite groups. American Journal of Mathematics, 93:857–866, 1971. [53] Kim Kiehn, Albin Ahlb¨ack, and Kathl´en Kohn. PLMP–Point-Line Minimal Problems for Projective SfM. 2025. [54] Joe Kileel and Kathl´en Kohn. Snapshot of algebraic vision. arXiv preprint arXiv:2210.11443, 2022. [55] Anatoliy Kouchnirenko. Poly`edres de Newton et nombres de Milnor. Inventiones Mathematicae, 32(1):1–31, 1976. [56] L. G. Kov´acs and Cheryl E. Praeger."
  },
  {
    "prompt": "Scientific context on permu, tation, groups and their role in graph theory.",
    "response": "On minimal faithful permu- tation representations of finite groups. Bulletin of the Australian Mathematical Society, 62(2):311–317, 2000. [57] Khazhgali Kozhasov, Alan Muniz, Yang Qi, and Luca Sodomaco. On the minimal algebraic complexity of the rank-one approximation problem for general inner products. arXiv preprint arXiv:2309.15105, 2023. [58] Erwin Kruppa. ¨Uber einige orientierungsprobleme der photogram- metrie."
  },
  {
    "prompt": "Understanding the computational aspects of Wissenschaften, Abteilung, der.",
    "response": "Sitzungsberichte der mathematisch-naturwissenschaftlichen Klasse der kaiserlichen Akademie der Wissenschaften, Abteilung II a, 121(1):3–16, 1912. 34 [59] Kaie Kubjas, Olga Kuznetsova, and Luca Sodomaco. Algebraic de- gree of optimization over a variety with an application to p-norm distance degree. arXiv preprint arXiv:2105.07785, 2021. [60] Zuzana Kukelova, Martin Bujnak, and Tom´as Pajdla. Polyno- mial eigenvalue solutions to minimal problems in computer vision."
  },
  {
    "prompt": "Read and understand this section about Intelligence, Transactions, Analysis from a research paper.",
    "response": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1381–1393, 2012. [61] Viktor Larsson, Kalle ˚Astr¨om, and Magnus Oskarsson. Efficient solvers for minimal problems by syzygy-based reduction. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2383–2392. IEEE Computer Society, 2017. [62] Viktor Larsson, Nicolas Zobernig, Kasim Taskin, and Marc Pollefeys."
  },
  {
    "prompt": "Scientific context on Calibration-free, computation, Anton and their role in graph theory.",
    "response": "Calibration-free structure-from-motion with calibrated radial trifocal tensors. In European Conference on Computer Vision, pages 382–399. Springer, 2020. [63] Anton Leykin and Frank Sottile. Galois groups of Schubert prob- lems via homotopy computation. Mathematics of Computation, 78(267):1749–1765, 2009. [64] Anton Leykin and Josephine Yu. Beyond polyhedral homotopies. Journal of Symbolic Computation, 91:173–180, 2019. [65] Shaowei Lin and Bernd Sturmfels."
  },
  {
    "prompt": "Scientific context on Julia, Lindberg, prin and their role in graph theory.",
    "response": "Polynomial relations among prin- cipal minors of a 4×4-matrix. Journal of Algebra, 322(11):4121–4131, 2009. [66] Julia Lindberg, Carlos Am´endola, and Jose Israel Rodriguez. Esti- mating gaussian mixtures using sparse polynomial moment systems. SIAM Journal on Mathematics of Data Science, 7(1):224–252, 2025. [67] Julia Lindberg, Leonid Monin, and Kemal Rose. The algebraic degree of sparse polynomial optimization. arXiv preprint arXiv:2308.07765, 2023."
  },
  {
    "prompt": "This is a technical discussion about Stefano, Soatto, Jana in theoretical computer science.",
    "response": "[68] Yi Ma, Stefano Soatto, Jana Koˇseck´a, and Shankar Sastry. An Invi- tation to 3-D Vision: from Images to Geometric Models, volume 26. Springer, 2004. [69] Abraham Mart´ın del Campo and Jose Israel Rodriguez. Critical points via monodromy and local methods. Journal of Symbolic Com- putation, 79:559–574, 2017. [70] Laurentiu G. Maxim, Jose I. Rodriguez, and Botong Wang. Euclidean distance degree of the multiview variety. SIAM Journal on Applied Algebra and Geometry, 4(1):28–48, 2020."
  },
  {
    "prompt": "Explain a concept involving David, Nist´er, IEEE in computational complexity.",
    "response": "[71] David Nist´er. An efficient solution to the five-point relative pose problem. IEEE transactions on pattern analysis and machine intel- ligence, 26(6):756–770, 2004. [72] David Nist´er, Richard I. Hartley, and Henrik Stew´enius. Using Galois theory to prove structure from motion algorithms are optimal. In 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007), 18-23 June 2007, Minneapolis, Minnesota, USA. IEEE Computer Society, 2007."
  },
  {
    "prompt": "Understanding the computational aspects of Giorgio, Ottaviani, Spaenlehauer.",
    "response": "35 [73] Giorgio Ottaviani, Pierre-Jean Spaenlehauer, and Bernd Sturmfels. Exact solutions in structured low-rank approximation. SIAM Journal on Matrix Analysis and Applications, 35(4):1521–1542, 2014. [74] Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71– 110, 1894. [75] Felix Rydell, Georg B¨okman, Fredrik Kahl, and Kathl´en Kohn."
  },
  {
    "prompt": "This is a technical discussion about bounds, framework, reducing in theoretical computer science.",
    "response": "A framework for reducing the complexity of geometric vision problems and its application to two-view triangulation with approximation bounds. arXiv preprint arXiv:2503.08142, 2025. [76] Jean-Pierre Serre. Topics in Galois theory, volume 1 of Research Notes in Mathematics. Jones and Bartlett Publishers, Boston, MA, 1992. Lecture notes prepared by Henri Damon [Henri Darmon], With a foreword by Darmon and the author. [77] Andrew J. Sommese and Charles W. Wampler, II."
  },
  {
    "prompt": "Scientific context on polynomials, numerical, solution and their role in graph theory.",
    "response": "The numerical solution of systems of polynomials. World Scientific Publishing Co. Pte. Ltd., Hackensack, NJ, 2005. Arising in engineering and science. [78] Frank Sottile and Thomas Yahl. Galois groups in enumerative geom- etry and applications. arXiv preprint arXiv:2108.07905, 2021. [79] Richard P. Stauduhar. The determination of Galois groups. Mathe- matics of Computation, 27:981–996, 1973. [80] Henrik Stew´enius, Frederik Schaffalitzky, and David Nist´er."
  },
  {
    "prompt": "Read and understand this section about Volume, pages, Vision from a research paper.",
    "response": "How hard is 3-view triangulation really? In Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1, volume 1, pages 686–693. IEEE, 2005. [81] Peter Sturm. A historical survey of geometric computer vision. In International Conference on Computer Analysis of Images and Pat- terns, pages 1–8. Springer, 2011. [82] Bernd Sturmfels and Simon Telen. Likelihood equations and scatter- ing amplitudes. Algebraic Statistics, 12(2):167–186, 2021. [83] Seth Sullivant."
  },
  {
    "prompt": "Understanding the computational aspects of Mathematics, Graduate, Studies.",
    "response": "Algebraic statistics, volume 194 of Graduate Studies in Mathematics. American Mathematical Society, Providence, RI, 2018. [84] SriRam Thirthala and Marc Pollefeys. Radial multi-focal tensors. International Journal of Computer Vision, 96(2):195–211, 2012. [85] Roger Y. Tsai. A versatile camera calibration technique for high- accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses. IEEE Journal on Robotics and Automation, 3(4):323–344, 1987. [86] Madeleine Aster Weinstein."
  },
  {
    "prompt": "Understanding the computational aspects of Geometry, Algebraic, Metric.",
    "response": "Metric Algebraic Geometry. PhD thesis, UC Berkeley, 2021. Department of Mathematics, University of Missouri, Columbia MO, 65211 E-mail address: tduff@missouri.edu 36"
  },
  {
    "prompt": "Read and understand this section about Science, log, Switzerland from a research paper.",
    "response": "Query-Efficient Fixpoints of ℓp-Contractions Sebastian Haslebacher1, Jonas Lill1, Patrick Schnider1,2, and Simon Weber1 1Department of Computer Science, ETH Z¨urich, Switzerland 2Department of Mathematics and Computer Science, University of Basel, Switzerland Abstract We prove that an ε-approximate fixpoint of a map f : [0, 1]d →[0, 1]d can be found with O(d2(log 1 ε + log 1 1−λ)) queries to f if f is λ-contracting with respect to an ℓp-metric for some p ∈[1, ∞) ∪{∞}."
  },
  {
    "prompt": "Understanding the computational aspects of Yannakakis, Chen, ℓp-metrics.",
    "response": "This generalizes a recent result of Chen, Li, and Yannakakis [STOC’24] from the ℓ∞-case to all ℓp-metrics. Previously, all query upper bounds for p ∈[1, ∞) \\ {2} were either exponential in d, log 1 ε , or log 1 1−λ. Chen, Li, and Yannakakis also show how to ensure that all queries to f lie on a discrete grid of limited granularity in the ℓ∞-case. We provide such a rounding for the ℓ1-case, placing an appropriately defined version of the ℓ1-case in FPdt."
  },
  {
    "prompt": "Understanding the computational aspects of bution, prove, centerpoint.",
    "response": "To prove our results, we introduce the notion of ℓp-halfspaces and generalize the classical centerpoint theorem from discrete geometry: for any p ∈[1, ∞) ∪{∞} and any mass distri- bution (or point set), we prove that there exists a centerpoint c such that every ℓp-halfspace defined by c and a normal vector contains at least a 1 d+1-fraction of the mass (or points)."
  },
  {
    "prompt": "Read and understand this section about Mar, Introduction, points from a research paper.",
    "response": "1 arXiv:2503.16089v1 [cs.CC] 20 Mar 2025 1 Introduction A contraction map is a function that maps any two points in such a way that their two images lie closer to each other than the original points. Formally, a function f : X →X on a metric space (X, dX) is a contraction map if there exists some λ < 1, such that any two points x, y ∈ X satisfy dX( f (x), f (y)) ≤λ · dX(x, y)."
  },
  {
    "prompt": "Understanding the computational aspects of fixpoint, point, Banach.",
    "response": "Banach’s fixpoint theorem [1] famously states that a contraction map must have a unique fixpoint, i.e., a point x ∈X with f (x) = x. In this paper, we consider the problem of finding approximate fixpoints based on the residual error criterion: we wish to find any point with dX( f (x), x) ≤ε, called an ε-approximate fixpoint. Banach’s proof actually provides a simple algorithm to find such an ε-approximate fixpoint: start at any point x ∈X and iteratively apply f until dX( f k+1(x), f k(x)) ≤ε."
  },
  {
    "prompt": "Scientific context on log, queries, diameter and their role in graph theory.",
    "response": "This algorithm requires at most O \u0010 log( 1 ε ) log( 1 λ ) \u0011 queries to f on metric spaces of constant diameter. Unfortunately, Banach’s iterative algorithm is quite slow when λ is close to 1, which is often the regime that is useful in practice [36]. For example, Simple Stochastic Games (SSG) reduce to approximating the fixpoint of a contraction map on the metric space induced on the unit cube [0, 1]d by the ℓ∞-norm, with 1 −λ and ε exponentially small [7]."
  },
  {
    "prompt": "Scientific context on ARRIVAL, Similarly, cube and their role in graph theory.",
    "response": "Similarly, the ARRIVAL problem reduces to approximating the fixpoint of a contraction map on the unit cube [0, 1]d, again with 1 −λ and ε exponentially small, but here the metric is induced by the ℓ1-norm instead [17]. Note that no polynomial-time algorithms are known for SSG and ARRIVAL, despite both of them being contained in NP ∩CoNP [7, 10]."
  },
  {
    "prompt": "Understanding the computational aspects of ARRIVAL, SSG, log.",
    "response": "If we could approximate the fixpoint of a λ-contraction map on [0, 1]d with respect to an ℓp-norm in time polynomial in d, log( 1 ε ), and log( 1 1−λ), then this would put both ARRIVAL and SSG in P. In the Euclidean case (p = 2), such efficient algorithms have been known for quite some time [19, 35, 36]. Concretely, the Inscribed Ellipsoid algorithm [19] requires O(d log( 1 ε )) queries (indepen- dent of λ) to find an ε-approximate fixpoint."
  },
  {
    "prompt": "Explain a concept involving algorithm, CLS, efficiently in computational complexity.",
    "response": "Moreover, each query point can be found efficiently, implying an overall time-efficient (and not just query-efficient) algorithm. In contrast, only little is known for p ̸= 2. The problem is known to be contained in CLS [8] and was explicitely mentioned as an open problem by Fearnley, Goldberg, Hollender, and Sa- vani [12] when they proved CLS = PPAD ∩EOPL."
  },
  {
    "prompt": "Understanding the computational aspects of Fearnley, Gordon, Mehta.",
    "response": "For p ∈N ∪{∞}, Fearnley, Gordon, Mehta, and Savani proved containment in UEOPL by reduction to One-Permutation Discrete Contrac- tion (OPDC) [13]. Moreover, their algorithm for OPDC implies an algorithm for finding an ε-approximate fixpoint in O(logd( 1 ε )) queries. Unfortunately, this is still exponential in d. Chen, Li, and Yannakakis [5] recently (STOC’24) gave the first query-efficient algorithm in the special case of the ℓ∞-norm."
  },
  {
    "prompt": "Scientific context on log, Concretely, independent and their role in graph theory.",
    "response": "Concretely, their algorithm can find an ε-approximate fixed point with O(d2 log 1 ε ) (independent of λ) queries to f. One of the main questions that they ask is whether similar query upper bounds can be shown for p ̸= {2, ∞} as well. We answer this question affirmatively by achieving a query upper bound of O(d2(log 1 ε + log 1 1−λ)) for all p ∈ [1, ∞) ∪{∞}."
  },
  {
    "prompt": "Understanding the computational aspects of Chen, Yannakakis, Note.",
    "response": "Note that using an observation of Chen, Li, and Yannakakis [5], we can also get rid of the factor log 1 1−λ for p = ∞, hence matching their bound. Neither our algorithm nor the algorithm of Chen, Li, and Yannakakis [5] is time-efficient, i.e., in both cases it is unclear how to algorithmically find the query points efficiently."
  },
  {
    "prompt": "Understanding the computational aspects of Chen, Yannakakis, ensure.",
    "response": "However, Chen, Li, and Yannakakis [5] ensure that all queries made by their algorithm lie on a discrete grid of limited granularity, which is certainly a prerequisite for any time-efficient algorithms. We provide a similar rounding of queries to a grid in the ℓ1-case. 2 Note that since our algorithm is only query-efficient, it does not imply polynomial-time algo- rithms for SSG, ARRIVAL, or other applications."
  },
  {
    "prompt": "This is a technical discussion about existence, Chen, Yannakakis in theoretical computer science.",
    "response": "However, as pointed out by Chen, Li, and Yannakakis [5], the existence of query-efficient algorithms could be viewed as evidence for the existence of time-efficient algorithms for ℓp-contraction (at least for p ∈{1, ∞}). 1.1 Results For any ℓp-norm with p ∈[1, ∞) ∪{∞}, we present an algorithm that can find an ε-approximate fixpoint of a λ-contracting function f : [0, 1]d →[0, 1]d by making at most O(d2(log 1 ε + log 1 1−λ)) queries to f."
  },
  {
    "prompt": "Read and understand this section about points, grid, show from a research paper.",
    "response": "In the ℓ1-case, we show how to ensure that all queries are made on points of a given discrete grid, as long as the distance between two neighboring grid points is at most 2d ε 1+λ 1−λ. A similar rounding was provided by Chen, Li, and Yannakakis [5] in the ℓ∞-case, and their rounding can also be used in combination with our algorithm."
  },
  {
    "prompt": "Read and understand this section about bits, function, output from a research paper.",
    "response": "If the function f produces only poly(k) bits of output when given k bits of input (as it is the case with many applications of the ℓ1-case and the ℓ∞-case), our algorithm only needs to input and output polynomially many bits of information into the function f. As a main technical tool, we consider a generalization of the notion of halfspaces from Euclidean geometry into ℓp-geometry."
  },
  {
    "prompt": "Understanding the computational aspects of centerpoint, theorem, works.",
    "response": "In particular, we prove a generalization of the classical centerpoint theorem from discrete and computational geometry that works for any p ∈[1, ∞) ∪{∞}. We think that this might be of independent interest. Our generalized centerpoint theorem works for both mass distributions and point sets and is tight in both cases."
  },
  {
    "prompt": "Scientific context on search, total, lies and their role in graph theory.",
    "response": "Our results also imply that a properly defined total search problem version of the ℓ1-case lies in FPdt, the class of black-box total search problems that can be solved efficiently by decision trees [15]. Again, the analogous observation for the ℓ∞-case was made by Chen, Li, and Yan- nakakis [5] already."
  },
  {
    "prompt": "Explain a concept involving promise, proof, grid in computational complexity.",
    "response": "In a total search problem, the algorithm has to either return a solution (an ε-approximate fixpoint) or a proof that the given instance does not fulfill the promise (i.e., that the given function on the grid does not extend to a contraction map on all of [0, 1]d). In the ℓ∞-case, it suffices to consider pairs of grid points violating the contraction property as proof for the violation of the promise [5]."
  },
  {
    "prompt": "Understanding the computational aspects of proof, suffice, problem.",
    "response": "For the ℓ1-case, this does not seem to suffice, which means that we need to consider a slightly more complicated proof of violation to properly define a total search version of the problem. 1.2 Proof Techniques Our algorithm is based on a simple observation that has also been used in previous work [5, 35]: due to the contraction property, given any query point x, the fixpoint x⋆must be closer to the query’s response f (x) than to the query x itself."
  },
  {
    "prompt": "Read and understand this section about remaining, search, response from a research paper.",
    "response": "To quickly hone in on the fixpoint, we therefore wish to query a point x such that for every possible response f (x) of f, a significantly large part of the remaining search space lies at least as close to x as to f (x). Querying x then allows us to considerably shrink the remaining search space no matter the response f (x)."
  },
  {
    "prompt": "Scientific context on mass, centerpoint, Euclidean and their role in graph theory.",
    "response": "In the Euclidean case (ℓ2-metric), a centerpoint of the remaining search space makes for a good query: a point c is a ρ-centerpoint of a mass distribution µ if any halfspace that contains c also contains at least a ρ-fraction of the mass of µ. The celebrated centerpoint theorem originally due to Rado [31] guarantees that a 1 d+1-centerpoint (commonly just called a centerpoint) always exists."
  },
  {
    "prompt": "Read and understand this section about search, space, querying from a research paper.",
    "response": "Since the set of points that are at least as close to c as to f (c) is a halfspace containing c, querying a centerpoint c of the remaining search space allows us to discard at least a 1 d+1-fraction 3 of the search space. We generalize these concepts from Euclidean geometry to arbitrary ℓp-metrics. We first define ℓp- halfspaces as a generalization of halfspaces."
  },
  {
    "prompt": "This is a technical discussion about Figure, Informally, speaking in theoretical computer science.",
    "response": "Informally speaking, an ℓp-halfspace Hp x,v is defined by a point x ∈Rd and a direction v ∈Sd−1 and contains all points that are at least as close (with respect to the ℓp-metric) to x as to x −εv for all ε > 0 (see Figure 1). We then prove a generalized centerpoint theorem for ℓp-metrics, which says that for every mass distribution µ on Rd, there exists a point c satisfying µ(Hp c,v) ≥ 1 d+1µ(Rd) for all v ∈Sd−1."
  },
  {
    "prompt": "Explain a concept involving query, algorithm, allowing in computational complexity.",
    "response": "This theorem implies existence of good query points for our algorithm, allowing us to discard a 1 d+1-fraction of the remaining search space with each query. Figure 1: The ℓ5-halfspace H5 0,v for v = (−2, −0.5, −0.6) is drawn in red. The vector shown in the image is −v. Image created with the Desmos 3D calculator. To prove our generalized centerpoint theorem, we use Brouwer’s fixpoint theorem [3]."
  },
  {
    "prompt": "Explain a concept involving Con, cretely, mass in computational complexity.",
    "response": "Con- cretely, given a mass distribution µ on Rd, we consider a function F : Rd →Rd which intuitively maps any point x by pushing it in all directions v ∈Sd−1 for which µ(Hp x,−v) is not yet large enough. Formally, we define it using the following integral Fi(x) := xi + Z Sd−1 vi max \u0012 1 d + 1 −µ(Hp x,−v), 0 \u0013 dv for all i ∈[d]. We show that F is continuous, which means that if we restrict it to a compact domain, Brouwer’s fixpoint theorem implies existence of a fixpoint."
  },
  {
    "prompt": "Scientific context on centerpoint, fixpoint, prove and their role in graph theory.",
    "response": "We then prove that this fixpoint must be a centerpoint. After proving that we can reduce our remaining search space by a 1 d+1-fraction with every query, we still need a termination condition for our algorithm. To this end, we show that as long as 4 a queried point x is not an ε-approximate fixpoint, any point sufficiently close to the fixpoint x⋆cannot lie closer to x than to f (x)."
  },
  {
    "prompt": "Explain a concept involving long, log, Vε,λ in computational complexity.",
    "response": "Thus, there exists a small ball around x⋆that cannot be discarded as long as we have not queried an ε-approximate fixpoint yet. This in turn implies a lower bound Vε,λ for the volume of the remaining search space that holds as long as the algorithm does not terminate. From this, we can then conclude the upper bound log d d+1 \u0012 1 Vε,λ \u0013 = O \u0012 d2 \u0012 log 1 ε + log 1 1 −λ + log d \u0013\u0013 on the number of queries needed to find an ε-approximate fixpoint."
  },
  {
    "prompt": "Scientific context on Chen, term, dominates and their role in graph theory.",
    "response": "To get rid of the log d term, we additionally observe that a simple iterative algorithm is faster if d dominates 1 ε and 1 1−λ. As mentioned before, Chen, Li, and Yannakakis [5] provide a rounding scheme in the ℓ∞-case that allows them to ensure that all queries lie on a discrete grid of limited granularity. Their rounding scheme can also be used in combination with our algorithm, but is specific to the ℓ∞- case."
  },
  {
    "prompt": "Explain a concept involving applications, scheme, motivated in computational complexity.",
    "response": "For the ℓ1-case, which is also motivated by applications, we therefore provide our own rounding scheme. Concretely, we show that for p = 1, each centerpoint query c of the original algorithm can be replaced with a query to the closest grid point c′ to c. To make this work, we need to use a discrete variant of our centerpoint theorem to measure the size of the remaining search space in terms of remaining grid points. 1.3 Discussion Comparison to Results for p = 2."
  },
  {
    "prompt": "This is a technical discussion about polyhedron, centerpoint, Euclidean in theoretical computer science.",
    "response": "The intersection of Euclidean halfspaces is a polyhedron, and Gr¨unbaum’s theorem says that the centroid of a polyhedron is a 1 e -centerpoint of the poly- hedron [16]. Thus, there always exists a 1 e -centerpoint of the remaining search space in the ℓ2-case, rather than just a 1 d+1-centerpoint."
  },
  {
    "prompt": "Understanding the computational aspects of Ellipsoid, Centroid, specialized.",
    "response": "Thanks to this, specialized algorithms such as the In- scribed Ellipsoid and the Centroid algorithm [35, 36] achieve a query upper bound that depends only linearly on d (compared to our quadratic dependency). An interesting question would be whether something analogous to Gr¨unbaum’s theorem exists for the intersection of ℓp-halfspaces in general. Such a bound could then be plugged into our analysis to improve our upper bounds from O(d2) to O(d log d) in terms of the dimension."
  },
  {
    "prompt": "Read and understand this section about Note, overhead, compared from a research paper.",
    "response": "Note that the log d overhead (compared to aforementioned results with O(d) dependency for the ℓ2-case) would come from using a lower bound of vol(Bp(0, b)) ≥2d d! bd for the volume of a general ℓp-ball in our analysis (see proof of Theorem 4.2 for more details). As we have discussed earlier, the Inscribed Ellipsoid algorithm is not just query-efficient, but can actually be implemented in polynomial time. However, this is not straightforward."
  },
  {
    "prompt": "Read and understand this section about P-hard, Euclidean, centroid from a research paper.",
    "response": "Even in the Euclidean case, computing the centroid of a polyhedron (given by its bounding hyperplanes) is #P-hard [30]. One thus has to rely on approximate centerpoints that can be computed effi- ciently by taking the centroid of an ellipsoid that approximates the polyhedron (hence the name Inscribed Ellipsoid algorithm)."
  },
  {
    "prompt": "Read and understand this section about algorithms, Euclidean, Yannakakis from a research paper.",
    "response": "In other words, using ellipsoids to approximate the remaining search space is the main tool to get from query-efficient algorithms to actual polynomial-time algorithms in the Euclidean case. For more information regarding the ℓ2-case, we refer to the survey by Sikorksi [36]. Comparison to Chen, Li, and Yannakakis [5] on p = ∞. Our approach is similar (but much more general) to the one of Chen, Li, and Yannakakis [5] for the ℓ∞-case."
  },
  {
    "prompt": "Explain a concept involving pyramids, space, pairs in computational complexity.",
    "response": "They prove that there always exists a query point x, such that any response f (x) will allow them to discard at least 5 a 1 2d-fraction of the remaining search space. Concretely, the area that is discarded is one out of 2d pyramids (d pairs of antipodal pyramids) that touch at x. To prove existence of a good query x, they use Brouwer’s fixpoint theorem on a function that balances the mass among the two pyramids in each of the d pairs of antipodal pyramids simultaneously."
  },
  {
    "prompt": "Read and understand this section about log, words, terminology from a research paper.",
    "response": "In other words, their good query point x is essentially a 1 2d-centerpoint, although they do not use this terminology. Their decomposition into pyramids is tailored to the ℓ∞-case and does not generalize. Our algorithm for arbitrary p ∈[1, ∞) ∪{∞} uses O(d2(log 1 ε + log 1 1−λ)) queries, essentially matching the bound in [5]."
  },
  {
    "prompt": "This is a technical discussion about instance, Note, upper in theoretical computer science.",
    "response": "Note that their upper bound does not include λ because they show how to get rid of the dependency on λ by applying a reduction that turns an instance with parameters ε and λ into an instance with ε′ = ε/2 and λ′ = 1 −ε′. This is again specific to the case p = ∞and unlikely to work in general. In order to prove the result for all p ∈[1, ∞) ∪{∞} simultaneously, we avoid a specific decom- position such as the pyramid decomposition in the ℓ∞-case."
  },
  {
    "prompt": "Read and understand this section about generality, approach, makes from a research paper.",
    "response": "We think that the generality of our approach also makes our arguments less technical than the ones used in [5]. Chen, Li, and Yannakakis [5] show that their algorithm also places the following total version of the problem in FPdt: given query access to the function f on a grid, either find an ε-approximate fixpoint, or two grid points violating the contraction property."
  },
  {
    "prompt": "Scientific context on contracting, Totality, cube and their role in graph theory.",
    "response": "Totality of this problem crucially relies on the fact that any ℓ∞-contracting function defined only on the grid can be extended to an ℓ∞-contracting map on the whole cube. Unfortunately, their extension construction, which is an implicit application of the extension theorem of McShane [25], does not generalize to other values of p. In fact, we do not expect a similar extension theorem to hold for general p."
  },
  {
    "prompt": "Scientific context on property, certificate, order and their role in graph theory.",
    "response": "In order to define a total version of the problem in the ℓ1-case, we therefore need to include a different certificate for violation of the contraction property. This certificate uses not just two, but polynomially many grid points, and exists whenever our algorithm would fail to find an ε-approximate fixpoint (on a non-contracting map). CLS-Completeness of General Banach."
  },
  {
    "prompt": "Understanding the computational aspects of Tzamos, Zampetakis, Daskalakis.",
    "response": "Daskalakis, Tzamos, and Zampetakis [9] showed that the problem of computing the fixpoint of a contraction map on the metric space ([0, 1]3, d[0,1]3) is CLS-complete if the metric d[0,1]3 is an arbitrary metric given as an algebraic circuit. This stands in stark contrast to our polynomial-query result for all ℓp-metrics, and highlights that the ℓp- metrics are quite special metrics. For example, as metrics induced by a norm, the ℓp-metrics are translation-invariant."
  },
  {
    "prompt": "This is a technical discussion about metrics, super, norm in theoretical computer science.",
    "response": "In future work, it would be interesting to investigate whether super- polynomial query lower bounds can be proven for some fixed metric, or whether our results can be generalized to more general classes of metrics, for example all metrics induced by some norm. Finding Centerpoints. An important question left unanswered by our work is whether our algorithms can be implemented not only using polynomially many queries, but also in overall polynomial time."
  },
  {
    "prompt": "Explain a concept involving algorithm, implications, polynomial-time in computational complexity.",
    "response": "Such a result would have important implications. For example, it would imply the existence of a polynomial-time algorithm for SSGs [7] in the ℓ∞-case and a polynomial-time algorithm for ARRIVAL [17] in the ℓ1-case. To be able to implement our algorithm in polynomial time, we would have to be able to find a centerpoint of an intersection of ℓp-halfspaces (the remaining search space) efficiently."
  },
  {
    "prompt": "Understanding the computational aspects of poly, note, suffice.",
    "response": "However, note that we would not necessarily need a 1 d+1- centerpoint; a 1 poly(d)-centerpoint would suffice. In Euclidean geometry, such approximate centerpoints can be computed efficiently with a variety 6 of approaches [6, 26]."
  },
  {
    "prompt": "Understanding the computational aspects of set, point, range.",
    "response": "One approach for a randomized algorithm that might potentially generalize to our setting is the one based on computing the centerpoint of a representative subset of a point set: given a point set X ⊆[0, 1]d, the set of all halfspaces induces a set system (or range space) on X. This range space has a Vapnik–Chervonenkis dimension [37] of at most d + 1, as implied by Radon’s lemma [32]."
  },
  {
    "prompt": "Explain a concept involving range, ε-approximation, VC-dimension in computational complexity.",
    "response": "For range spaces with bounded VC-dimension, a random sample of a constant number of points is a so-called ε-approximation [27] with high probability [22, 27]. Furthermore, the centerpoint of such an ε-approximation of the range space of halfspaces is an approximate centerpoint of X itself [24]. More details about this series of results can be found in Chapter 47 of the Handbook of Discrete and Computational Geometry [27]."
  },
  {
    "prompt": "This is a technical discussion about ℓp-halfspaces, intersection, setting in theoretical computer science.",
    "response": "To generalize this approach to our setting, one would need to bound the VC-dimension of the range space induced by ℓp-halfspaces, and to find an efficient way of sampling a random point from the intersection of ℓp-halfspaces. The first step towards such a sampling procedure may be to study the analogue of Linear Programming feasibility for ℓp-halfspaces: how quickly can we decide whether an intersection of ℓp-halfspaces is empty or not? Proof of Centerpoint Theorem."
  },
  {
    "prompt": "Read and understand this section about Brouwer, theorem, fixpoint from a research paper.",
    "response": "Our proof of the generalized centerpoint theorem makes use of Brouwer’s fixpoint theorem. The classical centerpoint theorem is usually proven using Helly’s theorem, which can in turn be proven using Brouwer’s fixpoint theorem [23]."
  },
  {
    "prompt": "Explain a concept involving Chapter, Geometry, proof in computational complexity.",
    "response": "A proof of the classical centerpoint theorem using Brouwer’s fixpoint theorem directly in the style of our proof seems to be somewhat folklore, as this strategy is mentioned in Chapter 27 of the Handbook of Discrete and Computational Geometry [38], but we were unable to find any reference containing the full proof. 1.4 Further Related Work Generalized Centerpoint Theorems. The classical centerpoint theorem has been generalized in many different directions."
  },
  {
    "prompt": "This is a technical discussion about concept, generalized, gen in theoretical computer science.",
    "response": "For example, it has been generalized from single centerpoints (gen- eralizing the concept of a median) to multiple centerpoints (generalizing the concept of quan- tiles) [29, 33]. It has also been generalized from centerpoints to centerdisks [2]. The centerpoint theorem has further been generalized to projective spaces [21], which is the only generalization to non-Euclidean geometries that we are aware of. Fixpoint Theorems."
  },
  {
    "prompt": "Scientific context on fixpoint, Finding, theorem and their role in graph theory.",
    "response": "Many other fixpoint theorems have been studied from the lens of compu- tational complexity. Finding a fixpoint of a continuous function from a hypercube to itself, as guaranteed by Brouwer’s fixpoint theorem, is famously PPAD-complete [28]. Finding a fixpoint of a monotone function from a lattice to itself, as guaranteed by Tarski’s fixpoint theorem, is known to lie in EOPL [11, 14], but no hardness result is known."
  },
  {
    "prompt": "Explain a concept involving Lyuu, Caristi, Banach in computational complexity.",
    "response": "Chang and Lyuu [4] gave lower bounds on the query complexity of finding fixpoints on finite metric spaces, as guaranteed by Banach’s and Caristi’s fixpoint theorems. The computational problem associated with Caristi’s fixpoint theorem on the metric space ([0, 1]3, ℓ∞) has been proven PLS-complete [20]. The com- putational version of Brøndsted’s fixpoint theorem on the same metric space is known to lie in PPAD and to be CLS-hard [20], with the exact complexity still open."
  },
  {
    "prompt": "This is a technical discussion about Section, Overview, Organization in theoretical computer science.",
    "response": "1.5 Overview and Organization We start our exposition in Section 2 by formally defining the problem ℓp-ContractionFixpoint that we are interested in. We also recall important notions and results from the literature that are needed to understand the rest of the paper. 7 We introduce the notion of ℓp-halfspaces in Section 3.1 and discuss some of their properties in Section 3.2. Proofs of those properties are deferred to Appendix A."
  },
  {
    "prompt": "Explain a concept involving Section, distributions, proof in computational complexity.",
    "response": "Section 3.3 contains the proof of our generalized centerpoint theorem for mass distributions. We also provide a discrete variant for point sets instead of mass distributions, the proof of which can be found in Appendix B. Finally, we use Section 3.4 to briefly argue that both versions of our generalized centerpoint theorem are tight. We use Section 4 to describe our algorithms. In Section 4.1, we prove our query upper bound for ℓp-ContractionFixpoint for all p ∈[1, ∞) ∪{∞}."
  },
  {
    "prompt": "This is a technical discussion about Section, solving, problem in theoretical computer science.",
    "response": "In Section 4.2, we then show how we can ensure that all queries lie on a discrete grid in the ℓ1-case, solving what we call the problem ℓ1-GridContractionFixpoint. Finally, we discuss membership in FPdt of a total search version ℓ1-GridContractionFixpoint in Section 4.3. 2 Preliminaries Definition 2.1 (Contraction Map)."
  },
  {
    "prompt": "This is a technical discussion about contraction, map, called in theoretical computer science.",
    "response": "Given a metric space (X, dX) and a contraction factor 0 ≤λ < 1, a function f : X →X is called a λ-contraction map (or λ-contracting) if dX( f (x), f (y)) ≤λ · dX(x, y) holds for all x, y ∈X. A function is called a contraction map if it is λ-contracting for some 0 ≤λ < 1. Theorem 2.2 (Banach Fixpoint Theorem [1]). Every contraction map f : X →X on a non-empty complete metric space (X, dX) admits a unique fixpoint x⋆∈X, i.e., a unique point satisfying f (x⋆) = x⋆."
  },
  {
    "prompt": "Read and understand this section about metric, ℓp-norm, paper from a research paper.",
    "response": "In this paper, we will consider metric spaces of the form ([0, 1]d, ℓp) for some p ∈[1, ∞) ∪{∞}, where ℓp denotes the metric induced by the ℓp-norm. Definition 2.3 (ℓp-Norm). The ℓp-norm of x ∈Rd is defined as ||x||p := \u0010 ∑d i=1 |xi|p\u00111/p for p ∈[1, ∞) and ||x||p := maxi |xi| for p = ∞. Concretely, the distance between two points x, y ∈([0, 1]d, ℓp) is given by ||y −x||p. We also use Bp(x, r) := {y ∈Rd | ||y −x||p ≤r} to denote the ℓp-ball of radius r around x."
  },
  {
    "prompt": "Understanding the computational aspects of fixpoint, Finding, ε-approximate.",
    "response": "Finding the exact fixpoint of a contraction map is often infeasible, which is why one usually considers the problem of finding ε-approximate fixpoints instead. Definition 2.4 (ε-Approximate Fixpoint). Given a contraction map f : X →X and an ε > 0, a point x ∈X is called an ε-approximate fixpoint if dX(x, f (x)) ≤ε. In the literature, this condition is often called the residual error criterion."
  },
  {
    "prompt": "Explain a concept involving fixpoint, Note, ε-approximate in computational complexity.",
    "response": "Note that the contraction property ensures that any ε-approximate fixpoint has a distance of at most ε 1−λ to the unique fixpoint x⋆, thus also bounding the so-called absolute error dX(x, x⋆). Definition 2.5. The ℓp-ContractionFixpoint problem is to find an ε-approximate fixpoint of a λ- contraction map f on ([0, 1]d, ℓp), given parameters d ∈N, ε > 0, λ ∈[0, 1) and p ∈[1, ∞) ∪{∞} as well as black-box query access to f. The goal is to minimize the number of queries made to f."
  },
  {
    "prompt": "This is a technical discussion about log, ℓp-ContractionFixpoint, algorithm in theoretical computer science.",
    "response": "We consider an algorithm for ℓp-ContractionFixpoint to be query-efficient if the number of queries made to f is polynomial in d, log( 1 ε ), and log( 1 1−λ), and independent of p. In ℓp-ContractionFixpoint, we are allowed to query f at any point x ∈[0, 1]d, even at points with irrational coordinates."
  },
  {
    "prompt": "This is a technical discussion about discretized, version, applications in theoretical computer science.",
    "response": "This might be unsuitable in some applications, and it seems rea- sonable to also consider a discretized version of this problem, where we are only allowed to 8 make queries to points on a discrete grid. Although we define this discretized version for all p ∈[1, ∞) ∪{∞}, we will only really study it for the cases p ∈{1, ∞} (motivated by applica- tions). Definition 2.6. Given an integer b ≥1, the grid Gd b ⊆[0, 1]d is the set of points x ∈[0, 1]d with rational coordinates x1, . . ."
  },
  {
    "prompt": "Read and understand this section about integers, form, Definition from a research paper.",
    "response": ", xd of the form xi = ki 2b for integers k1, . . . , kd ∈{0, 1, . . . , 2b}. Definition 2.7. A function f : Gd b →[0, 1]d is called a λ-contraction grid-map if there exists a λ- contraction map f ′ : [0, 1]d →[0, 1]d such that f (x) = f ′(x) for all x ∈Gd b (both f ′ and f are contracting with respect to the same fixed ℓp-metric). Definition 2.8. The ℓp-GridContractionFixpoint problem is to find an ε-approximate fixpoint x ∈ Gd b of a λ-contraction grid-map f : Gd b →[0, 1]d."
  },
  {
    "prompt": "Read and understand this section about priori, ℓp-GridContractionFixpoint, well-defined from a research paper.",
    "response": "A priori, ℓp-GridContractionFixpoint is not guaranteed to have a solution and might therefore not be well-defined. However, it is not hard to see that an ε-approximate fixpoint on the grid must exist if the input grid is fine enough. Lemma 2.9. For b ≥log2( d+dλ 2ε ), any λ-contraction grid-map f : Gd b →[0, 1]d admits an ε-approximate fixpoint. Proof. Let x⋆be the unique fixpoint of some λ-contraction map f ′ : [0, 1]d →[0, 1]d extending f."
  },
  {
    "prompt": "Explain a concept involving point, fixpoint, calculation in computational complexity.",
    "response": "The following calculation shows that any point x ∈[0, 1]d with ||x −x⋆||p ≤ ε 1+λ must be an ε-approximate fixpoint: || f (x) −x||p ≤|| f (x) −f (x⋆)||p + || f (x⋆) −x||p ≤λ||x −x⋆||p + ||x −x⋆||p = (1 + λ)||x −x⋆||p ≤ε. We now determine a lower bound on b needed to ensure the existence of such a point x ∈Gd b. Observe that since B∞(x⋆, r d) ⊆Bp(x⋆, r) (follows from ||x||p ≤d||x||∞) for all r > 0 and all p, it suffices to prove existence of a grid point in B∞(x⋆, ε d+dλ)."
  },
  {
    "prompt": "Scientific context on Observe, cube, axis-aligned and their role in graph theory.",
    "response": "Observe that this is the axis-aligned cube x⋆+ [− ε d+dλ, ε d+dλ]d. If 2−b ≤ 2ε d+dλ, this cube must always contain at least one point of Gd b. Thus, we get the lower bound b ≥log2( d+dλ 2ε ). 3 ℓp-Halfspaces and ℓp-Centerpoints In this section, we generalize halfspaces and centerpoints from Euclidean geometry to ℓp-norms for any p ∈[1, ∞) ∪{∞}. Note that we sometimes use ∡(v, w) to denote the angle between two vectors v, w ∈Rd. We also sometimes use −→ xy for the vector y −x."
  },
  {
    "prompt": "Read and understand this section about denotes, halfspace, angle from a research paper.",
    "response": "For example, ∡(−→ xy, v) denotes the angle between the vectors y −x and v. 3.1 ℓp-Halfspaces There are many different ways of defining a halfspace in Euclidean geometry (p = 2). One natural way that will appear in our algorithms for finding the fixpoints of contraction maps (see Section 4) is to define a halfspace using two distinct points x, y ∈Rd: a point z ∈Rd is considered to be in the halfspace if and only if ∥x −z∥2 ≤∥y −z∥2."
  },
  {
    "prompt": "Scientific context on Definition, Bisector, directly and their role in graph theory.",
    "response": "This directly generalizes to arbitrary p ∈[1, ∞) ∪{∞} as follows. 9 Definition 3.1 (Bisector ℓp-Halfspace). For fixed p ∈[1, ∞) ∪{∞} and distinct points x, y ∈Rd, the bisector ℓp-halfspace Hp x,y ⊆Rd is defined as Hp x,y := {z ∈Rd | ||x −z||p ≤||y −z||p}."
  },
  {
    "prompt": "Read and understand this section about Euclidean, product, point from a research paper.",
    "response": "In Euclidean geometry, rather than defining a halfspace using the principle of bisection, we can alternatively define it by a point x on the boundary and a normal vector v ∈Sd−1, making use of the Euclidean inner product: a point z ∈Rd belongs to the halfspace if and only if ⟨v, z −x⟩≥0. In fact, the ℓ2-norm is special because it is induced by an inner product. In general, ℓp-norms are not induced by an inner product, and therefore this definition does not directly generalize."
  },
  {
    "prompt": "This is a technical discussion about definition, observation, satisfies in theoretical computer science.",
    "response": "However, we can avoid the inner product with the following observation: a point z satisfies ⟨v, z −x⟩≥0 if and only if ∥x −z∥2 ≤∥x −εv −z∥2 holds for all ε > 0. In a way, this definition can be seen as the limit of the bisector definition with y = x −εv and ε →0. Stated like this, we can generalize the definition to arbitrary p ∈[1, ∞) ∪{∞}. Definition 3.2 (Limit ℓp-Halfspace)."
  },
  {
    "prompt": "Read and understand this section about direction, point, fixed from a research paper.",
    "response": "For fixed p ∈[1, ∞) ∪{∞}, point x ∈Rd, and direction v ∈Rd with v ̸= 0, the ℓp-halfspace Hp x,v ⊆Rd through x in the direction of v is defined as Hp x,v := {z ∈Rd | ∀ε > 0 : ||x −z||p ≤||x −εv −z||p}. Observe that scaling the direction v with a positive scalar does not change the halfspace Hp x,v. Hence, we usually assume v ∈Sd−1 ⊆Rd. In fact, we will frequently use the following charac- terization for containment in a limit ℓp-halfspace. Observation 3.3."
  },
  {
    "prompt": "Scientific context on direction, ℓp-halfspace, point and their role in graph theory.",
    "response": "For a given limit ℓp-halfspace Hp x,v and point z ∈Rd, let R−be the open ray from x in direction −v, and let Bz be the smallest closed ℓp-ball with center z that contains x. Then we have z ∈Hp x,v ⇐⇒R−∩B◦ z = ∅, where B◦ z denotes the interior of Bz. For our centerpoint theorem, we will exclusively work with limit halfspaces. However, as men- tioned before, bisector halfspaces naturally appear in the analysis of our algorithms in Section 4."
  },
  {
    "prompt": "Read and understand this section about observation, ℓp-halfspaces, translate from a research paper.",
    "response": "Thus, we will need the following observation that allows us to translate between the two. Observation 3.4. Let p ∈[1, ∞) ∪{∞}, x ∈Rn, and v ∈Sd−1. The limit ℓp-halfspace Hp x,v is the intersection of all bisector ℓp-halfspaces Hp x,x−εv for ε > 0. Thus, we have Hp x,v ⊆Hp x,x−εv for all ε > 0. For the rest of this section, we will exclusively work with limit ℓp-halfspaces and refer to them simply as ℓp-halfspaces."
  },
  {
    "prompt": "Explain a concept involving properties, ℓp-halfspaces, theorem in computational complexity.",
    "response": "In particular, we will first establish some properties of ℓp-halfspaces that we will then use to prove our centerpoint theorem. 3.2 Properties of ℓp-Halfspaces We now collect some useful properties of ℓp-halfspaces. The proofs of all the following properties and additional insights into ℓp-halfspaces can be found in Appendix A. We start with a simple observation for the case when the direction v is a standard unit vector (i.e., v is parallel to one of the coordinate axes)."
  },
  {
    "prompt": "Explain a concept involving Lemma, ℓp-halfspaces, classical in computational complexity.",
    "response": "It turns out that for those directions, ℓp-halfspaces are no different from the classical ℓ2-halfspaces. 10 Lemma 3.5. For any p ∈[1, ∞) ∪{∞}, any x ∈Rd, and any direction v ∈Sd−1 parallel to one of the coordinate axes, we have Hp x,v = H2 x,v. Lemma 3.5 really only describes a handful of special cases. For most directions v, ℓp-halfspaces are very different to classical ℓ2-halfspaces. In particular, they are in general not convex."
  },
  {
    "prompt": "This is a technical discussion about shape, give, nice in theoretical computer science.",
    "response": "Still, we can give a somewhat nice qualitative description of their shape. Lemma 3.6. For any p ∈[1, ∞) ∪{∞}, any limit halfspace Hp x,v is a union of rays originating in x. For p ∈(1, ∞), the boundary of any limit halfspace Hp x,v is a union of lines through x. Note that we will not really use the observation about the boundary of ℓp-halfspaces for p ∈ (1, ∞), but find it an interesting addition to the lemma."
  },
  {
    "prompt": "Explain a concept involving Figure, Lemma, breaks in computational complexity.",
    "response": "Unfortunately, it breaks down in some degenerate cases if p ∈{1, ∞} (see, e.g., Figure 2). x x v v Figure 2: Examples of Lemma 3.6. Two ℓ1-halfspaces, both are unions of rays starting at x, but only the left one (with non-degenerate v) has a boundary consisting of a union of lines through x. Lemma 3.6 provides a qualitative description of ℓp-halfspaces, but does not really give us any concrete points that must be contained in or outside of a given ℓp-halfspace."
  },
  {
    "prompt": "Scientific context on lying, roughly, direction and their role in graph theory.",
    "response": "For example, it seems intuitive that points lying roughly in the direction of v from x should be contained in Hp x,v, while points lying roughly in the direction of −v from x should never be contained. The following lemma formalizes this (see also Figure 3). Lemma 3.7. Let p ∈[1, ∞) ∪{∞} and Hp x,v be arbitrary. All z ∈Hp x,v satisfy ∡(−→ xz, v) ≤π −√ 1/d. Similarly, all z ∈Rd with ∡(−→ xz, v) ≤√ 1/d must be contained in Hp x,v."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, origin, Corollary.",
    "response": "With Lemma 3.7, it now also seems clear that any ℓp-halfspace Hp x,−x with x far away from the origin should contain every small compact set C around the origin. Corollary 3.8 makes this precise for C = [0, 1]d. Corollary 3.8. For arbitrary p ∈[1, ∞) ∪{∞} and x ∈Rd with ∥x∥2 > 2d, we have [0, 1]d ⊆Hp x,−x. Finally, we will need some results on the interaction of ℓp-halfspaces with mass distributions, defined as follows. Definition 3.9 (Mass Distribution)."
  },
  {
    "prompt": "Read and understand this section about Lebesgue, measure, mass from a research paper.",
    "response": "We call a measure µ on Rd a mass distribution if it is absolutely continuous with respect to the Lebesgue measure on Rd and satisfies µ(Rd) < ∞. Note that absolute continuity of µ says that all measurable sets A with Lebesgue measure 0 must also satisfy µ(A) = 0. By the Radon–Nikodym theorem, any such mass distribution must have 11 x v Hp x,v Figure 3: Sketch of Lemma 3.7. No point in the red cone is contained in the ℓp-halfspace Hp x,v, but all points in the green cone are."
  },
  {
    "prompt": "Scientific context on Figure, Sketch, Corollary and their role in graph theory.",
    "response": "x Hp x,−x 0 v Figure 4: Sketch of Corollary 3.8. The cube [0, 1]d is contained in all Hp x,−x for ||x||2 large enough. a density, and so one can think of mass distributions as probability distributions with a density (by assuming µ(Rd) = 1, without loss of generality). Concretely, we will adopt this point of view in the proofs of Lemma 3.10 and Lemma 3.11 (see Appendix A.2). We can now study how µ(Hp x,v) behaves with respect to its arguments x and v."
  },
  {
    "prompt": "Read and understand this section about lemma, continuous, ℓp-halfspace from a research paper.",
    "response": "For example, the next lemma says that the mass µ(Hp x,v) of an ℓp-halfspace is continuous under translation of the defining point x. Lemma 3.10. The function f (x) := µ(Hp x,v) is continuous in x ∈Rd for all mass distributions µ, all v ∈Sd−1, and all p ∈[1, ∞) ∪{∞}. For p ∈(1, ∞), one could also show continuity with respect to v ∈Sd−1 rather than x. Unfor- tunately, this breaks down for p ∈{1, ∞} again in some degenerate cases."
  },
  {
    "prompt": "Scientific context on turns, weaker, property and their role in graph theory.",
    "response": "However, it turns out that a weaker property will suffice for our needs. Concretely, for fixed x ∈Rd and some threshold t > 0, consider the set of directions Vx := {v ∈Sd−1 | µ(Hp x,v) < t} corresponding to halfspaces Hp x,v with strictly less than t mass. The following lemma says that Vx is an open subset of Sd−1. Lemma 3.11. The set Vx = {v ∈Sd−1 | µ(Hp x,v) < t} is an open subset of Sd−1 for all mass distributions µ, all t > 0, all p ∈[1, ∞) ∪{∞}, and all x ∈Rd."
  },
  {
    "prompt": "Understanding the computational aspects of Distributions, Mass, theorem.",
    "response": "12 3.3 ℓp-Centerpoints of Mass Distributions Using our notion of limit halfspaces, we can state the classical (Euclidean) centerpoint theorem for mass distributions as follows. Theorem 3.12 (Euclidean Centerpoint Theorem for Mass Distributions [31]). Let µ be a mass dis- tribution on Rd with bounded support. There exists c ∈Rd such that µ(H2 c,v) ≥ 1 d+1µ(Rd) for all v ∈Sd−1. We call such a point c an ℓ2-centerpoint of µ."
  },
  {
    "prompt": "Explain a concept involving theorem, section, centerpoint in computational complexity.",
    "response": "In this section, we will prove existence of an ℓp- centerpoint of µ for all p ∈[1, ∞) ∪{∞}, as stated in the following theorem. Theorem 3.13 (ℓp-Centerpoint Theorem for Mass Distributions). Let µ be a mass distribution on Rd with bounded support, and let p ∈[1, ∞) ∪{∞} be arbitrary. There exists c ∈Rd such that µ(Hp c,v) ≥ 1 d+1µ(Rd) for all v ∈Sd−1."
  },
  {
    "prompt": "Read and understand this section about Theorem, Helly, tion from a research paper.",
    "response": "A classical and simple proof of Theorem 3.12 uses Helly’s theorem [18] to show that all ℓ2- halfspaces containing strictly more than a d d+1-fraction of µ(Rd) must have a non-empty intersec- tion. Choosing c in this intersection guarantees the required property. However, this proof breaks down if we consider ℓp-halfspaces: they are in general non-convex and thus Helly’s theorem is no longer applicable. Instead, we prove Theorem 3.13 with Brouwer’s fixpoint theorem."
  },
  {
    "prompt": "Scientific context on Brouwer, Theorem, fixpoint and their role in graph theory.",
    "response": "The existence of a proof of Theorem 3.12 via Brouwer’s fixpoint theorem seems to be folklore1, but we were unable to find a reference containing the full proof. We briefly recall Brouwer’s fixpoint theorem. Theorem 3.14 (Brouwer’s fixpoint theorem [3]). Let C ⊆Rd be a non-empty compact convex set, and let f : C →C be a continuous function. Then f has a fixpoint. We will now use Brouwer’s fixpoint theorem to prove Theorem 3.13."
  },
  {
    "prompt": "Understanding the computational aspects of function, define, fixpoint.",
    "response": "For this, we first define a function F : Rd →Rd such that any fixpoint of F must be an ℓp-centerpoint of µ. Concretely, the intuition behind this function is that any point x is “pushed” in all directions v for which Hp x,−v does not contain a sufficient fraction of mass simultaneously. We achieve this by defining Fi(x) for all i ∈[d] using an integral over all directions, i.e., Fi(x) := xi + Z Sd−1 vi max \u0012 1 d + 1µ(Rd) −µ(Hp x,−v), 0 \u0013 dv."
  },
  {
    "prompt": "This is a technical discussion about Brouwer, uous, contin in theoretical computer science.",
    "response": "Since we eventually want to apply Brouwer’s fixpoint theorem to F, let us check that F is contin- uous. Lemma 3.15. Let p ∈[1, ∞) ∪{∞} be arbitrary. Let µ be a mass distribution on Rd with bounded support. Then the function F : Rd →Rd defined above is continuous. Proof. Fix x ∈Rd and let (xn)n∈N be an arbitrary sequence converging to x. By Lemma 3.10, the functions gn(v) := vi max \u0012 1 d + 1µ(Rd) −µ(Hp xn,−v), 0 \u0013 converge pointwise to g(v) = vi max \u00001 d+1µ(Rd) −µ(Hp x,−v), 0 \u0001 for all i ∈[d]."
  },
  {
    "prompt": "This is a technical discussion about bounded, yielding, globally in theoretical computer science.",
    "response": "Since they are also globally bounded, we can apply the dominated convergence theorem to exchange limit and integral, yielding continuity of F. 1Such a proof is vaguely outlined in [38]. 13 We next turn our attention to the fixpoints of F. Concretely, we wish to show that any fixpoint x of F must be an ℓp-centerpoint of µ. To argue this, we analyze the set Vx := {v ∈Sd−1 | max \u0012 1 d + 1µ(Rd) −µ(Hp x,−v), 0 \u0013 > 0} of directions which contribute towards the integral."
  },
  {
    "prompt": "This is a technical discussion about conv, show, impossible in theoretical computer science.",
    "response": "We will show that it is impossible for conv(Vx) to contain 0 (for any x ∈Rd). Later, we will then use this to argue that the inte- gral in the definition of F cannot be 0 without Vx being empty. This in turn allows us to conclude that fixpoints of F are ℓp-centerpoints of µ. To prove that 0 ̸∈conv(Vx), we need the following lemma. Lemma 3.16. Assume that 0 ∈Rd is contained in the convex hull of k ≤d + 1 points v1, . . . , vk ∈Sd−1. Let z ∈Rd and p ∈[1, ∞) ∪{∞} be arbitrary."
  },
  {
    "prompt": "Scientific context on exists, Proof, Observation and their role in graph theory.",
    "response": "Then there exists j ∈[k] such that z ∈Hp 0,−vj. Proof. Let R1, . . . , Rk be the open rays from 0 in the directions v1, . . . , vk, respectively. Let us now grow an ℓp-ball Bz centered at z, exactly until the ball contains 0. Recall that by Observation 3.3, we have z ∈Hp 0,−vj if and only if B◦ z does not intersect Rj. Now observe that if we had B◦ z ∩Rj ̸= ∅ for all j ∈[k], then this would imply 0 ∈B◦ z by convexity of B◦ z."
  },
  {
    "prompt": "Read and understand this section about contradiction, minimum-radius, ball from a research paper.",
    "response": "But then, Bz cannot have been the minimum-radius ball centered at z containing 0, a contradiction. Let us now see how this lemma implies 0 ̸∈conv(Vx) for all x ∈Rd. Corollary 3.17. Let p ∈[1, ∞) ∪{∞} be arbitrary and let µ be a mass distribution on Rd. For any point x ∈Rd, the set Vx = {v ∈Sd−1 | max \u00001 d+1µ(Rd) −µ(Hp x,−v), 0 \u0001 > 0} does not contain 0 in its convex hull. Proof. If 0 would lie in conv(Vx), this would mean that Vx is non-empty."
  },
  {
    "prompt": "Understanding the computational aspects of Carath´eodory’s, theorem, set.",
    "response": "In particular, using Carath´eodory’s theorem, we would get a set of k ≤d + 1 vectors v1, . . . , vk ∈Vx with 0 ∈ conv(v1, . . . , vk). Since these vectors are in Vx, we get on the one hand that k ∑ i=1 µ(Hp x,−vi) < k d + 1µ(Rd) ≤µ(Rd). On the other hand, Lemma 3.16 guarantees that every z ∈Rd is contained in at least one of the halfspaces Hp x,−v1, . . . , Hp x,−vk. Thus, we can derive k ∑ i=1 µ(Hp x,−vi) ≥µ k[ i=1 Hp x,−vi ! = µ(Rd), a contradiction."
  },
  {
    "prompt": "Scientific context on Theorem, Brouwer, proof and their role in graph theory.",
    "response": "We are now ready to put all of this together to get a proof of Theorem 3.13. The main technical thing that we still have to do, is to restrict F to a compact convex set (so that we can apply Brouwer’s fixpoint theorem). Proof of Theorem 3.13. Without loss of generality, assume that the bounded support of µ is con- tained in the box [0, 1]d. To apply Brouwer’s fixpoint theorem, we need a function going from a compact convex set C to itself, rather than from Rd to Rd like our function F."
  },
  {
    "prompt": "Read and understand this section about function, projection, origin from a research paper.",
    "response": "We create such a 14 function FC by defining a large compact convex set C ⊇[0, 1]d, and then restricting the function F to C using projection. More formally, we use the restricted function FC : C →C with FC(x) defined as the projection of F(x) onto C in the direction of the origin (for any x ∈C). As our set C, we choose the Euclidean ball of radius 1 + 2d around the origin, since this allows us to apply Corollary 3.8 for all x on the boundary of C."
  },
  {
    "prompt": "This is a technical discussion about continuous, continuity, max in theoretical computer science.",
    "response": "By continuity of F, it follows that FC is also continuous. Thus, we can use Brouwer’s fixpoint theorem, which tells us that there exists c ∈C with FC(c) = c. We now prove that c is a centerpoint by distinguishing the two cases F(c) = FC(c) and F(c) ̸= FC(c). In the first case, we must have that Z Sd−1 vi max \u0012 1 d + 1µ(Rd) −µ(Hp c,−v), 0 \u0013 dv = 0 for all i ∈[d]. Now consider the set of directions Vc = {v ∈Sd−1 | max \u00001 d+1µ(Rd) −µ(Hp c,−v), 0 \u0001 > 0}."
  },
  {
    "prompt": "Read and understand this section about Lemma, conv, open from a research paper.",
    "response": "By Lemma 3.11, this is an open subset of Sd−1. Moreover, 0 is not contained in conv(Vc) by Corollary 3.17. However, this implies that Vc must be empty: if Vc were non-empty, we could find a separating hyperplane between the two convex sets {0} and conv(Vc). But then, the definition of Vc and its openness contradicts that the above integral is zero (there would be a non-zero net push in the direction of the normal vector defining the separating hyperplane)."
  },
  {
    "prompt": "Scientific context on empty, centerpoint, expression and their role in graph theory.",
    "response": "Therefore, since Vc must be empty, the expression max( 1 d+1µ(Rd) −µ(Hp c,−v), 0) must be 0 for all v ∈Sd−1, and c is a centerpoint. In the second case, F(c) ̸= FC(c), we know that FC “used” the projection to map c. We thus know that c = FC(c) lies on the boundary of C. Furthermore, since the projection onto C is towards the origin, we get F(c) = (1 + ε)c for some ε > 0. Concretely, we must have Z Sd−1 vi max \u0012 1 d + 1µ(Rd) −µ(Hp c,−v), 0 \u0013 dv = εci for all i ∈[d]."
  },
  {
    "prompt": "Read and understand this section about conv, Corollary, segment from a research paper.",
    "response": "Considering the set Vc again, we can see that there hence must exist some δ > 0 with δc ∈conv(Vc): otherwise, we could find a hyperplane separating conv(Vc) from the line segment [0, c ∥c∥2 ], yielding a contradiction. But since C was chosen large enough, we also get − c ||c||2 ∈Vc by Corollary 3.8. Thus, we see that 0 ∈conv(Vc). This contradicts Corollary 3.17, and hence this case cannot occur."
  },
  {
    "prompt": "Understanding the computational aspects of fixpoint, Theorem, conclude.",
    "response": "We conclude that every fixpoint of FC is a fixpoint of F, and thus every fixpoint of FC must be a centerpoint. Theorem 3.12 can be adapted to point sets instead of mass distributions (see Theorem 3.18 below). The proof is not difficult but a bit technical, which is why we defer it to Appendix B. The main idea is to put a ball of small radius around each point and to apply Theorem 3.13. Letting the radius go to zero, we obtain a sequence of centerpoints."
  },
  {
    "prompt": "Scientific context on point, set, Theorem and their role in graph theory.",
    "response": "A subsequence of this sequence must converge to a discrete ℓp-centerpoint of the point set. Theorem 3.18 (ℓp-Centerpoint Theorem for Finite Point Sets). Let p ∈[1, ∞) ∪{∞} be arbitrary, and let P ⊆Rd be a finite set of points. There exists a point c such that |Hp c,v ∩P| ≥ |P| d+1 for all v ∈Sd−1. Finally, we observe that the centerpoint in both the discrete and continuous setting must lie inside any axis-aligned bounding box of the point set P or the support of µ, respectively. 15 Lemma 3.19."
  },
  {
    "prompt": "Read and understand this section about case, Theo, Theorem from a research paper.",
    "response": "Let B be an axis-aligned bounding box that contains the support of µ (in the case of Theo- rem 3.13) or all of P (in the case of Theorem 3.18), respectively. Any ℓp-centerpoint guaranteed by either theorem must lie inside B. Proof. Towards a contradiction, let c be an ℓp-centerpoint of µ (or P, respectively) not contained in B. Assume without loss of generality that every point x in the support of µ (or in P) satisfies x1 < c1."
  },
  {
    "prompt": "This is a technical discussion about Lemma, Theorem, centerpoint in theoretical computer science.",
    "response": "Then, by Lemma 3.5, we have that µ(Hp c,e1) = 0 (or Hp c,e1 ∩P = 0), where e1 is the first standard unit vector. This contradicts the centerpoint property of c. 3.4 Tightness of Centerpoint Theorems We want to remark that the fraction 1 d+1 in Theorem 3.13 and Theorem 3.18 is tight for all p ∈ [1, ∞) ∪{∞}. To prove this, we use a construction that has also been used to prove tightness of the classical Euclidean centerpoint theorem."
  },
  {
    "prompt": "Understanding the computational aspects of Theorem, bound, Appendix.",
    "response": "We will restrict ourselves to the discrete setting of Theorem 3.18, but tightness of Theorem 3.13 follows as well because any better bound for mass distributions could be used to get a better bound for point sets by following the proof of Theorem 3.18 (see Appendix B). Lemma 3.20. For every d ∈N, there exists a point set Pd ⊆[0, 1]d, such that for every p ∈[1, ∞) ∪{∞}, every point c ∈Rd has a direction v ∈Sd−1 with |Hp c,v ∩Pd| ≤|Pd| d+1. Proof. Let Pd = {0, e1, . . ."
  },
  {
    "prompt": "Scientific context on vector, i-th, standard and their role in graph theory.",
    "response": ", ed} ⊆[0, 1]d, where ei is the i-th standard unit vector. Let c ̸= 0 be arbitrary. Then c cannot lie on all axis-aligned facets of the convex hull of Pd simultaneously. Thus, by Lemma 3.5 there must exist a direction v ∈{−e1, . . . , −ed, e1, . . . , ed} such that Hp c,v does not contain any vertices of a facet that c does not lie on, and therefore |Hp c,v ∩Pd| ≤1 = |Pd| d+1. If we instead have c = 0, then Hp c,v ∩Pd = {0} for v = (−√ 1/d, . . . , −√ 1/d) ∈Sd−1 by direct calculation."
  },
  {
    "prompt": "This is a technical discussion about Finding, Fixpoints, Maps in theoretical computer science.",
    "response": "4 Finding Fixpoints of ℓp-Contraction Maps In this section, we describe our algorithms for the continuous problem ℓp-ContractionFixpoint and explain how, in the ℓ1-case, the algorithm can be adapted to the discretized setting ℓ1- GridContractionFixpoint. Note that in the ℓ∞-case, a suitable rounding strategy has already been provided by Chen, Li, and Yannakakis [5]."
  },
  {
    "prompt": "Understanding the computational aspects of solving, GridContractionFixpoint, round.",
    "response": "We will not repeat that, but note that their round- ing can also be used in combination with our methods (solving ℓ∞-GridContractionFixpoint). 4.1 Solving ℓp-ContractionFixpoint Our algorithm works as follows: we maintain a search space M that is guaranteed to always contain the fixpoint x⋆. At the beginning of the algorithm, we simply set M = [0, 1]d. We then iteratively query the centerpoint c of our remaining search space M."
  },
  {
    "prompt": "Understanding the computational aspects of vol, Lebesgue, measure.",
    "response": "Concretely, we use the measure vol (which we simply call volume) defined by the Lebesgue measure (such that vol([0, 1]d) = 1), with its support restricted to M. Whenever we query the centerpoint c of M, we get to discard at least a 1 d+1-fraction of the search space, because x⋆must lie closer to f (c) than to c itself. With each query, the volume of M is thus multiplied with a factor of at most d d+1. We terminate once we happen to query an ε-approximate fixpoint."
  },
  {
    "prompt": "Understanding the computational aspects of search, space, small.",
    "response": "It remains to prove that this must happen before the search space M gets too small. To that end, we show in the next lemma 16 that, whenever we query a point that is not an ε-approximate fixpoint, a ball of some radius rε,λ around x⋆cannot be discarded, and thus has to remain in the search space M. Note that while the centerpoint theorems use limit ℓp-halfspaces, we will now use bisector half- spaces in the analysis of our algorithm. Observation 3.4 allows us to translate between the two."
  },
  {
    "prompt": "Scientific context on arbitrary, rε,λ, fixpoint and their role in graph theory.",
    "response": "Lemma 4.1. Let p ∈[1, ∞) ∪{∞} be arbitrary. Let x⋆be the unique fixpoint of the λ-contracting map f : [0, 1]d →[0, 1]d. Let x ∈[0, 1]d be arbitrary. If x is not an ε-approximate fixpoint of f, then Bp(x⋆, rε,λ) ∩Hp x,f (x) = ∅ for rε,λ = ε−ελ 2+2λ. Proof. Let z ∈Bp(x⋆, rε,λ) be arbitrary. We need to show ||z −x||p > ||z −f (x)||p. Let us first collect some facts. Concretely, we know that x is not an ε-approximate fixpoint, z ∈Bp(x⋆, rε,λ), and that f is λ-contracting."
  },
  {
    "prompt": "Explain a concept involving rε,λ, inequalities, bound in computational complexity.",
    "response": "This gives us the following inequalities: || f (x) −x||p > ε (4.1) ||z −x⋆||p ≤rε,λ (4.2) ||x⋆−f (x)||p ≤λ||x⋆−x||p. (4.3) Combining these using the triangle inequality, we also get ||z −x||p ≥||x⋆−x||p −rε,λ (4.4) ||z −f (x)||p ≤||x⋆−f (x)||p + rε,λ. (4.5) Before putting everything together, we also want to lower bound ||z −x||p. We first lower bound ||x⋆−x||p using the calculation ε (4.1) < ||x −f (x)||p ≤||x⋆−x||p + ||x⋆−f (x)||p (4.3) ≤(1 + λ)||x⋆−x||p."
  },
  {
    "prompt": "Read and understand this section about rε,λ, Combining, Equations from a research paper.",
    "response": "(4.6) Combining Equations (4.4) and (4.6), we get ||z −x||p > ε 1 + λ −rε,λ. (4.7) We can now put everything together to obtain ||z −f (x)||p (4.5) ≤||x⋆−f (x)||p + rε,λ (4.3) ≤λ||x⋆−x||p + rε,λ (4.4) ≤λ||z −x||p + (1 + λ)rε,λ = ||z −x||p −(1 −λ)||z −x||p + (1 + λ)rε,λ (4.7) < ||z −x||p −(1 −λ) \u0012 ε 1 + λ −rε,λ \u0013 + (1 + λ)rε,λ = ||z −x||p + 2rε,λ −ε −ελ 1 + λ = ||z −x||p, and we conclude ||z −x||p > ||z −f (x)||p and thus z /∈Hp x,f (x). 17 We now have all the ingredients to prove Theorem 4.2."
  },
  {
    "prompt": "Read and understand this section about log, Theorem, ε-approximate from a research paper.",
    "response": "Theorem 4.2. For every p ∈[1, ∞) ∪{∞}, an ε-approximate fixpoint of a λ-contracting (in ℓp-norm) function f : [0, 1]d →[0, 1]d can be found using O(d2(log 1 ε + log 1 1−λ)) queries. Proof. We begin with the search space M = [0, 1]d with vol(M) = 1. We then repeatedly query the centerpoint c of M (or rather the measure induced by M), guaranteed to exist by Theorem 3.13. We terminate if c is an ε-approximate fixpoint. Otherwise, we remove the bisector halfspace Hc,f (c) from M."
  },
  {
    "prompt": "Read and understand this section about Observation, Theorem, volume from a research paper.",
    "response": "For each non-terminating query, we get the guarantee that the volume of M decreases to at most a d d+1-fraction of its previous volume, where we use Observation 3.4 to translate the centerpoint guarantee of Theorem 3.13 from limit halfspaces to bisector halfspaces. Now recall that Lemma 4.1 guarantees that Bp(x∗, rε,λ) stays in M as long as we do not query an ε-approximate fixpoint. Thus, we know that we must terminate before having reached vol(M) < vol(Bp(0, rε,λ))."
  },
  {
    "prompt": "Scientific context on log, rε,λ, vol and their role in graph theory.",
    "response": "Therefore, to compute our final query bound, we first note that it is well-known that the volume of the ℓp-norm ball Bp(0, rε,λ) can be bounded from below by vol(Bp 0,rε,λ) ≥2d d! rd ε,λ = 2d d! ( ε −ελ 2 + 2λ)d. Finally, we can upper bound the number k of queries by k ≤log d d+1 2d d! \u0012 ε −ελ 2 + 2λ \u0013d! = log \u0010 2d d! ( ε−ελ 2+2λ)d\u0011 log \u0010 d d+1 \u0011 = log \u0010 d! 2d ( 2 ε 1+λ 1−λ)d\u0011 log \u0010 d+1 d \u0011 ≤log d!"
  },
  {
    "prompt": "Scientific context on log, small, term and their role in graph theory.",
    "response": "−d + d \u0000log 4 + log( 1 ε ) + log( 1 1−λ) \u0001 log( d+1 d ) ≤O \u0000d \u0000log d + log( 1 ε ) + log( 1 1−λ) \u0001\u0001 log(1 + 1 d) . For small δ > 0, we have log(1 + δ) ≈δ, and thus we get k ≤O(d2(log d + log( 1 ε ) + log( 1 1−λ)). To get rid of the log d term, observe that if max( 1 ε , 1 1−λ) < d (i.e., whenever this term matters), a simple iteration algorithm can find an ε-approximate fixpoint after at most O(d log d) queries."
  },
  {
    "prompt": "Understanding the computational aspects of arbitrary, recursively, defined.",
    "response": "In particular, let x(0) ∈[0, 1]d be arbitrary and consider the recursively defined iterates x(i) := f (x(i−1)) for i ≥1. Since f is contracting, we have ∥x(i) −f (x(i))∥p ≤λ∥x(i−1) −f (x(i−1))∥p for all i ≥1. Together with the observation ∥x(0) −f (x(0))∥p ≤d, we get ∥x(i) −f (x(i))∥p ≤λid for all i. If i is large enough such that λid < ε, we must have found an ε-approximate fixpoint. This is equivalent to i > log d/ε log 1 λ ."
  },
  {
    "prompt": "Explain a concept involving log, theorem, max in computational complexity.",
    "response": "By using max( 1 ε , 1 1−λ) < d, we get log( 1 λ) = log(1 + 1 −λ λ ) ≥log(1 + 1 dλ) ≥log(1 + 1 d) and hence we obtain that O(d log d) iterations suffice. 4.2 Rounding to the Grid in the ℓ1-Case In this section, we adapt the algorithm from the previous section to also work in the discretized setting ℓ1-GridContractionFixpoint, proving the following theorem. 18 Theorem 4.3."
  },
  {
    "prompt": "Read and understand this section about log, queries, grid from a research paper.",
    "response": "For every b ≥log2 \u0010 2d ε 1+λ 1−λ \u0011 , an ε-approximate fixpoint of a λ-contracting (in ℓ1-norm) grid map f : Gd b →[0, 1]d can be found using O(d2(log 1 ε + log 1 1−λ)) queries. The main issue that we have to address is that we cannot always query the centerpoint c of the remaining search space, since c is not guaranteed to lie on the grid Gd b. In the ℓ∞-case, Chen, Li, and Yannakakis [5] solved this problem by choosing the measure for the remaining search space appropriately."
  },
  {
    "prompt": "Read and understand this section about measure, Concretely, centerpoint from a research paper.",
    "response": "Concretely, they design a special measure to determine the size of the remaining search space, and then show that rounding the centerpoint to the grid works for this measure. We do not repeat their argument here, but note that their technique also works if applied together with our (discrete) centerpoint theorem. Instead, we focus on the ℓ1-case. Fortunately, rounding in the ℓ1-case turns out to be simpler than in the ℓ∞-case."
  },
  {
    "prompt": "Read and understand this section about grid, measure, size from a research paper.",
    "response": "Indeed, we simply measure the size of the remaining search space as the number of grid points that still remain inside. It turns out that appropriately rounding the centerpoint to the grid does not change its centerpoint properties with respect to this measure of size. More formally, we get the following lemma. Lemma 4.4. Consider an arbitrary subset of points P ⊆Gd b on the grid. There exists a point c ∈Gd b such that |H1 c,v ∩P| ≥ |P| d+1 for all v ∈Sd−1."
  },
  {
    "prompt": "This is a technical discussion about Theorem, Lemma, proof in theoretical computer science.",
    "response": "We postpone the proof of Lemma 4.4 and instead first use it to derive Theorem 4.3. Proof of Theorem 4.3. Overall, we use a similar strategy as in the proof of Theorem 4.2, but we use Lemma 4.4 that is derived from our discrete centerpoint theorem. Concretely, let M ⊆Gd b be the subset of grid points that could still be ε-approximate fixpoints (they have not been ruled out yet). Applying Lemma 4.4 to M yields a centerpoint c on the grid Gd b."
  },
  {
    "prompt": "Explain a concept involving Querying, iteration, search in computational complexity.",
    "response": "Querying c guarantees that we can exclude at least |M| d+1 points from our current search space in each non-terminating iteration. Now Lemma 4.1 guarantees that as long as we have not queried an ε-approximate fixpoint, no point in B1(x⋆, rε,λ) ∩Gd b has been excluded from the search space yet (where rε,λ = ε−ελ 2+2λ). To guarantee that this intersection is non-empty, it suffices to have 2−b ≤rε,λ d , which translates to our assumption b ≥log2 \u0010 2d ε 1+λ 1−λ \u0011 ."
  },
  {
    "prompt": "Understanding the computational aspects of log, fact, generality.",
    "response": "In fact, we can assume b = \u0018 log2 \u00122d ε 1 + λ 1 −λ \u0013\u0019 without loss of generality (as we can use a subgrid of this size, if the given grid is finer). For this choice of b, we have |Gd b| = (2b + 1)d ≤4db. With the calculation log d d+1 4−db = log 4−db log d d+1 = db log(1 + 1 d) ≤O(d2b) ≤O \u0012 d2 \u0012 log d + log ε + log 1 1 −λ \u0013\u0013 we therefore conclude that the algorithm must find an ε-approximate fixpoint after at most O(d2(log 1 ε + log 1 1−λ + log d)) queries to grid points."
  },
  {
    "prompt": "Explain a concept involving Theorem, log, queries in computational complexity.",
    "response": "As in the proof of Theorem 4.2, we can get rid of the log d term in our query bound by observing that if max( 1 ε , 1 1−λ) < d, a simple iteration algorithm (this time with rounding) can find an ε- approximate fixpoint after at most O(d log d) queries. 19 It remains to prove Lemma 4.4. For this, we need some additional theory. In particular, we will proceed to give a characterization of containment in an ℓ1-halfspace based on tools from convex analysis."
  },
  {
    "prompt": "This is a technical discussion about Lemma, characterization, established in theoretical computer science.",
    "response": "Once this characterization is established, it will be quite easy to derive Lemma 4.4. Note that more details on this characterization can be found in Appendix A.1. The main tool that we need is the notion of subgradients of a convex function f : Rd →R: a vector u ∈Rd is a subgradient of f at x ∈Rd if and only if f (x′) −f (x) ≥⟨u, (x′ −x)⟩ for all x′ ∈Rd. The set ∂f (x) ⊆Rd of all subgradients of f at x is also called the subdifferential. If f is differentiable at x, then ∂f (x) = {∇f (x)}."
  },
  {
    "prompt": "Read and understand this section about Rockafellar, general, details from a research paper.",
    "response": "For more details on these concepts and convex analysis in general, we refer to the standard textbook by Rockafellar [34]. Subgradients are useful for us due to the following characterization of containment for ℓp- halfspaces. The proof can be found in Appendix A.1. Lemma 4.5. For any p ∈[1, ∞) ∪{∞}, a point z ∈Rd is contained in an ℓp-halfspace Hp x,v if and only if there exists a subgradient u ∈∂||z −x||p with ⟨u, v⟩≥0. In this section, we are interested in applying Lemma 4.5 in the ℓ1-case."
  },
  {
    "prompt": "Explain a concept involving subgradients, describe, Lemma in computational complexity.",
    "response": "Therefore, let us next describe the subgradients of || · ||1. Due to Lemma 3.6 (ℓp-halfspaces are unions of rays), it will suffice to characterize the subgradients at points z with ||z||1 = 1. Observation 4.6. Consider arbitrary z ∈Rd with ||z||1 = 1. A vector u ∈Rd is a subgradient of || · ||1 at z if and only if ui ∈      {1} if zi > 0 {−1} if zi < 0 [−1, 1] if zi = 0 for all i ∈[d]. In particular, we have ⟨u, z⟩= ||u||∞= ||z||1 = 1 for all u ∈∂||z||1."
  },
  {
    "prompt": "Scientific context on Lemma, Observation, grid and their role in graph theory.",
    "response": "Equipped with Lemma 4.5 and Observation 4.6, we are now ready to prove Lemma 4.4. Proof of Lemma 4.4. Let P ⊆Gd b be an arbitrary subset of the grid points, as given in the lemma. By Theorem 3.18, there exists a discrete ℓ1-centerpoint c of P. Moreover, by Lemma 3.19, c is contained in the convex hull conv(Gd b) = [0, 1]d of the grid. Let c′ ∈Gd b be the grid point closest to c (in ℓ1-distance). In other words, c′ is obtained from c by rounding each coordinate individually as little as possible."
  },
  {
    "prompt": "Read and understand this section about prove, discrete, claim from a research paper.",
    "response": "We claim that c′ is also a discrete ℓ1-centerpoint of P. To prove this, consider the ℓ1-halfspaces H1 c′,v and H1 c,v for an arbitrary direction v ∈Sd−1. We prove that every point z ∈P ∩H1 c,v must also be contained in H1 c′,v. Indeed, by Lemma 4.5, there exists a subgradient u ∈∂||z −c||1 such that ⟨u, v⟩≥0. We want to prove u ∈∂||z −c′||1. Observe that our choice of c′ guarantees zi −c′ i > 0 =⇒zi −ci > 0 and zi −c′ i < 0 =⇒zi −ci < 0 for all i ∈[d]."
  },
  {
    "prompt": "Scientific context on Observation, Lemma, Search and their role in graph theory.",
    "response": "With Observation 4.6, we therefore have u ∈∂||z −c′||1 and conclude by Lemma 4.5 that z is contained in H1 c′,v. 20 4.3 Total Search Version The class TFNPdt (standing for Total Function NP Search Problems, decision-tree view), as defined by G¨o¨os et al. [15], captures total search problems (i.e., problems where every possible instance has a solution) that are specified by a long hidden bitstring accessible only through a bit querying oracle."
  },
  {
    "prompt": "Understanding the computational aspects of TFNPdt, lie, efficiently.",
    "response": "To lie in TFNPdt, solutions must be efficiently verifiable (i.e., by decision trees of depth poly-logarithmic in the length of the bitstring). To lie in FPdt, the subclass of efficiently solvable problems in TFNPdt, one must be able to efficiently find a solution as well, i.e., there has to be a decision tree of poly-logarithmic depth that always outputs a correct solution."
  },
  {
    "prompt": "Scientific context on promise, TFNPdt, types and their role in graph theory.",
    "response": "To fit a promise search problem into TFNPdt, we must introduce solution types, usually called violations, that are guaranteed to exist when the promise is violated. Chen, Li, and Yannakakis [5] showed how the ℓ∞-GridContractionFixpoint problem can be made total: under the ℓ∞-norm, a grid-map f : Gd b →[0, 1]d extends to a λ-contraction f ′ on [0, 1]d if and only if f is λ-contracting for all pairs of points in Gd b."
  },
  {
    "prompt": "This is a technical discussion about violations, poly, suffices in theoretical computer science.",
    "response": "Thus, it suffices to introduce violations consisting of two points x, y ∈Gd b for which ∥f (x) −f (y)∥∞> λ · ∥x −y∥∞. If a function f : Gd b →Gd b′ (for b′ ∈poly(b)) is now encoded as a bitstring of length 2d·b · b′ by simply concatenating the output values f (x) for all x ∈Gd b, both ε-approximate fixpoints as well as these violations can be verified by querying O(b′) ∈poly log(2d·b · b′) bits, thus placing the resulting problem in TFNPdt."
  },
  {
    "prompt": "Explain a concept involving algorithm, Chen, Yannakakis in computational complexity.",
    "response": "Furthermore, the algorithm of Chen, Li, and Yannakakis can also be used to solve this total version of the problem: if the function f is λ-contracting for all pairs of points queried by the algorithm, the algorithm must return an ε-approximate fixpoint. Otherwise, the algorithm must have encountered a violation. Thus, ℓ∞-GridContractionFixpoint is in FPdt. The same strategy does not seem to work in the ℓ1-case."
  },
  {
    "prompt": "Read and understand this section about fact, ℓp-norms, Chen from a research paper.",
    "response": "In fact, we suspect that the statement that a grid-map extends to a contraction map if and only if it is contracting for all grid points is not true for the ℓ1-norm and general ℓp-norms. In the case of p = ∞, Chen, Li, and Yannakakis constructed an extension explicitly: they extended the grid-function to [0, 1]d using a formula that corresponds to applying McShane’s extension lemma [25] to every partial function fi : Gd b →[0, 1] independently."
  },
  {
    "prompt": "Explain a concept involving contraction, property, McShane in computational complexity.",
    "response": "McShane’s extension lemma guarantees that the partial functions f ′ i : [0, 1]d → [0, 1] then fulfill the same contraction property as fi, which together with the definition of the ℓ∞-norm as a maximum over coordinates, yields the contraction property for f ′. Trying this for p = 1, the analysis only yields that f ′ is (dλ)-Lipschitz, which is not enough to be contracting if we for example have d > 1 and λ > 0.5."
  },
  {
    "prompt": "Read and understand this section about cri, terion, algorithm from a research paper.",
    "response": "We get around this in a naive way by introducing a violation type based on the termination cri- terion of our algorithm for ℓ1-GridContractionFixpoint. Concretely, if our algorithm does not find an ε-approximate fixpoint after reaching its query bound, then the queried points x(1), . . . , x(k) must satisfy Gd b \\ S i∈[k] H1 x(i),f (x(i)) = ∅."
  },
  {
    "prompt": "Explain a concept involving points, grid, words in computational complexity.",
    "response": "In other words, we certify the existence of a violation of the contraction property with a set of points on the grid whose associated bisector halfspaces contain all grid points in their union. Definition 4.7. An instance of the Total-ℓ1-ContractionFixpoint problem consists of a bitstring encoding integers d, b, b′ ∈N and integral logarithms log( 1 1−λ), log( 1 ε ) in unary, as well as a function f : Gd b →Gd b′ encoded as a concatenation of its values."
  },
  {
    "prompt": "Scientific context on log, goal, produce and their role in graph theory.",
    "response": "The goal is to produce one of the following. (S) A point x ∈Gd b such that ∥f (x) −x∥p ≤ε. (V) A set P of poly(b, d, log \u0000 1 ε \u0001 , log \u00001 1−λ \u0001) points in Gd b with Gd b ⊆S x∈P H1 x,f (x). 21 Note that for our algorithm to work, we need to syntactically guarantee b ≥log2 \u0010 2d ε 1+λ 1−λ \u0011 . In fact, without this assumption, we would not even know if the problem is total."
  },
  {
    "prompt": "Understanding the computational aspects of poly, bounded, syntactically.",
    "response": "We also syntactically guarantee b′ ∈poly(b) to make sure that the number of bit queries that our algorithm makes to the encoding of f is polynomially bounded. Such syntactic guarantees can be realized by adding trivial-to-verify additional violation types for the cases where the input parameters do not fulfill these assumptions."
  },
  {
    "prompt": "Understanding the computational aspects of log, Theorem, queries.",
    "response": "We have shown in the proof of Theorem 4.3 that after O(d2(log 1 ε + log 1 1−λ + log d)) queries to f, the queries made by our algorithm must have produced either a solution (S), or a violation (V). Thus, our algorithm solves Total-ℓ1-ContractionFixpoint as well, placing it in TFNPdt and even in FPdt. Corollary 4.8. Total-ℓ1-ContractionFixpoint ∈FPdt."
  },
  {
    "prompt": "Understanding the computational aspects of unclear, Definition, Note.",
    "response": "Note that for general p /∈{1, 2, ∞}, it is unclear whether an analogue of Definition 4.7 would even yield a total problem: we do not provide any rounding strategy for those values and hence it is unclear whether any grid-map without ε-approximate fixpoints on the grid must have a certificate for this in terms of a violation (V) as described above. 22 References [1] Stefan Banach. Sur les op´erations dans les ensembles abstraits et leur application aux ´equations int´egrales."
  },
  {
    "prompt": "Read and understand this section about doi, Fundamenta, mathematicae from a research paper.",
    "response": "Fundamenta mathematicae, 3(1):133–181, 1922. doi:10.4064/ fm-3-1-133-181. [2] Abdul Basit, Nabil H. Mustafa, Saurabh Ray, and Sarfraz Raza. Centerpoints and Tverberg’s technique. Computational Geometry, 43(6):593–600, 2010. doi:10.1016/j.comgeo.2010.03. 002. [3] Luitzen E. J. Brouwer. ¨Uber Abbildung von Mannigfaltigkeiten. Math. Ann., 71(1):97–115, March 1911. doi:10.1007/BF01456931. [4] Ching-Lueh Chang and Yuh-Dauh Lyuu. Optimal bounds on finding fixed points of con- traction mappings."
  },
  {
    "prompt": "Scientific context on Science, Computer, doi and their role in graph theory.",
    "response": "Theoretical Computer Science, 411(16):1742–1749, 2010. doi:10.1016/j. tcs.2010.01.016. [5] Xi Chen, Yuhao Li, and Mihalis Yannakakis. Computing a Fixed Point of Contraction Maps in Polynomial Queries. In Proceedings of the 56th Annual ACM Symposium on Theory of Com- puting, STOC 2024, pages 1364–1373, New York, NY, USA, June 2024. Association for Com- puting Machinery. doi:10.1145/3618260.3649623. [6] Yeshwanth Cherapanamjeri. Computing approximate centerpoints in polynomial time."
  },
  {
    "prompt": "Explain a concept involving FOCS, USA, Annual in computational complexity.",
    "response": "In 2024 IEEE 65th Annual Symposium on Foundations of Computer Science (FOCS), pages 1654– 1668, Los Alamitos, CA, USA, 2024. IEEE Computer Society. doi:10.1109/FOCS61266. 2024.00104. [7] Anne Condon. The Complexity of Stochastic Games. Information and Computation, 96(2):203– 224, February 1992. doi:10.1016/0890-5401(92)90048-K. [8] Constantinos Daskalakis and Christos Papadimitriou. Continuous Local Search."
  },
  {
    "prompt": "Scientific context on Pro, SODA, Algorithms and their role in graph theory.",
    "response": "In Pro- ceedings of the 2011 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Pro- ceedings, pages 790–804. Society for Industrial and Applied Mathematics, January 2011. doi:10.1137/1.9781611973082.62. [9] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. A converse to Ba- nach’s fixed point theorem and its CLS-completeness. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 44–50, New York, NY, USA, June 2018."
  },
  {
    "prompt": "Explain a concept involving Machinery, Computing, Matouˇsek in computational complexity.",
    "response": "Association for Computing Machinery. doi:10.1145/3188745.3188968. [10] J´erˆome Dohrau, Bernd G¨artner, Manuel Kohler, Jiˇr´ı Matouˇsek, and Emo Welzl. ARRIVAL: A Zero-Player Graph Game in NP ∩coNP. In A Journey Through Discrete Mathematics: A Tribute to Jiˇr´ı Matouˇsek, pages 367–374. Springer International Publishing, Cham, 2017. doi: 10.1007/978-3-319-44479-6_14. [11] Kousha Etessami, Christos Papadimitriou, Aviad Rubinstein, and Mihalis Yannakakis."
  },
  {
    "prompt": "Read and understand this section about Theorem, Supermodular, Games from a research paper.",
    "response": "Tarski’s Theorem, Supermodular Games, and the Complexity of Equilibria. In 11th Inno- vations in Theoretical Computer Science Conference (ITCS 2020), volume 151 of Leibniz Interna- tional Proceedings in Informatics (LIPIcs), pages 18:1–18:19, Dagstuhl, Germany, 2020. Schloss Dagstuhl–Leibniz-Zentrum f¨ur Informatik. doi:10.4230/LIPIcs.ITCS.2020.18. 23 [12] John Fearnley, Paul Goldberg, Alexandros Hollender, and Rahul Savani. The Complexity of Gradient Descent: CLS = PPAD ∩PLS. J."
  },
  {
    "prompt": "Explain a concept involving ACM, December, 7:74 in computational complexity.",
    "response": "ACM, 70(1):7:1–7:74, December 2022. doi: 10.1145/3568163. [13] John Fearnley, Spencer Gordon, Ruta Mehta, and Rahul Savani. Unique End of Potential Line. Journal of Computer and System Sciences, 114:1–35, December 2020. doi:10.1016/j. jcss.2020.05.007. [14] M. G¨o¨os, A. Hollender, S. Jain, G. Maystre, W. Pires, R. Robere, and R. Tao. Further Collapses in TFNP. SIAM Journal on Computing, 53(3), 2024."
  },
  {
    "prompt": "Explain a concept involving Mika, Alexandros, Hollender in computational complexity.",
    "response": "[15] Mika G¨o¨os, Alexandros Hollender, Siddhartha Jain, Gilbert Maystre, William Pires, Robert Robere, and Ran Tao. Separations in Proof Complexity and TFNP. J. ACM, 71(4):26:1–26:45, August 2024. doi:10.1145/3663758. [16] B. Gr¨unbaum. Partitions of mass-distributions and of convex bodies by hyperplanes. Pacific Journal of Mathematics, 10(4):1257–1261, January 1960. [17] Sebastian Haslebacher. ARRIVAL: Recursive Framework & ℓ1-Contraction, February 2025."
  },
  {
    "prompt": "Understanding the computational aspects of arXiv, doi, Eduard.",
    "response": "arXiv:2502.06477, doi:10.48550/arXiv.2502.06477. [18] Eduard Helly. ¨Uber Mengen konvexer K¨orper mit gemeinschaftlichen Punkten. Jahresbericht der Deutschen Mathematiker-Vereinigung, 32:175–176, 1923. [19] Z. Huang, L. Khachiyan, and K. Sikorski. Approximating Fixed Points of Weakly Contract- ing Mappings. Journal of Complexity, 15(2):200–213, June 1999. doi:10.1006/jcom.1999. 0504. [20] Takashi Ishizuka. On TFNP Classes: Approaches from Fixed Point Theory and Algorithmic Game Theory."
  },
  {
    "prompt": "Scientific context on Kyushu, University, PhD and their role in graph theory.",
    "response": "PhD thesis, Kyushu University, 2022. [21] Roman Karasev and Benjamin Matschke. Projective center point and Tverberg the- orems. Discrete & Computational Geometry, 52(1):88–101, July 2014. doi:10.1007/ s00454-014-9602-9. [22] Yi Li, Philip M. Long, and Aravind Srinivasan. Improved bounds on the sample complexity of learning. Journal of Computer and System Sciences, 62(3):516–527, 2001. doi:10.1006/jcss. 2000.1741. [23] Jes´us A. De Loera, Xavier Goaoc, Fr´ed´eric Meunier, and Nabil H. Mustafa."
  },
  {
    "prompt": "Explain a concept involving Helly, Sperner, Tucker in computational complexity.",
    "response": "The discrete yet ubiquitous theorems of Carath´eodory, Helly, Sperner, Tucker, and Tverberg. Bulletin of the American Mathematical Society, 56:415–511, 2019. doi:10.1090/bull/1653. [24] Jiˇr´ı Matouˇsek. Computing the center of planar point sets. DIMACS Series in Discrete Mathe- matics and Theoretical Computer Science, 6:221–230, 1991. doi:10.1090/dimacs/006/14. [25] Edward J. McShane. Extension of range of functions. Bulletin of the American Mathematical Society, 40(12):837–842, December 1934."
  },
  {
    "prompt": "Read and understand this section about Gary, Sheehy, Miller from a research paper.",
    "response": "[26] Gary L. Miller and Donald R. Sheehy. Approximate center points with proofs. In Pro- ceedings of the Twenty-Fifth Annual Symposium on Computational Geometry, SCG ’09, pages 153–158, New York, NY, USA, June 2009. Association for Computing Machinery. doi: 10.1145/1542362.1542395. 24 [27] Nabil H. Mustafa and Kasturi Varadarajan. Epsilon-approximations & epsilon-nets. In Handbook of Discrete and Computational Geometry, chapter 47, pages 1241–1267. CRC Press LLC, 3 edition, 2017."
  },
  {
    "prompt": "Read and understand this section about Christos, Papadimitriou, June from a research paper.",
    "response": "[28] Christos H. Papadimitriou. On the complexity of the parity argument and other inefficient proofs of existence. Journal of Computer and System Sciences, 48(3):498–532, June 1994. doi: 10.1016/S0022-0000(05)80063-7. [29] Alexander Pilz and Patrick Schnider. Extending the centerpoint theorem to multiple points."
  },
  {
    "prompt": "Understanding the computational aspects of ISAAC, Germany, International.",
    "response": "In 29th International Symposium on Algorithms and Computation (ISAAC 2018), volume 123 of Leibniz International Proceedings in Informatics (LIPIcs), pages 53:1–53:13, Dagstuhl, Germany, 2018. Schloss Dagstuhl – Leibniz-Zentrum f¨ur Informatik. doi:10.4230/LIPIcs.ISAAC. 2018.53. [30] Luis A. Rademacher. Approximating the centroid is hard. In Proceedings of the Twenty-Third Annual Symposium on Computational Geometry, SCG ’07, pages 302–305, New York, NY, USA, June 2007."
  },
  {
    "prompt": "This is a technical discussion about Machinery, doi, Computing in theoretical computer science.",
    "response": "Association for Computing Machinery. doi:10.1145/1247069.1247123. [31] Richard Rado. A Theorem on General Measure. Journal of the London Mathematical Society, s1-21(4):291–300, 1946. doi:10.1112/jlms/s1-21.4.291. [32] Johann Radon. Mengen konvexer K¨orper, die einen gemeinsamen Punkt enthalten. Mathe- matische Annalen, 83(1):113–115, 1921. doi:10.1007/BF01464231. [33] Saurabh Ray and Nabil H. Mustafa. An optimal generalization of the centerpoint theorem, and its extensions."
  },
  {
    "prompt": "Understanding the computational aspects of USA, Geometry, SCG.",
    "response": "In Proceedings of the Twenty-Third Annual Symposium on Computational Geometry, SCG ’07, pages 138–141, New York, NY, USA, 2007. Association for Computing Machinery. doi:10.1145/1247069.1247097. [34] Ralph T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970. doi: 10.1515/9781400873173. [35] K. Sikorski, C.W. Tsay, and H. Wo´zniakowski. An Ellipsoid Algorithm for the Computation of Fixed Points. J. Complex., 9(1):181–200, March 1993. doi:10.1006/jcom.1993.1013."
  },
  {
    "prompt": "Read and understand this section about Krzysztof, Sikorski, doi from a research paper.",
    "response": "[36] Krzysztof Sikorski. Computational complexity of fixed points. J. Fixed Point Theory Appl., 6(2):249–283, December 2009. doi:10.1007/s11784-009-0128-3. [37] Vladimir N. Vapnik and Alexey Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264– 280, 1971. doi:10.1137/1116025. [38] Rade T. ˇZivaljevi´c. Topological methods in discrete geometry."
  },
  {
    "prompt": "This is a technical discussion about Geometry, Handbook, Discrete in theoretical computer science.",
    "response": "In Handbook of Discrete and Computational Geometry, chapter 27, pages 551–580. CRC Press LLC, 3 edition, 2017. 25 A More on ℓp-Halfspaces In this section, we will study limit ℓp-halfspaces in more detail. The main goal is to provide proofs for the halfspace properties in Section 3.2 and Section 4.2, but we will also develop some additional theory on the way."
  },
  {
    "prompt": "Explain a concept involving Appendix, ℓp-halfspaces, limit in computational complexity.",
    "response": "We split the exposition into two parts: fundamental properties of ℓp-halfspaces and insights about their shape are developed in Appendix A.1 while Appendix A.2 focuses on the interaction of ℓp-halfspaces with mass distributions. A.1 Fundamentals of ℓp-Halfspaces For completeness sake, we recall the definition of limit ℓp-halfspaces, which we will just refer to as ℓp-halfspaces in this appendix. Definition 3.2 (Limit ℓp-Halfspace)."
  },
  {
    "prompt": "Understanding the computational aspects of point, Lemma, direction.",
    "response": "For fixed p ∈[1, ∞) ∪{∞}, point x ∈Rd, and direction v ∈Rd with v ̸= 0, the ℓp-halfspace Hp x,v ⊆Rd through x in the direction of v is defined as Hp x,v := {z ∈Rd | ∀ε > 0 : ||x −z||p ≤||x −εv −z||p}. Recall that Hp x,v is invariant under scaling v with a positive scalar, and we therefore usually use v ∈Sd−1. As a warm-up, we prove Lemma 3.5, which considers ℓp-halfspaces with axis-aligned directions v. Lemma 3.5."
  },
  {
    "prompt": "This is a technical discussion about parallel, axes, direction in theoretical computer science.",
    "response": "For any p ∈[1, ∞) ∪{∞}, any x ∈Rd, and any direction v ∈Sd−1 parallel to one of the coordinate axes, we have Hp x,v = H2 x,v. Proof. Without loss of generality, assume that v = (1, 0, . . . , 0) ∈Sd−1. By our definition of ℓp- halfspaces, z is contained in Hp x,v if and only if ∥x −z∥p p ≤∥x −εv −z∥p p for all ε > 0, which happens if and only if |x1 −z1|p ≤|x1 −ε −z1|p. This last inequality holds if and only if z1 ≥x1, independently of p. This concludes the proof."
  },
  {
    "prompt": "Explain a concept involving Observation, effort, proofs in computational complexity.",
    "response": "The proofs of the other properties require some more effort. In particular, for Lemma 3.6, we will need Observation 3.3, which we restate here for convenience. Observation 3.3. For a given limit ℓp-halfspace Hp x,v and point z ∈Rd, let R−be the open ray from x in direction −v, and let Bz be the smallest closed ℓp-ball with center z that contains x. Then we have z ∈Hp x,v ⇐⇒R−∩B◦ z = ∅, where B◦ z denotes the interior of Bz."
  },
  {
    "prompt": "Scientific context on Lemma, rays, Observation and their role in graph theory.",
    "response": "With this, we are ready to prove Lemma 3.6, which states that ℓp-halfspaces are unions of rays originating at x. The proof mainly relies on convexity of ℓp-balls and Observation 3.3. Lemma 3.6. For any p ∈[1, ∞) ∪{∞}, any limit halfspace Hp x,v is a union of rays originating in x. For p ∈(1, ∞), the boundary of any limit halfspace Hp x,v is a union of lines through x. Proof. We start with the first part of the statement for arbitrary p ∈[1, ∞) ∪{∞}."
  },
  {
    "prompt": "Read and understand this section about direction, line, open from a research paper.",
    "response": "For this, let L be the line through x in direction v, and let R−be the open ray from x in direction −v. Let Bz be the smallest ℓp-ball around a point z ∈Rd that contains x. From Observation 3.3, we know that z ∈Hp x,v ⇐⇒R−∩B◦ z = ∅, 26 where B◦ z denotes the interior of Bz. Consider what happens when we move z along the ray through x and z. In other words, consider a point z′ = x + δ(z −x) for some δ > 0. By definition, x is on the boundary of both balls Bz′ and Bz."
  },
  {
    "prompt": "Understanding the computational aspects of Bz′, observe, translated.",
    "response": "Moreover, observe that the ball Bz′ is a translated (with center z′ instead of z) and scaled (such that x stays on the boundary) version of Bz. In particular, both Bz and Bz′ have the same tangent hyperplanes at the point x. Thus, we conclude R−∩B◦ z = ∅⇐⇒R−∩B◦ z′ = ∅, which means that z′ ∈Hp x,v if and only if z ∈Hp x,v. This proves that Hp x,v is a union of rays starting at x. It remains to prove that the boundary ∂Hp x,v of Hp x,v is a union of lines through x, assuming that p ∈(1, ∞)."
  },
  {
    "prompt": "Read and understand this section about point, observe, convex from a research paper.",
    "response": "For this, observe that the ℓp-balls for p ∈(1, ∞) are strictly convex. We claim that a point z is on the boundary of Hp x,v if and only if B◦ z ∩L = ∅. Indeed, any point z with B◦ z ∩L = ∅ is clearly contained in Hp x,v (by Observation 3.3). Moreover, moving z into the direction −v yields a point z′ with B◦ z′ ∩R−̸= ∅(by strict convexity of ℓp-balls). We conclude z ∈∂Hp x,v. Conversely, consider an arbitrary point z with B◦ z ∩L ̸= ∅."
  },
  {
    "prompt": "Read and understand this section about intersects, moving, change from a research paper.",
    "response": "If B◦ z intersects R−, slightly moving z does not change this. Concretely, any point z′ in a small neighborhood around z satisfies B◦ z′ ∩R−̸= ∅, and thus none of these points is contained in the halfspace. We conclude that z cannot be on the boundary. Otherwise, B◦ z intersects R+ := L \\ (R−∪{x}), but then slightly moving z does not change this."
  },
  {
    "prompt": "Read and understand this section about convexity, words, small from a research paper.",
    "response": "In other words, any point z′ in a small neighborhood around z satisfies B◦ z′ ∩R+ ̸= ∅, which also implies B◦ z′ ∩R−= ∅(as otherwise, x would be contained in B◦ z′ by convexity). We conclude that z is not on the boundary. With this characterization of the boundary points, we can now finish the proof. Consider an arbitrary z ∈∂Hp x,v. We must have B◦ z ∩L = ∅. In other words, L is tangential to Bz."
  },
  {
    "prompt": "Understanding the computational aspects of Bz′, Lemma, remains.",
    "response": "This remains true for any other point z′ = x + δ(z −x) for δ ∈R on the line through x and z with ball Bz′ (using point-symmetry of ℓp-balls around their center for δ < 0). This again implies z′ ∈∂Hp x,v. With Lemma 3.6 at our disposal, we now want to tackle Lemma 3.7 next. However, before getting there, we will need to recall some tools from convex analysis."
  },
  {
    "prompt": "Scientific context on subgradients, Concretely, notion and their role in graph theory.",
    "response": "Concretely, we will need the notion of subgradients of a convex function f : Rd →R: a vector u ∈Rd is a subgradient of f at x ∈Rd if and only if f (x′) −f (x) ≥⟨u, (x′ −x)⟩ for all x′ ∈Rd. The set ∂f (x) ⊆Rd of all subgradients of f at x is also called the subdifferential. If f is differentiable at x, then ∂f (x) = {∇f (x)}. For more details on convex analysis, we refer to the standard textbook by Rockafellar [34]."
  },
  {
    "prompt": "This is a technical discussion about ℓp-halfspace, point, subgradient in theoretical computer science.",
    "response": "Subgradients are useful because they allow us to further characterize containment of a given point z in an ℓp-halfspace Hp x,v as follows. Lemma 4.5. For any p ∈[1, ∞) ∪{∞}, a point z ∈Rd is contained in an ℓp-halfspace Hp x,v if and only if there exists a subgradient u ∈∂||z −x||p with ⟨u, v⟩≥0. Proof. Assume first that there exists such a subgradient u ∈∂||z −x||p with ⟨u, v⟩≥0."
  },
  {
    "prompt": "This is a technical discussion about Choosing, subgradients, conclude in theoretical computer science.",
    "response": "Choosing x′ = z −x + εv in the definition of subgradients, we conclude ||z −x + εv||p −||z −x||p ≥ ε⟨u, v⟩≥0 for all ε > 0. Thus, z is contained in the halfspace. Conversely, assume z ∈Hp x,v. By 27 Observation 3.3, this implies that the intersection of the open ray R−from x in direction −v with the interior B◦ z of the smallest ball Bz containing x around z is empty. Both of these objects are convex. Thus, there exists a hyperplane separating R−and B◦ z."
  },
  {
    "prompt": "Read and understand this section about Note, subgradient, hyperplane from a research paper.",
    "response": "Note that this hyperplane must go through x, and a scaling of its normal vector must be a subgradient in ∂||x −z||p. Naturally, this subgradient u at x points away from B◦ z, and we have ⟨u, −v⟩≥0. It remains to observe −u ∈∂||z −x||p (−u is the desired subgradient). In order to use Lemma 4.5, we need to determine the subgradients of the ℓp-norm for all p ∈ [1, ∞) ∪{∞}. We start with the cases p ∈(1, ∞), and focus on those points z ∈Rd with ||z||p = 1."
  },
  {
    "prompt": "This is a technical discussion about Lemma, Note, knowledge in theoretical computer science.",
    "response": "Note that by Lemma 3.6, knowledge about the cases with ||z||p = 1 already suffices to make full use of Lemma 4.5. Observation A.1. For p ∈(1, ∞) and arbitrary z ∈Rd with ||z||p = 1, || · ||p is differentiable at z and the gradient ∇||z||p is given by ∇||z||p =   |z1|p−1sign(z1) ... |zd|p−1sign(zd)  . In particular, we have ⟨∇||z||p, z⟩= ∇ \u0000||z||p \u0001 p p−1 = ||z||p = 1. Next, we characterize the subgradients for || · ||∞and || · ||1 on points z ∈Rd with ||z||∞= 1 (||z||1 = 1, respectively)."
  },
  {
    "prompt": "Scientific context on Note, literature, Observation and their role in graph theory.",
    "response": "Note that these subdifferentials can also be found in the literature (see, e.g., [34]). Observation A.2. Consider arbitrary z ∈Rd with ||z||∞= 1. The set of subgradients ∂||z||∞is given by ∂||z||∞= conv ({−ei | zi = −1} ∪{ei | zi = 1}) , where ei denotes the i-th standard unit vector in Rd. In particular, we have ⟨u, z⟩= ||u||1 = ||z||∞= 1 for all u ∈∂||z||∞. Observation 4.6. Consider arbitrary z ∈Rd with ||z||1 = 1."
  },
  {
    "prompt": "Read and understand this section about subgradient, Lemma, vector from a research paper.",
    "response": "A vector u ∈Rd is a subgradient of || · ||1 at z if and only if ui ∈      {1} if zi > 0 {−1} if zi < 0 [−1, 1] if zi = 0 for all i ∈[d]. In particular, we have ⟨u, z⟩= ||u||∞= ||z||1 = 1 for all u ∈∂||z||1. Consider a subgradient u of || · ||p at point z. Given the characterization of ℓp-halfspaces in Lemma 4.5, it will be useful to prove an upper bound on the angle between u and z. Lemma A.3. Let p ∈[1, ∞) ∪{∞} and consider arbitrary z ∈Rd with ||z||p = 1."
  },
  {
    "prompt": "Read and understand this section about subgradients, recall, Proof from a research paper.",
    "response": "Then we have ∡(u, z) ≤π 2 −√ 1/d for all subgradients u ∈∂||z||p. Proof. We heavily use the previous observations. In particular, recall that we have ⟨u, z⟩= ||u|| p p−1 = ||z||p = 1 for all subgradients u ∈∂||z||p (where p p−1 is interpreted as 1 for p = ∞ and as ∞for p = 1, respectively). We thus get cos(∡(u, z)) = ⟨u, z⟩ ||u||2||z||2 = 1 ||u||2||z||2 . 28 Recall the ℓp-norm inequalities ||x||2 ≤||x||p for p ≤2 and ||x||2 ≤ √ d||x||p for 2 ≤p."
  },
  {
    "prompt": "Scientific context on Lemma, obtain, conclude and their role in graph theory.",
    "response": "Further, for any p ∈[1, ∞) ∪{∞} we have p < 2 ⇐⇒ p p−1 > 2. Thus, we obtain 1 ||u||2||z||2 ≥ 1 √ d||u|| p p−1 ||z||p = 1 √ d . Thus, we conclude ∡(u, z) ≤π 2 − 1 √ d, as desired. With this theory in place, we are now ready to prove Lemma 3.7. Lemma 3.7. Let p ∈[1, ∞) ∪{∞} and Hp x,v be arbitrary. All z ∈Hp x,v satisfy ∡(−→ xz, v) ≤π −√ 1/d. Similarly, all z ∈Rd with ∡(−→ xz, v) ≤√ 1/d must be contained in Hp x,v. Proof."
  },
  {
    "prompt": "Explain a concept involving Lemma, rays, generality in computational complexity.",
    "response": "By Lemma 3.6 (every halfspace is a union of rays), we can assume ||z −x||p = 1, without loss of generality. Now consider the first statement of the lemma. Lemma 4.5 says that z ∈Hp x,v implies ⟨u, v⟩≥0 for some subgradient u ∈∂||z −x||p. Concretely, this means that ∡(u, v) ≤π 2 . Moreover, by Lemma A.3, we have ∡(u, z −x) ≤ π 2 −√ 1/d. Combining the two bounds, we conclude ∡(−→ xz, v) = ∡(z −x, v) ≤π −√ 1/d, as desired. For the second statement, assume that z satisfies ∡(−→ xz, v) ≤√ 1/d."
  },
  {
    "prompt": "Understanding the computational aspects of Lemma, Corollary, exists.",
    "response": "By Lemma A.3, there exists a subgradient u ∈∂||z −x||p with ∡(u, z −x) ≤π 2 −√ 1/d. We conclude ∡(u, v) ≤π, which implies z ∈Hp x,v by Lemma 4.5. Finally, we can prove Corollary 3.8 using Lemma 3.7. Corollary 3.8. For arbitrary p ∈[1, ∞) ∪{∞} and x ∈Rd with ∥x∥2 > 2d, we have [0, 1]d ⊆Hp x,−x. Proof. By Lemma 3.7, it suffices to prove that all points z ∈[0, 1]d satisfy ∡(−→ xz, −x) ≤√ 1/d. To see this, consider the triangle spanned by the origin 0 ∈Rd and the two points z and x."
  },
  {
    "prompt": "Scientific context on Mass, Distributions, sin and their role in graph theory.",
    "response": "We know that ||z −0||2 = ||z||2 ≤ √ d, and ||x −0||2 = ||x||2 > 2d. By the law of sines, we get sin(∡(−→ xz, −→ x0)) = ||z −0||2 sin(∡(−→ zx, −→ z0)) ||x −0||2 ≤ √ dsin(∡(−→ zx, −→ z0)) ||x −0||2 < √ d 1 2d = 1 2 √ d which implies ∡(−→ xz, −→ x0) < √ 1/d. A.2 ℓp-Halfspaces and Mass Distributions We now move on to the interaction of ℓp-halfspaces with mass distributions. We briefly recall that mass distributions are finite absolutely continuous (w.r.t. the Lebesgue measure) measures on Rd."
  },
  {
    "prompt": "Understanding the computational aspects of Section, distributions, theorem.",
    "response": "As mentioned in Section 3.2, by the Radon-Nikodym theorem, we can therefore think of mass distributions as probability distributions with a density (by assuming µ(Rd) = 1, without 29 loss of generality). We will use this point of view repeatedly in this section: it allows us to use the language of probability theory (instead of the language of measure theory)."
  },
  {
    "prompt": "This is a technical discussion about prove, Lemma, measure in theoretical computer science.",
    "response": "Before we prove Lemma 3.10, i.e., that µ(Hp x,v) is continuous in x, we first want to prove that the boundary of an ℓp-halfspace has measure 0. The following lemma will help with this. Lemma A.4 (Inside and Outside Orthant). Let Hp x,v be an arbitrary ℓp-halfspace for some p ∈[1, ∞) ∪ {∞}. Consider arbitrary z, z′ ∈Rd satisfying both • vi > 0 =⇒z′ i −zi ≥0 • and vi < 0 =⇒z′ i −zi ≤0 for all i ∈[d]. Then z ∈Hp x,v implies z′ ∈Hp x,v."
  },
  {
    "prompt": "Read and understand this section about Similarly, satisfying, contained from a research paper.",
    "response": "Similarly, for any z /∈Hp x,v, all z′ ∈Rd satisfying vi > 0 =⇒z′ i −zi ≤0 and vi < 0 =⇒z′ i −zi ≥0 for all i ∈[d] are not contained in Hp x,v. Proof. Let z ∈Hp x,v be arbitrary and consider first the case p ̸= ∞. Without loss of generality, we can consider the case where z′ and z only differ in one coordinate, i.e., z′ i = zi + αvi for some i ∈[d] and α > 0, and z′ j = zj for all j ∈[d] \\ {i}. By definition, we must have ∥x −z∥p p ≤∥x −εv −z∥p p for all ε > 0."
  },
  {
    "prompt": "Read and understand this section about εvi, prove, holds from a research paper.",
    "response": "If we can prove that |xi −εvi −zi|p −|xi −zi|p ≤|xi −εvi −z′ i|p −|xi −z′ i|p (A.1) for all ε > 0, we are done, because then 0 ≤∥x −εv −z∥p p −∥x −z∥p p ≤∥x −εv −z′∥p p −∥x −z′∥p p holds for all ε > 0 and thus z′ ∈Hp x,v. Equation (A.1) follows from the observation that |xi −εvi −z′ i|p −|xi −z′ i|p = |xi −(α + ε)vi −zi|p −|xi −αvi −zi|p and that the function f (β) := |β + γ|p −|β|p is for all p ∈[1, ∞) monotonically non-increasing for γ ≤0 and monotonically non-decreasing for γ ≥0."
  },
  {
    "prompt": "Explain a concept involving norm, proof, taking in computational complexity.",
    "response": "The proof for p = ∞follows from p = 1, since the ℓ∞-norm is just taking the maximum over the constituents of the ℓ1-norm. Consider now z /∈Hp x,v and consider again first the case p ̸= ∞. As before, we can assume without loss of generality that z′ i = zi −αvi for some i ∈[d] and α > 0, and z′ j = zj for all j ∈[d] \\ {i}. We want to prove z′ /∈Hp x,v. By z /∈Hp x,v, there must exist ε > 0 such that ∥x −z∥p p > ∥x −εv −z∥p p."
  },
  {
    "prompt": "Scientific context on εvi, prove, monotonically and their role in graph theory.",
    "response": "If we can prove |xi −zi|p −|xi −εvi −zi|p ≤|xi −z′ i|p −|xi −εvi −z′ i|p, (A.2) we get 0 < ∥x −z∥p p −∥x −εv −z∥p p ≤∥x −z′∥p p −∥x −εv −z′∥p p and thus z′ /∈Hp x,v. The inequality Equation (A.2) follows from the observation that |xi −z′ i|p −|xi −εvi −z′ i|p = |xi + αvi −zi|p −|xi + (α −ε)vi −zi|p and that the function f (β) := |β|p −|β −γ|p is for all p ∈[1, ∞) monotonically non-increasing for γ ≤0 and monotonically non-decreasing for γ ≥0. The proof for p = ∞again follows from p = 1."
  },
  {
    "prompt": "Read and understand this section about measure, boundary, conclude from a research paper.",
    "response": "30 This now lets us conclude that the boundary of the halfspace has measure 0. Corollary A.5 (Boundary has Measure 0). Let p ∈[1, ∞) ∪{∞}. The boundary ∂Hp x,v of the ℓp- halfspace Hp x,v has Lebesgue measure 0 for all x ∈Rd and v ∈Sd−1. Proof. We prove that ∂Hp x,v is porous, i.e., we prove that there exists 0 < α < 1 such that for every sufficiently small r > 0 and for every z ∈Rd, there is y ∈Rd such that Bp(y, αr) ⊆ Bp(z, r) \\ ∂Hp x,v."
  },
  {
    "prompt": "Explain a concept involving Lemma, sufficient, condition in computational complexity.",
    "response": "This is a sufficient condition for a subset of Rd to have measure 0. Porosity follows from Lemma A.4 by distinguishing between the cases z ∈Hp x,v and z /∈Hp x,v. We are now ready to prove Lemma 3.10. While this could be proven purely in terms of measure theory, we opted for the language of probability theory instead. Lemma 3.10. The function f (x) := µ(Hp x,v) is continuous in x ∈Rd for all mass distributions µ, all v ∈Sd−1, and all p ∈[1, ∞) ∪{∞}. Proof."
  },
  {
    "prompt": "This is a technical discussion about Fix, random, mass in theoretical computer science.",
    "response": "Fix a mass distribution µ, and fix v ∈Sd−1 and p ∈[1, ∞) ∪{∞}. Without loss of gener- ality, assume µ(Rd) = 1. Let x ∈Rd be arbitrary, and consider an arbitrary sequence (xn)n∈N converging to x. For n ∈N, let Xn denote the indicator random variable for a point drawn at random (w.r.t. µ) to lie in the set Hp xn,v. Similarly, let X denote the indicator random variable for containment in Hp x,v. We claim that Xn converges almost surely to X as n goes to infinity."
  },
  {
    "prompt": "Scientific context on observe, naturally, lim and their role in graph theory.",
    "response": "To see this, observe first that we naturally have lim n→∞∥z −xn∥p = ∥z −x∥p for all z ∈Rd. Now let z /∈Hp x,v be arbitrary (i.e., X(z) = 0). Then there exist ε, δ > 0 such that ∥x −εv −z∥p < ∥x −z∥p −δ. Thus, for n ∈N large enough we also have Xn(z) = 0. Consider now an arbitrary z in the interior of Hp x,v. By definition, there exists δ > 0 such that a δ-ball around z is contained in Hp x,v. In other words, we have ∥x −εv −z′∥p ≥∥x −z′∥p for all ε > 0 and z′ with ∥z′ −z∥p ≤δ."
  },
  {
    "prompt": "Understanding the computational aspects of limn, yields, convergence.",
    "response": "For large enough n ∈N, we can therefore choose z′ such that x −z′ = xn −z, which yields ∥xn −εv −z∥p ≥∥xn −z∥p and thus limn→∞Xn(z) = 1. We conclude that the set {z ∈Rd | limn→∞Xn(z) = X(z)} includes all of Rd except for the boundary of Hp x,v. By Corollary A.5, this boundary has measure zero and we get almost sure convergence. Using the dominated convergence theorem, this implies convergence in mean and thus lim n→∞µ(Hp xn,v) = lim n→∞E(Xn) = E(X) = µ(Hp x,v), as desired."
  },
  {
    "prompt": "Explain a concept involving Lemma, prove, set in computational complexity.",
    "response": "While we cannot prove for all p that µ(Hp x,v) is continuous in v, we at least prove Lemma 3.11, which states that the set V of directions v for which µ(Hp x,−v) is strictly smaller than some threshold t, is open. 31 Lemma 3.11. The set Vx = {v ∈Sd−1 | µ(Hp x,v) < t} is an open subset of Sd−1 for all mass distributions µ, all t > 0, all p ∈[1, ∞) ∪{∞}, and all x ∈Rd. Proof. Fix µ and p ∈[1, ∞) ∪{∞} and let x ∈Rd be arbitrary. Assume without loss of generality that µ(Rd)."
  },
  {
    "prompt": "Scientific context on prove, open, closed and their role in graph theory.",
    "response": "Instead of directly proving that V is open, we will prove that its complement Sd−1 \\ V = {v ∈Sd−1 | µ(Hp x,v) ≥t} is closed. To this end, consider an arbitrary sequence (vn)n∈N converging to v ∈Sd−1 satisfying vn ∈Sd−1 \\ V for all n ∈N. We will prove that then v ∈Sd−1 \\ V. Let Xn be the indicator random variable for containment of a point drawn according to µ in Hp x,vn."
  },
  {
    "prompt": "Explain a concept involving random, converges, variables in computational complexity.",
    "response": "Observe that by limn→∞∥x −εvn −z∥p = ∥x −εv −z∥p for all z ∈Rd, the sequence of random variables (Xn)n∈N converges point-wise to a random variable X. In other words, (Xn)n∈N converges almost surely to X, and by the dominated convergence theorem we get convergence in mean. Concretely, this means that we have lim n→∞µ(Hp x,vn) = EX ≥t. Let now z /∈Hp x,v be arbitrary. Then there exist ε, δ > 0 such that ∥x −εv −z∥p < ∥x −z∥p −δ. Using limn→∞∥x −εvn −z∥p = ∥x −εv −z∥p, we conclude that X(z) = 0."
  },
  {
    "prompt": "Read and understand this section about Theorem, words, Finite from a research paper.",
    "response": "In other words, we just proved that Rd \\ Hp x,v ⊆{z ∈Rd | X(z) = 0} and thus µ(Hp x,v) ≥EX. Plugging everything together, we obtain µ(Hp x,v) ≥t and hence v ∈Sd−1 \\ V, as desired. B Proof of Theorem 3.18 Theorem 3.18 (ℓp-Centerpoint Theorem for Finite Point Sets). Let p ∈[1, ∞) ∪{∞} be arbitrary, and let P ⊆Rd be a finite set of points. There exists a point c such that |Hp c,v ∩P| ≥ |P| d+1 for all v ∈Sd−1. Proof. Without loss of generality, assume that P ⊆(0, 1)d."
  },
  {
    "prompt": "Scientific context on generality, assume, ball and their role in graph theory.",
    "response": "Let r > 0 be arbitrary and consider the mass distribution µr on Rd obtained by replacing each point z in P with an ℓp-norm ball Bp(z, r) of radius r around z (without loss of generality, assume r is small enough such that the balls do not intersect and such that the support of µr is contained in [0, 1]d). Theorem 3.13 yields a centerpoint c(r) of µr. Moreover, by Lemma 3.19, these centerpoints are contained in [0, 1]d as well. Now consider the sequence of centerpoints c(r) as r goes to 0."
  },
  {
    "prompt": "This is a technical discussion about Weierstrass, Bolzano, convergent in theoretical computer science.",
    "response": "By the theorem of Bolzano- Weierstrass, this sequence has a convergent subsequence with limit c ∈[0, 1]d. From now on, we will use c(r) to refer to elements of this convergent subsequence. We claim that c is an ℓp-centerpoint of P. To prove this, fix v ∈Sd−1 and let z ∈P \\ Hp c,v be arbitrary. This means that there exists ε > 0 such that ∥c −z∥p > ∥c −εv −z∥p and thus even ∥c −z∥p > ∥c −εv −z∥p + δ for some δ > 0. By choosing r small enough, we can ensure both r ≤δ/4 and ∥c(r) −c∥p ≤δ/4."
  },
  {
    "prompt": "Read and understand this section about inequality, triangle, small from a research paper.",
    "response": "Using the triangle inequality, we then get ∥c(r) −z′∥p ≥∥c −z∥p −∥z′ −z∥p −∥c −c(r)∥p ≥∥c −z∥p −δ/2 > ∥c −εv −z∥p + δ/2 ≥∥c(r) −εv −z′∥p −∥z′ −z∥p −∥c −c(r)∥p + δ/2 ≥∥c(r) −εv −z′∥p 32 for all z′ ∈Bp(z, r). We conclude that Bp(z, r) ∩Hp c(r),v is empty. In fact, this derivation holds true for all sufficiently small r. This means that all balls around points z ∈P \\ Hp c,v will eventually (for small enough r) stop contributing to µr(Hp c(r),v)."
  },
  {
    "prompt": "This is a technical discussion about desired, conclude, points in theoretical computer science.",
    "response": "By µr(Hp c(r),v) ≥ 1 d+1µr(Rd), we conclude that there can be at most d d+1|P| such points z and thus we get |P ∩Hp c,v| ≥ |P| d+1, as desired. 33"
  }
]